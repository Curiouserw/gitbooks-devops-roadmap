{"./":{"url":"./","title":"简介","keywords":"","body":"Curiouser's Devops Roadmap This gitbook record the technical roadmap of Devops Curiouser. Link GitBook Access URL: https://gitbook.curiouser.top GitHub: https://github.com/Curiouserw What I had done at Openshift or Kubernetes Database Connect Secrity JumpServer结合VaultServer实现隔离数据库直连、自动生成有效期数据库账号 sequenceDiagram participant dev as 开发者 participant lb as 负载均衡器 participant J as JumpServer participant ecs as 服务器 participant v as VaultServer participant L as 审批流程 participant db as 数据库 rect rgba(0, 255, 0,.1) Note over J: 运维：添加开发者可使用的服务器信息 dev ->> L: ①邮件申请(邮件参照以下模板) L -->> L: ②审批结果 v -->> v: ③使用Vault创建申请数据库的临时账号 v -->> db: Vault根据模板SQL自动创建临时账号，并管理该账号的有效期,到期自动删除临时账号 J -->> J: ④运维：授予开发Jumpserver账号申请服务器的登录访问权限 L -->> dev: ⑤运维回复邮件：包含数据库临时账号密码和Jumpserver用户名密码 end dev ->> lb : ⑥使用JumpServer账号登录 lb -->> J: alt web页面登录服务器 activate dev J ->> ecs : ⑦web cli登录服务器 activate ecs ecs ->> ecs: ⑧开发者: 在服务器上部署报表程序代码及配置 deactivate ecs ecs ->> + db: ⑨报表程序连接数据库 activate ecs ecs ->> ecs: 报表程序完成运行 deactivate ecs deactivate dev else SSH客户端登录服务器 activate dev dev ->> lb : ⑦ssh -p负载均衡器监听转发至JumpserverSSH的端口 JumpServer用户名@JumpServer的域名 lb -->> J: 连接转发至JumpServer k8s NodePort J ->> ecs: 登录服务器 activate ecs ecs ->> ecs: ⑧开发者: 在服务器上部署报表程序代码及配置 deactivate ecs ecs ->> + db: ⑨报表程序连接数据库 activate ecs ecs ->> ecs: 报表程序完成运行 deactivate ecs deactivate dev end v -->> db: Vault自动回收删除到期数据库账号 J-->> J : JumpServer自动注销到期账号ECS登录会话 JumpServer使用自带数据库应用功能，实现隔离数据库直连、自动生成临时有效期数据库账号 sequenceDiagram participant dev as 开发者 participant lb as 负载均衡器 participant J as JumpServer participant L as 审批流程 participant db as 数据库 rect rgba(0, 255, 0,.1) dev -->> L: ①邮件申请(邮件参照以下模板) L -->> L: 审批 db-->> db: ②运维：无现成权限的账号则生成新的 J -->> J: ③运维：1：保存数据库账号和密码 2：Jumpserver账号新增或授权 L -->> dev: ④运维回复邮件，新增Jumpserver账号则回复Jumpserver的用户名密码。只授权则回复“已授权” end dev ->> lb: ⑤JumpServer账号登录 lb -->> J: alt Web页面直接连接 activate J J ->> db: ⑥JumpServer Web页面直接连接数据库 deactivate J activate dev dev ->> dev: ⑦问题排查 deactivate dev else 客户端工具连接 Note over J: JumpServer页面显示：1:数据库临时账号2:数据库临时密码3:负载均衡器域名4:负载均衡器MySQL转发端口 activate dev Note over dev: Navicat、MySQL CLI等工具 dev ->> lb: ⑥客户端工具使用JumpServer页面显示的信息连接 activate lb lb -->> J: 连接转发至JumpServer k8s NodePort deactivate lb activate J J ->> db: 连接数据库 deactivate J activate dev dev ->> dev: ⑦问题排查 deactivate dev deactivate dev end kubernetes Jenkins CI/CD Flow 1. Gitlab Webhook + Jenkins SharedLibraries/Kubernetes + SonarScanner Maven Plugin Gitlab CI/CD Workflow 1、Trigger pipeline to Scan code when create a MergeRequest Logging Logging与Metrics Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-04-27 09:44:02 "},"origin/openshift-allinone安装.html":{"url":"origin/openshift-allinone安装.html","title":"Allinone","keywords":"","body":"搭建Allinone全组件Openshift 3.11 一、Overviews Prerequisite IP地址：192.168.1.86 CentOS：7.5.1804 硬盘划分 系统盘60G / 数据盘100G /var/lib/docker ; 100G /data/nfs 开启Selinuxsed -i \"s/SELINUX=disabled/SELINUX=enforcing/\" /etc/selinux/config && \\ setenforce 0 Context Docker：版本 1.13，Overlay2(执行Ansible准备脚本时会进行安装) Openshift：版本 3.11 Kubernetes：版本 v1.11.0 二、使用Ansible安装部署 设置主机名并在本地Host文件中添加IP地址域名映射关系ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') && \\ echo $ipaddr $HOSTNAME >> /etc/hosts 配置中科大Openshift的YUM 源 mkdir /etc/yum.repos.d/bak && \\ mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak && \\ bash -c ' cat > /etc/yum.repos.d/all.repo 安装基础软件yum install -y git vim net-tools lrzsz unzip bind-utils yum-utils bridge-utils python-passlib wget java-1.8.0-openjdk-headless httpd-tools lvm2 安装Ansible 2.6.5 yum install -y https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ansible-2.6.5-1.el7.ans.noarch.rpm 获取openshift ansible部署脚本代码，禁用ansible脚本中的指定repo git clone https://github.com/openshift/openshift-ansible.git -b release-3.11 && \\ sed -i 's/enabled=1/enabled=0/g' /root/openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin.repo.j2 && \\ sed -i 's/enabled=1/enabled=0/g' /root/openshift-ansible/roles/openshift_repos/templates/CentOS-OpenShift-Origin311.repo.j2 (可选)将附件中定制化的OKD登陆页面文件放置/etc/origin/master/custom路径下（自定义的登陆首页）# 路径需要新建 mkdir -p /etc/origin/master/custom 配置Ansible部署Openshift的主机清单/etc/ansible/hosts [OSEv3:children] masters nodes etcd nfs [OSEv3:vars] openshift_ip=192.168.1.86 openshift_public_ip=192.168.1.86 ansible_default_ipv4.address=192.168.1.86 ansible_ssh_user=root openshift_deployment_type=origin deployment_type=origin openshift_release=3.11 openshift_image_tag=v3.11.0 ansible_ssh_pass=**Root用户SSH密码** ######################### Components Cert and CA Expire Days ################# openshift_hosted_registry_cert_expire_days=36500 openshift_ca_cert_expire_days=36500 openshift_node_cert_expire_days=36500 openshift_master_cert_expire_days=36500 etcd_ca_default_days=36500 ####################### Multitenant Network ####################### os_sdn_network_plugin_name=redhat/openshift-ovs-multitenant ####################### OKD ####################### openshift_clock_enabled=true openshift_enable_unsupported_configurations=True openshift_node_groups=[{'name': 'allinone', 'labels': ['node-role.kubernetes.io/master=true', 'node-role.kubernetes.io/infra=true', 'node-role.kubernetes.io/compute=true']}] openshift_disable_check=memory_availability,disk_availability,package_availability,package_update,docker_image_availability,docker_storage_driver,docker_storage ####################### OKD master config ####################### openshift_master_api_port=8443 openshift_master_cluster_public_hostname=allinone.okd311.curiouser.com openshift_master_cluster_hostname=allinone.okd311.curiouser.com openshift_master_default_subdomain=apps.okd311.curiouser.com openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}] openshift_master_htpasswd_users={'admin':'$apr1$eG8zNL.C$fvACBzDJ7.N7KdJORT12E0'} openshift_master_oauth_template=custom/login.html openshift_master_session_name=ssn openshift_master_session_max_seconds=3600 ####################### Docker ####################### container_runtime_docker_storage_setup_device=/dev/sdb container_runtime_docker_storage_type=overlay2 openshift_examples_modify_imagestreams=true openshift_docker_options=\"--selinux-enabled -l warn --ipv6=false --insecure-registry=0.0.0.0/0 --log-opt max-size=10M --log-opt max-file=3 --registry-mirror=https://zlsoueh7.mirror.aliyuncs.com\" ####################### Web Console ####################### openshift_web_console_extension_script_urls=[\"https://xhua-static.sh1a.qingstor.com/allinone/allinone-webconsole.js\"] openshift_web_console_extension_stylesheet_urls=[\"https://hermes-uat.curiouser.com/curiouser/M00/00/3A/rBACF1vz8NyALOS3AAApT8C9PDY549.css\"] ####################### Registry ####################### openshift_hosted_registry_storage_kind=nfs openshift_hosted_registry_storage_access_modes=['ReadWriteMany'] openshift_hosted_registry_storage_nfs_directory=/data/nfs openshift_hosted_registry_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_hosted_registry_storage_volume_name=registry openshift_hosted_registry_storage_volume_size=10Gi ####################### metrics ####################### openshift_metrics_install_metrics=true openshift_metrics_image_version=v3.11.0 openshift_metrics_storage_kind=nfs openshift_metrics_storage_access_modes=['ReadWriteOnce'] openshift_metrics_storage_nfs_directory=/data/nfs openshift_metrics_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_metrics_storage_volume_name=metrics openshift_metrics_storage_volume_size=10Gi ####################### logging ####################### openshift_logging_install_logging=true openshift_logging_image_version=v3.11.0 openshift_logging_es_ops_nodeselector={\"node-role.kubernetes.io/infra\":\"true\"} openshift_logging_es_nodeselector={\"node-role.kubernetes.io/infra\":\"true\"} openshift_logging_elasticsearch_pvc_size=5Gi openshift_logging_storage_kind=nfs openshift_logging_storage_access_modes=['ReadWriteOnce'] openshift_logging_storage_nfs_directory=/data/nfs openshift_logging_storage_nfs_options='*(rw,root_squash,sync,no_wdelay)' openshift_logging_storage_volume_name=logging openshift_logging_storage_volume_size=10Gi ################### Prometheus Cluster Monitoring ################### openshift_cluster_monitoring_operator_install=true openshift_cluster_monitoring_operator_prometheus_storage_enabled=true openshift_cluster_monitoring_operator_alertmanager_storage_enabled=true openshift_cluster_monitoring_operator_prometheus_storage_capacity=50Gi openshift_cluster_monitoring_operator_alertmanager_storage_capacity=5Gi ##################### Disable Components ############# openshift_enable_service_catalog=false ansible_service_broker_install=false [masters] allinone.okd311.curiouser.com [etcd] allinone.okd311.curiouser.com [nfs] allinone.okd311.curiouser.com [nodes] allinone.okd311.curiouser.com openshift_node_group_name='allinone' 创建NFS挂载目录 pvcreate /dev/sdc && \\ vgcreate -s 4m data /dev/sdc && \\ lvcreate --size 45G -n nfs data && \\ mkfs.xfs /dev/data/nfs && \\ echo \"/dev/data/nfs /data/nfs xfs defaults 0 0\" >> /etc/fstab && \\ mkdir /data/nfs -p && \\ mount -a && \\ df -mh （可选）预先拉取安装过程中可能使用的镜像docker pull docker.io/openshift/origin-node:v3.11.0 && \\ docker pull docker.io/openshift/origin-control-plane:v3.11.0 && \\ docker pull docker.io/openshift/origin-haproxy-router:v3.11.0 && \\ docker pull docker.io/openshift/origin-deployer:v3.11.0 && \\ docker pull docker.io/openshift/origin-pod:v3.11.0 && \\ docker pull docker.io/openshift/origin-docker-registry:v3.11.0 && \\ docker pull docker.io/openshift/origin-console:v3.11.0 && \\ docker pull docker.io/openshift/origin-service-catalog:v3.11.0 && \\ docker pull docker.io/openshift/origin-web-console:v3.11.0 && \\ docker pull docker.io/cockpit/kubernetes:latest && \\ docker pull docker.io/openshift/oauth-proxy:v1.1.0 && \\ docker pull docker.io/openshift/origin-docker-builder:v3.11.0 && \\ docker pull docker.io/openshift/prometheus-alertmanager:v0.15.2 && \\ docker pull docker.io/openshift/prometheus-node-exporter:v0.16.0 && \\ docker pull docker.io/openshift/prometheus:v2.3.2 && \\ docker pull docker.io/grafana/grafana:5.2.1 && \\ docker pull quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 quay.io/coreos/kube-rbac-proxy:v0.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/kube-rbac-proxy:v0.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/etcd:v3.2.22 && \\ docker tag quay-mirror.qiniu.com/coreos/etcd:v3.2.22 quay.io/coreos/etcd:v3.2.22 && \\ docker rmi quay-mirror.qiniu.com/coreos/etcd:v3.2.22 && \\ docker pull quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 quay.io/coreos/kube-state-metrics:v1.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/kube-state-metrics:v1.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 && \\ docker tag quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 quay.io/coreos/configmap-reload:v0.0.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/configmap-reload:v0.0.1 && \\ docker pull quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker tag quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 quay.io/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/cluster-monitoring-operator:v0.1.1 && \\ docker pull quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 && \\ docker tag quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 quay.io/coreos/prometheus-config-reloader:v0.23.2 && \\ docker rmi quay-mirror.qiniu.com/coreos/prometheus-config-reloader:v0.23.2 && \\ docker pull quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 && \\ docker tag quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 quay.io/coreos/prometheus-operator:v0.23.2 && \\ docker rmi quay-mirror.qiniu.com/coreos/prometheus-operator:v0.23.2 执行OKD Ansible Playbook先执行安装检查的Playbook ansible-playbook /root/openshift-ansible/playbooks/prerequisites.yml 再执行安装Playbook ansible-playbook /root/openshift-ansible/playbooks/deploy_cluster.yml 授予admin用户以管理员权限 oc adm policy add-cluster-role-to-user cluster-admin admin 三、配置Openshift的后端存储 使用Ceph RBD作为后端存储 搭建单节点的Ceph，详见（Ceph RBD单节点安装） 创建Storageclass 使用NFS作为后端存储：详见 参考文章链接 当主机有多网卡时指定组件监听的网卡IP地址：https://github.com/ViaQ/Main/blob/master/README-install.md Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-ocp43-install.html":{"url":"origin/openshift-ocp43-install.html","title":"OCP 4.3 集群","keywords":"","body":"OCP 4.3集群安装 一、简介 Openshift从4.0时代从OKD(Openshift Kubernetes Distribution)更名为OCP（openshift container platform) 二、安装操作 1、安装架构 VM Name 硬件配置 OS IP地址 服务 ocp43-tools 4C8G100G系统盘 CentOS 7.7 192.168.1.80 PXE、DHCP 、DNS、Web Server、TFTP、NFSv4、Load Balance ocp43-bootstrap 4C8G100G系统盘 暂不安装OS后续会使用辅助节点上的PXE安装上Readhat CoreOS 192.168.1.55 临时K8S集群，用来安装OCP、安装完后可以弃用 ocp43-master1 8C16G100G系统盘 暂不安装OS后续会使用PXE安装上Readhat CoreOS 192.168.1.81 OCP集群Master节点 ocp43-worker1 8C16G100G系统盘 暂不安装OS后续会使用PXE安装上Readhat CoreOS 192.168.1.91 OCP集群Worker节点 2、需预先提供的事项 2.1、在ESXI中创建以上配置的虚拟机 可使用govc命令快速创建，参考链接 ocp43-bootstrap、ocp43-master1、ocp43-worker1虚拟机创建后先关机 ocp43-bootstrap、ocp43-master1、ocp43-worker1虚拟机的网卡Mac地址，获取方式可使用govc命令：govc device.info -vm ocp4-worker1 |grep \"MAC Address:\" 2.2、辅助节点安装PXE/DHCP/DNS/LB 可使用红帽大神开源的Ansible部署脚本，github地址：https://github.com/RedHatOfficial/ocp4-helpernode 2.3、拉取镜像的pull secret 安装过程中需要从红帽镜像仓库拉取镜像时使用的pull secret。 可在https://cloud.redhat.com/openshift/install/metal/user-provisioned中获取（需要注册登录RedHat账号） 注意：pull secret的有效期只有一天，尽快在一天时间内安装好集群 后续安装过程中可通过echo | openssl s_client -connect api-int.ocp43.curiouser.com:6443 | openssl x509 -noout -text查看有效期 3、辅助节点准备 3.1、基础环境准备 开启SELinux 固定IP地址 生成SSH密钥 设置代理 设置主机名 开启ssh-agent 安装ansible、git等基础软件 sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config echo 'export HTTP_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export HTTPS_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export NO_PROXY=\"mirrors.ustc.edu.cn,api.ocp43.curiouser.com\"' >> ~/.zshrc source ~/.zshrc hostnamectl set-hostname --static tools.ocp43.curiouser.com echo \"PREFIX=24\\nIPADDR=192.168.1.80\\nGATEWAY=192.168.1.1\" >> /etc/sysconfig/network-scripts/ifcfg-ens32 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens32 yum -y install ansible git ssh-keygen -t rsa -b 4096 -N \"\" -f ~/.ssh/id_rsa eval \"$(ssh-agent -s)\" ssh-add ~/.ssh/id_rsa 由于主机名修改需要重启才能生效，所以该节点需要重启（虽然也可以不重启修改主机名、但是修改的地方较多、以防万一留下隐患、保险起见重启吧） 3.2、下载Ansible脚本 git clone https://github.com/RedHatOfficial/ocp4-helpernode 3.3、创建配置文件 bash -c 'cat > ~/ocp4-helpernode/vars.yaml 3.4、预下载安装OCP/RedHat CoreOS的二进制文件 辅助节点上的PXE服务需要给集群其他节点提供安装OCP和基础OS的二进制文件。在Ansilbe脚本执行时会去联网下载、我们可以收到先下载下来放到指定的文件夹下，加速Ansible执行速度。这些文件的下载地址可以在Ansible脚本文件中查看。 cat ~/ocp4-helpernode/vars/main.yml --- ocp_bios: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.3/4.3.8/rhcos-4.3.8-x86_64-metal.x86_64.raw.gz\" ocp_initramfs: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.3/4.3.8/rhcos-4.3.8-x86_64-installer-initramfs.x86_64.img\" ocp_install_kernel: \"https://mirror.openshift.com/pub/openshift-v4/dependencies/rhcos/4.3/4.3.8/rhcos-4.3.8-x86_64-installer-kernel-x86_64\" ocp_client: \"https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.8/openshift-client-linux-4.3.8.tar.gz\" ocp_installer: \"https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.3.8/openshift-install-linux-4.3.8.tar.gz\" 手动下载完后、需要将它们重名并放到指定路径下 cp openshift-client-linux-4.3.8.tar.gz /usr/local/src/openshift-client-linux.tar.gz cp openshift-install-linux-4.3.8.tar.gz /usr/local/src/openshift-install-linux.tar.gz mkdir -p /var/www/html/install/ /var/lib/tftpboot/rhcos/ /var/lib/tftpboot/rhcos/ cp rhcos-4.3.8-x86_64-metal.raw.gz /var/www/html/install/bios.raw.gz cp rhcos-4.3.8-x86_64-installer-initramfs.img /var/lib/tftpboot/rhcos/initramfs.img cp rhcos-4.3.8-x86_64-installer-kernel /var/lib/tftpboot/rhcos/kernel # 因为后续会使用到openshift安装工具openshift-instsall，所以在辅助节点也解压一份命令到可执行路径下 tar -zxvf ~/openshift-install-linux-4.3.8.tar.gz -C /usr/local/bin/ rm -rf /usr/local/bin/README.md 3.5、执行Ansible脚本 cd ~/ocp4-helpernode ansible-playbook -e @vars.yaml tasks/main.yml 3.6、检查辅助节点上安装的服务状况 /usr/local/bin/helpernodecheck {dns-masters|dns-workers|dns-etcd|dns-other|install-info|haproxy|services|nfs-info} 4、创建安装OCP配置文件 4.1、创建配置文件的定义信息文件 mkdir ~/ocp43 bash -c 'cat > ~/ocp43/install-config.yaml 4.2、使用openshift-install工具命令生成配置文件 ./openshift-install create manifests --dir=ocp43 4.3、使用openshift-install工具命令生成PXE安装CoreOS的ignition文件 ./openshift-install create ignition-configs --dir=ocp43 # 上述命令会在～/ocp43路径下生成以下文件 auth bootstrap.ign master.ign metadata.json worker.ign 将ign文件拷贝到/var/www/html/ignition目录下，并修改权限 cd ocp43 cp bootstrap.ign master.ign worker.ign /var/www/html/ignition chmod 775 /var/www/html/ignition/* cd .. 5、将集群节点虚拟机开机 要是辅助节点上所有的服务准备就绪了、此时将ocp43-bootstrap、ocp43-master1、ocp43-worker1节点虚拟机开机。它们就会发现局域网内辅助节点上的DHCP、TFTP、PXE服务，自动从网卡启动，然后在PXE服务的引导下拉取OS镜像文件安装操作系统。 6、执行等待安装完成命令 openshift-install --dir=ocp43 wait-for bootstrap-complete --log-level=info 7、注意事项！等待、等待、等待 在安装过程中，由于Bootstrap、Master、Worker节点默认时区是UTC时区，而pull secret的有效期是一天，申请完后立即安装的话。安装过程中安装组件式时报证书无效，需要等到8小时，让系统时间等到pull secret有效期。 可通过echo | openssl s_client -connect api-int.ocp43.curiouser.com:6443 | openssl x509 -noout -text查看pull secret相关证书的有效期 疑问待后续解决的：集群节点为什么会出现时区设置不一致的？为什么手动设置时区后，系统层面生效但POD没有及时生效？是不是需要改动tool节点上某个服务的配置？ 8、验证 tar -zxvf ~/openshift-client-linux-4.3.8.tar.gz -C /usr/local/bin/ export KUBECONFIG=~/ocp43/auth/kubeconfig oc get node # ./openshift-install --dir=ocp43 wait-for install-complete INFO Waiting up to 30m0s for the cluster at https://api.ocp43.curiouser.com:6443 to initialize... INFO Waiting up to 10m0s for the openshift-console route to be created... INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/root/ocp43/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp43.curiouser.com INFO Login to the console with user: kubeadmin, password: ***** 9、集群外访问 相关自带服务Web界面 本地hosts中配置访问的服务域名与tools节点IP地址的映射 192.168.1.80 api.ocp43.curiouser.com 192.168.1.80 oauth-openshift.apps.ocp43.curiouser.com # OCP的WebConsole 192.168.1.80 console-openshift-console.apps.ocp43.curiouser.com # OCP的用户认证服务地址 192.168.1.80 oauth-openshift.apps.ocp43.curiouser.com # OCP的HTTP WEB服务地址 192.168.1.80 downloads-openshift-console.apps.ocp43.curiouser.com # OCP的Prometheus 192.168.1.80 alertmanager-main-openshift-monitoring.apps.ocp43.curiouser.com 192.168.1.80 grafana-openshift-monitoring.apps.ocp43.curiouser.com 192.168.1.80 prometheus-k8s-openshift-monitoring.apps.ocp43.curiouser.com 192.168.1.80 thanos-querier-openshift-monitoring.apps.ocp43.curiouser.com oc/kubectl命令远程访问操作集群 kubeconfig配置文件在tools节点ocp安装配置目录下的auth路径下，拷贝出来配置在本地管理员电脑的kubectlconfig即可 oc/kubectl客户端可在OCP自带的HTTP服务中下载（https://downloads-openshift-console.apps.ocp43.curiouser.com) 三、节点扩容 参考 https://www.jianshu.com/p/72a981aec92a （非常感谢少坡同学予以的指导！） https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/openshift-Kubernetes的持久化存储.html":{"url":"origin/openshift-Kubernetes的持久化存储.html","title":"数据持久化","keywords":"","body":"一、Kubernetes持久化存储简介 通常情况下，我们可以认为容器或者Pod的生命周期时短暂的，当容器被销毁时，容器内部的数据也同时被清除。 对于容器，数据持久化存储的重要性不言而喻。Docker有存储卷的概念，用来将磁盘上的或另一个容器中的目录挂载到容器的某一个路径下。即使容器挂掉了，挂载Volume中的数据依旧存在。然而没有对其生命周期进行管理。而Kubernetes提供了多种不同类型资源的Volume存储卷，供POD挂载到容器的不同路径下,常见的有： emptyDir：pod被调度到某个宿主机上的时候才创建，而同一个pod内的容器都能读写EmptyDir中的同一个文件。删除容器并不会对它造成影响，只有删除整个Pod时，它才会被删除，它的生命周期与所挂载的Pod一致 hostPath：将宿主机的文件系统的文件或目录挂接到Pod中 secret：将Kubernetes中secret对象资源挂载到POD中 configMap：将Kubernetes中config对象资源挂载到POD中 persistentVolumeClaim：将PersistentVolume挂接到Pod中作为存储卷。使用此类型的存储卷，用户不需要关注存储卷的详细信息。 nfs glusterfs cephfs vspherevolume iscsi .... 对于以上大部分的volume类型，对使用用户是极其不友好的。理解他们体系中的概念配置是一件复杂的事情，有时我们其实并不关心他们的各种存储实现，只希望能够简单安全可靠地存储数据。所以K8S对存储的供应和使用做了抽象，以API形式提供给管理员和用户使用。因此引入了两个新的API资源：Persistent Volume（持久卷PV）和Persistent Volume Claim（持久卷申请PVC）。 PVC负责定义使用多大的存储空间，什么样的读写方式等常见要求即可，而PV负责抽象各种存储系统的技术细节（例如存储系统IP地址端口，客户端证书密钥等），满足PVC的存储需求，继而作为Kubernetes集群的存储对象资源。 apiVersion: \"v1\" kind: \"PersistentVolumeClaim\" metadata: name: \"ceph-pvc-test\" namespace: \"default\" spec: accessModes: - \"ReadWriteMany\" resources: requests: storage: \"2Gi\" volumeName: \"pv-nfs-test\" # 指定PV PersistentVolumesClaim的属性 Access Modes ReadWriteOnce —— 该volume只能被单个节点以读写的方式映射 ReadOnlyMany —— 该volume可以被多个节点以只读方式映射 ReadWriteMany —— 该volume只能被多个节点以读写的方式映射 Volume Modes：在Kubernetes 1.9之前，所有卷插件都在pv上创建了一个文件系统。现在，可以将volumeMode的值设置为block以使用原始块设备，或者将filesystem设置为使用文件系统。如果省略该值，则默认为filesystem。 Resources：指定使用多大的存储空间 Selector：PVC可以指定标签选择器进行更深度的过滤PV，只有匹配了选择器标签的PV才能绑定给PVC。选择器包含两个字段： matchLabels（匹配标签） - PV必须有一个包含该值得标签 matchExpressions（匹配表达式） - 一个请求列表，包含指定的键、值的列表、关联键和值的操作符。合法的操作符包含In，NotIn，Exists，和DoesNotExist。 　　所有来自matchLabels和matchExpressions的请求，都是逻辑与关系的，它们必须全部满足才能匹配上。 Class apiVersion: v1 kind: PersistentVolume metadata: name: ceph-pv-test spec: capacity: storage: 2Gi accessModes: - ReadWriteOnce rbd: monitors: - 192.168.122.133:6789 pool: rbd image: ceph-image user: admin secretRef: name: ceph-secret fsType: ext4 readOnly: false persistentVolumeReclaimPolicy: Retain claimRef: name: \"pvc-test\" namespace: \"default\" PersistentVolumes的属性 Capacity：指定存储容量大小 Volume Mode：在Kubernetes 1.9之前，所有卷插件都在pv上创建了一个文件系统。现在，可以将volumeMode的值设置为block以使用原始块设备，或者将filesystem设置为使用文件系统。如果省略该值，则默认为filesystem。 Class: 一个PV可以有一种class，通过设置storageClassName属性来选择指定的StorageClass。有指定class的PV只能绑定给请求该class的PVC。没有设置storageClassName属性的PV只能绑定给未请求class的PVC(过去，使用volume.beta.kubernetes.io/storage-class注解，而不是storageClassName属性。该注解现在依然可以工作，但在Kubernetes的未来版本中已经被完全弃用了) Reclaim Policy Mount Options Node Affinity Access Modes ReadWriteOnce —— 该volume只能被单个节点以读写的方式映射 ReadOnlyMany —— 该volume可以被多个节点以只读方式映射 ReadWriteMany —— 该volume只能被多个节点以读写的方式映射 PersistentVolumes的周期状态 Available: 空闲的，未绑定给PVC Bound: 绑定上了某个PVC Released: PVC已经删除了，但是PV还没有被回收 Failed: PV在自动回收中失败了 PV支持的存储系统: GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk CSI FC (Fibre Channel) Flexvolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath (Single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster) Portworx Volumes ScaleIO Volumes StorageOS PV和PVC 之间的关联遵循如下的生命周期： Provisioning-供应: PV的创建阶段，有以下两种创建方式 静态手工：集群管理员通过手工的方式创建pv 动态自动：通过PersistentVolume Controller动态调度，Kubernetes将能够按照用户的需要，根据PVC的资源请求，寻找StorageClasse定义的符合要求的底层存储自动创建其需要的存储卷。 Binding-绑定: PV分配绑定到PVC Using-使用： POD挂载使用PVC类型的Volume Reclaiming-回收：PV释放后的回收利用策略 Retain保留: 保留现场，人工回收 Delete删除: 自动删除，动态删除后端存储。需要IaaS层的支持，目前只有Ceph RBD和OpenStack Cinder支持 Recycle复用：通过rm -rf删除卷上的所有数据。目前只有NFS和HostPath支持（逐渐在抛弃该方式，建议使用） 二、使用StorageClass提供动态存储供应 通常情况下，Kubernetes集群管理员需要手工创建所需的PV存储资源。从Kubernetes 1.2以后可以使用Storageclass实现动态自动地根据用户需求创建某种存储系统类型的PV。同时，可以定义多个 StorageClass ，给集群提供不同存储系统类型的PV资源。 1. 定义创建StorageClass 每一个存储类都必须包含以下参数 provisioner: 决定由哪个Provisioner来创建PV parameters: Provisioner需要的参数,可选项：Delete(Default),Retain reclaimPolicy: PV的回收策略 可选参数： Mount Options Volume Binding Mode Allowed Topologies Note: StorageClass一旦被创建，将不能被更新 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd # 指定Provisioner provisioner: kubernetes.io/rbd parameters: monitors: 10.20.30.40:6789 adminId: kube adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-secret-user userSecretNamespace: default fsType: ext4 imageFormat: \"2\" imageFeatures: \"layering\" Kubernetes支持的Provisioner Provisioner 是否内置插件 配置例子 AWSElasticBlockStore ✓ AWS AzureFile ✓ Azure File AzureDisk ✓ Azure Disk CephFS – – Cinder ✓ OpenStack Cinder FC – – FlexVolume – – Flocker ✓ – GCEPersistentDisk ✓ GCE Glusterfs ✓ Glusterfs iSCSI – – PhotonPersistentDisk ✓ – Quobyte ✓ Quobyte NFS – – RBD ✓ Ceph RBD VsphereVolume ✓ vSphere PortworxVolume ✓ Portworx Volume ScaleIO ✓ ScaleIO StorageOS ✓ StorageOS Local – Local StorageClas可以支持第三方的Provisioner，只要该插件符合Kubernetes的规范 内置的Provisioner名称带有“kubernetes.io”前缀 Github仓库：https://github.com/kubernetes-incubator/external-storage 有官方支持的第三方Provisioner 2. 指定StorageClass动态创建PV 在Kubernetes v1.6之前的版本，通过volume.beta.kubernetes.io/storage-class注释类请求动态供应存储； 在Kubernetes v1.6版本之后，用户应该使用PersistentVolumeClaim对象的storageClassName参数来请求动态存储。 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim1 spec: accessModes: - ReadWriteOnce # 指定所使用的存储类，此存储类将会自动创建符合要求的PV storageClassName: ceph-rbd resources: requests: storage: 30Gi 3. 指定默认的StorageClass 创建StorageClass时可添加添加storageclass.kubernetes.io/is-default-class注解来指定为默认的存储类。 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" # 将此storageclass设置为默认 name: nfs-client-storageclass provisioner: fuseim.pri/ifs parameters: archiveOnDelete: \"true\" 一个集群中，最多只能有一个默认的存储类 如果没有默认的存储类，在PersistentVolumeClaim中也没有显示指定storageClassName，将无法创建PersistentVolume。 参考链接 https://kubernetes.io/docs/concepts/storage/volumes/ https://kubernetes.io/docs/concepts/storage/storage-classes/ https://www.kubernetes.org.cn/4078.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kubernetes-provisioner-nfs-client.html":{"url":"origin/openshift-Kubernetes-provisioner-nfs-client.html","title":"NFS Client provisioner","keywords":"","body":"一、NFS Client Provisioner https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client https://www.kubernetes.org.cn/3894.html Provisioner的定义原理: openshift-Kubernetes的持久化存储 二、安装部署 1. 创建NFS服务端 yum install -y nfs-utils rpcbind && \\ systemctl enable nfs && \\ systemctl enable rpcbind && \\ systemctl start nfs && \\ systemctl start rpcbind && \\ mkdir -p /data/nfs/appstorage-nfs-client-provisioner && \\ echo \"/data/nfs/appstorage-nfs-client-provisioner *(rw,no_root_squash,sync)\" >> /etc/exports && \\ exportfs -a && \\ showmount -e $HOSTNAME 2. 创建RBAC kind: ServiceAccount apiVersion: v1 metadata: name: nfs-client-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 3. 修改Deployment并以此部署POD 先拉取镜像 docker pull quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest quay.io/external_storage/nfs-client-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/external_storage/nfs-client-provisioner:latest kind: Deployment apiVersion: extensions/v1beta1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: $HOSTNAME # NFS Server的地址 - name: NFS_PATH value: /data/nfs/appstorage-nfs-client-provisioner # NFS Server要挂载的路径 volumes: - name: nfs-client-root nfs: server: $HOSTNAME #指定NFS Server的地址 path: /data/nfs/appstorage-nfs-client-provisioner #指定NFS Server要挂载的路径 三、使用 1. 创建StorageClass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-client-storageclass provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # When set to \"false\" your PVs will not be archived by the provisioner upon deletion of the PVC. =======================================================补充内容========================================================= #如果要将此storageclass设置为默认，在metadata里面添加以下注解。（这样创建PVC时就可以不用特意指定StorageClass） apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" name: nfs-client-storageclass provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # \"false\" 删除PVC时不会保留数据，\"true\"将保留PVC的数据，形成以\"archived-\"开头的文件夹 2. 创建PVC时使用 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-pvc #当默认storageclass就是nfs-client-storageclass，可不要该注解 annotations: volume.beta.kubernetes.io/storage-class: \"nfs-client-storageclass\" spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi 四、测试 1. 创建一个PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi ​ #======================================================================================================================== $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test Bound pvc-e8a15786-5a09-11e9-ad53-000c296286d8 100Mi RWX nfs-client-storageclass 10m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-e8a15786-5a09-11e9-ad53-000c296286d8 100Mi RWX Delete Bound default/test nfs-client-storageclass 10m 2. 创建一个POD使用PVC apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: test 3. 查看NFS目录 /data/nfs/k8s-app-nfs-storage/ └── [drwxrwxrwx 32] default-test-pvc-e8a15786-5a09-11e9-ad53-000c296286d8 ├── [-rw-r--r-- 947] 1.log └── [-rw-r--r-- 1.0K] 2.log Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kubernetes-provisioner-nfs-server.html":{"url":"origin/openshift-Kubernetes-provisioner-nfs-server.html","title":"NFS Server Provisioner","keywords":"","body":"一、NFS Server Provisioner Github项目地址：https://github.com/kubernetes-incubator/external-storage/tree/v5.2.0/nfs Provisioner的定义原理：openshift-Kubernetes的持久化存储 NFS Provisioner的部署文档：https://github.com/kubernetes-incubator/external-storage/blob/master/nfs/docs/deployment.md NFS Provisioner的使用文档：https://github.com/kubernetes-incubator/external-storage/blob/master/nfs/docs/usage.md 二、在Kubernetes上部署 1、（可选）预拉取镜像 docker pull quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest quay.io/kubernetes_incubator/nfs-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/kubernetes_incubator/nfs-provisioner:latest 2、创建PodSecurityPolicy apiVersion: extensions/v1beta1 kind: PodSecurityPolicy metadata: name: nfs-provisioner spec: fsGroup: rule: RunAsAny allowedCapabilities: - DAC_READ_SEARCH - SYS_RESOURCE runAsUser: rule: RunAsAny seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny volumes: - configMap - downwardAPI - emptyDir - persistentVolumeClaim - secret - hostPath 3、创建RBAC kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\", \"endpoints\"] verbs: [\"get\"] - apiGroups: [\"extensions\"] resources: [\"podsecuritypolicies\"] resourceNames: [\"nfs-provisioner\"] verbs: [\"use\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: ClusterRole name: nfs-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-provisioner subjects: - kind: ServiceAccount name: nfs-provisioner # replace with namespace where provisioner is deployed namespace: default roleRef: kind: Role name: leader-locking-nfs-provisioner apiGroup: rbac.authorization.k8s.io 4、使用deployment创建POD（推荐） apiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner --- kind: Service apiVersion: v1 metadata: name: nfs-provisioner labels: app: nfs-provisioner spec: ports: - name: nfs port: 2049 - name: mountd port: 20048 - name: rpcbind port: 111 - name: rpcbind-udp port: 111 protocol: UDP selector: app: nfs-provisioner --- kind: Deployment apiVersion: apps/v1 metadata: name: nfs-provisioner spec: selector: matchLabels: app: nfs-provisioner replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-provisioner spec: serviceAccount: nfs-provisioner containers: - name: nfs-provisioner image: quay.io/kubernetes_incubator/nfs-provisioner:latest ports: - name: nfs containerPort: 2049 - name: mountd containerPort: 20048 - name: rpcbind containerPort: 111 - name: rpcbind-udp containerPort: 111 protocol: UDP securityContext: capabilities: add: - DAC_READ_SEARCH - SYS_RESOURCE args: # 定义提供者的名称，存储类通过此名称指定提供者 - \"-provisioner=example.com/nfs\" env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: SERVICE_NAME value: nfs-provisioner - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: export-volume mountPath: /export volumes: - name: export-volume hostPath: path: /srv 5、（可选）使用StatefulSet创建POD apiVersion: v1 kind: ServiceAccount metadata: name: nfs-provisioner --- kind: Service apiVersion: v1 metadata: name: nfs-provisioner labels: app: nfs-provisioner spec: ports: - name: nfs port: 2049 - name: mountd port: 20048 - name: rpcbind port: 111 - name: rpcbind-udp port: 111 protocol: UDP selector: app: nfs-provisioner --- kind: StatefulSet apiVersion: apps/v1 metadata: name: nfs-provisioner spec: selector: matchLabels: app: nfs-provisioner serviceName: \"nfs-provisioner\" replicas: 1 template: metadata: labels: app: nfs-provisioner spec: serviceAccount: nfs-provisioner terminationGracePeriodSeconds: 10 containers: - name: nfs-provisioner image: quay.io/kubernetes_incubator/nfs-provisioner:latest ports: - name: nfs containerPort: 2049 - name: mountd containerPort: 20048 - name: rpcbind containerPort: 111 - name: rpcbind-udp containerPort: 111 protocol: UDP securityContext: capabilities: add: - DAC_READ_SEARCH - SYS_RESOURCE args: - \"-provisioner=example.com/nfs\" env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: SERVICE_NAME value: nfs-provisioner - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace imagePullPolicy: \"IfNotPresent\" volumeMounts: - name: export-volume mountPath: /export volumes: - name: export-volume hostPath: path: /srv 三、使用 创建StorageClass kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: example-nfs provisioner: example.com/nfs parameters: mountOptions: \"vers=4.1\" ​ =======================================================补充内容========================================================= ​ #其他参数： gid: # \"none\" or a supplemental group like \"1001\". NFS shares will be created with permissions such that pods running with the supplemental group can read & write to the share, but non-root pods without the supplemental group cannot. Pods running as root can read & write to shares regardless of the setting here, unless the rootSquash parameter is set true. If set to \"none\", anybody root or non-root can write to the share. Default (if omitted) \"none\". rootSquash: # \"true\" or \"false\". Whether to squash root users by adding the NFS Ganesha root_id_squash or kernel root_squash option to each export. Default \"false\". mountOptions: # a comma separated list of mount options for every PV of this class to be mounted with. The list is inserted directly into every PV's mount options annotation/field without any validation. Default blank \"\". ​ #如果要将此storageclass设置为默认，在metadata里面添加以下注解。（这样创建PVC时就可以不用特意指定StorageClass） kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: #注解 annotations: \"storageclass.kubernetes.io/is-default-class\": \"true\" name: example-nfs provisioner: example.com/nfs parameters: mountOptions: \"vers=4.1\" 创建PVC时指定storageclass kind: PersistentVolumeClaim apiVersion: v1 metadata: name: nfs annotations: volume.beta.kubernetes.io/storage-class: \"example-nfs\" spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi 四、测试 创建一个PVC kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test annotations: volume.beta.kubernetes.io/storage-class: \"example-nfs\" spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi ​ #======================================================================================================================== $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE test Bound pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 100Mi RWX example-nfs 5m3s $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 100Mi RWX Delete Bound default/test example-nfs 5m9s 创建一个POD使用PVC apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: test 查看/srv目录 /srv ├── [-rw-r--r-- 4.4K] ganesha.log ├── [-rw------- 36] nfs-provisioner.identity ├── [drwxrwsrwx 32] pvc-e00e9603-56a5-11e9-95dd-080027c8ba17 │ ├── [-rw-r--r-- 5.1K] 1.log │ └── [-rw-r--r-- 5.7K] 2.log └── [-rw------- 1.1K] vfs.conf 注意：删除掉PVC，PV也会自动删除，底层的NFS目录也会跟着删除 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kubernetes-provisioner-glusterfs.html":{"url":"origin/openshift-Kubernetes-provisioner-glusterfs.html","title":"Glusterfs Provisioner","keywords":"","body":"一、OKD集群中添加容器化的GlusteFS Prerequisite OKD集群（3.11）至少有三个节点 OKD官方操作指南：https://docs.okd.io/3.11/install_config/persistent_storage/persistent_storage_glusterfs.html#install-config-persistent-storage-persistent-storage-glusterfs GlusterFS官方操作指南：https://docs.gluster.org/en/latest/Administrator%20Guide/overview/ heketi-cli官方操作指南：https://github.com/heketi/heketi 配置ansible主机清单/etc/ansible/hosts [OSEv3:children] ... glusterfs ​ [OSEv3:vars] ... openshift_storage_glusterfs_namespace=app-storage openshift_storage_glusterfs_storageclass=true openshift_storage_glusterfs_storageclass_default=false openshift_storage_glusterfs_block_deploy=true openshift_storage_glusterfs_block_host_vol_size=100 openshift_storage_glusterfs_block_storageclass=true openshift_storage_glusterfs_block_storageclass_default=false ​ [glusterfs] allinone311.okd.curiouser.com glusterfs_devices='[ \"/dev/vdf\" ]' node1.okd.curiouser.com glusterfs_devices='[ \"/dev/vdd\" ]' node2.okd.curiouser.com glusterfs_devices='[ \"/dev/vdd\" ]' #至少是三个节点 glusterfs节点上安装软件 yum install glusterfs-fuse && \\ yum update glusterfs-fuse 配置glusterfs节点上的Selinux setsebool -P virt_sandbox_use_fusefs on && \\ setsebool -P virt_use_fusefs on 执行openshift ansible playbook ansible-playbook /root/openshift-ansible/playbooks/openshift-glusterfs/config.yml 二、向OKD集群中添加集群外的GlusteFS 配置ansible主机清单/etc/ansible/hosts [OSEv3:children] ... glusterfs ​ [OSEv3:vars] ... openshift_storage_glusterfs_namespace=app-storage openshift_storage_glusterfs_storageclass=true openshift_storage_glusterfs_storageclass_default=false openshift_storage_glusterfs_block_deploy=true openshift_storage_glusterfs_block_host_vol_size=100 openshift_storage_glusterfs_block_storageclass=true openshift_storage_glusterfs_block_storageclass_default=false openshift_storage_glusterfs_is_native=false openshift_storage_glusterfs_heketi_is_native=true openshift_storage_glusterfs_heketi_executor=ssh openshift_storage_glusterfs_heketi_ssh_port=22 openshift_storage_glusterfs_heketi_ssh_user=root openshift_storage_glusterfs_heketi_ssh_sudo=false openshift_storage_glusterfs_heketi_ssh_keyfile=\"/root/.ssh/id_rsa\" ​ [glusterfs] gluster1.example.com glusterfs_ip=192.168.10.11 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' gluster2.example.com glusterfs_ip=192.168.10.12 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' gluster3.example.com glusterfs_ip=192.168.10.13 glusterfs_devices='[ \"/dev/xvdc\", \"/dev/xvdd\" ]' 执行openshift ansible playbook ansible-playbook /root/openshift-ansible/playbooks/openshift-glusterfs/config.yml 三、卸载 ansible-playbook -e \"openshift_storage_glusterfs_wipe=true\" /root/openshift-ansible/playbooks/openshift-glusterfs/uninstall.yml 四、OKD中通过storage动态使用glusterfs作为PVC的后端存储 1. 创建storage class 创建storage class(ansible playbook执行过程中会自动创建storageclass) kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: glusterfs-storage provisioner: kubernetes.io/glusterfs parameters: resturl: 'http://heketi-storage.app-storage.svc:8080' restuser: admin secretName: heketi-storage-admin-secret secretNamespace: app-storage reclaimPolicy: Delete volumeBindingMode: Immediate 如果使用的集群外的Glusterfs集群，需要手动创建storage class。 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: glusterfs-storage provisioner: kubernetes.io/glusterfs parameters: resturl: \"http://10.42.0.0:8080\" restauthenabled: \"false\" 2. 创建PVC时使用Glusterfs的storage class apiVersion: v1 kind: PersistentVolumeClaim metadata: name: gluster1 spec: accessModes: - ReadWriteMany resources: requests: storage: 30Gi storageClassName: glusterfs-storage 五、主机上mount挂载使用容器化的GlusterFS 挂载命令格式： mount -t glusterfs GlusterFS容器化pod所在的节点IP地址:/volume_name /mnt/glusterfs 示例： $ mount -t glusterfs 172.16.1.4:/vol_fe0de9d2f43731d1af7a5dc296041d83 /mnt/glusterfs && \\ df -mh 172.16.1.4:/vol_fe0de9d2f43731d1af7a5dc296041d83 10G 136M 9.9G 2% /mnt/glusterfs Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kubernetes-provisioner-cephfs.html":{"url":"origin/openshift-Kubernetes-provisioner-cephfs.html","title":"Ceph FileSystem Provisioner","keywords":"","body":"相关链接 官方文档： https://github.com/kubernetes-incubator/external-storage/tree/master/ceph/cephfs Provisioner的定义原理：Kubernetes的存储--> StorageClass provisioner 姊妹篇： Preflight 1. Openshift创建cephfs命名空间 oc new-project cephfs --display-name=\"Ceph FileSystem Provisioner\" 2. 拉取镜像 docker pull quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest && \\ docker tag quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest quay.io/external_storage/cephfs-provisioner:latest && \\ docker rmi quay-mirror.qiniu.com/external_storage/cephfs-provisioner:latest 一、安装部署 1. 获取Ceph Filesystem Client.admin用户的密钥环 ceph auth get client.admin # [client.admin] # key = AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== # caps mds = \"allow *\" # caps mon = \"allow *\" # caps osd = \"allow *\" 2. 创建Secrets oc create secret generic cephfs-secret-admin --from-literal=key='AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ==' --namespace=cephfs 3. 创建RBAC --- apiVersion: v1 kind: ServiceAccount metadata: name: cephfs-provisioner namespace: cephfs --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\", \"get\", \"delete\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cephfs-provisioner namespace: cephfs roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner namespace: cephfs rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: cephfs-provisioner subjects: - kind: ServiceAccount name: cephfs-provisioner namespace: cephfs roleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io 4. 使用Deployment创建Ceph-FileSystem-provisioner的POD apiVersion: extensions/v1beta1 kind: Deployment metadata: name: cephfs-provisioner namespace: cephfs spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: \"quay.io/external_storage/cephfs-provisioner:latest\" env: - name: PROVISIONER_NAME value: ceph.com/cephfs - name: PROVISIONER_SECRET_NAMESPACE value: cephfs command: - \"/usr/local/bin/cephfs-provisioner\" args: - \"-id=cephfs-provisioner-1\" serviceAccount: cephfs-provisioner 二、使用 1. 创建StorageClass kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: cephfs provisioner: ceph.com/cephfs parameters: monitors: allinone.okd311.curiouser.com:6789 adminId: admin adminSecretName: cephfs-secret-admin adminSecretNamespace: \"cephfs\" claimRoot: /pvc-volumes 2、创建PVC时使用 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cephfs-test spec: storageClassName: cephfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kubernetes-provisioner-cephrbd.html":{"url":"origin/openshift-Kubernetes-provisioner-cephrbd.html","title":"Ceph RBD Provisioner","keywords":"","body":"一、获取ceph client admin用户的密钥环keyring 查看Ceph集群Admin节点的集群配置文件夹my-cluster下的ceph.client.admin.keyring文件来获取key值 $> cat ceph.client.admin.keyring [client.admin] key = AQBUilha86ufLhAA2BxJn7sG8qVYndokVwtvyA== caps mds = \"allow *\" caps mon = \"allow *\" caps osd = \"allow *\" $ ceph auth list #获取所有客户端用户 $ ceph auth get client.admin #获取客户端指定用户 二、使用admin的keyring在openshift上创建secret CLI $> oc create secret generic ceph-secret --type=\"kubernetes.io/rbd\" --from-literal=key='AQAil11anEPOORAArxzRkH9iS1IOGKQfK87+Ag==' --namespace=default YAML kind: Secret apiVersion: v1 metadata: name: ceph-secret namespace: default selfLink: /api/v1/namespaces/default/secrets/ceph-secret data: key: QVFDcFNlMWJ0Y3VxSFJBQWlST25zY1VDMWpnTWRwZkRJMFd0THc9PQ== type: kubernetes.io/rbd 三、创建storageclass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd-sc provisioner: kubernetes.io/rbd parameters: monitors: 192.168.0.26:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: default pool: rbd userId: admin userSecretName: ceph-secret #说明:adminId默认值为admin,pool默认值为rbd, userId默认值与adminId一样.所以这三个值可以不填写。 四、可以在console界面创建，也可以通过PVC的YAML配置文件中指定使用Ceph kind: PersistentVolumeClaim apiVersion: v1 metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ceph-rbd-sc 结果如下图： Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-资源对象常见操作.html":{"url":"origin/openshift-资源对象常见操作.html","title":"常见资源对象操作","keywords":"","body":"常用资源对象操作 1、登录 oc project oc login -u 用户名 集群master的URL oc whoami #查看当前登录的用户，加-t参数可查看当前用户的token 2、切换Project oc project 3、查看集群节点 oc get node/no oc get node/no node1.test.openshift.com 4、查看集群节点的详细信息 oc describe node node1.test.openshift.com 5、查看某个节点上的所有Pods oc adm manage-node node1.test.openshift.com --list-pods 6、使节点禁止参与调度 oc adm manage-node router1.test.openshift.com --schedulable=false 7、疏散某个节点上的所有POD oc adm drain router1.test.openshift.com --ignore-daemonsets 8、清除旧的Build和Deployments历史版（所有namespace） 统计要清除的资源个数 #oc adm prune deployments --keep-younger-than=24h --keep-complete=5 --keep-failed=5|wc -l 确认清除动作 # oc adm prune [deployments|builds|images] --confirm --keep-younger-than=24h --keep-complete=5 --keep-failed=5 参数详解 --confirm 确认操作 --keep-younger-than=1h0m0s Specify the minimum age of a Build for it to be considered a candidate for pruning. --keep-complete=5 Per BuildConfig, specify the number of builds whose status is complete that will be preserved. --keep-failed=1 Per BuildConfig, specify the number of builds whose status is failed, error, or cancelled that will be preserved. --orphans=false If true, prune all builds whose associated BuildConfig no longer exists and whose status is complete, failed, error, or cancelled. 示例： 清理images（在admin用户下执行） # oc adm prune images --keep-younger-than=400m --keep-tag-revisions=10 --registry-url=docker-registry.default.svc:5000 --certificate-authority=/etc/origin/master/registry.crt --confirm 9、删除所有Namespace中非Running的pods for i in `oc get po --all-namespaces|grep -v \"Running\"|grep -v \"NAMESPACE\"|awk '{print $1}'|sort -u` ; do echo \"===================Namespace $i===================\"; oc -n $i delete po `oc get po -n $i |grep -v \"Running\"|grep -v \"NAME\"|awk 'BEGIN{ORS=\" \"}{print $1}'`; done 10、强制删除POD oc delete po gitlab-ce-16-ntzst --force --grace-period=0 11、资源的查看 #查看当前项目的所有资源 oc get all #查看当前项目的所有资源，外加输出label信息 oc get all --show-labels # 查看指定资源 oc get pod/po oc get service/svc oc get persistentvolumes/pv 12、通过label选择器删除namespace下所有的资源 #如果namespace下所有的资源都打上了“name=test”标签 oc delete all -l name=test 13、项目的管理 #创建项目 oc new-project --display-name=显示的项目名 --description=项目描述 project_name #删除项目 oc delete project 项目名 #查看当前处于哪个项目下 oc project #查看所有项目 oc projects 14、模板的管理 #创建模板(模板文件格式为YAML/JSON.也可以在Openshift的web页面上直接导入) oc create -f #查看模板 oc get templates #编辑模板 oc edit template #删除模板 oc delete template 附录 buildconfigs (aka 'bc') #构建配置 builds #构建版本 certificatesigningrequests (aka 'csr') clusters (valid only for federation apiservers) clusterrolebindings clusterroles componentstatuses (aka 'cs') configmaps (aka 'cm') daemonsets (aka 'ds') deployments (aka 'deploy') deploymentconfigs (aka 'dc') endpoints (aka 'ep') events (aka 'ev') horizontalpodautoscalers (aka 'hpa') imagestreamimages (aka 'isimage') imagestreams (aka 'is') imagestreamtags (aka 'istag') ingresses (aka 'ing') groups jobs limitranges (aka 'limits') namespaces (aka 'ns') networkpolicies nodes (aka 'no') persistentvolumeclaims (aka 'pvc') persistentvolumes (aka 'pv') poddisruptionbudgets (aka 'pdb') podpreset pods (aka 'po') podsecuritypolicies (aka 'psp') podtemplates policies projects replicasets (aka 'rs') replicationcontrollers (aka 'rc') resourcequotas (aka 'quota') rolebindings roles routes secrets serviceaccounts (aka 'sa') services (aka 'svc') statefulsets users storageclasses thirdpartyresources Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-将Secret和ConfigMap以文件的形式挂载到容器.html":{"url":"origin/openshift-将Secret和ConfigMap以文件的形式挂载到容器.html","title":"将Secret和ConfigMap以文件的形式挂载到容器","keywords":"","body":"将Secret和ConfigMap以文件的形式挂载到容器 一、Context ConfigMap或者Secret在默认挂载到容器是以Volumes的形式，如果挂载路径下原有的其他文件，则会覆盖掉。 如果将挂载路径直接写成文件的绝对路径，这会在挂载路径下创建以文件名为名字的文件夹，文件会在这个文件夹下 containers: - image: 'busybox:latest' name: test volumeMounts: - mountPath: /etc/test/test.txt name: test-volume volumes: - name: test-volume secret: defaultMode: 420 secretName: test-secret 二、操作 挂载Secret或Config类型的volume时，添加一个subPath字段即可，可将其以文件的形式挂载，而不是以目录的形式。 Secret 1. 创建secret apiVersion: v1 kind: Secret metadata: name:test-secret type: Opaque data: test.txt: >- ************************ 2. 容器中挂载secret containers: - image: 'busybox:latest' name: test volumeMounts: - mountPath: /etc/test/test.txt name: test-volume readOnly: true subPath: test.txt # .... volumes: - name: test-volume secret: defaultMode: 420 secretName: test-secret secret中的test.txt文件将会单个文件的形式挂载到/etc/test/目录下 Configmap 1. 创建ConfigMap oc create configmap crack-jar --from-file=atlassian-extras-3.2.jar --from-literal=text=atlassian-extras-3.2.jar 2. 容器中挂载ConfigMap # .... volumeMounts: - mountPath: /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/atlassian-extras-3.2.jar name: crack-jar readOnly: true subPath: atlassian-extras-3.2.jar # .... volumes: - configMap: defaultMode: 420 name: crack-jar name: crack-jar Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-集群节点管理.html":{"url":"origin/openshift-集群节点管理.html","title":"节点管理","keywords":"","body":"一、集群添加Node节点 Ansible脚本有新增节点的Playbook脚本，准备好新增节点的基础环境，在集群的ansible管理节点上执行该Playbook就行。 　Context OKD版本 OS版本 Docker版本 Ansible版本 3.11 CentOS 7.5.1804 1.13.1 2.6.5 1. 新增节点Prerequisite 新增node节点IP地址及主机名：192.168.1.23 node6.okd.curiouser.com 开启seLinux sed -i \"s/SELINUX=disabled/SELINUX=enforcing/\" /etc/sysconfig/selinux && \\ setenforce 1 安装docker，jdk及基础软件 yum install -y docker vim lrzsz wget unzip net-tools telnet bind-utils && \\ systemctl enable docker && \\ systemctl start docker && \\ systemctl status docker && \\ yum localinstall -y jdk-8u191-linux-x64.rpm && \\ docker info && \\ java -version 配置DNS，发现集群其他节点的IP地址与域名的映射关系.(注意DNSMasq服务端的iptables是否放行DNS的53 UDP端口) 由于集群内有DNSMasq服务端，配置/etc/resolv.conf echo \"nameserver 192.168.1.22\" >> /etc/resolv.conf && \\ ping allinone311.okd.curiouser.com Note: #DNSMasq服务端放行DNS的53 UDP端口 iptables -I OS_FIREWALL_ALLOW -p udp -m udp --dport 53 -j ACCEPT && \\ iptables-save 配置Openshift的YUM源 mkdir /etc/yum.repos.d/bak && \\ mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak && \\ scp allinone311.okd.curiouser.com:/etc/yum.repos.d/all.repo /etc/yum.repos.d/ && \\ yum clean all && \\ yum makecache 2. ansible管理节点 打通ansible管理节点到新增node节点的SSH免密通道 ssh-copy-id -i root@node6.okd.curiouser.com && \\ ssh root@node1.okd.curiouser.com ansible管理节点的ansible主机清单文件inventory中添加新增节点相关信息 [OSEv3:children] ... new_nodes [new_nodes] node1.okd.curiouser.com openshift_node_group_name=\"node-config-all-in-one\" ansible管理节点执行新增节点的Ansible Playbookansible-playbook /root/openshift-ansible/playbooks/openshift-node/scaleup.yml 注意1： 当执行脚本时tower主机会把它的dnsmasq配置/etc/dnsmasq.d/origin-upstream-dns.conf同步到新增节点/etc/dnsmasq.d/路径下。由于tower主机的/etc/dnsmasq.d/origin-upstream-dns.conf设置的上游DNS服务器为外网的。不希望新增节点的上游DNS服务器走外网，而是走tower主机，形成集群只有Tower主机一个节点的dns对外，其他主机作为Tower主机dns服务的客户端。所以当tower主机/etc/dnsmasq.d/origin-upstream-dns.conf同步到新增节点/etc/dnsmasq.d/路径下的时候，及时修改上游dns服务器为tower主机。然后重启dnsmasq。有两个明显的坑: ① 无法重启dnsmasq，报以下错误： DBus error: Connection \":1.50\" is not allowed to own the service \"uk.org.thekelleys.dnsmasq\" due to security policies in the configuration file 解决方案：重启dbus，再重启dnsmasq systemctl restart dbus && \\ systemctl restart dnsmasq ②tower主机的iptables服务开启，dns的53端口没有放开，导致新增节点的dns无法连接上游dns服务器（即Tower主机的dns服务） 解决方案：tower主机放行dns服务的UDP 53端口。（可在新增节点尝试nslookup解析域名试一下） iptables -I OS_FIREWALL_ALLOW -p udp -m udp --dport 53 -j ACCEPT && \\ iptables-save 注意2： 如果出现收集allinone节点facts超时的报错，出现一下错误提示 The full traceback is: Traceback (most recent call last): File \"/tmp/ansible_d9POp0/ansible_modlib.zip/ansible/module_utils/basic.py\", line 2853, in run_command cmd = subprocess.Popen(args, **kwargs) File \"/usr/lib64/python2.7/subprocess.py\", line 711, in __init__ errread, errwrite) File \"/usr/lib64/python2.7/subprocess.py\", line 1308, in _execute_child data = _eintr_retry_call(os.read, errpipe_read, 1048576) File \"/usr/lib64/python2.7/subprocess.py\", line 478, in _eintr_retry_call return func(*args) File \"/tmp/ansible_d9POp0/ansible_modlib.zip/ansible/module_utils/facts/timeout.py\", line 37, in _handle_timeout raise TimeoutError(msg) TimeoutError: Timer expired after 10 seconds TimeoutError: Timer expired after 10 seconds 请在/etc/ansible/ansible.cfg 设置\"gather_subset = !all\"或者\"gather_timeout=300\"。原因可能是已经运行allinone节点上的facts（特别是docker images layer的挂载信息）过多，造成收集facts超时，默认收集facts超时时间是10。 相关连接：https://github.com/ansible/ansible/issues/43884 二、删除节点 疏散要删除节点上的POD oc adm drain [--pod-selector=] --force=true --grace-period=-1 --timeout=5s --delete-local-data=true 删除Node oc delete node Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-使用Cockpit监控集群节点的系统状态.html":{"url":"origin/openshift-使用Cockpit监控集群节点的系统状态.html","title":"节点状态监控","keywords":"","body":"一、Cockpit简介 Cockpit 是一个自由开源的服务器管理软件，它使得我们可以通过它好看的 web 前端界面轻松地管理我们的 GNU/Linux 服务器。Cockpit 使得 linux 系统管理员、系统维护员和开发者能轻松地管理他们的服务器并执行一些简单的任务，例如管理存储、检测日志、启动或停止服务以及一些其它任务。它的报告界面添加了一些很好的功能使得可以轻松地在终端和 web 界面之间切换。另外，它不仅使得管理一台服务器变得简单，更重要的是只需要一个单击就可以在一个地方同时管理多个通过网络连接的服务器。它非常轻量级，web 界面也非常简单易用。在这篇博文中，我们会学习如何安装 Cockpit 并用它管理我们的运行着 Fedora、CentOS、Arch Linux 以及 RHEL 发行版操作系统的服务器。下面是 Cockpit 在我们的 GNU/Linux 服务器中一些非常棒的功能： 它包含 systemd 服务管理器。 有一个用于故障排除和日志分析的 Journal 日志查看器。 包括 LVM 在内的存储配置比以前任何时候都要简单。 用 Cockpit 可以进行基本的网络配置。 可以轻松地添加和删除用户以及管理多台服务器。 二、Cockpit安装 所有集群节点安装cockpit并启动服务 yum install -y cockpit cockpit-docker cockpit-kubernetes ;\\ systemctl enable cockpit ;\\ systemctl start cockpit ;\\ netstat -lanp |grep 9090 iptables放行端口 vi /etc/sysconfig/iptables #-A INPUT -p tcp -m state --state NEW -m tcp --dport 9090 -j ACCEPT systemctl restart iptables Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-集群组件TLS证书管理.html":{"url":"origin/openshift-集群组件TLS证书管理.html","title":"集群组件TLS证书管理","keywords":"","body":"Openshift组件:Master、Node、Etcd、Router、Registry之间的TLS证书管理 一、安装时指定证书的有效期 默认情况下，etcd证书、openshift证书的有效期为5年，kubelet证书、私有镜像仓库registry证书、Route证书的有效期为2年。在集群安装时可以通过设置ansible/hosts中的参数来指定证书的有效期 [OSEv3:vars] openshift_hosted_registry_cert_expire_days=730 openshift_ca_cert_expire_days=1825 openshift_node_cert_expire_days=730 openshift_master_cert_expire_days=730 etcd_ca_default_days=1825 二、使用openshift的ansible playbook查看当前集群所有证书的有效期 在/etc/ansible/hosts中添加变量 [OSEv3:vars] ... openshift_is_atomic=false ansible_distribution=centos openshift_certificate_expiry_config_base=/etc/origin openshift_certificate_expiry_warning_days=30 openshift_certificate_expiry_show_all=no # 可选项 # openshift_certificate_expiry_generate_html_report=no # openshift_certificate_expiry_html_report_path=$HOME/cert-expiry-report.yyyymmddTHHMMSS.html # openshift_certificate_expiry_save_json_results=no # openshift_certificate_expiry_json_results_path=$HOME/cert-expiry-report.yyyymmddTHHMMSS.json ... 检查 $ ansible-playbook playbooks/openshift-checks/certificate_expiry/easy-mode.yaml #执行完成后可在roles/openshift_certificate_expiry/defaults/main.yml中的openshift_certificate_expiry_html_report_path变量指定路径下看到证书检查报告文件。分别是HTML格式和JSON格式的文件。 # （默认证书检查报告文件路径是：当前用户家目录下~/cert-expiry-report.时间戳.html和cert-expiry-report.时间戳.JSON）查看所有证书的过期时间 它将会展示出所有Master oc证书、etcd证书、kube证书、router默认证书、私有镜像仓库registry证书的过期时间 三、更新证书 更新证书方法可以只针对Master oc证书、etcd证书、kube证书、router默认证书、私有镜像仓库registry证书中的一种进行更新，也可以全部进行更新。 确保ansible/hosts中的参数有如下信息 openshift_master_cluster_hostname=master.example.com openshift_master_cluster_public_hostname=master.example.com 重新生成证书进行更新 ①全部一次性更新 ansible-playbook playbooks/redeploy-certificates.yml ②只更新master CA证书 ansible-playbook playbooks/openshift-master/redeploy-openshift-ca.yml ③只更新etcd CA证书 ansible-playbook playbooks/openshift-etcd/redeploy-ca.yml ④只更新master Certificates证书 ansible-playbook playbooks/openshift-master/redeploy-certificates.yml ⑤只更新etcd Certificates证书 ansible-playbook playbooks/openshift-etcd/redeploy-certificates.yml ⑥只更新node Certificates证书 ansible-playbook playbooks/openshift-node/redeploy-certificates.yml ⑦只更新私有镜像仓库Rgistry Certificates证书 ansible-playbook playbooks/openshift-hosted/redeploy-registry-certificates.yml ⑧只更新Router Certificates证书 ansible-playbook playbooks/openshift-hosted/redeploy-router-certificates.yml 四、安装时使用自定义Master CA证书（以Master的CA证书为例） 将证书的路径写在inventory的配置参数中 ... [OSEv3.vars] ... openshift_master_ca_certificate={'certfile': '', 'keyfile': ''} ... 执行正常部署 ansible-playbook playbooks/deploy_cluster.yml 五、已运行的集群，更新自定义证书 同步骤四，将证书的路径写在inventory的配置参数中，运行更新Master CA证书的playbook ansible-playbook playbooks/openshift-master/redeploy-openshift-ca.yml 六、更新完成后可能遇到的问题 The installer detected the wrong host names and the issue was identified too late The certificates are expired and you need to update them You have a new CA and want to create certificates using it instead allinone的集群下更新所有证书时，在重启docker那一步中，容易卡住 参考连接 https://docs.openshift.com/container-platform/3.11/install/configuring_inventory_file.html#advanced-install-custom-certificates https://docs.openshift.com/container-platform/3.11/install_config/redeploying_certificates.html#install-config-cert-expiry https://www.jianshu.com/p/ffc4d6369d4e Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-WebConsole定制化.html":{"url":"origin/openshift-WebConsole定制化.html","title":"定制WebConsole界面","keywords":"","body":"一、定制WebConsole中左上角的logo 制作图标 使用Windows 10自带的Paint 3D。制作高度40pixel，宽度为logo字体宽的透明画布（建议logo字体宽度为100-300pixel之间）。保存为PNG格式。 将PNG图片转成SVG格式 http://www.bejson.com/convert/image_to_svg/ 将SVG文件进行Base64加密 https://www.css-js.com/tools/base64.html 将下面CSS文件上传到一个HTTPS的静态资源服务器上 #header-logo { background-image: url('data:image/svg+xml;base64,base64加过密的SVG图片源码'); width: 230px; height: 40px; } # 参考 #header-logo{ background-image:url('data:image/svg+xml;base64,77u/PD******'); width: 230px; height: 40px; } 或者 #header-logo{ background-image: url(\"logo图片的访问UTRL（必须是HTTPS）\"); width: 300px; height: 40px; } 修改WebConsole的配置文件 待Webconsole的容器重启过后（等待约5分钟），再次刷新页面可见修改过后的效果。可使用F12调出浏览器开发者模式，查看页面渲染的元素。 二、汉化项目左侧导航栏 创建js (function() { window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[0].label=\"概览\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[1].label=\"应用\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[2].label=\"构建\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[3].label=\"资源\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[4].label=\"存储\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[5].label=\"监控\" window.OPENSHIFT_CONSTANTS.PROJECT_NAVIGATION[6].label=\"商店\" window.OPENSHIFT_CONSTANTS.APP_LAUNCHER_NAVIGATION = [ { title: \"Sharing Videos\", iconClass: \"fa fa-video-camera\", href: \"https://yun.baidu.com/s/1xIwYILHQebEHZOcW4yvsAw\", tooltip: \"一键部署Openshift相关视频\" }]; }()); 上传到https服务器上 修改WebConsole的配置文件 三、定制登陆页面 导出login模板文件 oc adm create-login-template > login.html 修改该HTML文件，然后放到master节点上的/etc/origin/master/login-template/路径下（示例可见附件） 修改Master节点的/etc/origin/master/master-config.yaml文件 oauthConfig: ... templates: login: login-template/login.html #login-template/login.html是相对于/etc/origin/master/master-config.yaml文件路径的相对位置 重启master节点上的OKD的api进程 # 使用okd3.11新命令：master-restart master-restart api Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-no-IP-addresses-available-in-range-set解决方案.html":{"url":"origin/openshift-no-IP-addresses-available-in-range-set解决方案.html","title":"集群管理遇到的问题","keywords":"","body":"问题一 描述 正在运行的Openshift Allinone 3.11集群创建POD突然报出 network: failed to allocate for range 0: no IP addresses available in range set 现象 整个Allinone集群所有的POD不超多100个，但是/var/lib/cni/networks/openshift-sdn/的IP地址文件却有252个。造成当前节点的容器网络无法再为POD分配IP地址 解决方案 将对应节点标记为不可调用 oc adm manage-node node1.test.openshift.com --schedulable=false 驱散对应节点上的POD oc adm drain node1.test.openshift.com --ignore-daemonsets 停止docker和origin-node服务 systemctl stop docker origin-node.service 删除/var/lib/cni/networks/openshift-sdn/路径下所有文件 rm -rf /var/lib/cni/networks/openshift-sdn/* 重启docker和origin-node服务 systemctl start docker origin-node.service 将节点标记为可调度 oc adm manage-node node1.test.openshift.com --schedulable=true 相关链接 https://access.redhat.com/solutions/3328541 https://github.com/debianmaster/openshift-examples/issues/59 https://github.com/cloudnativelabs/kube-router/issues/383 https://github.com/jsenon/api-cni-cleanup/blob/master/k8s/deployment.yml#L42 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-openshift的用户认证.html":{"url":"origin/openshift-openshift的用户认证.html","title":"用户认证","keywords":"","body":"一、用户认证 Openshift通过OAuth进行用户的认证。在Openshift的master节点上运行着一个内置的OAuth服务对用户的请求进行认证检查。一旦OAuth服务器通过登录信息确认了用户的信息，OAuth服务器就返回用户的访问Token。通过这个Token，用户可以在有效的时间内对系统进行访问。 #登录命令 $ oc login -u 用户名 ​ #查看以哪个用户登录的 $ oc whoami $ oc whoami -t 查看当前用户当前Session的Token ​ #system:admin是集群默认的管理员，该用户是一个特殊用户，它不能通过用户名密码登录，它也没有Token。 作为身份验证的登录信息，如用户名密码，并非保存在Openshift集群中，而是保存在用户信息管理系统中，这些用户信息管理系统在Openshift中被称为Identity Provider。但Openshift并不提供用户信息管理系统，而是提供了不同的适配器连接不同的用户信息管理系统。通过配置，Openshift可以连接到以下用户信息管理系统： LADP（Lightweight Directory Access Protocol） 微软的活动目录（Active Directory） AllowALL DenyAll HTPasswd Github #查看当前Openshift集群支持的用户信息管理系统 cat /etc/origin/master/master-config.yaml|grep provider -A 3 provider: apiVersion: v1 file: /etc/origin/master/htpasswd kind: HTPasswdPasswordIdentityProvider #Htpasswd是Apache提供的一个基于文本文件管理用户名密码的用户信息管理工具 Openshift的用户管理，在后台创建用户时，会同时创建一个User对象和Identity对象（该对象保存了用户来源哪一个Identity Provider及用户信息）。 #查看集群中所有用户 $oc get user NAME UID FULL NAME IDENTITIES admin a04e0467-c8e7-11e7-b9d9-5254ac31d0ec htpasswd_auth:admin dev 1ffbda60-cb72-11e7-bd9b-5254c1caedf4 htpasswd_auth:dev #查看用户的Identity对象 $oc get identity NAME IDP NAME IDP USER NAME USER NAME USER UID htpasswd_auth:admin htpasswd_auth admin admin a04e0467-c8e7-11e7-b9d9-5254ac31d0ec htpasswd_auth:dev htpasswd_auth dev dev 1ffbda60-cb72-11e7-bd9b-5254c1caedf4 Openshift的用户组管理。用户组的信息来源有两个：一个是Identity Provider，二是通过用户在Openshift中定义的。 #通过oadm groups命令在Openshift中对组及组成员进行管理 $> oadm groups #添加用户到用户组 $> oadm groups add-users group_name user_name #查看用户组 $> oc get group #创建用户组 $> oadm groups new group_name #删除组 $> oc delete group group_name 二、用户权限管理 用户角色权限管理 授予及撤销用户某种角色 oc policy add-role-to-user view test oc policy remove-role-from-user view test 查看项目的角色绑定关系 oc get rolebinding -n 项目名 授予某用户对某项目的某角色 oc policy add-role-to-user view test -n test 查看角色绑定的规则 oc describe clusterrole registry-viewer 用户管理 新增用户 ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd test test\" 查看已创建的用户 oc get user 或 cat /etc/origin/master/htpasswd 删除用户 oc delete user test ansible masters -m shell -a \"htpasswd -D /etc/origin/master/htpasswd ha\" 用户组管理 创建用户组、添加用户到用户组 oc adm groups new test oc adm groups add-users test 用户1 用户2 用户3 查看创建的用户组及组内的成员用户 oc get group 删除用户组 oc delete group test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-openshift用户权限管理实例.html":{"url":"origin/openshift-openshift用户权限管理实例.html","title":"用户权限管理实例","keywords":"","body":"Openshift用户权限管理实例 由于公司的日常项目开发测试环境都迁移到openshift上了。有众多开发测试人员需要登陆到openshift上进行操作，如果直接给admin权限，肯定是不行的。而openshift是支持多租户的权限管理。所以，就在创建普通用户的基础上赋予各种不同的权限限制来自控制对openshift上project的操作。 一、Prerequisite 开发人员对CI环境有操作权限，对SIT、UAT环境只有查看权限 测试人员对SIT环境有操作权限，对CI环境只有查看权限 所有人员有自己的登录账户，均可见openshift上所有的业务项目，不可见系统项目 二、实现过程 创建登录用户 ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd dev1 dev1\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd dev2 dev2\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd dev3 dev3\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd tester1 tester1\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd tester2 tester2\" ansible masters -m shell -a \"htpasswd -b /etc/origin/master/htpasswd tester3 tester3\" 创建用户组 oc adm groups new developer oc adm groups new tester 将用户添加到用户组中 oc adm groups add-users developer dev1 dev2 dev3 oc adm groups add-users tester tester1 tester2 tester3 针对项目，给用户组赋予系统角色 oc adm policy add-role-to-group edit developer -n aci oc adm policy add-role-to-group edit developer -n bci oc adm policy add-role-to-group edit developer -n cci oc adm policy add-role-to-group view developer -n asit oc adm policy add-role-to-group view developer -n auat oc adm policy add-role-to-group view developer -n bsit oc adm policy add-role-to-group view developer -n buat oc adm policy add-role-to-group view developer -n csit oc adm policy add-role-to-group view developer -n cuat ​ oc adm policy add-role-to-group edit tester -n asit oc adm policy add-role-to-group edit tester -n auat oc adm policy add-role-to-group edit tester -n bsit oc adm policy add-role-to-group edit tester -n buat oc adm policy add-role-to-group edit tester -n csit oc adm policy add-role-to-group edit tester -n cuat oc adm policy add-role-to-group view tester -n aci oc adm policy add-role-to-group view tester -n bci oc adm policy add-role-to-group view tester -n cci 实际操作过程中，在以某以开发人员登录过程openshift过程中，依旧会看到openshift 其他一些项目的namespace。例如base namespace，该namespace项目是在registry镜像注册仓库中创建镜像项目时自动创建的openshift namespace（在registry镜像注册仓库中创建base镜像项目是为了存放一些自定义的s2i镜像）。为了使其他openshift namespace使用其中的s2i镜像，特别在registry镜像注册仓库中是镜像项目的访问策略设置为共享的。种种以上，导致openshift上的base namespace是能被所有的已认证的用户查看到。 在openshift中查看base项目的membership 可以发现，凡是在registry镜像注册仓库中设置问访问策略设置为共享的，都会在openshift 项目中添加一个系统用户system:authenticated 。这个系统用户上绑定的是这个角色registry-viewer。在openshift后台查看该角色的详细信息 # oc describe clusterrole registry-viewer Name: registry-viewer Created: About an hour ago Labels: Annotations: authorization.openshift.io/system-only=true openshift.io/reconcile-protect=false Verbs Non-Resource URLs Resource Names API Groups Resources [get list watch] [] [] [ image.openshift.io] [imagestreamimages imagestreammappings imagestreams imagestreamtags] [get] [] [] [ image.openshift.io] [imagestreams/layers] [get] [] [] [] [namespaces] [get] [] [] [project.openshift.io ] [projects] 发现该角色有对namespace资源拥有get动作。仔细想想，该system:authenticated用户只是让openshift其他项目的系统用户能够拉取其下的镜像流。而在Kubernetes中使用命名空间的概念来分隔资源。在同一个命名空间中，某一个对象的名称在其分类中必须唯一，但是分布在不同命名空间中的对象则可以同名。OpenShift中继承了Kubernetes命名空间的概念，而且在其之上定义了Project对象的概念。每一个Project会和一个Namespace相关联，甚至可以简单地认为，Project就是Namespace。所以，该用户对project资源有获取权限，那就把对namespace的权限给去掉试试。 先导出角色registry-viewer的bindding配置文件 oc export clusterrole registry-viewer > registry-viewer.yml 然后修改配置文件，注释掉get namespace的动作 apiVersion: v1 kind: ClusterRole metadata: annotations: authorization.openshift.io/system-only: \"true\" openshift.io/reconcile-protect: \"false\" creationTimestamp: null name: registry-viewer rules: - apiGroups: - \"\" - image.openshift.io attributeRestrictions: null resources: - imagestreamimages - imagestreammappings - imagestreams - imagestreamtags verbs: - get - list - watch - apiGroups: - \"\" - image.openshift.io attributeRestrictions: null resources: - imagestreams/layers verbs: - get #- apiGroups: # - \"\" # attributeRestrictions: null # resources: # - namespaces # verbs: # - get - apiGroups: - project.openshift.io - \"\" attributeRestrictions: null resources: - projects verbs: - get 再将集群中角色删掉（此时特别注意:从集群中删掉registry-viewer角色后会导致已有镜像注册仓库中镜像的访问策略从共有变成私有,base项目的membership中会删掉system:authenticated该用户） oc delete clusterrole registry-viewer 接着再从配置文件中创建角色 oc create -f registry-viewer.yml 最后再次修改镜像注册仓库中镜像的访问策略从私有变成共有。再次查看base项目中membership. 最有再以测试人员账户登录查看。不再显示base项目。测试其他项目去拉取base项目中的镜像，看去掉registry-viewer角色中role是否有影响。 实际使用过程中，测试人员需要以openshift上的用户名密码登录openshift上的jenkins，还要对jenkins做操作，比如在jenkins上做构建操作，查看构建日志等。需要对测试人员分组tester赋予对jenkins的编辑权限。初步思路是直接给tester分组服务系统角色clusterrole edit（oc adm policy add-cluster-role-to-group edit tester）。但是再以测试人员登录时还是能看到jenkins的项目，甚至能操作openshift上jenkins pod的重新部署。这是不可接受的。 那就换个思路。自己创建一个集群角色clusterrole，在角色上绑定若干规则，再将这个集群角色赋予测试组，相应的测试组成员能登录jenkins，并对jenkins做操作。 具体过程如下： 先查看集群角色edit的配置，看edit都对那些资源都有什么动作 oc describe clusterrole edit ​ Name: edit Created: 7 months ago Labels: Annotations: openshift.io/description=A user that can create and edit most objects in a project, but can not update the project's membership. Verbs Non-Resource URLs Resource Names API Groups Resources [create delete deletecollection get list patch update watch] [] [] [] [pods pods/attach pods/exec pods/portforward pods/proxy] [create delete deletecollection get list patch update watch] [] [] [] [configmaps endpoints persistentvolumeclaims replicationcontrollers replicationcontrollers/scale secrets serviceaccounts services services/proxy] [get list watch] [] [] [] [bindings events limitranges namespaces namespaces/status pods/log pods/status replicationcontrollers/status resourcequotas resourcequotas/status] [impersonate] [] [] [] [serviceaccounts] [create delete deletecollection get list patch update watch] [] [] [autoscaling] [horizontalpodautoscalers] [create delete deletecollection get list patch update watch] [] [] [batch] [cronjobs jobs scheduledjobs] [create delete deletecollection get list patch update watch] [] [] [extensions] [deployments deployments/rollback deployments/scale horizontalpodautoscalers jobs replicasets replicasets/scale replicationcontrollers/scale] [get list watch] [] [] [extensions] [daemonsets] [create delete deletecollection get list patch update watch] [] [] [apps] [deployments deployments/scale deployments/status statefulsets] [create delete deletecollection get list patch update watch] [] [] [build.openshift.io ] [buildconfigs buildconfigs/webhooks builds] [get list watch] [] [] [build.openshift.io ] [builds/log] [create] [] [] [build.openshift.io ] [buildconfigs/instantiate buildconfigs/instantiatebinary builds/clone] [update] [] [] [build.openshift.io ] [builds/details] [edit view] [] [] [build.openshift.io] [jenkins] [create delete deletecollection get list patch update watch] [] [] [apps.openshift.io ] [deploymentconfigs deploymentconfigs/scale generatedeploymentconfigs] [create] [] [] [apps.openshift.io ] [deploymentconfigrollbacks deploymentconfigs/instantiate deploymentconfigs/rollback] [get list watch] [] [] [apps.openshift.io ] [deploymentconfigs/log deploymentconfigs/status] [create delete deletecollection get list patch update watch] [] [] [image.openshift.io ] [imagestreamimages imagestreammappings imagestreams imagestreams/secrets imagestreamtags] [get list watch] [] [] [image.openshift.io ] [imagestreams/status] [get update] [] [] [image.openshift.io ] [imagestreams/layers] [create] [] [] [image.openshift.io ] [imagestreamimports] [get] [] [] [project.openshift.io ] [projects] [get list watch] [] [] [quota.openshift.io ] [appliedclusterresourcequotas] [create delete deletecollection get list patch update watch] [] [] [route.openshift.io ] [routes] [create] [] [] [route.openshift.io ] [routes/custom-host] [get list watch] [] [] [route.openshift.io ] [routes/status] [create delete deletecollection get list patch update watch] [] [] [template.openshift.io ] [processedtemplates templateconfigs templateinstances templates] [create delete deletecollection get list patch update watch] [] [] [build.openshift.io ] [buildlogs] [get list watch] [] [] [] [resourcequotausages] 导出集群角色edit的配置文件到本地文件，在其上做修改 oc export clusterrole edit > jenkins-clusterrole.yml 编辑 jenkins-clusterrole.yml（只保留相重要的，其他的都删掉） apiVersion: v1 kind: ClusterRole metadata: annotations: openshift.io/description: A user that can view jenkins project, and edit jenkins job. #添加clusterrole角色的说明简介 creationTimestamp: null name: jenkins #修改clusterrole名字为jenkins rules: - apiGroups: - \"\" attributeRestrictions: null resources: #clusterrole edit中有好多对其他资源的操作。例persistentvolumeclaims、replicationcontrollers、replicationcontrollersscale。对这些资源没有什么用处，就可以删掉啦。 - configmaps - endpoints - secrets - serviceaccounts - services - services/proxy verbs: - get - list - apiGroups: - \"\" attributeRestrictions: null resources: - serviceaccounts verbs: - impersonate - apiGroups: - build.openshift.io attributeRestrictions: null resources: - jenkins verbs: - edit - view 导入jenkins clusterrole oc create -f jenkins-clusterrole.yml 将jenkins clusterrole赋予测试组 oc adm policy add-cluster-role-to-group jenkins tester 以测试组成员登录openshift，jenkins项目不可见了。再以测试组成员登录jenkins，发现登录 出现以下界面 点击\"Allow selected permissions\"，发现也能正常登录jenkins。然后进行一次构建触发。发现一切正常。Bazinga！Everything is ok ! Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-开启router的haproxy-statisc.html":{"url":"origin/openshift-开启router的haproxy-statisc.html","title":"openshift开启router的haproxy-statisc","keywords":"","body":" 设置router POD 所在节点的iptables对1936端口的放行 iptables -I OS_FIREWALL_ALLOW -p tcp -m tcp --dport 1936 -j ACCEPT 获取访问router haproxy statics 页面的用户名密码。 删除掉router dc中的环境变量”ROUTER_METRICS_TYPE“ 这个环境变量默认值为“haproxy”。不删除的话，访问的时候会报一下错误 Forbidden: User \"system:anonymous\" cannot get routers/metrics.route.openshift.io at the cluster scope 将健康检查readiness的HTTP GET URL由“/healthz/ready”改为\"/healthz\"。（不然router POD无法通过健康检查） 验证监听端口80，443，1936 ss -ntl|grep 80 ss -ntl|grep 443 ss -ntl|grep 1936 访问router haproxy statistics 页面。 访问方式是：http://:@router所在节点IP地址:1936 例如：http://admin:MJbJFvODhP@allinone.curiouser.com:1936 相关链接 https://docs.openshift.com/container-platform/3.11/install_config/router/default_haproxy_router.html#using-wildcard-routes https://bugzilla.redhat.com/show_bug.cgi?id=1579054 https://github.com/openshift/origin/issues/17025 https://blog.chmouel.com/2016/09/27/how-to-view-openshift-haproxy-stats/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-多租户网络.html":{"url":"origin/openshift-多租户网络.html","title":"openshift的多租户网络","keywords":"","body":"一、Openshift容器网络简介 Openshift容器网络默认是基于Open vSwitch（OVS）实现的。 Openshift提供两种网络方案： ovs-subnet(子网模式)：为集群节点上的容器提供一个扁平化的二层虚拟网路，所有在这个二层网路中容器可直接通信。 ovs-multitenet(多租户模式)：基于项目的网络隔离，即不同项目间的容器之间不能直接通信。启动多租户网络隔离后，每个项目创建后都会被分配一个虚拟网络ID（Virtual Network ID ,VNID）.OVS网桥会为该项目的所有数据流量标记上VNID，在默认情况下，只有数据包上的VNID与目标容器所在项目的VNID匹配上后，数据包才允许被转发到目标容器中。当有些项目的容器应用是通过公共服务的，后期可通过配置将多个项目见的网络连通，或者将项目设置为全局可访问。 二、启动多租户网络 需要将集群中所有的master节点配置文件/etc/origin/master/master-config.yaml和node节点配置文件/etc/origin/node/node-config.yaml中的networkPluginName的属性值从redhat/openshift-ovs-subnet修改为redhat/openshift-ovs-multitenant，然后重启Openshift集群Master节点的origin-master-controllers.service服务和Node节点的origin-node.service服务 三、测试，查看网络隔离 在一个项目中的一个pod的终端中ping/telnet/curl/nslook另一个项目中的pod的ip地址或者对应svc的FQDN（..svc.cluster.local） 查看namespace的Netid是否一致 $ oc get netnamespaces NAME NETID EGRESS IPS default 0 [] kube-public 5899696 [] kube-service-catalog 0 [] demo 13843039 [] dubbo 11344186 [] jenkins 13843039 [] 当NETID相同时，表示这个两个project的网络是相通的 当NETID为0时，表示这个Project的网络全局可访问 四、连通隔离的网络 # project 1,2,3中所有的pod，service可以通过容器IP相互访问（通过service的FQDN不能相互访问） oc adm pod-network join-projects --to= #将某个project中所有的pod和service设置为全局可访问 oc adm pod-network make-projects-global 参考链接 https://docs.okd.io/3.11/admin_guide/managing_networking.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-25 16:31:03 "},"origin/openshift-kubernetes的审计日志功能.html":{"url":"origin/openshift-kubernetes的审计日志功能.html","title":"Kubernetes的审计日志功能","keywords":"","body":"一、Overviews kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。 审计的目的 Kubernetes 审计功能提供了与安全相关的按时间顺序排列的记录集，记录单个用户、管理员或系统其他组件影响系统的活动顺序。 它能帮助集群管理员处理以下问题： 发生了什么？ 什么时候发生的？ 谁触发的？ 活动发生在哪个（些）对象上？ 在哪观察到的？ 它从哪触发的？ 活动的后续处理行为是什么？ 产生的阶段 kube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有： RequestReceived ：apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。 ResponseStarted ：在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。 ResponseComplete ：当响应 body 发送完并且不再发送数据。 Panic：内部服务器出错，请求未完成。 也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成 审计记录日志级别 当前支持的日志记录级别有： None: 符合这条规则的日志将不会记录。 Metadata: 记录请求的 metadata（请求的用户、timestamp、resource、verb 等等），但是不记录请求或者响应的消息体。 Request: 记录事件的 metadata 和请求的消息体，但是不记录响应的消息体。这不适用于非资源类型的请求。 RequestResponse: 记录事件的 metadata，请求和响应的消息体。这不适用于非资源类型的请求。 日志记录策略 在记录日志的时候尽量只记录所需要的信息，不需要的日志尽可能不记录，避免造成系统资源的浪费。 一个请求不要重复记录，每个请求有三个阶段，只记录其中需要的阶段 不要记录所有的资源，不要记录一个资源的所有子资源 系统的请求不需要记录，kubelet、kube-proxy、kube-scheduler、kube-controller-manager 等对 kube-apiserver 的请求不需要记录 对一些认证信息（secerts、configmaps、token 等）的 body 不记录 审计日志格式 json{ \"kind\": \"Event\", \"apiVersion\": \"audit.k8s.io/v1beta1\", \"metadata\": { \"creationTimestamp\": \"2019-07-23T09:02:19Z\" }, \"level\": \"Request\", \"timestamp\": \"2019-07-23T09:02:19Z\", \"auditID\": \"eb481add-fdac-48a3-a302-1c33d73bfdbf\", \"stage\": \"RequestReceived\", \"requestURI\": \"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\", \"verb\": \"update\", \"user\": { \"username\": \"system:openshift-master\", \"groups\": [ \"system:masters\", \"system:authenticated\" ] }, \"sourceIPs\": [ \"192.168.1.96\" ], \"objectRef\": { \"resource\": \"configmaps\", \"namespace\": \"kube-system\", \"name\": \"openshift-master-controllers\", \"apiVersion\": \"v1\" }, \"requestReceivedTimestamp\": \"2019-07-23T09:02:19.148057Z\", \"stageTimestamp\": \"2019-07-23T09:02:19.148057Z\" } legacy 2019-07-23T23:50:06.223368641+08:00 AUDIT: id=\"3574e2e0-06b1-44d8-bc6c-5983c402d55e\" stage=\"ResponseComplete\" ip=\"192.168.1.96\" method=\"update\" user=\"system:openshift-master\" groups=\"\\\"system:masters\\\",\\\"system:authenticated\\\"\" as=\"\" asgroups=\"\" namespace=\"kube-system\" uri=\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\" response=\"200\" 支持的审计日志存储后端 审计后端可以将审计事件导出到外部存储。 Kube-apiserver 提供两个后端： Log 后端: 将事件写入文件，落到磁盘。如果有多个api-server，审计日志文件会分散，无法集中分析。此时可以使用logstash或fluend进行日志采集汇聚到elsticsearch中 Webhook 后端: 将事件发送到外部存储系统的API接口中。例如可以将审计日志发生到logstash监听的http接口中进行处理并吐到elsticsearch中进行汇聚查看 注意 审计日志记录功能会增加 API server 的内存消耗，因为需要为每个请求存储审计所需的某些上下文。 此外，内存消耗取决于审计日志记录的配置。 二、openshift开启自定义策略的审计功能 创建审计日志的存储路径 mkdir /etc/origin/master/audit # 注意：审计日志文件的存储路径必须是kube-system命名空间下apiservser pod挂载目录下的子路径。 # ocp 3.11版本的apiserver是以pod的形式运行在kube-system命名空间下的，它所需要的配置文件等Volume资源都是以hostpath的形式挂载上去的，例如ocp节点上的/etc/origin/master目录 编辑/etc/origin/master/master-config.yaml ****省略******** auditConfig: auditFilePath: \"/etc/origin/master/audit/audit-ocp.log\" # 指定审计日志文件的存储路径 enabled: true # 开启审计功能 logFormat: \"json\" # 指定输出审计日志的格式。可指定为\"json\"或\"legacy\" maximumFileRetentionDays: 10 # 指定审计日志文件的保留天数 maximumFileSizeMegabytes: 100 # 指定审计日志文件的最大Byte maximumRetainedFiles: 5 # 指定审计日志文件的保留个数 policyConfiguration: null # 是否使用默认的审计策略 policyFile: \"/etc/origin/master/audit-policy.yaml\" # 自定义的审计策略配置文件 ****省略******** 创建自定义的审计策略配置文件 kind: Policy omitStages: - \"ResponseStarted\" rules: - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" resources: [\"endpoints\", \"services\"] - level: None userGroups: [\"system:authenticated\"] nonResourceURLs: - \"/api*\" # Wildcard matching. - \"/version\" - level: Request verbs: [\"update\"] resources: - group: \"\" # core API group resources: [\"configmaps\",\"secrets\"] # This rule only applies to resources in the \"kube-system\" namespace. # The empty string \"\" can be used to select non-namespaced resources. namespaces: [\"kube-system\"] # Log configmap and secret changes in all other namespaces at the metadata level. - level: None verbs: [\"update\"] resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] # Log all other resources in core and extensions at the request level. - level: None resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. # Log login failures from the web console or CLI. Review the logs and refine your policies. - level: Metadata nonResourceURLs: - /login* - /oauth* - level: Metadata userGroups: [\"system:authenticated:oauth\"] verbs: [\"create\", \"delete\"] resources: - group: \"project.openshift.io\" resources: [\"projectrequests\", \"projects\"] omitStages: - RequestReceived 重启APIServer和Controller #对于OCP版本大于3.9的 /usr/local/bin/master-restart api /usr/local/bin/master-restart controllers # 对于OCP版本小于3.9的 systemctl restart atomic-openshift-master-api systemctl restart atomic-openshift-master-controllers 三、使用openshift集群的Fluentd收集审计日志到集群内的elasticsearch 配置OCP集群中的Fluentd挂载审计日志的存储目录(OCP集群中日志系统的fluentd是以DaemonSet形式收集节点上容器的日志到elasticsearch的，它是将节点的/var/lib/docker目录以hostpath形式挂载到容器中的) oc set volume ds/logging-fluentd --add --mount-path=/etc/origin/master/audit --name=audit --type=hostPath --path=/etc/origin/master/audit -n openshift-logging 配置OCP集群中的Fluentd监控审计日志目录下的日志 oc edit cm/logging-fluentd -n openshift-logging *****省略******* ## sources *****省略******* @include configs.d/user/input-audit.conf *****省略******* input-audit.conf: | @type tail @id audit-ocp path /etc/origin/master/audit/audit-ocp.log pos_file /etc/origin/master/audit/audit.pos tag audit.requests format json @type copy @type elasticsearch log_level debug host \"#{ENV['OPS_HOST']}\" port \"#{ENV['OPS_PORT']}\" scheme https ssl_version TLSv1_2 index_name .audit user fluentd password changeme client_key \"#{ENV['OPS_CLIENT_KEY']}\" client_cert \"#{ENV['OPS_CLIENT_CERT']}\" ca_file \"#{ENV['OPS_CA']}\" type_name com.redhat.ocp.audit reload_connections \"#{ENV['ES_RELOAD_CONNECTIONS'] || 'false'}\" reload_after \"#{ENV['ES_RELOAD_AFTER'] || '100'}\" sniffer_class_name \"#{ENV['ES_SNIFFER_CLASS_NAME'] || 'Fluent::ElasticsearchSimpleSniffer'}\" reload_on_failure false flush_interval \"#{ENV['ES_FLUSH_INTERVAL'] || '5s'}\" max_retry_wait \"#{ENV['ES_RETRY_WAIT'] || '300'}\" disable_retry_limit true buffer_type file buffer_path '/var/lib/fluentd/buffer-output-es-auditlog' buffer_queue_limit \"#{ENV['BUFFER_QUEUE_LIMIT'] || '1024' }\" buffer_chunk_limit \"#{ENV['BUFFER_SIZE_LIMIT'] || '1m' }\" buffer_queue_full_action \"#{ENV['BUFFER_QUEUE_FULL_ACTION'] || 'exception'}\" request_timeout 2147483648 *****省略******* 重启Fluentd oc delete po -l component=fluentd -n openshift-logging 在ocp集群系统的Kibana上添加\".audit*\"的Index Pattern,并在\"Discover\"查看、筛选审计日志 四、将审计日志通过WebHook 发送到OCP外部的Logstash或者Fluentd 接收后端 可使用Logstash或者Fluentd作为后端来接收Api-Server通过web hook方式发送的审计日志。Logstash和Fluentd可以是ocp集群外二进制方式安装运行的，也可以是原生Docker运行的，甚至可以是另外一个集群中容器化的。一个原则就是不要放到审计日志产生集群的内部。防止apiserver启动起来了，有了一些操作，logstash还没有启动起来，丢失审计日志。再者审计日志后端最好选择适合自己的，审计日志落一份，重复记录也没多大意义。 方式一：使用OCP集群外二进制方式安装的Logstash来接收ApiServer通过web hook方式发送过来的审计日志并过滤、存储到本地文件中 安装logstash bash -c 'cat > /etc/yum.repos.d/elasticsearch.repo 设置logstash，/etc/logstash/logstash.yml # ------------ Pipeline Configuration Settings -------------- # Where to fetch the pipeline configuration for the main pipeline path.config: /etc/logstash/conf.d/ *************************省略****************************** # ------------ Data path ------------------ # Which directory should be used by logstash and its plugins for any persistent needs. Defaults to LOGSTASH_HOME/data path.data: /data/logs/logstash/data/ *************************省略****************************** # ------------ Debugging Settings ------------- # Options for log.level: fatal/error/warn/info (default)/debug/trace log.level: info path.logs: /data/logs/logstash/logs 创建监听HTTP 8081端口的pipeline cat /etc/logstash/conf.d/accept-audit-log.conf input{ http{ host => \"0.0.0.0\" port => 8081 } } filter{ split{ # Webhook audit backend sends several events together with EventList # split each event here. field=>[items] # We only need event subelement, remove others. remove_field=>[headers, metadata, apiVersion, kind, \"@version\", host] } mutate{ rename => {items=>event} } } output{ file{ # Audit events from different users will be saved into different files. path=>\"/data/logs/logstash/ocp-audit-logs/ocp-audit-%{[event][user][username]}/audit-%{+YYYY-MM-dd}.log\" } } EOF 启动logstash mkdir -p /data/logs/logstash/{data,logs,ocp-audit-logs} chown -R logstash:logstash /data/logs/logstash system start logstash # 或者 /usr/share/logstash/bin/logstash -f /etc/logstash/config --path.settings /etc/logstash/ 测试logstash的联通性。一是看logstash pipeline监听的HTTP端口是否开启。二是尝试发送一个带有模拟数据的POST请求，看其是否会pipeline指定的数据目录生成日志文件 ss -ntl |grep 8081 curl -X POST \\ http://192.168.1.96:8081 \\ -H 'Accept: */*' \\ -H 'Cache-Control: no-cache' \\ -H 'Connection: keep-alive' \\ -H 'Content-Type: application/json' \\ -H 'accept-encoding: gzip, deflate' \\ -d '{\"kind\":\"Event\",\"apiVersion\":\"audit.k8s.io/v1beta1\",\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:27:54Z\"},\"level\":\"Request\",\"timestamp\":\"2019-07-23T14:27:54Z\",\"auditID\":\"29bf32ba-4bea-4b4f-a1fb-cd091b2188ff\",\"stage\":\"ResponseComplete\",\"requestURI\":\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\",\"verb\":\"update\",\"user\":{\"username\":\"system:openshift-master\",\"groups\":[\"system:masters\",\"system:authenticated\"]},\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"resource\":\"configmaps\",\"namespace\":\"kube-system\",\"name\":\"openshift-master-controllers\",\"uid\":\"d54578ea-425e-11e9-b1bd-000c2976c04e\",\"apiVersion\":\"v1\",\"resourceVersion\":\"8285989\"},\"responseStatus\":{\"metadata\":{},\"code\":200},\"requestObject\":{\"kind\":\"ConfigMap\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"openshift-master-controllers\",\"namespace\":\"kube-system\",\"selfLink\":\"/api/v1/namespaces/kube-system/configmaps/openshift-master-controllers\",\"uid\":\"d54578ea-425e-11e9-b1bd-000c2976c04e\",\"resourceVersion\":\"8285989\",\"creationTimestamp\":\"2019-03-09T11:31:08Z\",\"annotations\":{\"control-plane.alpha.kubernetes.io/leader\":\"{\\\"holderIdentity\\\":\\\"allinone.okd311.curiouser.com\\\",\\\"leaseDurationSeconds\\\":15,\\\"acquireTime\\\":\\\"2019-03-09T11:31:01Z\\\",\\\"renewTime\\\":\\\"2019-07-23T14:27:54Z\\\",\\\"leaderTransitions\\\":0}\"}}},\"requestReceivedTimestamp\":\"2019-07-23T14:27:54.894767Z\",\"stageTimestamp\":\"2019-07-23T14:27:54.899643Z\",\"annotations\":{\"authorization.k8s.io/decision\":\"allow\",\"authorization.k8s.io/reason\":\"\"}}' 创建audit的webhook配置文件/etc/origin/master/audit-policy.yaml cat /etc/origin/master/audit-policy.yaml apiVersion: v1 clusters: - cluster: server: http://192.168.1.96:8081 name: logstash contexts: - context: cluster: logstash user: \"\" name: default-context current-context: default-context kind: Config preferences: {} users: [] EOF 编辑/etc/origin/master/master-config.yaml，添加webhook相关的参数 ****省略******** auditConfig: auditFilePath: \"/etc/origin/master/audit/audit-ocp.log\" # 指定审计日志文件的存储路径 enabled: true # 开启审计功能 logFormat: \"json\" # 指定输出审计日志的格式。可指定为\"json\"或\"legacy\" maximumFileRetentionDays: 10 # 指定审计日志文件的保留天数 maximumFileSizeMegabytes: 100 # 指定审计日志文件的最大Byte maximumRetainedFiles: 5 # 指定审计日志文件的保留个数 policyConfiguration: null # 是否使用默认的审计策略 policyFile: \"/etc/origin/master/audit-policy.yaml\" # 自定义的审计策略配置文件 #==========以下配置项为添加的webhook参数=========================================================================== webHookKubeConfig: /etc/origin/master/audit-webhook-config.yaml # 指定WebHook的配置文件（同样路径要指定在ApiServer POD已挂载的路径下） webHookMode: batch # 可选参数\"batch\"和\"blocking\" ****省略******** 重启APIServer和Controller #对于OCP版本大于3.9的 /usr/local/bin/master-restart api /usr/local/bin/master-restart controllers # 对于OCP版本小于3.9的 systemctl restart atomic-openshift-master-api systemctl restart atomic-openshift-master-controllers 验证，用除\"system:admin\"用户外的其他用户创建project，然后再删除project，最后查看logstash配置的审计日志存储目录下是否生成对应的文件 oc login -u admin -p oc new-project test oc delete project test $ tree -L 2 /data/logs/logstash/ocp-audit-logs/ /data/logs/logstash/ocp-audit-logs/ ├── ocp-audit-admin │ └── audit-2019-07-23.log └── ocp-audit-system:openshift-master └── audit-2019-07-23.log $ cat /data/logs/logstash/ocp-audit-logs/ocp-audit-admin/audit-2019-07-23.log 产生以下内容。显示一次创建成功，另一次创建失败，原因是project已经存在（特意测试），一次删除project等日志。 {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:03Z\"},\"stageTimestamp\":\"2019-07-23T14:45:03.019249Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:02Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projectrequests\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"code\":201},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:02.964347Z\",\"auditID\":\"eec24884-b70a-4b27-80c1-431111d2f4f5\",\"verb\":\"create\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"cluster-admin-0\\\" of ClusterRole \\\"cluster-admin\\\" to User \\\"admin\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projectrequests\"}} {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:10Z\"},\"stageTimestamp\":\"2019-07-23T14:45:10.842999Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:10Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projectrequests\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Failure\",\"reason\":\"AlreadyExists\",\"code\":409},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:10.836487Z\",\"auditID\":\"a7b64f47-94eb-4723-b101-24e112cd0735\",\"verb\":\"create\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"self-provisioners\\\" of ClusterRole \\\"self-provisioner\\\" to Group \\\"system:authenticated:oauth\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projectrequests\"}} {\"event\":{\"metadata\":{\"creationTimestamp\":\"2019-07-23T14:45:19Z\"},\"stageTimestamp\":\"2019-07-23T14:45:19.813911Z\",\"level\":\"Metadata\",\"timestamp\":\"2019-07-23T14:45:19Z\",\"sourceIPs\":[\"192.168.1.96\"],\"objectRef\":{\"apiGroup\":\"project.openshift.io\",\"resource\":\"projects\",\"namespace\":\"test\",\"name\":\"test\",\"apiVersion\":\"v1\"},\"responseStatus\":{\"metadata\":{},\"status\":\"Success\",\"code\":200},\"user\":{\"extra\":{\"scopes.authorization.openshift.io\":[\"user:full\"]},\"uid\":\"7775eba0-426e-11e9-b1bd-000c2976c04e\",\"groups\":[\"system:authenticated:oauth\",\"system:authenticated\"],\"username\":\"admin\"},\"stage\":\"ResponseComplete\",\"requestReceivedTimestamp\":\"2019-07-23T14:45:19.805940Z\",\"auditID\":\"9d3260ca-3bff-49da-9fe9-346043a29991\",\"verb\":\"delete\",\"annotations\":{\"authorization.k8s.io/reason\":\"RBAC: allowed by ClusterRoleBinding \\\"cluster-admin-0\\\" of ClusterRole \\\"cluster-admin\\\" to User \\\"admin\\\"\",\"authorization.k8s.io/decision\":\"allow\"},\"requestURI\":\"/apis/project.openshift.io/v1/projects/test\"}} 五、审计策略配置详解 apiVersion: audit.k8s.io/v1 kind: Policy omitStages: - \"RequestReceived\" # 审计阶段 rules: # rule按顺序匹配 - level: Request # 审计级别 verbs: - create - delete resources: - group: \"\" resources: - pods 参考链接 https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh#L1101 https://www.kubernetes.org.cn/2611.html?spm=a2c6h.12873639.article-detail.11.616c3b5fDgcMZv https://developer.aliyun.com/article/686982 https://zhuanlan.zhihu.com/p/112864670 http://blog.itpub.net/31555606/viewspace-2636723/ https://austindewey.com/2018/10/17/integrating-advanced-audit-with-aggregated-logging-in-openshift-3-11/#test-it-out https://www.outcoldsolutions.com/docs/monitoring-openshift/v4/audit/ https://docs.openshift.com/container-platform/3.11/install_config/master_node_configuration.html#master-node-config-advanced-audit https://docs.openshift.com/container-platform/3.11/security/monitoring.html https://kubernetes.io/docs/tasks/debug-application-cluster/audit/ https://medium.com/@noqcks/kubernetes-audit-logging-introduction-464a34a53f6c https://www.jianshu.com/p/8117bc2fb966 https://cloud.google.com/kubernetes-engine/docs/concepts/audit-policy?hl=zh-cn https://github.com/rbo/openshift-examples/tree/master/efk-auditlog https://github.com/openshift/origin-aggregated-logging/issues/1226 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:35:33 "},"origin/openshift-elasticsearch容器化部署.html":{"url":"origin/openshift-elasticsearch容器化部署.html","title":"Elasticsearch容器化部署","keywords":"","body":"一、拉取镜像 docker pull docker.io/elasticsearch/elasticsearch:6.6.1 #或者 docker pull docker.elastic.co/elasticsearch/elasticsearch:6.6.1 二、Docker部署 修改系统 echo \"vm.max_map_count=262144\" >> /etc/sysctl.conf sysctl -w vm.max_map_count=262144 Docker单节点部署 docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.io/elasticsearch/elasticsearch:6.6.1 Docker compose集群部署 version: '2.2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.1 container_name: elasticsearch environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata1:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - esnet elasticsearch2: image: docker.elastic.co/elasticsearch/elasticsearch:6.6.1 container_name: elasticsearch2 environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - \"discovery.zen.ping.unicast.hosts=elasticsearch\" ulimits: memlock: soft: -1 hard: -1 volumes: - esdata2:/usr/share/elasticsearch/data networks: - esnet volumes: esdata1: driver: local esdata2: driver: local networks: esnet: 三、OKD上部署 DeploymentConfig apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: labels: app: elasticsearch name: elasticsearch spec: replicas: 1 selector: app: elasticsearch deploymentconfig: elasticsearch strategy: type: Recreate template: metadata: labels: app: elasticsearch deploymentconfig: elasticsearch spec: containers: - env: - name: discovery.type value: single-node - name: cluster.name value: curiouser - name: bootstrap.memory_lock value: 'true' - name: path.repo value: /usr/share/elasticsearch/snapshots-repository - name: TZ value: Asia/Shanghai - name: ES_JAVA_OPTS value: '-Xms1g -Xmx2g' - name: xpack.monitoring.collection.enabled value: 'true' - name: xpack.security.enabled value: 'true' - name: ELASTIC_USERNAME value: \"elastic\" - name: \"ELASTIC_PASSWORD\" value: \"elastic\" image: 'docker.elastic.co/elasticsearch/elasticsearch:7.1.1' imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 initialDelaySeconds: 90 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 name: elasticsearch ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP readinessProbe: failureThreshold: 3 initialDelaySeconds: 80 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 resources: limits: cpu: '2' memory: 3Gi requests: cpu: '1' memory: 2Gi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /usr/share/elasticsearch/data name: elasticsearch-data - mountPath: /usr/share/elasticsearch/snapshots-repository name: elasticsearch-snapshots-repository dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: elasticsearch-data persistentVolumeClaim: claimName: elasticsearch-data - name: elasticsearch-snapshots-repository persistentVolumeClaim: claimName: elasticsearch-snapshots-repository test: false triggers: - type: ConfigChange SVC apiVersion: v1 kind: Service metadata: labels: app: elasticsearch name: elasticsearch spec: ports: - name: 9200-tcp port: 9200 protocol: TCP targetPort: 9200 - name: 9300-tcp port: 9300 protocol: TCP targetPort: 9300 selector: deploymentconfig: elasticsearch sessionAffinity: None type: ClusterIP 数据目录PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: nfs-client-storageclass name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi snapshot repository存储目录PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: nfs-client-storageclass name: elasticsearch-snapshots-repository spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi 四. Kubernetes部署 Deployment kind: Deployment apiVersion: apps/v1 metadata: labels: elastic-app: elasticsearch role: master name: elasticsearch-master namespace: elk spec: replicas: 1 revisionHistoryLimit: 10 strategy: type: Recreate selector: matchLabels: elastic-app: elasticsearch role: master template: metadata: labels: elastic-app: elasticsearch role: master spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1.k8s.curiouser.com initContainers: - name: init-scheduler image: busybox:latest imagePullPolicy: IfNotPresent command: ['sh', '-c', 'chmod -R 777 /usr/share/elasticsearch/data /usr/share/elasticsearch/snapshots-repository && chown -R 1000.0 /usr/share/elasticsearch/data /usr/share/elasticsearch/snapshots-repository'] volumeMounts: - name: elasticsearch-data mountPath: /usr/share/elasticsearch/data - name: elasticsearch-snapshots-repository mountPath: /usr/share/elasticsearch/snapshots-repository containers: - name: elasticsearch-master-data image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0 imagePullPolicy: IfNotPresent ports: - containerPort: 9200 protocol: TCP - containerPort: 9300 protocol: TCP env: - name: \"cluster.name\" value: \"Curiouser\" - name: \"bootstrap.memory_lock\" value: \"false\" - name: discovery.type value: single-node - name: \"node.master\" value: \"true\" - name: \"node.data\" value: \"true\" - name: \"node.ingest\" value: \"false\" - name: xpack.monitoring.collection.enabled value: \"true\" - name: \"xpack.monitoring.elasticsearch.collection.enabled\" value: \"true\" - name: \"xpack.security.enabled\" value: \"true\" - name: \"path.repo\" value: \"/usr/share/elasticsearch/snapshots-repository\" - name: \"ES_JAVA_OPTS\" value: \"-Xms2048m -Xmx2048m\" - name: TZ value: Asia/Shanghai - name: \"xpack.monitoring.exporters.my_local.type\" value: \"local\" - name: \"xpack.monitoring.exporters.my_local.use_ingest\" value: \"false\" resources: requests: memory: \"2Gi\" cpu: \"2\" limits: memory: \"4096Mi\" cpu: \"3\" readinessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 9200 timeoutSeconds: 1 volumeMounts: - name: elasticsearch-data mountPath: \"/usr/share/elasticsearch/data\" - name: elasticsearch-snapshots-repository mountPath: \"/usr/share/elasticsearch/snapshots-repository\" restartPolicy: Always terminationGracePeriodSeconds: 30 dnsPolicy: ClusterFirst securityContext: {} schedulerName: default-scheduler volumes: - name: elasticsearch-data persistentVolumeClaim: claimName: elasticsearch-data - name: elasticsearch-snapshots-repository persistentVolumeClaim: claimName: elasticsearch-snapshots-repository PersistentVolumeClaim --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: elasticsearch name: elasticsearch-data namespace: elk spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: elasticsearch name: elasticsearch-snapshots-repository namespace: elk spec: accessModes: - ReadWriteMany resources: requests: storage: 30Gi Service kind: Service apiVersion: v1 metadata: labels: elastic-app: elasticsearch-service name: elasticsearch namespace: elk spec: ports: - port: 9200 targetPort: 9200 protocol: TCP selector: elastic-app: elasticsearch type: ClusterIP Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openshift-Kibana容器化部署.html":{"url":"origin/openshift-Kibana容器化部署.html","title":"Kibana容器化部署","keywords":"","body":"三、OKD上部署 DeploymentConfig apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: labels: app: kibana name: kibana spec: replicas: 1 selector: app: kibana deploymentconfig: kibana strategy: activeDeadlineSeconds: 21600 resources: {} rollingParams: intervalSeconds: 1 maxSurge: 25% maxUnavailable: 25% timeoutSeconds: 600 updatePeriodSeconds: 1 type: Rolling template: metadata: labels: app: kibana deploymentconfig: kibana spec: containers: - env: - name: ELASTICSEARCH_USERNAME value: kibana - name: ELASTICSEARCH_PASSWORD value: uLAWAfW1b7UHZdHEigCW - name: TZ value: Asia/Shanghai image: docker.elastic.co/kibana/kibana:7.1.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 name: kibana ports: - containerPort: 5601 protocol: TCP readinessProbe: failureThreshold: 3 initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 resources: limits: cpu: \"1\" memory: 1500Mi requests: cpu: 500m memory: 800Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 test: false triggers: - type: ConfigChange SVC apiVersion: v1 kind: Service metadata: labels: app: kibana name: kibana spec: ports: - name: 5601-tcp port: 5601 protocol: TCP targetPort: 5601 selector: deploymentconfig: kibana sessionAffinity: None type: ClusterIP Route apiVersion: route.openshift.io/v1 kind: Route metadata: labels: app: apache-kibana name: kibana spec: port: targetPort: 5601-tcp to: kind: Service name: kibana weight: 100 wildcardPolicy: None 四. Kubernetes上部署 Deployment apiVersion: apps/v1beta2 kind: Deployment metadata: labels: app: kibana name: kibana namespace: elk spec: progressDeadlineSeconds: 600 replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: app: kibana strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: labels: app: kibana spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1.k8s.curiouser.com containers: - image: kibana/kibana:7.2.0 imagePullPolicy: IfNotPresent name: kibana envFrom: - secretRef: name: kibana-config-env env: - name: TZ value: Asia/Shanghai - name: ELASTICSEARCH_HOSTS value: '[\"http://elasticsearch.elk.svc:9200\"]' ports: - containerPort: 5601 name: web protocol: TCP resources: requests: memory: \"1Gi\" cpu: \"0.5\" limits: memory: \"2Gi\" cpu: \"1\" readinessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 60 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 5601 timeoutSeconds: 1 securityContext: allowPrivilegeEscalation: false capabilities: {} privileged: false procMount: Default readOnlyRootFilesystem: false runAsNonRoot: false stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true dnsConfig: {} dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 Secret apiVersion: v1 kind: Secret metadata: labels: app: kibana name: kibana-config-env namespace: elk stringData: ELASTICSEARCH_USERNAME: kibana ELASTICSEARCH_PASSWORD: kibana Service apiVersion: v1 kind: Service metadata: name: kibana namespace: elk labels: app: kibana spec: ports: - port: 5601 name: web selector: app: kibana Ingress apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kibana namespace: elk spec: rules: - host: kibana.apps.k8s.curiouser.com http: paths: - path: / backend: serviceName: kibana servicePort: 5601 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-22 14:58:39 "},"origin/kubernetes-集群角色及插件.html":{"url":"origin/kubernetes-集群角色及插件.html","title":"Kubernetes的集群角色及插件","keywords":"","body":"一、Master节点上的组件 kube-apiserver：对外暴露了Kubernetes API。它是的 Kubernetes 前端控制层。它被设计为水平扩展，即通过部署更多实例来缩放。 kube-controller-manager：运行控制器，它们是处理集群中常规任务的后台线程。逻辑上，每个控制器是一个单独的进程，但为了降低复杂性，它们都被编译成独立的可执行文件，并在单个进程中运行。这些控制器包括: 节点控制器: 当节点移除时，负责注意和响应。 副本控制器: 负责维护系统中每个副本控制器对象正确数量的 Pod。 端点控制器: 填充端点(Endpoints) 对象(即连接 Services & Pods)。 服务帐户和令牌控制器: 为新的命名空间创建默认帐户和 API 访问令牌 kube-scheduler：监视没有分配节点的新创建的 Pod，选择一个节点供他们运行。 etcd：用于 Kubernetes 的后端存储。存储所有集群数据。 cloud-controller-manager：用于与底层云提供商交互的控制器。云控制器管理器可执行组件是 Kubernetes v1.6 版本中引入的 Alpha 功能。仅运行云提供商特定的控制器循环。您必须在 - kube-controller-manager 中禁用这些控制器循环，您可以通过在启动 kube-controller-manager 时将 --cloud-provider 标志设置为external来禁用控制器循环。允许云供应商代码和 Kubernetes 核心彼此独立发展，在以前的版本中，Kubernetes 核心代码依赖于云提供商特定的功能代码。在未来的版本中，云供应商的特定代码应由云供应商自己维护，并与运行 Kubernetes 的云控制器管理器相关联。以下控制器具有云提供商依赖关系: 节点控制器: 用于检查云提供商以确定节点是否在云中停止响应后被删除 路由控制器: 用于在底层云基础架构中设置路由 服务控制器: 用于创建，更新和删除云提供商负载平衡器 数据卷控制器: 用于创建，附加和装载卷，并与云提供商进行交互以协调卷 二、Node节点上的组件 kubelet：是主要的节点代理,它监测已分配给其节点的 Pod(通过 apiserver 或通过本地配置文件)，提供如下功能: 挂载 Pod 所需要的数据卷(Volume)。 下载 Pod 的 secrets。 通过 Docker 运行(或通过 rkt)运行 Pod 的容器。 周期性的对容器生命周期进行探测。 如果需要，通过创建镜像Pod（Mirror Pod） 将 Pod 的状态报告回系统的其余部分。 将节点的状态报告回系统的其余部分。 kube-proxy：通过维护主机上的网络规则并执行连接转发，实现了Kubernetes服务抽象 Container Runtime：运行容器的底层平台。Kubernetes支持的容器平台：Docker、containerd、cri-o、rktlet 三、插件 网络插件 ACI: provides integrated container networking and network security with Cisco ACI. Calico is a secure L3 networking and network policy provider. Canal unites Flannel and Calico, providing networking and network policy. Cilium is a L3 network and network policy plugin that can enforce HTTP/API/L7 - policies transparently. Both routing and overlay/encapsulation mode are - supported. CNI-Genie enables Kubernetes to seamlessly connect to a choice of CNI plugins,such as Calico, Canal, Flannel, Romana, or Weave. Contiv provides configurable networking (native L3 using BGP, overlay using - vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich policy - framework. Contiv project is fully open sourced. The installer provides both - kubeadm and non-kubeadm based installation options. Contrail, based on Tungsten Fabric, is a open source, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods and bare metal workloads. Flannel is an overlay network provider that can be used with Kubernetes. Knitter is a network solution supporting multiple networking in Kubernetes. Multus is a Multi plugin for multiple network support in Kubernetes to support - all CNI plugins (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, - DPDK, OVS-DPDK and VPP based workloads in Kubernetes. NSX-T Container Plug-in (NCP) provides integration between VMware NSX-T and - container orchestrators such as Kubernetes, as well as integration between - NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service - (PKS) and OpenShift. Nuage is an SDN platform that provides policy-based networking between - Kubernetes Pods and non-Kubernetes environments with visibility and security - monitoring. Romana is a Layer 3 networking solution for pod networks that also supports the - NetworkPolicy API. Kubeadm add-on installation details available here. Weave Net provides networking and network policy, will carry on working on both - sides of a network partition, and does not require an external database 服务发现插件 CoreDNS is a flexible, extensible DNS server which can be installed as the in-cluster DNS for pods 可视化及控制插件 Dashboard is a dashboard web interface for Kubernetes. Weave Scope is a tool for graphically visualizing your containers, pods, services etc. Use it in conjunction with a Weave Cloud account or host the UI yourself 参考链接 https://kubernetes.io/docs/concepts/cluster-administration/addons/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/k8s-pod-info-to-env.html":{"url":"origin/k8s-pod-info-to-env.html","title":"将POD信息传递容器环境变量或文件","keywords":"","body":"一、将POD信息传递容器环境变量或文件 ​ 对于容器来说，有时候拥有自己的信息是很有用的，可避免与 Kubernetes 过度耦合。 Downward API 使得容器使用自己或者集群的信息，而不必通过 Kubernetes 客户端或 API 服务器来获得。 可以通过环境变量和 downwardAPI 卷提供给容器： 能通过fieldRef获得的： metadata.name : Pod 名称 metadata.namespace : Pod 名字空间 metadata.uid : Pod 的 UID metadata.labels[''] Pod 标签 的值 (例如, metadata.labels['mylabel']）。以 label-key=\"escaped-label-value\" 格式显示，每行显示一个标签 metadata.annotations[''] Pod 的注解 的值（例如, metadata.annotations['myannotation']）。以 annotation-key=\"escaped-annotation-value\" 格式显示，每行显示一个标签 能通过resourceFieldRef获得的： 容器的 CPU 约束值 容器的 CPU 请求值 容器的内存约束值 容器的内存请求值 容器的巨页限制值（前提是启用了 DownwardAPIHugePages 特性门控） 容器的巨页请求值（前提是启用了 DownwardAPIHugePages 特性门控） 容器的临时存储约束值 容器的临时存储请求值 有两种方式可以将 Pod 和 Container 字段呈现给运行中的容器： 1、传递给环境变量 ....... spec: containers: - name: test-container ...... env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: MY_POD_SERVICE_ACCOUNT valueFrom: fieldRef: fieldPath: spec.serviceAccountName - name: MY_CPU_REQUEST valueFrom: resourceFieldRef: containerName: test-container resource: requests.cpu - name: MY_CPU_LIMIT valueFrom: resourceFieldRef: containerName: test-container resource: limits.cpu - name: MY_MEM_REQUEST valueFrom: resourceFieldRef: containerName: test-container resource: requests.memory - name: MY_MEM_LIMIT valueFrom: resourceFieldRef: containerName: test-container resource: limits.memory env 字段是一个 EnvVars. 对象的数组。 数组中第一个元素指定 MY_NODE_NAME 这个环境变量从 Pod 的 spec.nodeName 字段获取变量值。 2、传递给文件进行挂载 ..... spec: containers: - name: client-container image: k8s.gcr.io/busybox command: [\"sh\", \"-c\"] args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\n\\n'; cat /etc/podinfo/labels; fi; if [[ -e /etc/podinfo/annotations ]]; then echo -en '\\n\\n'; cat /etc/podinfo/annotations; fi; if [[ -e /etc/podinfo/cpu_limit ]]; then echo -en '\\n'; cat /etc/podinfo/cpu_limit; fi; if [[ -e /etc/podinfo/cpu_request ]]; then echo -en '\\n'; cat /etc/podinfo/cpu_request; fi; if [[ -e /etc/podinfo/mem_limit ]]; then echo -en '\\n'; cat /etc/podinfo/mem_limit; fi; if [[ -e /etc/podinfo/mem_request ]]; then echo -en '\\n'; cat /etc/podinfo/mem_request; fi; sleep 5; done; volumeMounts: - name: podinfo mountPath: /etc/podinfo volumes: - name: podinfo downwardAPI: items: - path: \"labels\" fieldRef: fieldPath: metadata.labels - path: \"annotations\" fieldRef: fieldPath: metadata.annotations - path: \"cpu_limit\" resourceFieldRef: containerName: client-container resource: limits.cpu divisor: 1m - path: \"cpu_request\" resourceFieldRef: containerName: client-container resource: requests.cpu divisor: 1m - path: \"mem_limit\" resourceFieldRef: containerName: client-container resource: limits.memory divisor: 1Mi - path: \"mem_request\" resourceFieldRef: containerName: client-container resource: requests.memory divisor: 1Mi 如果容器以subPath卷挂载方式来使用 Downward API，则该容器无法收到更新事件 在输出中可以看到，labels 和 annotations 文件都在一个临时子目录中。 在这个例子，..2982_06_02_21_47_53.299460680。 在 /etc/podinfo 目录中，..data 是一个指向临时子目录 的符号链接。/etc/podinfo 目录中，labels 和 annotations 也是符号链接。 drwxr-xr-x ... Feb 6 21:47 ..2982_06_02_21_47_53.299460680 lrwxrwxrwx ... Feb 6 21:47 ..data -> ..2982_06_02_21_47_53.299460680 lrwxrwxrwx ... Feb 6 21:47 annotations -> ..data/annotations lrwxrwxrwx ... Feb 6 21:47 labels -> ..data/labels /etc/podinfo/..2982_06_02_21_47_53.299460680: total 8 -rw-r--r-- ... Feb 6 21:47 annotations -rw-r--r-- ... Feb 6 21:47 labels 用符号链接可实现元数据的动态原子性刷新；更新将写入一个新的临时目录， 然后通过使用rename(2) 完成 ..data 符号链接的原子性更新。 参考： https://kubernetes.io/zh/docs/tasks/inject-data-application/environment-variable-expose-pod-information/ https://kubernetes.io/zh/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#the-downward-api Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/k8s-deployment.html":{"url":"origin/k8s-deployment.html","title":"无状态应用deployment部署文件","keywords":"","body":"无状态应用K8s部署文件Deployment 一、简介 二、说明 1、添加注解以标注发布变更历史 Deployment或者statefulset添加kubernetes.io/change-cause的注解用以标注发布变更历史。可以使用镜像版本号作为说明，也可以时间戳等。 ... metadata: annotations: kubernetes.io/change-cause: $IMAGE_NAME ... $ kubectl rollout history deployment test-nginx REVISION CHANGE-CAUSE 1 harbor.curiouser.com/test-nginx/stg:20210908-a3das215 2 harbor.curiouser.com/test-nginx/stg:20210908-8020cdfh 2、command字段和args字段 描述 Docker字段名称 Kubernetes字段名称 容器执行的命令 Entrypoint command 传给命令的参数 Cmd args 如果要覆盖Docker容器默认的 Entrypoint 与 Cmd，需要遵循如下规则： 如果在 Pod 配置中没有设置 command 或者 args，那么将使用 Docker 镜像自带的命令及其参数。 如果在 Pod 配置中只设置了 command 但是没有设置 args，那么容器启动时只会执行该命令，Docker 镜像中自带的命令及其参数会被忽略。 如果在 Pod 配置中只设置了 args，那么 Docker 镜像中自带的命令会使用该新参数作为其执行时的参数。 如果在 Pod 配置中同时设置了 command 与 args，那么 Docker 镜像中自带的命令及其参数会被忽略。容器启动时只会执行配置中设置的命令，并使用配置中设置的参数作为命令的参数。 spec: containers: - name: command-demo-container image: debian command: [\"/bin/sh\"] args: [\"-c\", \"可以前台运行的进程命令\"] 环境变量需要加上括号，类似于 \"$(VAR)\"。这是在 command 或 args 字段使用变量的格式要求。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-19 21:46:10 "},"origin/kubernetes-容器的访问方式.html":{"url":"origin/kubernetes-容器的访问方式.html","title":"kubernetes容器的访问方式","keywords":"","body":"Kubernetes容器的访问方式 一、简介 当在kubernetes中使用docker镜像启动成一个容器，形成一个POD时，kubernetes的CNI组件(例如Calico、OVS)会随机动态给分配一个IP地址。通过访问这个POD IP地址加应用服务监听暴露出来的端口即可访问POD中的服务。可是POD生命周期是动态化的，IP地址重启后会改变。为屏蔽POD IP地址的动态变化和对多POD实例的负载均衡，引入了Service这个资源对象。 二、Service Service的类型大致可分成4种： ClusterIP： 默认方式。根据是否生成ClusterIP又可分为普通Service和Headless Service两类： 普通Service：通过为Kubernetes的Service分配一个集群内部可访问的固定虚拟IP（Cluster IP），实现集群内的访问。为最常见的方式。 Headless Service：该服务不会分配Cluster IP，也不通过kube-proxy做反向代理和负载均衡。而是通过DNS提供稳定的网络ID来访问，DNS会将headless service的后端直接解析为podIP列表。主要供StatefulSet使用。 NodePort：通过将service的port映射到集群内每个节点的相同一个端口，实现通过nodeIP:nodePort从集群外访问服务。 LoadBalancer：和nodePort类似，不过除了使用一个Cluster IP和nodePort之外，还会向所使用的公有云申请一个负载均衡器(负载均衡器后端映射到各节点的nodePort)，实现从集群外通过LB访问服务。 ExternalName：是 Service 的特例。此模式主要面向运行在集群外部的服务，通过它可以将外部服务映射进k8s集群，且具备k8s内服务的一些特征（如具备namespace等属性），来为集群内部提供服务。此模式要求kube-dns的版本为1.7或以上。这种模式和前三种模式（除headless service）最大的不同是重定向依赖的是dns层次，而不是通过kube-proxy。 Service中主要涉及三种Port： * port 这里的port表示service暴露在clusterIP上的端口，clusterIP:Port 是提供给集群内部访问kubernetes服务的入口。 targetPort containerPort，targetPort是pod上的端口，从port和nodePort上到来的数据最终经过kube-proxy流入到后端pod的targetPort上进入容器。 nodePort nodeIP:nodePort 是提供给从集群外部访问kubernetes服务的入口。 总的来说，port和nodePort都是service的端口，前者暴露给从集群内访问服务，后者暴露给从集群外访问服务。从这两个端口到来的数据都需要经过反向代理kube-proxy流入后端具体pod的targetPort，从而进入到pod上的容器内。 1.3 IP 使用Service服务还会涉及到几种IP： ClusterIP Pod IP 地址是实际存在于某个网卡(可以是虚拟设备)上的，但clusterIP就不一样了，没有网络设备承载这个地址。它是一个虚拟地址，由kube-proxy使用iptables规则重新定向到其本地端口，再均衡到后端Pod。当kube-proxy发现一个新的service后，它会在本地节点打开一个任意端口，创建相应的iptables规则，重定向服务的clusterIP和port到这个新建的端口，开始接受到达这个服务的连接。 Pod IP Pod的IP，每个Pod启动时，会自动创建一个镜像为gcr.io/google_containers/pause的容器，Pod内部其他容器的网络模式使用container模式，并指定为pause容器的ID，即：network_mode: \"container:pause容器ID\"，使得Pod内所有容器共享pause容器的网络，与外部的通信经由此容器代理，pause容器的IP也可以称为Pod IP。 节点IP Node-IP，service对象在Cluster IP range池中分配到的IP只能在内部访问，如果服务作为一个应用程序内部的层次，还是很合适的。如果这个service作为前端服务，准备为集群外的客户提供业务，我们就需要给这个服务提供公共IP了。指定service的spec.type=NodePort，这个类型的service，系统会给它在集群的各个代理节点上分配一个节点级别的端口，能访问到代理节点的客户端都能访问这个端口，从而访问到服务。 三、Ingress 参考 https://blog.csdn.net/liukuan73/article/details/82585732 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes的容器网络.html":{"url":"origin/kubernetes的容器网络.html","title":"kubernetes的容器网络CNI","keywords":"","body":"Kubernetes容器网络CNI 一、简介 二、Calico 三、OpenvSwitch 四、Flannel Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-09-20 10:31:26 "},"origin/kubernetes-kube-proxy-iptables-ipvs.html":{"url":"origin/kubernetes-kube-proxy-iptables-ipvs.html","title":"kube-proxy的实现方式之iptables与ipvs模式","keywords":"","body":"Kube-proxy的iptables与ipvs实现方式 一、什么是IPVS？ IPVS (IP Virtual Server，IP虚拟服务器)是基于Netfilter的、作为linux内核的一部分实现传输层负载均衡的技术，通常称为第4层LAN交换。 IPVS集成在LVS(Linux Virtual Server)中，它在主机中运行，并在真实服务器集群前充当负载均衡器。IPVS可以将对TCP/UDP服务的请求转发给后端的真实服务器，并使真实服务器的服务在单个IP地址上显示为虚拟服务。因此IPVS天然支持Kubernetes Service。 二、为什么选择IPVS？ 随着kubernetes使用量的增长，其资源的可扩展性变得越来越重要。特别是对于使用kubernetes运行大型工作负载的开发人员或者公司来说，service的可扩展性至关重要。 kube-proxy是为service构建路由规则的模块，之前依赖iptables来实现主要service类型的支持，比如(ClusterIP和NodePort)。但是iptables很难支持上万级的service，因为iptables纯粹是为防火墙而设计的，并且底层数据结构是内核规则的列表。 kubernetes早在1.6版本就已经有能力支持5000多节点，这样基于iptables的kube-proxy就成为集群扩容到5000节点的瓶颈。举例来说，如果在一个5000节点的集群，我们创建2000个service，并且每个service有10个pod，那么我们就会在每个节点上有至少20000条iptables规则，这会导致内核非常繁忙。 ipvs (IP Virtual Server) 实现了传输层负载均衡，也就是我们常说的4层LAN交换，作为Linux 内核的一部分。ipvs运行在主机上，在真实服务器集群前充当负载均衡器。ipvs可以将基于TCP和UDP的服务请求转发到真实服务器上，并使真实服务器的服务在单个IP 地址上显示为虚拟服务。 基于IPVS的集群内负载均衡就可以完美的解决这个问题。IPVS是专门为负载均衡设计的，并且底层使用哈希表这种非常高效的数据结构，几乎可以允许无限扩容。 三、IPVS与IPTABLES的区别 IPVS模式在Kubernetes v1.8中引入，并在v1.9中进入了beta。 1.11中实现了GA(General Availability)。IPTABLES模式在v1.1中添加，并成为自v1.2以来的默认操作模式。 IPVS和IPTABLES都基于netfilter。 IPVS模式和IPTABLES模式之间的差异如下： IPVS为大型集群提供了更好的可扩展性和性能。 IPVS支持比iptables更复杂的负载平衡算法（最小负载，最少连接，位置，加权等）。 IPVS支持服务器健康检查和连接重试等。 四、IPVS要求 k8s版本 >= v1.11 使用ipvs需要安装相应的工具来处理”yum install ipset ipvsadm -y“ 确保 ipvs已经加载内核模块， ip_vs、ip_vs_rr、ip_vs_wrr、ip_vs_sh、nf_conntrack_ipv4(如果这些内核模块不加载，当kube-proxy启动后，会退回到iptables模式) 五、IPVS原理分析 参考 https://blog.csdn.net/fanren224/article/details/86548398 https://www.jianshu.com/p/89f126b241db https://segmentfault.com/a/1190000016333317 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-23 18:24:03 "},"origin/k8s-cni-traefik-common-operation.html":{"url":"origin/k8s-cni-traefik-common-operation.html","title":"部署及操作","keywords":"","body":"Traefik Ingress Controller 一、简介 Traefik文档：https://doc.traefik.io/traefik/ Traefik-Helm-Chart Github：https://github.com/traefik/traefik-helm-chart Traefik 创建路由规则有多种方式： 原生 Ingress 使用 CRD IngressRoute 使用 GatewayAPI 相较于原生 Ingress 写法，ingressRoute 是 2.1 以后新增功能，简单来说，他们都支持路径 (path) 路由和域名 (host) HTTP 路由，以及 HTTPS 配置，区别在于 IngressRoute 需要定义 CRD 扩展，但是它支持了 TCP、UDP 路由以及中间件等新特性，强烈推荐使用 ingressRoute 二、部署安装 开起功能 设置 HTTP 服务端口为 80、HTTPS 服务端口 443、TCP 服务端口为 799 以 HostPort 方式暴露相关服务端口 开起Prometheus Metrics监控端点 持久化记录响应状态码为400~599的请求日志 创建Dashboard的 IngressRoute 1、Helm 部署 helm upgrade --install --atomic traefik-ingress-controller traefik/traefik \\ --version 24.0.0 \\ --namespace kube-system \\ --set log.general.level=INFO \\ --set securityContext.capabilities.add={NET_BIND_SERVICE} \\ --set securityContext.runAsGroup=0 \\ --set securityContext.runAsUser=0 \\ --set securityContext.runAsNonRoot=false \\ --set podSecurityContext.runAsNonRoot=false \\ --set ports.traefik.hostPort=9000 \\ --set ports.tcp.port=799 \\ --set ports.tcp.hostPort=799 \\ --set ports.web.port=80 \\ --set ports.web.hostPort=80 \\ --set ports.websecure.port=443 \\ --set ports.websecure.hostPort=443 \\ --set ports.metrics.port=9110 \\ --set ports.metrics.exposedPort=9110 \\ --set deployment.replicas=2 \\ --set persistence.enabled=true \\ --set persistence.size=20G \\ --set metrics.prometheus.serviceMonitor.enable=true \\ --set metrics.prometheus.serviceMonitor.additionalLabels.release=prometheus \\ --set persistence.storageClass=local-nfs-storage \\ --set globalArguments=\"{\"--entryPoints.tcp.address=:799/tcp\",\"--entrypoints.websecure.address=:443\",\"--entrypoints.web.address=:80\",\"--entryPoints.metrics.address=:9110\",\"--api.disabledashboardad=false\",\"--global.sendanonymoususage=false\",\"--global.checknewversion=false\",\"--accesslog=true\",\"--accesslog.fields.names.accesslog\",\"--accesslog.fields.headers.defaultmode=keep\",\"--accesslog.filepath=/data/400-599-reponse-json.log\",\"--accesslog.format=json\",\"--accesslog.filters.statuscodes=400-599\"}\" \\ --set service.type=ClusterIP \\ --set ingressRoute.dashboard.dashboard=\"Host(\\`traefik-dashboard.test.com\\`) && PathPrefix(\\`/dashboard\\`) || PathPrefix(\\`/api\\`)\" \\ --set ingressRoute.dashboard.entryPoints[0]=web \\ --set hostNetwork=true 通过域名访问 Traefik Dashboard：http://traefik-dashboard.test.com/dashboard/ 2、验证测试 ①创建测试 POD kubectl -n kube-public run nginx2 --image=nginx --port=80 --labels=\"app=nginx2\" && \\ kubectl -n kube-public expose pod nginx2 --port=80 --target-port=80 --protocol=TCP && \\ cat ②访问测试 通过命令行访问： ingress_controller_node=`kubectl -n kube-system get pod -l app.kubernetes.io/name=traefik -ojson | jq -r '.items[0] | .status.hostIP'` && \\ curl -s --noproxy \"traefik-dashboard.test.com\" --resolve traefik-dashboard.test.com:80:$ingress_controller_node http://traefik-dashboard.test.com/dashboard/ && \\ curl -s --noproxy \"nginx.test.com\" --resolve nginx.test.com:80:$ingress_controller_node http://nginx.test.com && \\ curl -s -k --noproxy \"nginx.test.com\" --resolve nginx.test.com:443:$ingress_controller_node https://nginx.test.com 三、路由TCP服务 TLS Extensions:TLS 扩展于 2003 年以一个独立的规范（RFC 3546）被提出，经过不断的发展：RFC 4366、RFC 6066 等，先后被加入到 TLS1.1、TLS1.2、TLS1.3 中。它能让 Client 和 Server 在不更新 TLS 的基础上，获得新的功能。 在 ClientHello 中声明多个自己可以支持的 Extensions，Server 收到 ClientHello 以后，依次解析 Extensions，有些如果需要立即回应，就在 ServerHello 中作出回应，有些不需要回应，或者 Server 不支持的 Extensions 就不用响应，忽略不处理。ClientHello 中，Extension 字段位于 Compression Methods 字段之后 SNI(Server Name Indication)：在 Nginx 中可以通过指定不同的 server_name 来配置多个站点。HTTP/1.1 协议请求头中的 Host 字段可以标识出当前请求属于哪个站点。但是在 TLS 协议中，没有提供一种机制来告诉 Server 它正在建立连接的 Server 的名称，那么对于在同一个地址，并且还使用不同证书的情况下，Server 怎么知道该发送哪个证书？为了解决这个问题，SNI 应运而生。最初是 2003 年标准化的，在 RFC 6066 中有更新。它允许 Server 在同一个网络地址上托管多个启用了 TLS 的服务，要求 Client 在初始 TLS 握手期间指定要连接到哪个服务。 Traefik的TCP路由与SNI：从 Traefik 2.0 开始支持 TCP 路由，也支持在相同的 entryPoints（traefik 中的入口端口） 中定义不同的 TCP 路由，但是我们都知道，TCP 是传输层协议，没有任何 SNI 类的机制来保证同一地址入口可以处理不同的服务。Traefik 支持通过 SNI 在每台主机上进行路由，因为这是通过 TCP 进行路由的惟一标准方法，但是 TCP 本身没有 SNI，因此必须使用 TLS。 kubectl -n kube-public run redis --image=redis:alpine --port=6379 --labels=\"app=redis\" && \\ kubectl -n kube-public expose pod redis --name=redis --port=6379 --target-port=6379 --protocol=TCP && \\ cat 使用同一个 Entrypoint通过 SNI 路由不同的 TCP 服务暂调试不成功 kubectl -n kube-public run mysql --image=mysql --port=3306 --env=\"MYSQL_ROOT_PASSWORD=test123\" --labels=\"app=mysql\" && \\ kubectl -n kube-public expose pod mysql --name=mysql --port=3306 --target-port=3306 --protocol=TCP && \\ kubectl create secret tls redis-tls --cert=redis-tls.crt --key=redis-tls.key openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout mysql-tls.key -out mysql-tls.crt -subj \"/CN=mysql.test.com\" kubectl create secret tls mysql-tls --cert=mysql-tls.crt --key=mysql-tls.key openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout redis-tls.key -out redis-tls.crt -subj \"/CN=redis.test.com\" #### openssl genpkey -algorithm RSA -out mysql-tls.key openssl req -new -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=Curiouser/OU=devops/CN=mysql.test.com\" -key mysql-tls.key -out mysql-tls.csr openssl x509 -req -days 3650 -in mysql-tls.csr -signkey mysql-tls.key -out mysql-tls.crt kubectl create secret tls mysql-tls --cert=mysql-tls.crt --key=mysql-tls.key openssl genpkey -algorithm RSA -out redis-tls.key openssl req -new -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=Curiouser/OU=devops/CN=redis.test.com\" -key redis-tls.key -out redis-tls.csr openssl x509 -req -days 3650 -in redis-tls.csr -signkey redis-tls.key -out redis-tls.crt kubectl create secret tls redis-tls --cert=redis-tls.crt --key=redis-tls.key #### cat 四、插件使用 1、sablier cat 五、常用操作 1、设置简单认证 ①使用htpasswd生成密码文件 htpasswd -bc basic-auth-secret username password ②创建包含用户名密码的secret kubectl create secret generic test-basic-auth --from-file=basic-auth-secret --namespace=test ③ingress配置中添加注解 apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: traefik ingress.kubernetes.io/auth-type: basic ingress.kubernetes.io/auth-secret: test-basic-auth name: test namespace: test .....省略....... 参考： https://doc.traefik.io/traefik/v1.7/configuration/backends/kubernetes/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:46:29 "},"origin/kubernetes-使用Kubeadm安装单机版Kubernetes.html":{"url":"origin/kubernetes-使用Kubeadm安装单机版Kubernetes.html","title":"Kubeadm安装单机版Kubernetes","keywords":"","body":"使用Kubeadm安装单机版Kubernetes kubeadm是Kubernetes官方提供的用于快速安装 Kubernetes 集群的工具，通过将集群的各个组件进行容器化安装管理，通过kubeadm的方式安装集群比二进制的方式安装要方便 Prerequisite Hostname IP 地址 硬件 Kubernetes版本 Docker版本 allinone.k8s114.curiouser.com 172.16.1.12 最低2C2G v1.14.0 18.09.4 关闭防火墙 关闭Selinux 关闭Swap 加载br_netfilter 添加配置内核参数 hosts文件添加主机名与IP的映射关 #关闭防火墙 \\ #关闭Swap \\ #关闭Selinux \\ #加载br_netfilter \\ #添加配置内核参数 \\ #加载配置 \\ systemctl disable firewalld && systemctl stop firewalld \\ swapoff -a && sed -i 's/.\\\\*swap.\\\\*/#&/' /etc/fstab \\ setenforce 0 \\ sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/sysconfig/selinux && \\ sed -i \"s/^SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config && \\ sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/sysconfig/selinux && \\ sed -i \"s/^SELINUX=permissive/SELINUX=disabled/g\" /etc/selinux/config && \\ modprobe br_netfilter && \\ bash -c 'cat > /etc/sysctl.d/k8s.conf > /etc/hosts 一、安装 安装docker kubeadm kubelet kubectl yum -y install yum-utils && \\ yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo && \\ bash -c 'cat > /etc/yum.repos.d/kubernetes.repo (可选)添加dockers日志相关配置 --log-driver=json-file --log-opt=max-size=10m --log-opt=max-file=5 (可选)预拉取镜像 docker pull mirrorgooglecontainers/kube-apiserver:v1.14.0 && \\ docker pull mirrorgooglecontainers/kube-controller-manager:v1.14.0 && \\ docker pull mirrorgooglecontainers/kube-scheduler:v1.14.0 && \\ docker pull mirrorgooglecontainers/kube-proxy:v1.14.0 && \\ docker pull mirrorgooglecontainers/pause:3.1 && \\ docker pull mirrorgooglecontainers/etcd:3.3.10 && \\ docker pull coredns/coredns:1.3.1 && \\ docker pull quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 && \\ docker pull calico/cni:v3.3.6 && \\ docker pull calico/node:v3.3.6 && \\ docker tag mirrorgooglecontainers/kube-proxy:v1.14.0 k8s.gcr.io/kube-proxy:v1.14.0 && \\ docker tag mirrorgooglecontainers/kube-scheduler:v1.14.0 k8s.gcr.io/kube-scheduler:v1.14.0 && \\ docker tag mirrorgooglecontainers/kube-apiserver:v1.14.0 k8s.gcr.io/kube-apiserver:v1.14.0 && \\ docker tag mirrorgooglecontainers/kube-controller-manager:v1.14.0 k8s.gcr.io/kube-controller-manager:v1.14.0 && \\ docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1 && \\ docker tag mirrorgooglecontainers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10 && \\ docker tag coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1 && \\ docker tag quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 quay.io/coreos/flannel:v0.11.0-amd64 && \\ docker rmi mirrorgooglecontainers/kube-apiserver:v1.14.0 && \\ docker rmi mirrorgooglecontainers/kube-controller-manager:v1.14.0 && \\ docker rmi mirrorgooglecontainers/kube-scheduler:v1.14.0 && \\ docker rmi mirrorgooglecontainers/kube-proxy:v1.14.0 && \\ docker rmi mirrorgooglecontainers/pause:3.1 && \\ docker rmi mirrorgooglecontainers/etcd:3.3.10 && \\ docker rmi coredns/coredns:1.3.1 && \\ docker rmi quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64 初始化apiserver kubeadm init --apiserver-advertise-address=0.0.0.0 --kubernetes-version=v1.14.0 --pod-network-cidr=192.168.0.0/16 配置常规用户或root用户如何使用kubectl访问集群 mkdir -p $HOME/.kube && \\ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config && \\ chown $(id -u):$(id -g) $HOME/.kube/config 设置Master节点可被调度 kubectl taint nodes --all node-role.kubernetes.io/master- #该参数node-role.kubernetes.io/master会污染所有节点，包括master节点,这意味着调度器可以调度POD到所有节点。 (可选)设置Kubectl命令别名及命令补全 yum install -y bash-completion && \\ echo \"alias k='kubectl'\" >> /etc/bashrc && \\ source 二、安装容器网络插件 Calico kubeadm初始化apiserver时添加\"--pod-network-cidr=192.168.0.0/16\" 网络工作在amd64，arm64，ppc64le kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml && \\ kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml Flannel Prerequisite 设置 /proc/sys/net/bridge/bridge-nf-call-iptables为1，将桥接的IPv4流量传递到iptables的链 sysctl net.bridge.bridge-nf-call-iptables=1 kubeadm初始化apiserver时添加\"--pod-network-cidr=10.244.0.0/16\" flannel网络工作在amd64, arm, arm64, ppc64le,s390x 安装 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml 三、验证 查看所有namespace下的POD kubectl get pods --all-namespaces kubectl get pod -n kube-system 查看集群Node节点 kubectl get node systemctl status kubelet.service 查看版本 kubectl version 显示集群信息 kubectl cluster-info 四、添加Node节点 kubeadm join 172.16.1.12:6443 --token i7xcb9.sz5t4sa8xx3ntc2h --discovery-token-ca-cert-hash sha256:487275a22ea4af5a1ea30ee4b0f21f8c27104d17f6a259bf4990f1569a3301cd 查看Master的Token kubeadm token list Master创建Token kubeadm token create Master节点创建\"--discovery-token-ca-cert-hash\"值 openssl x509 -pubkey -in /etc/kubernet 五、安装UI管理界面 DashBoard 项目GitHub：https://github.com/kubernetes/dashboard.git # ------------------- Dashboard Secret ------------------- # apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-system type: Opaque --- # ------------------- Dashboard Service Account ------------------- # apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Role & Role Binding ------------------- # kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kubernetes-dashboard-minimal namespace: kube-system rules: # Allow Dashboard to create 'kubernetes-dashboard-key-holder' secret. - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\"] # Allow Dashboard to create 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"create\"] # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\"] verbs: [\"get\", \"update\", \"delete\"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] # Allow Dashboard to get metrics from heapster. - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\"] verbs: [\"proxy\"] - apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\"] verbs: [\"get\"] --- apiVersion: rbac.authorization.k8s.io/v1 #===修改原rolebind类型RoleBinding为ClusterRoleBinding kind: ClusterRoleBinding metadata: name: kubernetes-dashboard-minimal namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io #修改原role类型Role为ClusterRole kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Deployment ------------------- # kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 ports: #====修改原容器端口8443为9090 - containerPort: 9090 protocol: TCP args: #==== #- --auto-generate-certificates # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTP path: / #修改原健康检查端口8443为9090 port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule --- # ------------------- Dashboard Service ------------------- # #使用Nodeport的方式访问Dashboard kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-external namespace: kube-system spec: ports: - port: 9090 targetPort: 9090 nodePort: 30090 type: NodePort selector: k8s-app: kubernetes-dashboard 拉取Template中使用的Image docker pull mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 && \\ docker tag mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 && \\ docker rmi mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1 Weave Scope 官方文档： https://www.weave.works/docs/scope/latest/installing/#k8s 安装部署 kubectl apply -f \"https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 | tr -d '\\n')\" （可选）修改svc使用NodePort访问 $ kubectl edit service/weave-scope-app -n weave apiVersion: v1 kind: Service #.........省略........ spec: externalTrafficPolicy: Cluster ports: - name: app #===== nodePort: 30040 port: 80 protocol: TCP targetPort: 4040 selector: app: weave-scope name: weave-scope-app weave-cloud-component: scope weave-scope-component: app sessionAffinity: None #====== type: NodePort Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes-Kubeasz二进制安装集群.html":{"url":"origin/kubernetes-Kubeasz二进制安装集群.html","title":"Kubeasz二进制安装Kubernetes集群","keywords":"","body":"使用Kubeasz离线安装二进制Kubernetes集群 一、集群规划 k8s版本：1.28.1 CNI：Cilium kubeproxy模式：IPVS 证书有效期：100年 IngressContoller：Traefik 2.0+ Registry：Habor CSI：NFS Provisioner、Ceph RBD Provisioner、Ceph Filesystem Provisioner、LocalVolume Provisioner 主机FQDN Domain：k8s118.curiouser.com 主机名 硬件配置 IP地址 服务 tools 8C16G100G /data100G 192.168.1.60 NFS Server、Ceph、Harbor、Nginx、Chrony Node1 8C16G100G /data100G 192.168.1.61 AnsibleK8S Master(etcd、apiserver、controllermanager、scheduler、kubelet、kueb-proxy、docker) Node2 8C16G100G /data100G 192.168.1.62 K8S Worker(kubelet、kube-proxy、docker) Node3 8C16G100G /data100G 192.168.1.63 K8S Worker(kubelet、kube-proxy、docker) 二、Kubeasz ansible脚本简介 Kubeasz Github地址：https://github.com/easzlab/kubeasz kubeasz 2.0.1 开始支持完全离线安装，目前已测试 Ubuntu1604|1804 CentOS7 Debian9|10 系统。 kubeasz 项目代码 --> /etc/ansible kubernetes 集群组件二进制 --> /etc/ansible/bin 其他集群组件二进制（etcd/CNI等） --> /etc/ansible/bin 操作系统基础依赖软件包（haproxy/ipvsadm/ipset/socat等） --> /etc/ansible/down/packages 集群基本插件镜像（coredns/dashboard/metrics-server等） --> /etc/ansible/down # 分步安装 ansible-playbook 01.prepare.yml ansible-playbook 02.etcd.yml ansible-playbook 03.docker.yml ansible-playbook 04.kube-master.yml ansible-playbook 05.kube-node.yml ansible-playbook 06.network.yml ansible-playbook 07.cluster-addon.yml # 一步安装 ansible-playbook 90.setup.yml kubeasz创建集群主要在以下两个地方进行配置： ansible hosts 文件（模板在examples目录）：集群主要节点定义和主要参数配置、全局变量 roles/xxx/defaults/main.yml文件：其他参数配置或者部分组件附加参数 配置 lb 节点负载均衡算法：修改roles/lb/defaults/main.yml 变量 BALANCE_ALG: \"roundrobin\" 配置 docker 国内镜像加速站点：修改 roles/docker/defaults/main.yml相关变量 配置 apiserver 支持公网域名：修改roles/kube-master/defaults/main.yml 相关变量 配置 flannel 使用镜像版本：修改roles/flannel/defaults/main.yml相关变量 配置选择不同 addon 组件：修改roles/cluster-addon/defaults/main.yml 作为 kubeasz 项目的推荐命令行脚本，easzctl 十分轻量、简单；（后续会不断完善补充） 命令集 1：集群层面操作 切换/创建集群 context 删除当前集群 显示所有集群 创建集群 创建单机集群（类似 minikube） 命令集 2：集群内部操作 增加工作节点 增加主节点 增加 etcd 节点 删除 etcd 节点 删除任意节点 升级集群 命令集3：额外操作 开启/关闭基础认证 集群 context 由 ansible hosts 配置、roles 配置等组成，用以区分不同的 k8s 集群，从而实现多集群的创建和管理；当然 easzctl 命令行不是必须的，你仍旧可以使用之前熟悉的方式安装/管理集群。 典型 easzctl 创建管理的集群拓扑如下 +----------------+ +-----------------+ |easzctl 1.1.1.1 | |cluster-aio: | +--+---+---+-----+ | | | | | |master 4.4.4.4 | | | +-------------------->+etcd 4.4.4.4 | | | |node 4.4.4.4 | | +--------------+ +-----------------+ | | v v +--+------------+ +---+----------------------------+ | cluster-1: | | cluster-2: | | | | | | master 2.2.2.1| | master 3.3.3.1/3.3.3.2 | | etcd 2.2.2.2| | etcd 3.3.3.1/3.3.3.2/3.3.3.3 | | node 2.2.2.3| | node 3.3.3.4/3.3.3.5/3.3.3.6 | +---------------+ +--------------------------------+ 使用 easzctl 举例 随时运行 easzctl help 获取命令行提示信息 1.创建 context：准备集群名称（例如：test-cluster1），运行 easzctl checkout test-cluster1 如果 context: test-cluster1 不存在，那么会根据 default 配置创建它；如果存在则切换当前 context 为 test-cluster1 2.准备 context 以后，根据你的需要配置 ansible hosts 文件和其他配置，然后运行 easzctl setup 3.安装成功后，运行 easzctl list 显示当前所有集群信息 4.重复步骤 1/2 可以创建多个集群 5.切换到某个集群 easzctl checkout xxxx，然后执行增加/删除节点操作 三、安装k8s 1.28.1集群 0、参考 https://github.com/easzlab/kubeasz/blob/master/docs/setup/00-planning_and_overall_intro.md https://github.com/easzlab/kubeasz/blob/master/docs/setup/config_guide.md 1、各节点基础OS配置 rm -rf anaconda-ks.cfg original-ks.cfg post-install.log sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config echo 'export HTTP_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export HTTPS_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo \"PREFIX=24\\nIPADDR=192.168.1.61\\nGATEWAY=192.168.1.1\" >> /etc/sysconfig/network-scripts/ifcfg-ens32 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens32 source ~/.zshrc hostnamectl set-hostname --static node1.k8s118.curiouser.com reboot now rm -rf anaconda-ks.cfg original-ks.cfg post-install.log sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config echo 'export HTTP_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export HTTPS_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo \"PREFIX=24\\nIPADDR=192.168.1.62\\nGATEWAY=192.168.1.1\" >> /etc/sysconfig/network-scripts/ifcfg-ens32 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens32 source ~/.zshrc hostnamectl set-hostname --static node2.k8s118.curiouser.com reboot now rm -rf anaconda-ks.cfg original-ks.cfg post-install.log sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config echo 'export HTTP_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export HTTPS_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo \"PREFIX=24\\nIPADDR=192.168.1.63\\nGATEWAY=192.168.1.1\" >> /etc/sysconfig/network-scripts/ifcfg-ens32 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens32 source ~/.zshrc hostnamectl set-hostname --static node3.k8s118.curiouser.com reboot now 2、Node1节点安装ansible pip install pip --upgrade -i https://mirrors.aliyun.com/pypi/simple/ pip install ansible==2.6.18 netaddr==0.7.19 -i https://mirrors.aliyun.com/pypi/simple/ 3、Node1节点配置节点FQDN与IP的映射并打通SSH免密钥登录 echo \"192.168.1.60 tools.k8s118.curiouser.com tools\" >> /etc/hosts echo \"192.168.1.61 node1.k8s118.curiouser.com node1\" >> /etc/hosts echo \"192.168.1.62 node2.k8s118.curiouser.com node2\" >> /etc/hosts echo \"192.168.1.63 node3.k8s118.curiouser.com node3\" >> /etc/hosts ssh-keygen -t rsa -b 2048 -N '' -f ~/.ssh/id_rsa ssh-copy-id tools ssh-copy-id node1 ssh-copy-id node2 ssh-copy-id node3 4、Node1节点下载kubeasz中的安装准备工具脚本easzup export release=3.6.2 wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown chmod +x ./ezdown mv ezdown /usr/local/bin # 下载kubeasz代码、二进制、默认容器镜像（更多关于ezdown的参数，运行./ezdown 查看） ezdown -D # 海外环境 ezdown -D -m standard # 按需下载额外容器镜像（cilium,flannel,prometheus等） ezdown -X flannel ezdown -X prometheus # 下载离线系统包 (适用于无法使用yum/apt仓库情形) ezdown -P 执行成功后，所有文件均已整理好放入目录/etc/ansible，只要把该目录整体复制到任何离线的机器上，即可开始安装集群 离线文件不包括： 管理端 ansible 安装，但可以使用 kubeasz 容器运行 ansible 脚本 其他更多 kubernetes 插件镜像 ezdown -S docker exec -it kubeasz ezctl new new-k8s # 2023-09-15 14:05:15 DEBUG generate custom cluster files in /etc/kubeasz/clusters/new-k8s # 2023-09-15 14:05:15 DEBUG set versions # 2023-09-15 14:05:15 DEBUG cluster new-k8s: files successfully created. # 2023-09-15 14:05:15 INFO next steps 1: to config '/etc/kubeasz/clusters/new-k8s/hosts' # 2023-09-15 14:05:15 INFO next steps 2: to config '/etc/kubeasz/clusters/new-k8s/config.yml' 5、配置k8s集群参数的主机清单 模版配置：https://github.com/easzlab/kubeasz/blob/master/example/hosts.multi-node cp /etc/ansible/example/hosts.multi-node /etc/ansible/hosts [etcd] 192.168.1.61 NODE_NAME=etcd1 [kube-master] 192.168.1.61 [kube-node] 192.168.1.62 192.168.1.63 # [optional] loadbalance for accessing k8s from outside [ex-lb] 192.168.1.60 LB_ROLE=backup EX_APISERVER_VIP=192.168.1.6250 EX_APISERVER_PORT=8443 # [optional] ntp server for the cluster [chrony] 192.168.1.60 [all:vars] # Cluster container-runtime supported: docker, containerd CONTAINER_RUNTIME=\"docker\" # Network plugins supported: calico, flannel, kube-router, cilium, kube-ovn CLUSTER_NETWORK=\"calico\" # Service proxy mode of kube-proxy: 'iptables' or 'ipvs' PROXY_MODE=\"ipvs\" # K8S Service CIDR, not overlap with node(host) networking SERVICE_CIDR=\"10.68.0.0/16\" # Cluster CIDR (Pod CIDR), not overlap with node(host) networking CLUSTER_CIDR=\"172.20.0.0/16\" # NodePort Range NODE_PORT_RANGE=\"20000-40000\" # Cluster DNS Domain CLUSTER_DNS_DOMAIN=\"cluster.local.\" # -------- Additional Variables (don't change the default value right now) --- # Binaries Directory bin_dir=\"/opt/kube/bin\" # CA and other components cert/key Directory ca_dir=\"/etc/kubernetes/ssl\" # Deploy Directory (kubeasz workspace) base_dir=\"/etc/ansible\" 6、修改Ansible中K8S服务配置 ⓪设置离线安装 sed -i 's/^INSTALL_SOURCE.*$/INSTALL_SOURCE: \"offline\"/g' /etc/ansible/roles/chrony/defaults/main.yml sed -i 's/^INSTALL_SOURCE.*$/INSTALL_SOURCE: \"offline\"/g' /etc/ansible/roles/ex-lb/defaults/main.yml sed -i 's/^INSTALL_SOURCE.*$/INSTALL_SOURCE: \"offline\"/g' /etc/ansible/roles/kube-node/defaults/main.yml sed -i 's/^INSTALL_SOURCE.*$/INSTALL_SOURCE: \"offline\"/g' /etc/ansible/roles/prepare/defaults/main.yml ①修改集群证书相关配置 参考：https://github.com/easzlab/kubeasz/blob/master/docs/setup/01-CA_and_prerequisite.md sed -i 's/HangZhou/Shanghai/g' /etc/ansible/roles/deploy/templates/* vi /etc/ansible/roles/deploy/defaults/main.yml # CA 证书相关参数 CA_EXPIRY: \"876000h\" CERT_EXPIRY: \"876000h\" # apiserver 默认第一个master节点 KUBE_APISERVER: \"https://{{ groups['kube-master'][0] }}:6443\" CLUSTER_NAME: \"Curiouser\" CREATE_READONLY_KUBECONFIG: false ②修改Docker配置 vi /etc/ansible/roles/docker/defaults/main.yml # docker日志相关 LOG_DRIVER: \"json-file\" LOG_LEVEL: \"warn\" LOG_MAX_SIZE: \"50m\" LOG_MAX_FILE: 10 # docker容器存储目录 STORAGE_DIR: \"/data/docker\" # 开启Restful API ENABLE_REMOTE_API: false # 启用 docker 仓库镜像 ENABLE_MIRROR_REGISTRY: true # 设置 docker 仓库镜像 REG_MIRRORS: '[\"https://dockerhub.azk8s.cn\", \"https://docker.mirrors.ustc.edu.cn\"]' # 信任的HTTP仓库 INSECURE_REG: '[\"127.0.0.1/8\",\"192.168.1.60\"]' ③其他插件配置 vi /etc/ansible/roles/cluster-addon/defaults/main.yml # dns 自动安装，'dns_backend'可选\"coredns\"和“kubedns” dns_install: \"yes\" dns_backend: \"coredns\" # 设置 dns svc ip (这里选用 SERVICE_CIDR 中第2个IP) CLUSTER_DNS_SVC_IP: \"{{ SERVICE_CIDR | ipaddr('net') | ipaddr(2) | ipaddr('address') }}\" kubednsVer: \"1.14.13\" corednsVer: \"1.6.6\" kubedns_offline: \"kubedns_{{ kubednsVer }}.tar\" coredns_offline: \"coredns_{{ corednsVer }}.tar\" dns_offline: \"{%- if dns_backend == 'coredns' -%} \\ {{ coredns_offline }} \\ {%- else -%} \\ {{ kubedns_offline }} \\ {%- endif -%}\" # metric server 自动安装 metricsserver_install: \"yes\" metricsVer: \"v0.3.6\" metricsserver_offline: \"metrics-server_{{ metricsVer }}.tar\" # dashboard 自动安装 # dashboard v2.x.x 不依赖于heapster dashboard_install: \"no\" dashboardVer: \"v2.0.0-rc3\" dashboard_offline: \"dashboard_{{ dashboardVer }}.tar\" dashboardMetricsScraperVer: \"v1.0.3\" metricsscraper_offline: \"metrics-scraper_{{ dashboardMetricsScraperVer }}.tar\" # ingress 自动安装，可选 \"traefik\" 和 \"nginx-ingress\" ingress_install: \"no\" ingress_backend: \"traefik\" traefikVer: \"v1.7.20\" nginxingVer: \"0.21.0\" traefik_offline: \"traefik_{{ traefikVer }}.tar\" nginx_ingress_offline: \"nginx_ingress_{{ nginxingVer }}.tar\" # metallb 自动安装 metallb_install: \"no\" metallbVer: \"v0.7.3\" # 模式选择: 二层 \"layer2\" 或者三层 \"bgp\" metallb_protocol: \"layer2\" metallb_offline: \"metallb_{{ metallbVer }}.tar\" metallb_vip_pool: \"192.168.1.240/29\" # efk 自动安装 #efk_install: \"no\" # prometheus 自动安装 #prometheus_install: \"no\" 7、执行ansible playbook ansible-playbook /etc/ansible/90.setup.yml 8、验证 kubectl get node -owide kubectl get pod --all-namespaces /opt/kube/bin/calicoctl get node 三、Tools安装基础服务 1、安装NFS Server yum install -y nfs-utils rpcbind && \\ systemctl enable nfs && \\ systemctl enable rpcbind && \\ systemctl start nfs && \\ systemctl start rpcbind && \\ echo \"/data/nfs/k8s-storage 192.168.1.0/24(rw,no_root_squash,sync)\" >> /etc/exports && \\ mkdir -p /data/nfs/k8s-storage && \\ exportfs -a && \\ showmount -e $HOSTNAME 2、安装Harbor 最新kubeasz安装脚本中不支持安装最新版本的harbor,所以要使用docker compose在tools节点安装 ①安装docker、docker-compose yum install -y yum-utils && \\ yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo && \\ yum list docker-ce --showduplicates | sort -r && \\ yum install docker-ce-18.06.3.ce docker-compose && \\ bash -c 'cat > /etc/docker/daemon.json ②部署Harbor export harbor_ver=v2.0.0 && \\ wget https://github.com/goharbor/harbor/releases/download/$harbor_ver/harbor-online-installer-$harbor_ver.tgz && \\ tar -zxvf harbor-online-installer-* && \\ rm -rf harbor-online-installer-* && \\ mkdir -p /data/harbor/{data,logs} && \\ mv harbor /data/harbor/ && \\ cd /data/harbor/harbor && \\ bash -c 'cat > /data/harbor/harbor/harbor.yml ③验证 访问http://192.168.1.60 四、K8S集群配置 1、在K8S集群中添加harbor用户认证的Secret ①Harbor中创建用户并授权 在Harbor中创建用户k8s，在指定的仓库中授予访客的权限，仅限于可以拉取镜像 ②在K8S指定命名空间下创建harbor账号的Secret for i in {default,kube-system} ; do kubectl create secret docker-registry harbor-secret --docker-server=192.168.1.60 --docker-username=k8s --docker-password=**** --docker-email=***@163.com -n $i ;done ③指定默认default ServiceAccount的镜像拉取Secret 不用每个Deployment中都要添加imagepullsecret kubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"harbor-secret\"}]}' -n kube-system 2、Node1节点安装Helm 3 ①下载安装helm3 export helm_ver=v3.2.1 wget https://get.helm.sh/helm-$helm_ver-linux-amd64.tar.gz tar -zxvf helm-* linux-amd64/helm mv linux-amd64/helm /usr/local/bin chmod +x /usr/local/bin/helm rm -rf linux-amd64 helm-* ②添加远程charts仓库 helm repo add googleapis-incubator https://kubernetes-charts-incubator.storage.googleapis.com helm repo add googleapis-stable https://kubernetes-charts.storage.googleapis.com helm repo add bitnami https://charts.bitnami.com/bitnami helm repo add traefik https://containous.github.io/traefik-helm-chart helm repo add harbor https://helm.goharbor.io helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts helm repo add elastic https://helm.elastic.co helm repo add kong https://charts.konghq.com helm repo add pingcap https://charts.pingcap.org/ helm repo list helm repo update 3、配置CSI Ceph Filesystem Provisoner ①tools节点安装单节点Ceph Filesystem，并获取admin用户的密钥环 参考： ceph-filesystem单节点安装 ceph-filesystem-provisioner ceph auth get client.admin ②创建Ceph admin用户的Secret kubectl create secret generic ceph-admin --type=\"kubernetes.io/rbd\" --from-literal=key='******' --namespace=default ③部署带有RBAC的Ceph Filesystem Provisoner 对象资源 --- apiVersion: v1 kind: ServiceAccount metadata: name: ceph-fs-provisioner namespace: default --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: ceph-fs-provisioner namespace: default rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"create\", \"get\", \"delete\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: ceph-fs-provisioner namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ceph-fs-provisioner subjects: - kind: ServiceAccount name: ceph-fs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ceph-fs-provisioner namespace: default rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ceph-fs-provisioner subjects: - kind: ServiceAccount name: ceph-fs-provisioner namespace: default roleRef: kind: ClusterRole name: ceph-fs-provisioner apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: ceph-fs-provisioner namespace: default spec: selector: matchLabels: app: ceph-fs-provisioner replicas: 1 strategy: type: Recreate template: metadata: labels: app: ceph-fs-provisioner spec: imagePullSecrets: - name: harbor-secret containers: - name: ceph-fs-provisioner image: \"192.168.1.60/tools/cephfs-provisioner:latest\" env: - name: PROVISIONER_NAME value: ceph.com/cephfs - name: PROVISIONER_SECRET_NAMESPACE value: default command: - \"/usr/local/bin/cephfs-provisioner\" args: - \"-id=cephfs-provisioner-1\" serviceAccount: ceph-fs-provisioner ④创建storageclass kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: ceph-fs provisioner: ceph.com/cephfs parameters: monitors: 192.168.1.60:6789 adminId: admin adminSecretName: ceph-admin adminSecretNamespace: \"default\" claimRoot: /ceph-fs-pvc-volumes ⑤验证测试 创建pvc kind: PersistentVolumeClaim apiVersion: v1 metadata: name: ceph-fs-pvc-test spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ceph-fs 创建临时POD挂载PVC，写入测试数据 apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: ceph-fs-pvc-test 将Ceph Filesystem的pool挂载到Node1节点上查看其中是否产生文件 mkdir /mnt/mycephfs mount -t ceph tools.k8s118.curiouser.com:/ /mnt/mycephfs -o name=admin,secret=***** tree /mnt/mycephfs/pvc-volumes/kubernetes /mnt/mycephfs/pvc-volumes/kubernetes └── kubernetes-dynamic-pvc-33ee21eb-984f-11ea-be0a-52a5fa47eee8 └── 1.log └── 2.log Ceph RBD Provisioner ①tools节点安装单节点Ceph，并获取admin用户的密钥环 参考：ceph-rbd单节点安装 ②部署带有RBAC的Ceph RBD Provisioner对象资源 --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ceph-rbd-provisioner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"kube-dns\",\"coredns\"] verbs: [\"list\", \"get\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: ceph-rbd-provisioner rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\"] - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: ceph-rbd-provisioner roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ceph-rbd-provisioner subjects: - kind: ServiceAccount name: ceph-rbd-provisioner namespace: default --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: ceph-rbd-provisioner subjects: - kind: ServiceAccount name: ceph-rbd-provisioner namespace: default roleRef: kind: ClusterRole name: ceph-rbd-provisioner apiGroup: rbac.authorization.k8s.io --- apiVersion: v1 kind: ServiceAccount metadata: name: ceph-rbd-provisioner --- apiVersion: apps/v1 kind: Deployment metadata: name: ceph-rbd-provisioner spec: replicas: 1 selector: matchLabels: app: ceph-rbd-provisioner strategy: type: Recreate template: metadata: labels: app: ceph-rbd-provisioner spec: imagePullSecrets: - name: harbor-secret containers: - name: ceph-rbd-provisioner image: \"192.168.1.60/tools/ceph-rbd-provisioner:latest\" env: - name: PROVISIONER_NAME value: ceph-rbd-provisioner - name: PROVISIONER_SECRET_NAMESPACE value: default serviceAccount: ceph-rbd-provisioner ③创建storageclass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd provisioner: ceph-rbd-provisioner parameters: monitors: 192.168.1.60:6789 adminId: admin adminSecretName: ceph-admin adminSecretNamespace: default pool: rbd #ceph创建是默认rbd池 userId: admin userSecretName: ceph-admin userSecretNamespace: default fsType: ext4 imageFormat: \"2\" imageFeatures: \"layering\" # fsType：Kubernetes 支持的 fsType。默认：\"ext4\"。 # imageFormat：Ceph RBD 镜像格式，“1” 或者 “2”。默认值是 “1”。 # imageFeatures：这个参数是可选的，只能在你将 imageFormat 设置为 “2” 才使用。 目前支持的功能只是 # # layering。默认是 “\"，没有功能打开。 ④所有K8s节点安装ceph-common包 kubernetes 的所有节点（尤其是 master 节点）上需要安装 ceph-common客户端,不然稍后测试时，pvc/pv创建都正常,但是pod挂载失败，报failed to create rbd image: executable file not found in $PATH, command output [ceph] name=Ceph packages for $basearch baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/$basearch enabled=1 gpgcheck=1 priority=1 type=rpm-md gpgkey=http://mirrors.163.com/ceph/keys/release.asc [ceph-noarch] name=Ceph noarch packages baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/noarch enabled=1 gpgcheck=1 priority=1 type=rpm-md gpgkey=http://mirrors.163.com/ceph/keys/release.asc yum install -y ceph-common ⑤验证测试 创建pvc kind: PersistentVolumeClaim apiVersion: v1 metadata: name: ceph-rbd-pvc-test spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: ceph-rbd 创建临时POD挂载PVC，写入测试数据 apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: ceph-rbd-pvc-test ⑥排错方式 persistent-volume-controller服务受kube-controller-manager控制，可以通过查看kube-controller-manager的日志排错 journalctl -xe -u kube-controller-manager.service ⑦参考 https://github.com/kubernetes-incubator/external-storage/tree/master/ceph/rbd https://blog.51cto.com/wangzhijian/2159701 NFS Client Provisoner ①拉取镜像并推送到harbor中 docker login -u admin -p ***** 192.168.1.60 docekr pull quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11 docekr tag quay.io/external_storage/nfs-client-provisioner:v3.1.0-k8s1.11 192.168.1.60/tools/nfs-client-provisioner:v3.1.0-k8s1.11 docekr push 192.168.1.60/tools/nfs-client-provisioner:v3.1.0-k8s1.11 ②使用helm部署NFS Client Provisoner helm install nfs-client-provisioner --set nfs.server=192.168.1.60 --set nfs.path=/data/nfs/k8s-storage googleapis-stable/nfs-client-provisioner --namespace default --set image.repository=192.168.1.60/tools/nfs-client-provisioner --set image.tag=v3.1.0-k8s1.11 --set storageClass.defaultClass=true bash -c 'cat > /tmp/a ③测试验证 创建一个PVC vi /tmp/test.pvc kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi kubectl apply -f /tmp/test.pvc -n default 创建一个POD使用PVC apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: busybox args: - /bin/sh - -c - > i=0; while true; do echo \"$i: $(date)\" >> /var/log/1.log; echo \"$(date) INFO $i\" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog persistentVolumeClaim: claimName: test 验证NFS Server节点tools上NFS目录下是否产生文件 ls /data/nfs/k8s-storage/default-test-pvc-59663468-2352-4be5-8b08-432045ce8a18/ 1.log 2.log 删除对应测试pod、pvc kubectl -n default delete pod/counter pvc/test --force --grace-period 0 五、K8S Worker节点管理 参考：https://github.com/easzlab/kubeasz/blob/master/docs/op/op-node.md 1、添加节点 新增kube-node节点大致流程为：/etc/ansible/tools/02.addnode.yml [可选]新节点安装 chrony 时间同步 新节点预处理 prepare 新节点安装 docker 服务 新节点安装 kube-node 服务 新节点安装网络插件相关 ①创建新节点VM并配置OS基础配置 rm -rf anaconda-ks.cfg original-ks.cfg post-install.log sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config echo 'export HTTP_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export HTTPS_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo \"PREFIX=24\\nIPADDR=192.168.1.64\\nGATEWAY=192.168.1.1\" >> /etc/sysconfig/network-scripts/ifcfg-ens32 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens32 source ~/.zshrc hostnamectl set-hostname --static node4.k8s118.curiouser.com reboot now ②Node1节点配置新增节点FQDN与IP的映射并打通SSH免密钥登录 echo \"192.168.1.64 node4.k8s118.curiouser.com node4\" >> /etc/hosts ssh-copy-id node4 ③Node1节点运行easzctl add-node命令 easzctl add-node 192.168.1.64 ④验证 kubectl get node -owide ⑤（可选）添加非标准 ssh 22端口的节点 目前 easzctl 暂不支持自动添加非标准 ssh 端口的节点，可以手动操作如下： 假设待添加节点192.168.2.1，ssh 端口 10022；配置免密登录ssh-copy-id -p 10022 192.168.2.1，按提示输入密码 在 /etc/ansible/hosts文件 [kube-node] 组下添加一行： 192.168.2.1 ansible_ssh_port=10022 最后执行 ansible-playbook /etc/ansible/tools/02.addnode.yml -e NODE_TO_ADD=192.168.2.1 2、删除节点 删除 node 节点流程：/etc/ansible/tools/12.delnode.yml 检测是否可以删除 迁移节点上的 pod 删除 node 相关服务及文件 从集群删除 node ①Node1节点运行easzctl del-node命令 easzctl del-node 192.168.1.64 六、K8S Master节点管理 参考：https://github.com/easzlab/kubeasz/blob/master/docs/op/op-master.md 1、添加Master节点 新增kube-master节点大致流程为：/etc/ansible/tools/03.addmaster.yml [可选]新节点安装 chrony 时间同步 新节点预处理 prepare 新节点安装 docker 服务 新节点安装 kube-master 服务 新节点安装 kube-node 服务 新节点安装网络插件相关 禁止业务 pod调度到新master节点 更新 node 节点 haproxy 负载均衡并重启 ①创建新节点VM并配置OS基础配置 rm -rf anaconda-ks.cfg original-ks.cfg post-install.log sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config echo 'export HTTP_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo 'export HTTPS_PROXY=\"http://192.168.1.7:8001\"' >> ~/.zshrc echo \"PREFIX=24\\nIPADDR=192.168.1.64\\nGATEWAY=192.168.1.1\" >> /etc/sysconfig/network-scripts/ifcfg-ens32 sed -i 's/dhcp/static/g' /etc/sysconfig/network-scripts/ifcfg-ens32 source ~/.zshrc hostnamectl set-hostname --static node4.k8s118.curiouser.com reboot now ②Node1节点配置新增节点FQDN与IP的映射并打通SSH免密钥登录 echo \"192.168.1.64 node4.k8s118.curiouser.com node4\" >> /etc/hosts ssh-copy-id node4 ③Node1节点运行easzctl add-master命令 easzctl add-master 192.168.1.64 ④验证 # 在新节点master 服务状态 systemctl status kube-apiserver systemctl status kube-controller-manager systemctl status kube-scheduler # 查看新master的服务日志 journalctl -u kube-apiserver -f # 查看集群节点，可以看到新 master节点 Ready, 并且禁止了POD 调度功能 kubectl get node 2、删除 Master 节点 删除kube-master节点大致流程为：/etc/ansible/tools/13.delmaster.yml 检测是否可以删除 迁移节点 pod 删除 master 相关服务及文件 删除 node 相关服务及文件 从集群删除 node 节点 从 ansible hosts 移除节点 在 ansible 控制端更新 kubeconfig 更新 node 节点 haproxy 配置 ①Node1节点运行easzctl del-master命令 easzctl del-master 192.168.1.64 七、集群备份与恢复 参考：https://github.com/easzlab/kubeasz/blob/master/docs/op/cluster_restore.md 在高可用k8s集群中 etcd集群保存了整个集群的状态，因此这里的备份与恢复重点就是： 从运行的etcd集群备份数据到磁盘文件 从etcd备份文件恢复数据，从而使集群恢复到备份时状态 备份与恢复操作说明 1.首先搭建一个测试集群，部署几个测试deployment，验证集群各项正常后，进行一次备份： $ ansible-playbook /etc/ansible/23.backup.yml 执行完毕可以在备份目录下检查备份情况，示例如下： /etc/ansible/.cluster/backup/ ├── hosts ├── hosts-201907030954 ├── snapshot-201907030954.db ├── snapshot-201907031048.db └── snapshot.db 2.模拟误删除操作（略） 3.恢复集群及验证 可以在 roles/cluster-restore/defaults/main.yml 文件中配置需要恢复的 etcd备份版本（从上述备份目录中选取），默认使用最近一次备份；执行恢复后，需要一定时间等待 pod/svc 等资源恢复重建。 $ ansible-playbook /etc/ansible/24.restore.yml 如果集群主要组件（master/etcd/node）等出现不可恢复问题，可以尝试使用如下步骤 清理 --> 创建 --> 恢复 ansible-playbook /etc/ansible/99.clean.yml ansible-playbook /etc/ansible/90.setup.yml ansible-playbook /etc/ansible/24.restore.yml 参考 https://github.com/coreos/etcd/blob/master/Documentation/op-guide/recovery.md 八、升级 快速k8s版本升级 快速升级是指只升级k8s版本，比较常见如Bug修复 重要特性发布时使用。 首先去官网release下载待升级的k8s版本，例如https://dl.k8s.io/v1.11.5/kubernetes-server-linux-amd64.tar.gz 解压下载的tar.gz文件，找到如下 kube* 开头的二进制，复制替换ansible控制端目录 /etc/ansible/bin 对应文件 kube-apiserver kube-controller-manager kubectl kubelet kube-proxy kube-scheduler 在ansible控制端执行ansible-playbook -t upgrade_k8s 22.upgrade.yml即可完成k8s 升级，不会中断业务应用 如果使用 easzctl 命令行，可按如下执行： 首先确认待升级的集群（如果有多集群的话） easzctl checkout 执行升级 easzctl upgrade 其他升级说明 其他升级是指升级k8s组件包括：etcd版本 docker版本，一般不需要用到，以下仅作说明。 1.下载所有组件相关新的二进制解压并替换 /etc/ansible/bin/ 目录下文件 2.升级 etcd: ansible-playbook -t upgrade_etcd 02.etcd.yml，注意：etcd 版本只能升级不能降低！ 3.升级 docker （建议使用k8s官方支持的docker稳定版本） 如果可以接受短暂业务中断，执行 ansible-playbook -t upgrade_docker 03.docker.yml 如果要求零中断升级，执行ansible-playbook -t download_docker 03.docker.yml ，然后手动执行如下 待升级节点，先应用kubectl cordon和kubectl drain命令迁移业务pod 待升级节点执行 systemctl restart docker 恢复节点可调度 kubectl uncordon Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:43:41 "},"origin/new-kubeasz-install-bin-k8s.html":{"url":"origin/new-kubeasz-install-bin-k8s.html","title":"新版Kubeasz二进制安装Kubernetes集群","keywords":"","body":"集群节点规划 kubeasz：3.6.2 k8s版本：1.28.1 服务器OS：Ubuntu 22.04.2 CRI: Containerd 1.6.23 CNI：Cilium 1.13.6 kubeproxy模式：IPVS 证书有效期：100年 IngressContoller：Traefik 2.10.4 CSI：NFS Provisioner、LocalVolume Provisioner 开起功能： 审计日志 额外组件： 日志体系：EFK Stack Fluentd ElasticSearch Kibana 监控体系：Prometheus Stack Metrics-server Prometheus Grafana Alertmanager Kube-state-metrics Node-exporter 主机名 硬件配置 IP地址 服务 Node1 8C16G100G 192.168.150.206 kubeasz AnsibleK8S Master(etcd、apiserver、controllermanager、scheduler、kubelet、kueb-proxy、docker) Node2 8C16G100G 192.168.150.207 K8S Master、K8S Worker(kubelet、kube-proxy、docker)、etcd Node3 8C16G100G 192.168.150.208 K8S Master、K8S Worker(kubelet、kube-proxy、docker)、etcd Node4 8C16G100G 192.168.150.209 K8S Worker(kubelet、kube-proxy、docker) 第一步：初始化部署节点 sed -i '' -e '/192.168.150.206/d' -e '/192.168.150.207/d' -e '/192.168.150.208/d' -e '/192.168.150.209/d' ~/.ssh/known_hosts && \\ local_ssh_pub=$(cat ~/.ssh/id_rsa.pub) && \\ ssh -o \"StrictHostKeyChecking no\" root@192.168.150.206 \\ ' echo \"'\"$local_ssh_pub\"'\" >> ~/.ssh/authorized_keys && apt update && apt install -y git nfs-kernel-server && nk8s_node01_ip=$(ip addr show eth0 | grep -oP \"inet \\K[\\d.]+\") && IFS='.' read -r ip_part1 ip_part2 ip_part3 ip_part4 > ~/.ssh/authorized_keys && sed -i \"/$nk8s_node01_ip/d\" /etc/hosts && echo -e \"\\n$nk8s_node01_ip nk8s-node01\\n$nk8s_node02_ip nk8s-node02\\n$nk8s_node03_ip nk8s-node03\\n$nk8s_node04_ip nk8s-node04\\n\" >> /etc/hosts && release=3.6.2 && wget -e use_proxy=yes -e https_proxy=http://home.curiouser.top:8001 https://github.com/easzlab/kubeasz/releases/download/$release/ezdown && chmod +x ./ezdown && mv ezdown /usr/local/bin && ezdown -D && ezdown -S && ezdown -X cilium && ezdown -X prometheus && ezdown -X nfs-provisioner && ezdown -X local-path-provisioner && git config --global user.email \"test@example.com\" && git config --global user.name \"test\" && rm -rf /etc/kubeasz/.github && sed -i \"/clusters\\//d\" /etc/kubeasz/.gitignore && cd /etc/kubeasz && git init /etc/kubeasz && git add /etc/kubeasz && git commit -am \"init commit\" && docker exec -i kubeasz ezctl new new-k8s && cd /etc/kubeasz && git add /etc/kubeasz && git commit -am \"add new-k8s cluster resource\" && echo -e \"alias dk='\"'docker exec -it kubeasz'\"'\" >> ~/.bashrc && systemctl enable nfs-server.service && \\ systemctl start nfs-server.service && \\ echo \"/data/nfs/k8s-storage 192.168.150.0/24(rw,insecure,no_subtree_check,no_root_squash,sync)\" >> /etc/exports && \\ mkdir -p /data/nfs/k8s-storage && \\ exportfs -a && \\ showmount -e $HOSTNAME reboot now ' 第二步：设置其他节点基础设置 local_ssh_pub=$(cat ~/.ssh/id_rsa.pub) nk8s_node01_ssh_pub=$(ssh root@192.168.150.206 'cat .ssh/id_rsa.pub') k8s_nodes_ips=(\"192.168.150.207\" \"192.168.150.208\" \"192.168.150.209\") for ((i=1; i> ~/.ssh/authorized_keys && echo \"'\"$nk8s_node01_ssh_pub\"'\" >> ~/.ssh/authorized_keys && apt update && apt install -y git && nodename=\"nk8s-node'\"$(printf \"%02d\" $((i+1)))\"'\" && sed -i \"/'\"${k8s_nodes_ips[$i]}\"'/d\" /etc/hosts && hostnamectl --static set-hostname $nodename && reboot now ' done ansible --user root -i \"192.168.150.206,192.168.150.207,192.168.150.208,192.168.150.209\" all -m shell -a \"hostname\" 第三步：修改kubeasz部署配置文件 scp -r root@192.168.150.206:/etc/kubeasz/clusters/new-k8s ~/Desktop ~/Desktop/new-k8s/config.yml ~/Desktop/new-k8s/hosts 修改后同步到 Tool节点 rsync --exclude='.DS_Store' -r ~/Desktop/new-k8s/ root@192.168.150.206:/etc/kubeasz/clusters/new-k8s/ 第四步：修改集群部署配置 修改集群 SSL/TLS证书中组织的名字 新增随机生成 Grafana admin用户的密码，修改部署时安装图标插件，新增ServiceMonitor的标签 新增部署 Prometheus时挂载持久化PVC 修改 NFS Provisioner部署时的PVC 回收策略 开起 k8s 审计日志功能 random_grafana_admin_passw=`specialchars='!@#$%^&*()_+' && \\ astring=$(echo $(date +%s)$RANDOM | md5sum | base64 | tr -dc A-Za-z0-9 | head -c 15 ) && \\ randomchar=${specialchars:$RANDOM % ${#specialchars}:1} && \\ randompos=$(( $RANDOM % ( ${#astring} + 1 ) )) && \\ echo ${astring:0:$randompos}${randomchar}${astring:$randompos}` && \\ ssh root@192.168.150.206 ' \\ for i in `grep -rl \"XS\" /etc/kubeasz/roles/deploy/templates`;do sed -i \"s/XS/ZZMED/g\" $i ;done && \\ sed -i -e \"s#adminPassword: Admin1234\\!#adminPassword: '$random_grafana_admin_passw'#g\" -e \"/skipTlsVerify:\\ true/a \\ \\ plugins:\\n\\ \\ \\ \\ -\\ grafana-piechart-panel\\n\\ \\ \\ \\ -\\ grafana-clock-panel\\n\\ \\ persistence:\\n\\ \\ \\ \\ enabled: true\\n\\ \\ serviceMonitor:\\n\\ \\ \\ \\ labels:\\n\\ \\ \\ \\ \\ \\ release: prometheus\" -e \"s/storageSpec: {}/storageSpec:\\n volumeClaimTemplate:\\n spec:\\n storageClassName: \\\"local-nfs-storage\\\"\\n accessModes: [\\\"ReadWriteOnce\\\"]\\n resources:\\n requests:\\n storage: 50Gi\\n selector:\\n matchLabels:\\n app: prometheus/g\" /etc/kubeasz/roles/cluster-addon/templates/prometheus/values.yaml.j2 && \\ sed -i \"s/archiveOnDelete: \\\"false\\\"/archiveOnDelete: \\\"true\\\"/g\" /etc/kubeasz/roles/cluster-addon/templates/nfs-provisioner/nfs-provisioner.yaml.j2 && \\ sed -i -e \"s/--v=2/--v=2 \\\\\\\\\\n --audit-log-path=\\/var\\/log\\/audit\\/audit.log \\\\\\\\\\n --audit-policy-file=\\/etc\\/kubernetes\\/audit-policy.yaml \\\\\\\\\\n --audit-log-maxage=5 \\\\\\\\\\n --audit-log-compress \\\\\\\\\\n --audit-log-maxbackup=5 \\\\\\\\\\n --audit-log-maxsize=200/g\" /etc/kubeasz/roles/kube-master/templates/kube-apiserver.service.j2 && \\ sed -i \"/\\/etc\\/kubernetes\\/{{ item }}/ {N;N;N; s/\\(.*\\)/\\1\\n - audit-policy.yaml/}\" /etc/kubeasz/roles/kube-master/tasks/main.yml && \\ cat > /etc/kubeasz/clusters/new-k8s/audit-policy.yaml 第五步：开始部署 K8s 集群 ssh root@192.168.150.206 'docker exec -i kubeasz ezctl setup new-k8s all' 第六步：验证k8s kubectl config delete-cluster test-k8s-cluster ; \\ kubectl config delete-user test-k8s-cluster-admin ; \\ kubectl config delete-context test-k8s-cluster ; \\ kubectl config set-cluster test-k8s-cluster --server=https://192.168.1.1:6443 && \\ kubectl config set-credentials test-k8s-cluster-admin && \\ ssh root@192.168.150.206 'cat .kube/config' | yq -r '.clusters.[0].cluster.certificate-authority-data ' | xargs kubectl config set clusters.test-k8s-cluster.certificate-authority-data && \\ ssh root@192.168.150.206 'cat .kube/config' | yq -r '.users.[0].user.client-certificate-data' | xargs kubectl config set users.test-k8s-cluster-admin.client-certificate-data && \\ ssh root@192.168.150.206 'cat .kube/config' | yq -r '.users.[0].user.client-key-data' | xargs kubectl config set users.test-k8s-cluster-admin.client-key-data && \\ kubectl config set-context test-k8s-cluster --cluster=test-k8s-cluster --user=test-k8s-cluster-admin --namespace=kube-system && \\ kubectl config use-context test-k8s-cluster && \\ kubectl -n kube-public run nginx --image=nginx --port=80 --labels=\"app=nginx\" && \\ kubectl -n kube-public expose pod nginx --name=nginx --port=80 --target-port=80 --protocol=TCP && \\ kubectl get nodes && \\ kubectl get ns && \\ kubectl get pod -A && \\ kubectl -n kube-public get pod -l app=nginx -o wide && \\ kubectl -n kube-public get svc -l app=nginx Prometheus：http://192.168.150.206:30901 Alertmanager：http://192.168.150.206:30902/ Grafana：http://192.168.150.206:30903 (可选)第七步：其他操作 ssh root@192.168.150.206 \\ ' apt update && apt install -y zsh && chsh -s /bin/zsh root && curl -x http://home.curiouser.top:8001 -sLo install.sh https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh && chmod +x install.sh && REPO=\"mirrors/oh-my-zsh\" REMOTE=\"https://gitee.com/mirrors/oh-my-zsh.git\" sh install.sh --skip-chsh && git config --global https.proxy http://home.curiouser.top:8001 && git config --global https.proxy https://home.curiouser.top:8001 && git clone https://github.com/zsh-users/zsh-autosuggestions.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions && git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting && sed -i -e \"/ZSH_THEME/d\" -e \"10 a ZSH_THEME=\\\"ys\\\"\" -e \"s/plugins=(git)/plugins=(git kubectl docker sudo history extract zsh-autosuggestions zsh-syntax-highlighting )/g\" ~/.zshrc && echo -e \"alias dk='\"'docker exec -it kubeasz'\"'\\nexport PATH=/opt/kube/bin:/opt/kube/bin/containerd-bin/:\\$PATH\\nsource > ~/.zshrc ' 第八步：安装配置其他系统 1、安装 Traefik 参照：Traefik的部署及操作 2、配置应用暴露metrics Prometheus Service Monitor # 创建 Nginx 配置文件 configmap cat 3、安装 EFK 参照：ELK系列安装部署 第二章节 3.3 使用Fluent Operator部署 Fluentd 文档：https://docs.fluentbit.io/manual/ Helm Charts：https://github.com/fluent/helm-charts ①使用Helm安装Fluent Operator fluent-operator helm repo add fluent https://fluent.github.io/helm-charts && \\ helm search repo fluent helm upgrade --install --atomic \\ -n kube-system \\ fluent-operator fluent/fluent-operator \\ --set containerRuntime=containerd \\ --set fluentbit.enable=false \\ --set fluentbit.crdsEnable=false \\ --set fluentbit.output.es.enable=true \\ --set fluentbit.output.es.host=\"logging-es-http.logging.svc\" fluent-bit helm upgrade --install \\ -n logging \\ fluent-bit fluent/fluent-bit \\ --set testFramework.enabled=false kubectl delete cm fluent-bit && \\ cat export POD_NAME=$(kubectl get pods --namespace logging -l \"app.kubernetes.io/name=fluent-bit,app.kubernetes.io/instance=fluent-bit\" -o jsonpath=\"{.items[0].metadata.name}\") kubectl --namespace logging port-forward $POD_NAME 2020:2020 curl http://127.0.0.1:2020 fluent helm upgrade --install \\ -n logging \\ fluentd fluent/fluentd \\ sed -i -e 's/deb.debian.org/mirrors.ustc.edu.cn/g' -e 's/security.debian.org/mirrors.ustc.edu.cn/g' /etc/apt/sources.list && apt update && apt install -y curl curl -XGET -s -k -u \"elastic:ky673H5s3Q3JMSm90N5qAWP0\" \"https://logging-es-http.logging.svc:9200/\" ②安装 Fluentd采集 k8s 容器日志与审计日志 fluentd输出到elasticsearch文档： https://github.com/uken/fluent-plugin-elasticsearch#usage https://docs.fluentd.org/output/elasticsearch bash 参考 https://github.com/prometheus-community/helm-charts/tree/main/charts https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus/values.yaml#L870 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-21 16:14:42 "},"origin/k8s-install-sealos.html":{"url":"origin/k8s-install-sealos.html","title":"Sealos安装","keywords":"","body":"Sealos部署Kubernetes 一、简介 Github：https://github.com/fanux/sealos 官网：https://www.sealyun.com/ 二、安装部署 1、部署前准备 主机名 IP地址 master0 192.168.1.2 master1 192.168.1.3 master2 192.168.1.4 node0 192.168.1.5 必须同步所有服务器时间 所有服务器主机名不能重复 操作系统支持：centos7.6以上、ubuntu16.04以上，推荐：centos7.7 内核推荐4.14以上 kubernetes .0版本不建议上生产环境!!! 如果使用阿里云/华为云主机部署。 默认的pod网段会和阿里云的dns网段冲突， 建议自定义修改pod网段, 在init的时候指定--podcidr 来修改 2、部署 ①下载安装 下载并安装二进制工具sealos和离线安装资源包 wget -c https://sealyun.oss-cn-beijing.aliyuncs.com/latest/sealos && \\ chmod +x sealos && mv sealos /usr/bin wget -c https://sealyun.oss-cn-beijing.aliyuncs.com/2fb10b1396f8c6674355fcc14a8cda7c-v1.20.0/kube1.21.0.tar.gz ②安装 sealos init --passwd '123456' \\ --master 192.168.1.2-192.168.1.4 \\ --node 192.168.1.5 \\ --pkg-url /root/kube1.21.0.tar.gz \\ --version v1.20.0 参数含义 参数名 含义 示例 passwd 服务器密码 123456 master k8s master节点IP地址 192.168.1.2-192.168.1.4 node k8s node节点IP地址 192.168.1.3 pkg-url 离线资源包地址，支持下载到本地，或者一个远程地址 /root/kube1.20.0.tar.gz version 资源包对应的版本 v1.20.0 apiserver apiserver的虚拟IP地址与域名映射。配置在/etc/hosts中 默认值\"apiserver.cluster.local\" cert-sans kubernetes apiServerCertSANs ex. 47.0.0.22 sealyun.com interface 默认值为`\"eth.* en.* em.*\"` ipip 是否开起ipip模式 默认值为true kubeadm-config kubeadm-config.yaml模板文件地址 lvscare-image lvscare镜像名 默认值为\"fanux/lvscare\" lvscare-tag lvscare镜像标签 默认值为“latest” mtu ipip模式的mtu值 默认值为\"1440\" network 指定安装安装的CNI组件 默认值为\"calico\" pk SSH私钥文件路径 默认值为\"/root/.ssh/id_rsa\" pk-passwd SSH私钥密码 podcidr POD的IP地址网络段 默认值为\"100.64.0.0/10\" repo 指定安装时镜像拉取的镜像注册仓库地址 默认值为\"k8s.gcr.io\" svccidr k8s service的IP地址网络段 默认值为\"10.96.0.0/12\" user ssh用户名 默认值为\"root\" vip 虚拟IP地址 默认值为\"10.103.97.2\" vlog 设置kubeadm的日志级别 without-cni 如果设置为true，则不安装CNI组件 config 集群配置文件路径（全局配置参数） 默认为$HOME/.sealos/config.yaml info 指定安装时的日志输出级别 true为Info级别，false为Debug级别 3、验证操作 mv ~/kube/bin/kubectl /usr/local/bin/ kubectl completion bash > ~/.kube/completion.bash.inc kubectl get pod --all-namespaces kubectl get nodes apt install -y ipvsadm ipvsadm -Ln 4、其他管理操作 ①增加master节点 sealos join --master 192.168.1.6 --master 192.168.1.7 sealos join --master 192.168.1.6-192.168.1.9 # 或者多个连续IP ②增加node节点 sealos join --node 192.168.1.6 --node 192.168.1.7 sealos join --node 192.168.1.6-192.168.1.9 # 或者多个连续IP ③删除指定master节点 sealos clean --master 192.168.1.6 --master 192.168.1.7 sealos clean --master 192.168.1.6-192.168.1.9 # 或者多个连续IP ④删除指定node节点 sealos clean --node 192.168.1.6 --node 192.168.1.7 sealos clean --node 192.168.1.6-192.168.1.9 # 或者多个连续IP ⑤清理集群 sealos clean --all -f ⑥备份集群 sealos etcd save Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/prometheus-Kubernetes或Openshift的Prometheus监控体系.html":{"url":"origin/prometheus-Kubernetes或Openshift的Prometheus监控体系.html","title":"kubernetes集群性能监控","keywords":"","body":"Kubernetes或Openshift的Prometheus监控体系 一、Overview 在早期，也就是 1.10 以前的 K8s 版本。大家都会使用类似像 Heapster 这样的组件来去进行监控的采集。那为什么 Kubernetes 会将 Heapster 放弃掉而转换到 metrics-service 呢？其实这个主要的一个动力来源是由于 Heapster 在做监控数据接口的标准化。为什么要做监控数据接口标准化呢？ 第一点在于客户的需求是千变万化的，比如说今天用 Heapster 进行了基础数据的一个资源采集，那明天的时候，我想在应用里面暴露在线人数的一个数据接口，放到自己的接口系统里进行数据的一个展现，以及类似像 HPA 的一个数据消费。那这个场景在 Heapster 下能不能做呢？答案是不可以的，所以这就是 Heapster 自身扩展性的弊端； 第二点是 Heapster 里面为了保证数据的离线能力，提供了很多的 sink，而这个 sink 包含了类似像 influxdb、sls、钉钉等等一系列 sink。这个 sink 主要做的是把数据采集下来，并且把这个数据离线走，然后很多客户会用 influxdb 做这个数据离线，在 influxdb 上去接入类似像 grafana 监控数据的一个可视化的软件，来实践监控数据的可视化。 但是后来社区发现，这些 sink 很多时候都是没有人来维护的。这也导致整个 Heapster 的项目有很多的 bug，这个 bug 一直存留在社区里面，是没有人修复的，这个也是会给社区的项目的活跃度包括项目的稳定性带来了很多的挑战。 基于这两点原因，K8s 把 Heapster 抛弃掉了，然后做了一个精简版的监控采集组件，叫做 metrics-server。 上图是 Heapster 内部的一个架构。大家可以发现它分为几个部分，第一个部分是 core 部分，然后上层是有一个通过标准的 http 或者 https 暴露的这个 API。然后中间是 source 的部分，source 部分相当于是采集数据暴露的不同的接口，然后 processor 的部分是进行数据转换以及数据聚合的部分。最后是 sink 部分，sink 部分是负责数据离线的，这个是早期的 Heapster 的一个应用的架构。那到后期的时候呢，K8s 做了这个监控接口的一个标准化，逐渐就把 Heapster 进行了裁剪，转化成了 metrics-server。 目前版本的 metrics-server 大致的一个结构就变成了上图这样，是非常简单的：有一个 core 层、中间的 source 层，以及简单的 API 层，额外增加了 API Registration 这层。这层的作用就是它可以把相应的数据接口注册到 K8s 的 API server 之上，以后客户不再需要通过这个 API 层去访问 metrics-server，而是可以通过这个 API 注册层，通过 API server 访问 API 注册层，再到 metrics-server。这样的话，真正的数据消费方可能感知到的并不是一个 metrics-server，而是说感知到的是实现了这样一个 API 的具体的实现，而这个实现是 metrics-server。这个就是 metrics-server 改动最大的一个地方。 Kubernetes 的监控接口标准 在 K8s 里面针对于监控，有三种不同的接口标准。它将监控的数据消费能力进行了标准化和解耦，实现了一个与社区的融合，社区里面主要分为三类。 第一类 Resource Metrice 对应的接口是 metrics.k8s.io，主要的实现就是 metrics-server，它提供的是资源的监控，比较常见的是节点级别、pod 级别、namespace 级别、class 级别。这类的监控指标都可以通过 metrics.k8s.io 这个接口获取到。 第二类 Custom Metrics 对应的 API 是 custom.metrics.k8s.io，主要的实现是 Prometheus。它提供的是资源监控和自定义监控，资源监控和上面的资源监控其实是有覆盖关系的，而这个自定义监控指的是：比如应用上面想暴露一个类似像在线人数，或者说调用后面的这个数据库的 MySQL 的慢查询。这些其实都是可以在应用层做自己的定义的，然后并通过标准的 Prometheus 的 client，暴露出相应的 metrics，然后再被 Prometheus 进行采集。 而这类的接口一旦采集上来也是可以通过类似像 custom.metrics.k8s.io 这样一个接口的标准来进行数据消费的，也就是说现在如果以这种方式接入的 Prometheus，那你就可以通过 custom.metrics.k8s.io 这个接口来进行 HPA，进行数据消费。 第三类 External Metrics External Metrics 其实是比较特殊的一类，因为我们知道 K8s 现在已经成为了云原生接口的一个实现标准。很多时候在云上打交道的是云服务，比如说在一个应用里面用到了前面的是消息队列，后面的是 RBS 数据库。那有时在进行数据消费的时候，同时需要去消费一些云产品的监控指标，类似像消息队列中消息的数目，或者是接入层 SLB 的 connection 数目，SLB 上层的 200 个请求数目等等，这些监控指标。 那怎么去消费呢？也是在 K8s 里面实现了一个标准，就是 external.metrics.k8s.io。主要的实现厂商就是各个云厂商的 provider，通过这个 provider 可以通过云资源的监控指标。在阿里云上面也实现了阿里巴巴 cloud metrics adapter 用来提供这个标准的 external.metrics.k8s.io 的一个实现。 二、Kube-State-Metrics 监听 Kubernetes API server 并自动生成相关对象的metrics信息(并不修改相关对象的配置)，在80端口(默认)暴露出HTTP的endpoint /metric GIthub：https://github.com/kubernetes/kube-state-metrics kube-state-metrics VS metrics-server 三、Metrics-Server 四、OpenShfit cluster-monitoring-operator cluster-monitoring-operator ：负责在 OpenShfit 环境中部署基于 Prometheus 的监控系统 GIthub：https://github.com/openshift/cluster-monitoring-operator 部署基于 Prometheus 监控系统中的组件 Prometheus Operator Prometheus Alertmanager cluster for cluster and application level alerting kube-state-metrics node_exporter 五、Prometheus Operator prometheus operator：使用operator部署、配置、管理Prometheus和Alertmanager GIthub： https://github.com/coreos/prometheus-operator 相关博客：https://blog.csdn.net/ygqygq2/article/details/83655552 功能： Create/Destroy: 在Kubernetes namespace中更容易启动一个Prometheus实例，一个特定的应用程序或团队更容易使用Operator。 Simple Configuration: 配置Prometheus的基础东西，比如在Kubernetes的本地资源versions, persistence, retention policies, 和replicas。 Target Services via Labels: 基于常见的Kubernetes label查询，自动生成监控target 配置；不需要学习普罗米修斯特定的配置语言。 架构 node-exporter：以Daemonset的形式部署在Openshift集群的各个节点上，采集OS级别的metrics信息 监控的Target Prometheus itself Prometheus-Operator cluster-monitoring-operator Alertmanager cluster instances Kubernetes apiserver kubelets (the kubelet embeds cAdvisor for per container metrics) kube-controllers kube-state-metrics node-exporter etcd (if etcd monitoring is enabled) Note: Prometheus Pod 中，除了 Prometheus 容器外，还有一个 prometheus-config-reloader 容器。它负责导入在需要的时候让Prometheus 重新加载配置文件。 配置文件被以 Secret 形式创建并挂载给 prometheus-config-reloader Pod。一旦配置有变化，它会调用 Prometheus 的接口，使其重新加载配置文件。 相关链接 https://docs.okd.io/3.11/install_config/prometheus_cluster_monitoring.html#prometheus-cluster-monitoring http://www.cnblogs.com/sammyliu/p/10155442.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/kubernetes-集群组件.html":{"url":"origin/kubernetes-集群组件.html","title":"kubernetes集群组件","keywords":"","body":"Kubernetes的集群组件 一、集群架构 二、etcd 是高可用的 key/value 存储系统，用于持久化存储集群中的所有资源对象，比如：Node，Pod，Serivce，RC,namespace 等。API server 提供了操作 etcd 的封装接口 API，以 Rest 的方式提供，这些 API 基本上都是集群中资源对象的增删改查及监听资源变化的接口，比如创建 Pod、RC，监听 Pod 的变化等接口。API server 是连接其他所有服务组件的中间枢纽。 三、kube-apiserver 提供了资源对象的唯一操作入口，其他组件都必须通过它提供的 API 来操作资源数据，通过对相关的资源数据\"全量查询\" + \"变化监听\"，这些组件可以很\"实时\"的完成相关的业务功能。比如提交一个新的 Pod 到 API server 中，Controller Manger 可以立即就发现并开始作用。它还有一套完备的安全机制，包括认证、授权及准入控制等相关模块。 四、kube-controllermanager 集群内部的管理控制中心，主要完成了集群的故障检测和恢复的自动化工作。比如对 RC 定义的 Pod 进行维护；根据 service 和 Pod 的关系，完成服务的 Endpoints 对象的创建和更新；还有 Node 的发现、管理和状态监控，死亡容器所占资源及本地缓存的镜像文件的清理等工作 保障集群中各种资源处于期望状态，当监控到某个资源状态不正常时，管理控制器会触发对应的调度操作，主要由以下几个部分组成: 节点控制器(Node Controller) 副本控制器(Replication Controller) 端点控制器(Endpoints Controller) 命名空间控制器(Namespace Controller) 身份认证控制器(Serviceaccounts Controller) 五、kube-scheduler 集群的调度器，负责 Pod 在集群节点中的调度分配，也负责 Volume（CVI）和网络（CNI）的管理，按照预定的调度策略将 Pod 调度到相应的机器上； 调度器，接收来自于管理控制器(kube-controller-manager)触发的调度操作请求，然后根据请求规格、调度约束、整体资源情况等因素进行调度计算，最后将任务写到etcd，目标节点的kubelet 组件监听到由其负责的资源创建工作，然后执行具体调度任务 六、kube-proxy 实现 Service 的代理及软件模式的负载均衡器。 七、kubelet 负责本地节点上 Pod 的创建、修改、监控、删除等生命周期管理，同时会上报本 Node 的状态信息到 API server。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-03-06 19:16:22 "},"origin/kubernetes-kubectl.html":{"url":"origin/kubernetes-kubectl.html","title":"kubectl","keywords":"","body":"Kubectl 一、下载kubectl kubectl github下载地址：https://github.com/kubernetes/kubectl/releases 二、创建配置文件夹 Linux mkdir ~/.kube Windows CMD mkdir %USERPROFILE%\\.kube # %USERPROFILE% 当前用户目录 三、创建编辑kubectl配置文件 apiVersion: v1 # 集群信息 clusters: - cluster: certificate-authority-data: **CA证书*** server: https://开发k8s环境APIServer的IP地址:6443 name: k8s-dev - cluster: certificate-authority-data: **CA证书*** server: https://测试k8s环境APIServer的IP地址:8443 name: k8s-test - cluster: certificate-authority-data: **CA证书*** server: https://UAT k8s环境APIServer的IP地址:8443 name: k8s-uat - cluster: certificate-authority-data: **CA证书*** server: https://生产k8s环境APIServer的IP地址:8443 name: k8s-pro # 集群上下文环境 contexts: - context: cluster: k8s-dev user: k8s-dev-admin name: k8s-dev - context: cluster: k8s-test user: k8s-test-admin name: k8s-test - context: cluster: k8s-uat user: k8s-uat-admin name: k8s-uat - context: cluster: k8s-pro user: k8s-pro-readonly name: k8s-pro # 当前使用的上下文环境 current-context: k8s-dev kind: Config preferences: {} #集群用户信息及证书信息 users: - name: k8s-dev user: client-certificate-data: **用户证书** client-key-data： **用户私钥** - name: k8s-test user: client-certificate-data: **用户证书** client-key-data： **用户私钥** - name: k8s-uat user: client-certificate-data: **用户证书** client-key-data： **用户私钥** - name: k8s-pro user: client-certificate-data: **用户证书** client-key-data： **用户私钥** 四、切换Kubernetes集群上下文 #切换至开发k8s环境上下文 kubectl config use-context k8s-dev #切换至开发k8s环境上下文 kubectl config use-context k8s-test #切换至开发k8s环境上下文 kubectl config use-context k8s-uat #切换至开发k8s环境上下文 kubectl config use-context k8s-pro 五、kubectl命令快捷操作 1、设置别名快速使用kubectl命令 Windows doskey k=kubectl $* # $*表示这个命令还可能有其他参数 Linux alias k='kubectl' 2、设置别名快速切换集群上下文 Windows doskey k2d=kubectl config use-context k8s-dev doskey k2t=kubectl config use-context k8s-test doskey k2u=kubectl config use-context k8s-uat doskey k2p=kubectl config use-context k8s-pro Linux alias k2d='kubectl config use-context k8s-dev' alias k2t='kubectl config use-context k8s-test' alias k2u='kubectl config use-context k8s-uat' alias k2p='kubectl config use-context k8s-pro' 3、设置别名永久生效 Windows ①创建bat脚本cmdalias.cmd @doskey k=kubectl $* @doskey k2d=kubectl config use-context k8s-dev @doskey k2t=kubectl config use-context k8s-test @doskey k2u=kubectl config use-context k8s-uat @doskey k2p=kubectl config use-context k8s-pro # @表示执行这条命令时不显示这条命令本身 ②修改注册表 方式1：手动在注册HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor下添加一项AutoRun，把值设为bat脚本的路径 方式2：创建编写一个注册表修改文件，名为：add-regkey.reg，双击行这个文件,导入注册表添加的值 Windows Registry Editor Version 5.00 [HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor] \"AutoRun\"=\"%USERPROFILE%\\\\.kube\\\\cmdalias.cmd\" Linux echo \"alias k='kubectl'\" >> /etc/profile && \\ echo \"alias k2d='kubectl config use-context k8s-dev'\" >> /etc/profile && \\ echo \"alias k2t='kubectl config use-context k8s-test'\" >> /etc/profile && \\ echo \"alias k2u='kubectl config use-context k8s-uat'\" >> /etc/profile && \\ echo \"alias k2p='kubectl config use-context k8s-pro'\" >> /etc/profile && \\ source /etc/profile 六、Kubectl Config命令详解 1. If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes place. 2. If $KUBECONFIG environment variable is set, then it is used a list of paths (normal path delimitting rules for your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the last file in the list. 3. Otherwise, ${HOME}/.kube/config is used and no merging takes place. Usage: kubectl config SUBCOMMAND [options] Available Commands: current-context Displays the current-context delete-cluster Delete the specified cluster from the kubeconfig delete-context Delete the specified context from the kubeconfig get-clusters Display clusters defined in the kubeconfig get-contexts Describe one or many contexts rename-context Renames a context from the kubeconfig file. set Sets an individual value in a kubeconfig file set-cluster Sets a cluster entry in kubeconfig set-context Sets a context entry in kubeconfig set-credentials Sets a user entry in kubeconfig unset Unsets an individual value in a kubeconfig file use-context Sets the current-context in a kubeconfig file view Display merged kubeconfig settings or a specified kubeconfig file 七、Kubectl常用操作 1、节点维护 # 1. 设置节点不可被调度 kubectl cordon 节点名 # 2. 驱逐节点上的POD kubectl drain 节点名 --delete-local-data --ignore-daemonsets --force # --delete-local-data 即使pod使用了emptyDir也删除 # --ignore-daemonsets 忽略deamonset控制器的pod，如果不忽略，deamonset控制器控制的pod被删除后可能马上又在此节点上启动起来,会成为死循环； # --force 不加force参数只会删除该NODE上由ReplicationController, ReplicaSet, DaemonSet,StatefulSet or Job创建的Pod，加了后还会删除'裸奔的pod'(没有绑定到任何replication controller) # 3. 下掉节点 kubectl delete node stg-k8s-node1 # 4. 修复操作 # 5. 设置节点可被调度 kubectl uncordon 节点名 2、Deployment历史版本管理 # 查看版本历史版本 kubectl rollout history deployment test # 查看指定历史版本的详细信息 kubectl rollout history deployment test --revision=1 # 回滚到上一个版本 kubectl rollout undo deployment/test # 回滚到指定版本 kubectl rollout undo deployment/test --to-revision=2 # 查看Deployment的发布状态 kubectl rollout status deploy/test 3、直接修改资源对象参数 kubectl set image deployment/test nginx=nginx:1.15 # 直接更新所有容器镜像 kubectl set image daemonset abc *=nginx:1.9.1 4、运行测试容器 # 创建POD kubectl run nginx --image=nginx --port=80 --labels=\"app=nginx\" kubectl expose pod nginx --name=nginx --port=80 --target-port=80 --protocol=TCP # 创建Deployment kubectl create deployment nginx --image=nginx --port=80 --replicas=1 kubectl expose deployment nginx --port=80 --target-port=80 5、等待验证k8s资源pod部署状态 kubectl apply -f k8s-job-application.yaml kubectl -n $NAMESPACE wait --for=condition=complete --timeout=50s job/$CI_PROJECT_NAME || exit_code=$? | \\ if [ $exit_code -ne 0 ];then ROLLBACK_ID=$(kubectl -n $NAMESPACE rollout undo deployment/$CI_PROJECT_NAME -ojson | jq -r '.status.observedGeneration') ; curl -s https://oapi.dingtalk.com/robot/send?access_token=\"$PIPELINE_DINGDING_ROBOT_TOKEN\" -H 'Content-Type: application/json' -d '{\"msgtype\": \"markdown\",\"markdown\": {\"title\": \"Gitlab流水线部署失败\",\"text\": \"['$CI_PROJECT_NAME-cronjob']('$CI_PROJECT_URL'/-/tree/'$CI_BUILD_REF_NAME')的'$APPENV'环境第['$CI_PIPELINE_ID']('$CI_PIPELINE_URL')号流水线'$CI_JOB_STAGE'阶段失败，已回滚至最近一个稳定版本'$ROLLBACK_ID'，请检查相关错误！\"},\"at\": {\"isAtAll\": true}}' > /dev/null; kubectl -n $NAMESPACE logs -l job=$CI_PROJECT_NAME ; kubectl -n $NAMESPACE delete job $CI_PROJECT_NAME ; exit 1; fi 6、kubectl apply从标准输入中读取资源文件 cat 7、kubectl patch更新k8s资源 Kubectl patch 支持以下 3 种 patch 类型： strategic patch(默认)：根据不同字段 patchStrategy 决定具体的合并 patch 策略。 Strategic merge patch 并非通用的 RFC 标准，而是 Kubernetes 特有的一种更新 Kubernetes 资源对象的方式。与 JSON merge patch 和 JSON patch 相比，strategic merge patch 更为强大。 例如：给deployment添加configmap类型的 Volume 声明与挂载 kubectl patch deployment nginx \\ -p '{\"spec\":{\"template\":{\"spec\":{\"volumes\":[{\"name\":\"nginx-stubstatus-config\",\"configMap\":{\"name\":\"nginx-stubstatus-config\"}}],\"containers\":[{\"name\":\"nginx\",\"volumeMounts\":[{\"name\":\"nginx-stubstatus-config\",\"mountPath\":\"/etc/nginx/conf.d\"}]}]}}}}' json merge patch：遵循 JSON Merge Patch, RFC 7386[1] 规范，根据 patch 中提供的期望更改的字段及其对应的值，更新到目标中。 json patch：遵循 JSON Patch, RFC 6902[2] 规范，通过明确的指令表示具体的操作。 例如：给Service暴露的端口添加名字 kubectl patch service nginx-exporter -n kube-public \\ --type='json' \\ -p='[{\"op\": \"add\", \"path\": \"/spec/ports/0/name\", \"value\": \"application-metrics-exporter\"}]' 参考连接 https://blog.csdn.net/u013360850/article/details/83315188 https://www.awaimai.com/2445.html https://stackoverflow.com/questions/44686568/tell-when-job-is-complete Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:43:06 "},"origin/kubernetes-NetworkPolicy.html":{"url":"origin/kubernetes-NetworkPolicy.html","title":"Network Policy容器流量管理","keywords":"","body":"Kubernetes Network Policy容器网络流量限制 一、Overview Kubernetes使用命名空间namesapce做多租户隔离，但是如果不配置网络策略，namespace的隔离也仅仅是作用于在kubernetes编排调度时的隔离，实际上不同namespace下的pod还是可以相互联通的。此时就需要使用Kubernetes提供的networkPolicy,用于隔离不同租户应用的网络流量来减少攻击面。它使用标签选择器模拟传统的分段网络，并通过策略控制它们之间的流量以及来自外部的流量。Kubernetes中的Network Policy只定义了规范，并没有提供实现，而是把实现留给了网络插件 Kubernetes v1.7+版本GA，API版本为http://networking.k8s.io/v1 二、使用Calico做Network policy Network Policy的实现仰赖CNI插件的支持，目前已经支持的cni插件包括： Calico Kube-router Romana Weave Net trireme Calico的NetworkPolicy功能支持以下特性： 支持多种endpoint: pods/containers, VMs 支持限制入站和出站的流量访问 规则策略支持: 动作: allow, deny, log, pass 源和目标的匹配标准: 端口: numbered, ports in a range, and Kubernetes named ports 协议: TCP, UDP, ICMP, SCTP, UDPlite, ICMPv6, protocol numbers (1-255) HTTP attributes (if using Istio service mesh) ICMP attributes IP version (IPv4, IPv6) IP or CIDR Endpoint selectors (using label expression to select pods, VMs, host - interfaces, and/or network sets) Namespace selectors Service account selectors Optional packet handling controls: disable connection tracking, apply before DNAT, apply to forwarded traffic and/or locally terminated traffic Preflight k8s集群版本大于v1.3.0 calico-cni网络插件的二进制文件 kubelet添加配置Flag 需要配置kubelet 让pod启动时使用calico网络插件，kubelet可以配置使用calico在启动时配置参数： --network-plugin=cni --cni-conf-dir=/etc/cni/net.d # CNI插件的配置文件目录,该目录下的配置文件内容需要符合CNI规范 --cni-bin-dir=/opt/cni/bin # CNI插件的可执行文件目录，默认为/opt/cni/bin API Server添加配置Flag --allow-privileged=true # calico-node的POD需要以特权模式运行在各node上 三、策略规则的声明配置 Network Policy策略规则是用来定义命名空间有哪些POD，能被谁访问，能访问谁的。相应地，在Network Policy声明文件中的字段有： spec.podSelector: 定义该命名空间有哪些POD遵循本networkpolicy约束 spec.ingress.from: 定义受本networkpolicy约束的POD的入站规则 spec.egress.to：定义受本networkpolicy约束的POD的出站规则 spec.ingress.from的选择器有： podSelector namespaceSelector ...上文省略... ingress: - from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client ...下文省略... namespaceSelector和podSelector（注意YAML语法的区别） ...上文省略... ingress: - from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client ...下文省略... ipBlock 四、示例说明 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: # POD选择器，选择遵循本networkpolicy约束的POD podSelector: matchLabels: role: db # 流量访问策略类型 policyTypes: - Ingress - Egress # 流量访问入站规则 ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 # POD流量访问出站规则 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978 上述例子，网络流量访问策略规则可解释为： \"default\"命名空间下标签为\"role=db\"的POD的入站规则： 允许被\"default\"命名空间下，所有带标签\"role=frontend\"的POD访问TCP 6379端口 允许被标签为\"project=myproject\"的命名空间下所有的POD访问TCP 6379端口 允许被IP地址为172.17.0.0–172.17.0.255或172.17.2.0–172.17.255.255的POD访问TCP 6379端口 \"default\"命名空间下标签为\"role=db\"的POD的出站规则： 允许访问10.0.0.0/24网段的POD的5978端口 五、默认策略 默认情况下，如果namespace下没有network policy,则该namespace下所有POD的入站规则和出站规则都是开放的。network policy只影响命名空间下被Pod Selector选择的POD，其他依旧是默认规则。 namespace下的所有pod，入站规则为全部禁止 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny namespace: test spec: podSelector: {} policyTypes: - Ingress namespace下的所有pod，入站规则为全部开放 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all namespace: test spec: podSelector: {} ingress: - {} policyTypes: - Ingress namespace下的所有pod，出站规则为全部禁止 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny namespace: test spec: podSelector: {} policyTypes: - Egress namespace下的所有pod，出站规则为全部开放 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all namespace: test spec: podSelector: {} egress: - {} policyTypes: - Egress 同namespace的pod，入站和出站规则为全部禁止 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: {} policyTypes: - Ingress - Egress 六、场景测试 限制服务只能被带有特定label的应用访问 $ kubectl create ns test1 $ kubectl -n test1 run nginx --image=nginx $ kubectl -n test1 expose deployment nginx --port=80 kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: test1 name: access-nginx spec: podSelector: matchLabels: run: nginx ingress: - from: - podSelector: matchLabels: access: \"true\" # 创建没有Label的临时POD去访问Nginx的SVC $ kubectl -n test1 run busybox --rm -ti --image=busybox /bin/sh / # wget http://nginx.test1.svc:80/ (无法访问) # 创建没有Label的临时POD去访问Nginx的SVC $ kubectl -n test1 run busybox --labels=\"access=true\" --rm -ti --image=busybox /bin/sh / # wget http://nginx.test1.svc:80/ Connecting to nginx.test1.svc:80 (10.68.86.216:80) saving to 'index.html' index.html 100% | *********************************************************************************************************************| 612 0:00:00 ETA 'index.html' saved 限制带\"run=busybox\"标签的Pod只能访问www.baidu.com kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: busybox-egress-baidu-a-policy namespace: test1 spec: podSelector: matchLabels: run: busybox egress: - to: - ipBlock: cidr: 180.101.49.11/32 - to: - ipBlock: cidr: 0.0.0.0/0 ports: - protocol: UDP port: 53 $ kubectl -n test1 run busybox --labels=\"run=busybox\" --rm -ti --image=busybox /bin/sh / # wget www.baidu.com Connecting to www.baidu.com (180.101.49.11:80) saving to 'index.html' index.html 100% |*********************************************************************************************************************| 2381 0:00:00 ETA 'index.html' saved / # wget www.sohu.com Connecting to www.sohu.com (101.227.172.11:80) ^C / # wget https://github.com Connecting to github.com (13.250.177.223:443) ^C / # 参考链接 https://kubernetes.io/docs/concepts/services-networking/network-policies/#sctp-support https://docs.projectcalico.org/v3.8/security/kubernetes-network-policy#best-practice-create-deny-all-default-network-policy https://yq.aliyun.com/articles/640190 https://www.jianshu.com/p/c0d2618d2849 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/k8s-pressure-test.html":{"url":"origin/k8s-pressure-test.html","title":"k8s集群的压力测试","keywords":"","body":"K8S集群压力测试 参考 https://kubestone.io/en/latest/quickstart/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/k8s-cluster-node-autoscaler.html":{"url":"origin/k8s-cluster-node-autoscaler.html","title":"TODO：k8s集群节点自动伸缩容","keywords":"","body":"k8s集群节点自动伸缩容 一、简介 自建k8s集群，使用阿里云的伸缩组服务进行集群节点的自动伸缩容。在使用kubernetes的AutoScaler服务监控k8s节点资源使用状态，调用阿里云的API触发集群ECS伸缩容 由于node弹性伸缩存在一定的时延，这个时延主要包含：采集时延(分钟级) + 判断时延(分钟级) + 伸缩时延(分钟级)结合业务，根据时间段，自动伸缩业务(CronHPA)来处理高峰数据，底层自动弹性伸缩kubernetes node增大容器资源池 https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/alicloud 二、准备节点的模板镜像 1、创建一个按量付费的ECS节点 OS：Ubuntu 20.04.5 硬件：8核16G 磁盘：100G系统盘 200G数据盘 2、准备ECS节点的基础环境 ①优化系统参数 # 一次性设置文件句柄量限制 sysctl -w fs.inotify.max_user_watches=\"81920\" # 永久性设置文件数量限制 echo \"fs.inotify.max_user_watches = 81920\" >>/etc/sysctl.conf ②安装基础软件 apt update && apt install -y lvm2 socat nfs-common docker.io ③创建基础文件路径、挂载NFS mkdir -p /log/app/{test,stg} # 挂载NFS apt install -y nfs-common echo \"192.168.1.111:/k8s/ /mnt nfs vers=3,nolock,proto=tcp,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,_netdev,noresvport 0 0\" >>/etc/fstab # mount -a ④安装docker apt install -y docker.io socat # 下载docker配置 wget http://nexus.curiouser.com:8081/repository/resources/k8s/daemon.json -P /etc/docker/ # 启动docker systemctl daemon-reload systemctl enable docker systemctl restart docker docker info ⑤下载解压sealos资源包 wget http://nexus.curiouser.com:8081/repository/resources/k8s/kube1.17.0.tar.gz -P /root tar -zxvf /root/kube1.17.0.tar.gz -C /root/ # 执行检测脚本 cd /root/kube/shell/ sh init.sh rm -rf /root/kube ⑥下载SSH密钥对 wget http://nexus.curiouser.com:8081/repository/resources/k8s/id_ed25519 -P /root/.ssh chmod 600 /root/.ssh/id_ed25519 wget http://nexus.curiouser.com:8081/repository/resources/k8s/id_ed25519.pub -P /root/.ssh chmod 600 /root/.ssh/id_ed25519.pub cat /root/.ssh/id_ed25519.pub >>/root/.ssh/authorized_keys ⑦安装采集日志的组件 wget http://nexus.curiouser.com:8081/repository/resources/elk-install-packages/filebeat-7.5.1-amd64.deb -P /root dpkg -i /root/filebeat-7.5.1-amd64.deb mv /etc/filebeat/filebeat.yml /etc/filebeat/filebeat.yml.bak wget http://nexus.curiouser.com:8081/repository/resources/k8s/filebeat.yml -P /etc/filebeat rm -rf /root/filebeat-7.5.1-amd64.deb systemctl enable filebeat ⑧清理 rm -rf /var/lib/apt/lists/* 3、创建ECS节点的镜像 三、阿里云的ESS 1、创建阿里云伸缩配置 2、创建阿里云伸缩组 获取伸缩组的ASG_ID 3、创建阿里云子账号授予 https://help.aliyun.com/document_detail/185763.html 创建子账号，获取子账号的AccessKey ID和 AccessKey Secret 授予子账号AliyunESSFullAccess的系统权限 创建以下自定义权限策略，并授予子账号 { \"Version\": \"1\", \"Statement\": [ { \"Action\": [ \"ess:Describe*\", \"ess:CreateScalingRule\", \"ess:RemoveInstances\", \"ess:ExecuteScalingRule\", \"ess:ModifyScalingRule\", \"ess:DetachInstances\", \"ecs:DescribeInstanceTypes\" ], \"Resource\": [ \"*\" ], \"Effect\": \"Allow\" } ] } 参考 https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler/cloudprovider/alicloud https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md https://www.jianshu.com/p/02304dd32056 https://developer.aliyun.com/article/392170 https://help.aliyun.com/document_detail/169655.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-12-05 15:15:54 "},"origin/k8s-pod-autoscaler.html":{"url":"origin/k8s-pod-autoscaler.html","title":"HPA：POD自动伸缩容","keywords":"","body":"K8S 容器自动伸缩容 一、简介 HPA(Horizontal Pod Autoscaler)是kubernetes的一种资源对象，能够根据某些指标对在statefulset、replicacontroller、replicaset等集合中的pod数量进行动态伸缩，使运行在上面的服务对指标的变化有一定的自适应能力。 HPA目前支持四种类型的指标，分别是Resource、Object、External、Pods。其中在稳定版本autoscaling/v1只支持对CPU指标的动态伸缩，在测试版本autoscaling/v2beta2中支持memory和自定义指标的动态伸缩，并以annotation的方式工作在autoscaling/v1版本中。 k8s API Resource Version： autoscaling/v1：只支持基于CPU指标的缩放。 autoscaling/v2beta1：支持Resource Metrics（资源指标，如pod的CPU）和Custom Metrics（自定义指标）的缩放。 autoscaling/v2beta2：支持Resource Metrics（资源指标，如pod的CPU）和Custom Metrics（自定义指标）和ExternalMetrics（额外指标）的缩放。 HPA Controller默认30s轮询一次（可通过 kube-controller-manager 的--horizontal-pod-autoscaler-sync-period 参数进行设置），查询指定的资源中的 Pod 资源使用率，并且与创建时设定的值和指标做对比，从而实现自动伸缩的功能。 2、伸缩过程 扩容过程 当开始测试30秒后，HPA检测到测试容器CPU使用率 扩容POD的个数是根据的倍数进行设置的 缩容过程 默认需要等待5分钟后才会开始自动缩容。 从 Kubernetes v1.12 版本开始我们可以通过设置 kube-controller-manager 组件的--horizontal-pod-autoscaler-downscale-stabilization 参数来设置一个持续时间，用于指定在当前操作完成后，HPA 必须等待多长时间才能执行另一次缩放操作。 实测在5~6分钟后开始自动缩容 自动删除新创建的扩容POD。保留原始deployment创建的POD 3、HPA伸缩流程 HPA的主要伸缩流程如下： 判断当前Pod数量是否在HPA设定的Pod数量空间中，如果不在，过小返回最小值，过大返回最大值，结束伸缩。 判断指标的类型，并向api server发送对应的请求，拿到设定的监控指标。一般来说指标会从下面系列聚合API中获取(metrics.k8s.io，custom.metrics.k8s.io和external.metrics.k8s.io)。其中metrics.k8s.io一般由kubernetes自带的metrics-server来提供，主要是cpu、memory使用率指标。另外两种需要第三方的adapter来提供。custom.metrics.k8s.io提供的自定义指标数据，一般与kubernetes集群有关，比如跟特定的pod相关。external.metrics.k8s.io同样提供自定义指标数据，但一般与kubernetes集群无关，许多知名的第三方监控平台提供了adapter实现上述api(如prometheus)，可以将监控和adapter一同部署在kubenetes集群中提供服务。甚至能够替换原来的metrics-server来提供上述三类api指标，达到深度定制监控数据的目标。 根据获取的指标，使用相关的算法计算出一个伸缩系数，并乘以当前pod数量以获得期望的pod数量。这里系数是指标的期望值与目前值的比值，如果大于1表示扩容，小于1表示缩容。指数数值有平均值(AverageValue)、平均使用率(Utilization)、裸值(Value)三种类型 每种类型的数值都有对应的算法。注意下面事项：如果系数有小数点，统一进一；系数如果未达到某个容忍值，HPA认为变化太小，会忽略这次变化，容忍值默认为0.1。 这里HPA扩容算法比较保守，如果出现获取不到指标的情况，扩容时算最小值，缩容时算最大值。如果需要计算平均值，出现pod没准备好的情况，我们保守地假设尚未就绪的pods消耗了试题指标的0%，从而进一步降低了伸缩的幅度。 一个HPA支持多个指标的监控，HPA会循环获取所有的指标，并计算期望的pod数量，并从期望结果中获得最大的pod数量作为最终的伸缩的pod数量。一个伸缩对象在k8s中允许对应多个HPA，但是只是k8s不会报错而已，事实上HPA彼此不知道自己监控的是同一个伸缩对象，在这个伸缩对象中的pod会被多个HPA无意义地来回修改pod数量，给系统增加消耗，如果想要指定多个监控指标，可以如上述所说，在一个HPA中添加多个监控指标。 检查最终pod数量是否在HPA设定的pod数量范围的区间，如果超过最大值或不足最小值都会修改为最大值或者最小值。然后会向kubernetes发出请求，修改伸缩对象的子对象scale的pod数量，结束一个HPA的检查，获取下一个HPA，完成一个伸缩流程。 二、测试验证 1. 基于CPU进行扩缩容 1.1 创建测试应用容器 apiVersion: apps/v1 kind: Deployment metadata: name: hpa-demo namespace: hpa spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 resources: requests: memory: 50Mi cpu: 50m 1.2 创建暴露服务访问的SVC kubectl expose deployment hpa-demo --port=80 --target-port=80 1.3 创建HPA规则资源对象 kubectl autoscale deployment hpa-demo --cpu-percent=10 --min=1 --max=10 # 最小的 Pod 副本数为1，最大为10。HPA 会根据设定的 cpu 使用率（10%）动态的增加或者减少 Pod 数量。 或者通过YAML方式创建HPA资源 apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: hpa-demo namespace: hpa spec: maxReplicas: 10 minReplicas: 1 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: hpa-demo targetCPUUtilizationPercentage: 10 1.4 测试 创建测试容器 kubectl run -n hpa -it --image busybox test-hpa --restart=Never --rm /bin/sh # 测试命令 date && while true; do wget -q -O- http://hpa-demo.hpa.svc > /dev/null ; done 1.5 扩缩容过程 # 观察hpa控制器扩缩容动作 kubectl get hpa test-hpa --watch # 获取负载pod的指标 kubectl top pod -l app=nginx 2. 基于内存进行扩缩容 2.1 创建测试应用容器 --- apiVersion: v1 kind: ConfigMap metadata: name: increase-mem-config namespace: hpa data: increase-mem.sh: | #!/bin/bash mkdir /tmp/memory mount -t tmpfs -o size=40M tmpfs /tmp/memory dd if=/dev/zero of=/tmp/memory/block sleep 60 rm /tmp/memory/block umount /tmp/memory rmdir /tmp/memory --- apiVersion: apps/v1 kind: Deployment metadata: name: hpa-mem-demo namespace: hpa spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: volumes: - name: increase-mem-script configMap: name: increase-mem-config containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: increase-mem-script mountPath: /etc/script resources: requests: memory: 100Mi cpu: 50m securityContext: privileged: true 2.2 创建HPA规则资源对象 --- apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: hpa-mem-demo namespace: hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: hpa-mem-demo minReplicas: 1 maxReplicas: 5 metrics: - type: Resource resource: name: memory target: type: Utilization averageValue: 50Mi 2.3 测试 kubectl exec -it hpa-mem-demo容器 bash source /etc/script/increase-mem-script 2.4 扩缩容测试 三、影响扩容响应时间的因素 1、POD的监控获取时间 默认30s 2、扩容POD的启动时间 POD镜像的拉取时间 按最大的镜像1-2G的大小，最多20~30s POD健康检测的时间 1~3分钟 参考： https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#autoscaling-during-rolling-update https://www.sobyte.net/post/2021-11/k8s-hpa-usage/ https://meixuhong.com/how-to-make-hpa-policy-based-on-basic-metrics-in-kubernetes.html https://www.qikqiak.com/post/k8s-hpa-usage https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale https://kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-24 22:36:01 "},"origin/kubernetes-serviceaccount-rbac.html":{"url":"origin/kubernetes-serviceaccount-rbac.html","title":"用户认证ServiceAccount与授权策略RBAC","keywords":"","body":"K8S安全之认证与授权策略机制 一、简介 官方文档：https://kubernetes.io/docs/reference/access-authn-authz/rbac/ 还是得先从kubernetes集群角色说起 ETCD：存储所有k8s资源状态数据 API Server：对外暴露操作ETCD等REST API接口 Kubernetes的API Server有众多的资源REST API接口，同时还有众多依赖API Server进行操作的集群组件，例如Controller Manager等。API Server为了保护API请求的合法性。API Serve内部需要先验证请求的权限。需要验证 目前Kubernetes支持的授权策略有： ABAC：基于属性的访问控制 RBAC：基于角色的访问控制 Webhook： Node AlwaysDeny：拒绝所有的请求，一般用于测试。 AlwaysAllow：表示允许所有的请求，不进行认证授权。（默认配置） 从1.6版本起，Kubernetes 默认启用RBAC访问控制策略。从1.8开始，RBAC已成为稳定的功能。API Server启用RABC需要设置启动参数–-authorization-mode=RBAC，。 二、RBAC API 资源对象 Kubernetes的RBAC认证授权策略使用rbac.authorization.k8s.io API组 Role ClusterRole RoleBinding ClusterRoleBind 三、应用 1、限制客户端用户只能访问或操作指定命名空间的特定资源 场景用户 用户角色 限制Namespace 限制资源对象 语义化的限制动作 开发者 developer test pod、configmap 只能查看pod/configmap能登录到pod中进行操作 ①创建RBAC相关的资源声明文件 ServiceAccount ClusterRole RoleBinding ClusterRoleBinding #=========================ServiceAccount====================== --- apiVersion: v1 kind: ServiceAccount metadata: name: developer namespace: default # 或者使用命令：kubectl -n default create sa developer #=========================ClusterRole====================== --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: developer rules: - apiGroups: - \"\" resources: - pods - pods/attach - pods/exec - pods/log - pods/status - configmaps verbs: - get - list - watch - apiGroups: - \"\" resources: - services verbs: - get - list - watch - apiGroups: - \"\" resources: - pods/exec verbs: - create # Portforward - apiGroups: - \"\" resources: - pods - pods/portforward verbs: - get - list - create #=========================RoleBinding====================== --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: developer namespace: test roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: developer subjects: - kind: ServiceAccount name: developer namespace: default # 或者使用命令：kubectl create rolebinding developer --clusterrole=developer --serviceaccount=default:developer --namespace=test #=========================ClusterRoleBinding====================== --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: developer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: developer subjects: - kind: ServiceAccount name: developer namespace: default ②获取serviceaccount的secret k -n default get secrets k -n default get secrets developer-token-** -oyaml 获得如下secrets的详细内容 apiVersion: v1 type: kubernetes.io/service-account-token kind: Secret metadata: namespace: default data: ca.crt: ******12345678******** # API Server服务端的CA证书 namespace: ZGVmYXVsdA== # 该字符串为“default”base64转码后的值 token: *******ABCDEF********* # 该token是经过base64处理的，需要进行解码处理 base64解码secret中的Token echo \"*******ABCDEF*********\" | base64 -d ③组装kubeconfig apiVersion: v1 clusters: - cluster: certificate-authority-data: ******12345678******** server: https://master.k8s117.curiouser.com:8443 name: k8s117 contexts: - context: cluster: k8s117 namespace: test user: k8s117-developer name: k8s117-developer current-context: k8s117-developer kind: Config preferences: {} users: - name: k8s117-developer user: token: \"*******ABCDEF*********base64解码后的值\" ④测试 看是否能获取所有的命名空间（不能） 看是否能查看test命名空间下的所有POD和ConfigMap（能） 看是否能登录到test命名空间下的POD（能） 看是否能使用kube-proxy端口转发test命名空间下POD的端口（能） Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:42:29 "},"origin/kubernetes-helm.html":{"url":"origin/kubernetes-helm.html","title":"K8S应用管理工具Helm","keywords":"","body":"Helm简介、安装、配置、使用 一、简介 Helm 是 Kubernetes 的包管理器。包管理器类似于我们在 Ubuntu 中使用的apt、Centos中使用的yum 或者Python中的 pip 一样，能快速查找、下载和安装软件包。Helm 由客户端组件 helm 和服务端组件 Tiller 组成, 能够将一组K8S资源打包统一管理, 是查找、共享和使用为Kubernetes构建的软件的最佳方式。 Helm3之前是C/S架构的。主要分为客户端 helm 和服务端 Tiller。Tiller负责对charts的解析生成k8s资源声明文件，然后调用k8s api进行部署。同时还保存chart部署的版本信息。 Helm3移除了 Tiller，直接在客户端就对charts进行解析，调用k8s api部署资源声明文件。同时将charts release的版本信息保存至对应k8s应用部署所在命名空间下的secret中。(例如：名为sh.helm.release.v1.sentry-kubernetes-events.v1 helm.sh/release.v1类型的secret) 全面拥抱Helm3 二、安装 Github下载地址：https://github.com/helm/helm/releases 1、二进制包安装 下载二进制文件解压至系统环境路径下即可。 命令脚本 curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh # 或者 curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash 2、包管理器安装 Brew brew install helm 3、源码编译安装 $ cd $GOPATH $ mkdir -p src/helm.sh $ cd src/helm.sh $ git clone https://github.com/helm/helm.git $ cd helm $ make 三、配置 helm3默认读取当前用户目录下～/.kube/config文件中的当前k8s环境上下文来配置部署charts到哪个k8s集群。相关权限跟随着kuectl配置的用户权限。（开箱即用的感觉） 1、配置helm的环境变量 Name Description $XDG_CACHE_HOME set an alternative location for storing cached files. $XDG_CONFIG_HOME set an alternative location for storing Helm configuration. $XDG_DATA_HOME set an alternative location for storing Helm data. $HELM_DRIVER set the backend storage driver. Values are: configmap, secret, memory $HELM_NO_PLUGINS disable plugins. Set HELM_NO_PLUGINS=1 to disable plugins. $KUBECONFIG set an alternative Kubernetes configuration file (default \"~/.kube/config\") 2、Helm相关文件存储的默认路径 cached文件都存在$XDG_CACHE_HOME/helm 配置文件存在 $XDG_CONFIG_HOME/helm 数据文件存在$XDG_DATA_HOME/helm 3、各个操作操作系统的默认配置 操作系统 Cache文件路径 配置文件路径 数据文件路径 Linux $HOME/.cache/helm $HOME/.config/helm $HOME/.local/share/helm macOS $HOME/Library/Caches/helm $HOME/Library/Preferences/helm $HOME/Library/helm Windows %TEMP%\\helm %APPDATA%\\helm %APPDATA%\\helm 4、命令行的命令补全 helm completion zsh source 四、charts的管理 全局通用的命令行参数 --add-dir-header 添加文件路径到Header中 --alsologtostderr log to standard error as well as files --debug 输出Debug级别的日志 --kube-context string 指定使用哪个kubeconfig context --kubeconfig string 指定kubeconfig文件路径 --log-backtrace-at traceLocation when logging hits line file:N, emit a stack trace (default :0) --log-dir string 指定日志输出到哪个路径下 --log-file string 指定日志输出到哪个文件中 --log-file-max-size uint Defines the maximum size a log file can grow to. Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800) --logtostderr log to standard error instead of files (default true) -n, --namespace string 指定在哪个K8S命名空间下进行操作 --registry-config string path to the registry config file (default \"/Users/curiouser/Library/Preferences/helm/registry.json\") --repository-cache string path to the file containing cached repository indexes (default \"/Users/curiouser/Library/Caches/helm/repository\") --repository-config string path to the file containing repository names and URLs (default \"/Users/curiouser/Library/Preferences/helm/repositories.yaml\") --skip-headers If true, avoid header prefixes in the log messages --skip-log-headers If true, avoid headers when opening log files --stderrthreshold severity logs at or above this threshold go to stderr (default 2) -v, --v Level number for the log level verbosity --vmodule moduleSpec comma-separated list of pattern=N settings for file-filtered logging 1、远程Charts仓库的管理 添加远程charts仓库 helm repo add 远程仓库别名 https://kubernetes-charts-incubator.storage.googleapis.com/ 查看当前所有的远程charts仓库 helm repo list 删除指定的远程charts仓库 helm repo rm/remove 远程仓库别名 查看远程仓库中的所有charts helm search repo 查看Github中的所有charts helm search hub 2、Charts的管理 从远程仓库中下载Charts到本地 helm pull 远程仓库别名/chart名 参数项 # 参数项 --ca-file string verify certificates of HTTPS-enabled servers using this CA bundle --cert-file string identify HTTPS client using this SSL certificate file -d/--destination string location to write the chart. If this and tardir are specified, tardir is appended to this (default \".\") --devel use development versions, too. Equivalent to version '>0.0.0-0'. If --version is set, this is ignored. -h/--help help for pull --key-file string identify HTTPS client using this SSL key file --keyring string location of public keys used for verification (default \"/Users/curiouser/.gnupg/pubring.gpg\") --password string chart repository password where to locate the requested chart --prov fetch the provenance file, but don't perform verification --repo string chart repository url where to locate the requested chart --untar 下载后解压 --untardir string 下载后解压到指定目录(默认是当前路径\".\") --username string chart repository username where to locate the requested chart --verify verify the package before installing it --version string specify the exact chart version to install. If this is not specified, the latest version is installed # 支持全局通用参数 五、部署Charts到k8s集群 命令格式 helm install [NAME] [CHART] [参数项] # 参数项 --atomic 原子部署。当charts部署失败时，所有操作进行回滚删除。同时如果设置该参数， 一并的\"--wait\"也会被设置 --ca-file string verify certificates of HTTPS-enabled servers using this CA bundle --cert-file string identify HTTPS client using this SSL certificate file --dependency-update 在部署前更新charts依赖 --description string 添加自定义描述 --devel use development versions, too. Equivalent to version '>0.0.0-0'. If --version is set, this is ignored --disable-openapi-validation if set, the installation process will not validate rendered templates against the Kubernetes OpenAPI Schema --dry-run 模拟部署 -g, --generate-name generate the name (and omit the NAME parameter) -h, --help 显示帮助信息 --key-file string identify HTTPS client using this SSL key file --keyring string location of public keys used for verification (default \"/Users/curiouser/.gnupg/pubring.gpg\") --name-template string specify template used to name the release --no-hooks prevent hooks from running during install -o, --output format 指定日志输出的格式（可选项table, json, yaml 默认是table) --password string 远程chart仓库用户的密码 --post-renderer postrenderer the path to an executable to be used for post rendering. If it exists in $PATH, the binary will be used, otherwise it will try to look for the executable at the given path (default exec) --render-subchart-notes if set, render subchart notes along with the parent --replace re-use the given name, only if that name is a deleted release which remains in the history. This is unsafe in production --repo string 设置远程chart仓库的url --set stringArray 设置vaules。(覆盖values.yaml中的值可设置多个，以“,”分割。例如 key1=val1,key2=val2) --set-file stringArray 从文件中读取va luset values from respective files specified via the command line (can specify multiple or separate values with commas: key1=path1,key2=path2) --set-string stringArray set STRING values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --skip-crds if set, no CRDs will be installed. By default, CRDs are installed if not already present --timeout duration time to wait for any individual Kubernetes operation (like Jobs for hooks) (默认5分0秒) --username string 远程chart仓库的用户名 -f, --values strings 指定values文件或URL(可设置多个) --verify verify the package before installing it --version string specify the exact chart version to install. If this is not specified, the latest version is installed --wait 设置等待charts涉及的k8s资源变为ready状态的时间才认为部署成功。它的值等 同timeout设置的值例如Pods, PVCs, Services, Deployment的最少POD数, StatefulSet, or ReplicaSet ） # 支持全局通用参数 1、部署远程仓库中的charts到k8s集群 helm install 部署名 远程仓库别名/chart名 参数项 2、部署本地的Charts到k8s集群 helm install 部署名 -f values.yaml . 3、更新charts的部署 helm upgrade charts的部署名 -f values.yaml . # 参数项 --atomic 原子更新。当charts更新部署失败时，所有操作进行回滚删除。同时如果设置该参 数，一并的\"--wait\"也会被设置 --ca-file string verify certificates of HTTPS-enabled servers using this CA bundle --cert-file string identify HTTPS client using this SSL certificate file --cleanup-on-fail allow deletion of new resources created in this upgrade when upgrade fails --description string 添加自定义描述 --devel use development versions, too. Equivalent to version '>0.0.0-0'. If --version is set, this is ignored --dry-run 模拟更新部署 --force force resource updates through a replacement strategy -h, --help 显示帮助信息 --history-max int limit the maximum number of revisions saved per release. Use 0 for no limit (default 10) -i, --install 如果指定的chart部署名不存在，就直接安装 --key-file string identify HTTPS client using this SSL key file --keyring string 指定验证时公钥的路径(默认当前用户路径下的.gnupg/pubring.gpg\") --no-hooks disable pre/post upgrade hooks -o, --output format 指定日志输出的格式（可选项table, json, yaml 默认是table) --password string 远程chart仓库用户的密码 --post-renderer postrenderer the path to an executable to be used for post rendering. If it exists in $PATH, the binary will be used, otherwise it will try to look for the executable at the given path (default exec) --render-subchart-notes if set, render subchart notes along with the parent --repo string 设置远程chart仓库的url --reset-values when upgrading, reset the values to the ones built into the chart --reuse-values when upgrading, reuse the last release's values and merge in any overrides from the command line via --set and -f. If '--reset-values' is specified, this is ignored --set stringArray 设置vaules。(覆盖values.yaml中的值可设置多个，以“,”分割。例如 key1=val1,key2=val2) --set-file stringArray set values from respective files specified via the command line (can specify multiple or separate values with commas: key1=path1,key2=path2) --set-string stringArray set STRING values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --timeout duration time to wait for any individual Kubernetes operation (like Jobs for hooks) (默认5分0秒) --username string 远程chart仓库的用户名 -f, --values strings 指定values文件或URL(可设置多个) --verify verify the package before installing it --version string specify the exact chart version to install. If this is not specified, the latest version is installed --wait 设置等待charts涉及的k8s资源变为ready状态的时间才认为部署成功。它的值等 同timeout设置的值例如Pods, PVCs, Services, Deployment的最少POD数, StatefulSet, or ReplicaSet ） # 支持全局通用参数 4、删除部署charts的资源 默认删除charts涉及的所有资源和charts的发布版本 helm del/uninstall/del/delete/un charts的部署名 参数项 # 参数项 --description string 添加自定义描述 --dry-run 模拟删除 -h, --help 显示帮助信息 --keep-history 删除charts涉及的所有资源，然后标记该charts的发布为删除状态，但保留删除历史 --no-hooks prevent hooks from running during uninstallation --timeout duration time to wait for any individual Kubernetes operation (like Jobs for hooks) (默认5m0s) # 支持全局通用参数 六、其他操作 1、value文件中的List数组配置映射到命令行 set中 # values.yaml中参数 globalArguments: - \"--api.disabledashboardad=false\" - \"--global.checknewversion=false\" - \"--global.sendanonymoususage=false\" - \"--api.insecure=false\" - \"--accesslog=true\" - \"--accesslog.fields.names.accesslog\" - \"--accesslog.fields.headers.defaultmode=keep\" - \"--accesslog.filepath=/data/400-599-reponse-json.log\" - \"--accesslog.format=json\" - \"--accesslog.filters.statuscodes=400-599\" # 映射为 set参数值 helm upgrade --install traefik-ingress-controller \\ --version 24.0.0 \\ --namespace kube-system \\ --set ports.traefik.hostPort=9000 \\ --set deployment.replicas=2 \\ --set globalArguments=\"{\"--api.disabledashboardad=false\",\"--global.sendanonymoususage=false\",\"--global.checknewversion=false\",\"--accesslog=true\",\"--accesslog.fields.names.accesslog\",\"--accesslog.fields.headers.defaultmode=keep\",\"--accesslog.filepath=/data/400-599-reponse-json.log\",\"--accesslog.format=json\",\"--accesslog.filters.statuscodes=400-599\"}\" \\ --set service.type=ClusterIP \\ --set hostNetwork=true \\ traefik/traefik 2、value文件中的对象数组配置映射到命令行 set中 # values.yaml中参数 server: ingress: hosts: - host: chart-example.local paths: [] # 映射为 set参数值 helm upgrade --install vault --namespace tools hashicorp/vault \\ --set \"server.ingress.enabled=true\" \\ --set \"server.ingress.hosts[0].host=vault.test.com\" 3、value文件中的完整对象数组配置映射到命令行 set中 extraObjects: - apiVersion: v1 kind: Service metadata: name: traefik-api spec: type: ClusterIP selector: app.kubernetes.io/name: traefik app.kubernetes.io/instance: traefik-default ports: - port: 8080 name: traefik targetPort: 9000 protocol: TCP - apiVersion: v1 kind: Secret metadata: name: traefik-dashboard-auth-secret type: kubernetes.io/basic-auth stringData: username: admin password: changeme - apiVersion: traefik.io/v1alpha1 kind: Middleware metadata: name: traefik-dashboard-auth spec: basicAuth: secret: traefik-dashboard-auth-secret - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: traefik-dashboard annotations: traefik.ingress.kubernetes.io/router.entrypoints: websecure traefik.ingress.kubernetes.io/router.middlewares: default-traefik-dashboard-auth@kubernetescrd helm upgrade --install --atomic traefik-ingress-controller \\ --version 24.0.0 \\ --namespace kube-system \\ --set extraObjects[0].apiVersion=v1 \\ --set extraObjects[0].kind=Service \\ --set extraObjects[0].metadata.name=traefik-api \\ --set extraObjects[0].spec.type=ClusterIP \\ --set extraObjects[0].spec.ports[0].port=8080 \\ --set extraObjects[0].spec.ports[0].name=traefik \\ --set extraObjects[0].spec.ports[0].targetPort=9000 \\ --set extraObjects[0].spec.ports[0].protocol=TCP \\ --set extraObjects[0].spec.selector.\"app\\.kubernetes\\.io\\/name\"=\"traefik\" \\ --set extraObjects[0].spec.selector.\"app\\.kubernetes\\.io\\/instance\"=\"traefik-default\" \\ --set extraObjects[1].apiVersion=v1 \\ --set extraObjects[1].kind=Secret \\ --set extraObjects[1].metadata.name=traefik-dashboard-auth-secret \\ --set extraObjects[1].type=kubernetes.io/basic-auth \\ --set extraObjects[1].stringData.username=admin \\ --set extraObjects[1].stringData.password=changeme \\ --set extraObjects[2].apiVersion=traefik.io/v1alpha1 \\ --set extraObjects[2].kind=Middleware \\ --set extraObjects[2].metadata.name=traefik-dashboard-auth \\ --set extraObjects[2].spec.basicAuth.secret=traefik-dashboard-auth-secret \\ --set extraObjects[3].apiVersion=networking.k8s.io/v1 \\ --set extraObjects[3].kind=Ingress \\ --set extraObjects[3].metadata.name=traefik-dashboard \\ --set extraObjects[3].spec.rules[0].host=traefik-dashboard.test.com \\ --set extraObjects[3].spec.rules[0].http.paths[0].path=/ \\ --set extraObjects[3].spec.rules[0].http.paths[0].pathType=Prefix \\ --set extraObjects[3].spec.rules[0].http.paths[0].backend.service.name=traefik-api \\ --set extraObjects[3].spec.rules[0].http.paths[0].backend.service.port.name=traefik \\ --set extraObjects[3].metadata.annotations.\"traefik\\.ingress\\.kubernetes\\.io\\/router\\.entrypoints\"=\"websecure\" \\ --set extraObjects[3].metadata.annotations.\"traefik\\.ingress\\.kubernetes\\.io\\/router\\.middlewares\"=\"default-traefik-dashboard-auth@kubernetescrd\" \\ traefik/traefik 参考：https://stackoverflow.com/questions/59632924/how-to-set-annotations-for-a-helm-install Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:44:44 "},"origin/Kubernetes-helm-charts编写规则.html":{"url":"origin/Kubernetes-helm-charts编写规则.html","title":"helm charts编写规则","keywords":"","body":"Helm Charts编写规则 一、Charts的文件目录结构 Prerequisite：初始化一个模板charts helm create charts名 以创建一个test charts为例进行说明 helm create test # 会在当前路径下产生以下目录结构的文件 test/ Chart.yaml # Charts的描述信息。例如作者信息，使用的k8s api版本等 LICENSE # Charts的许可证等信息(可选) README.md # Charts的README(可选) requirements.yaml # Charts的依赖管理文件(可选) values.yaml # charts的默认配置项 charts/ # 存放Charts所依赖的其他charts templates/ # 存放Charts的k8s资源声明文件模板 ｜-- NOTES.txt # 简短使用说明文件(可选) 二、Chart.yaml Chart.yaml 文件是编写Charts 所必需的。描述Chart的描述信息 apiVersion: The chart API version (必须) name: Chart名 (必须) version: Chart的版本信息 (必须) kubeVersion: A SemVer range of compatible Kubernetes versions (可选) description: 一句描述chart的话 (可选) type: chart类型 (可选) keywords: - chart关健字 (可选) home: The URL of this project's home page (可选) sources: - chart的源代码仓库URL(可选) dependencies: # 依赖的chart (可选) - name: 依赖的chart名 version: 依赖的chart版本 repository: The repository URL (\"https://example.com/charts\") or alias (\"@repo-name\") condition: (可选) A yaml path that resolves to a boolean, used for enabling/disabling charts (e.g. subchart1.enabled ) tags: # (可选) - Tags can be used to group charts for enabling/disabling together enabled: (可选) Enabled bool determines if chart should be loaded import-values: # (可选) - ImportValues holds the mapping of source values to parent key to be imported. Each item can be a string or pair of child/parent sublist items. alias: (可选) Alias usable alias to be used for the chart. Useful when you have to add the same chart multiple times maintainers: # (可选) - name: The maintainer's name (必须 for each maintainer) email: The maintainer's email (可选 for each maintainer) url: A URL for the maintainer (可选 for each maintainer) icon: A URL to an SVG or PNG image to be used as an icon (可选). appVersion: The version of the app that this contains (可选). This needn't be SemVer. 三、 参考 https://github.com/whmzsu/helm-doc-zh-cn/blob/master/chart/charts-zh_cn.md Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/Service与SpringBoot应用启动参数冲突的问题排查及解决方案.html":{"url":"origin/Service与SpringBoot应用启动参数冲突的问题排查及解决方案.html","title":"Service与SpringBoot应用启动参数冲突的问题排查及解决方案","keywords":"","body":"kubernetes Service与SpringBoot应用启动参数冲突 上下文 部署Springboot应用到kubernetes集群 创建了名为\"server\"的Service 描述 1.部署到kubernetes集群时,原因容器报错日志如下 ```bash Caused by: org.springframework.core.convert.ConversionFailedException: Failed to convert from type [java.lang.String] to type [java.lang.Integer] for value 'tcp://192.168.1.55:8080'; nested exception is java.lang.NumberFormatException: For input string: \"tcp://192.168.1.55:8080\" ``` 2.当单独使用Docker部署时,则不报错 3.报错表现为SpringBoot框架无法将字符串“tcp://192.168.1.55:8080”转换为数字类型 原因 SpringBoot应用会读取POD中系统环境变量\"SERVER_PORT\"的值作为应用监听的端口，值应为数字类型 而当该应用在kubernetes中的Service名字命名为\"server\"时,kubernetes会默认在该命名空间下所有的POD中注入一个名叫\"SERVER_PORT=tcp://service-ip地址:镜像Dockerfile暴露出来的端口\"环境变量(该值为字符类型，报错示例中的为\"SERVER_PORT=tcp://192.168.1.55:8080\")来进行POD间的服务发现 解决方案 禁止SpringBoot应用部署在kubernetes中的Service名字命名为“server”，建议命名为“项目名-应用名” 在部署Deployment声明文件中添加\"SERVER_PORT=8080\"进行覆盖默认值 附录: Kubernetes中的服务发现 创建Pod资源时，kubelet会将其所属名称空间内的每个活动的Service对象以一系列环境变量的形式注入其中。它支持使用Kubernetes Service环境变量以及与Docker的links兼容的变量。 简单来说，服务发现就是服务或者应用之间互相定位的过程。不过，服务发现并非什么新概念，传统的单体应用架构时代也会用到，只不过单体应用的动态性不强，更新和重新发布频度较低，通常以月甚至以年计，基本不会进行自动伸缩，因此服务发现的概念无须显性强调。在传统的单体应用网络位置发生变化时，由IT运维人员手工更新一下相关的配置文件基本就能解决问题。但在微服务应用场景中，应用被拆分成众多的小服务，它们按需创建且变动频繁，配置信息基本无法事先写入配置文件中并及时跟踪反映动态变化，服务发现的重要性便随之凸显。 服务发现机制的基本实现，一般是事先部署好一个网络位置较为稳定的服务注册中心（也称为服务总线），服务提供者（服务端）向注册中心注册自己的位置信息，并在变动后及时予以更新，相应地，服务消费者则周期性地从注册中心获取服务提供者的最新位置信息从而“发现”要访问的目标服务资源。复杂的服务发现机制还能够让服务提供者提供其描述信息、状态信息及资源使用信息等，以供消费者实现更为复杂的服务选择逻辑。 实践中，根据其发现过程的实现方式，服务发现还可分为两种类型：客户端发现和服务端发现。 客户端发现：由客户端到服务注册中心发现其依赖到的服务的相关信息，因此，它需要内置特定的服务发现程序和发现逻辑。 服务端发现：这种方式额外要用到一个称为中央路由器或服务均衡器的组件；服务消费者将请求发往中央路由器或者负载均衡器，由它们负责查询服务注册中心获取服务提供者的位置信息，并将服务消费者的请求转发给服务提供者。 由此可见，服务注册中心是服务发现得以落地的核心组件。事实上，DNS可以算是最为原始的服务发现系统之一，不过，在服务的动态性很强的场景中，DNS记录的传播速度可能会跟不上服务的变更速度，因此它不并适用于微服务环境。另外，传统实践中，常见的服务注册中心是ZooKeeper和etcd等分布式键值存储系统。不过，它们只能提供基本的数据存储功能，距离实现完整的服务发现机制还有大量的二次开发任务需要完成。另外，它们更注重数据一致性，这与有着更高的服务可用性要求的微服务发现场景中的需求不太相吻合。 Netflix的Eureka是目前较流行的服务发现系统之一，它是专门开发用来实现服务发现的系统，以可用性目前为先，可以在多种故障期间保持服务发现和服务注册的功能可用，其设计原则遵从“存在少量的错误数据，总比完全不可用要好”。另一个同级别的实现是Consul，它是由HashiCorp公司提供的商业产品，不过还有一个开源基础版本提供。它于服务发现的基础功能之外还提供了多数据中心的部署能力等一众出色的特性。 尽管传统的DNS系统不适于微服务环境中的服务发现，但SkyDNS项目（后来称kubedns）却是一个有趣的实现，它结合古老的DNS技术和时髦的Go语言、Raft算法并构建于etcd存储系统之上，为Kubernetes系统实现了一种服务发现机制。Service资源为Kubernetes提供了一个较为稳定的抽象层，这有点类似于服务端发现的方式，于是也就不存在DNS服务的时间窗口的问题。 Kubernetes自1.3版本开始，其用于服务发现的DNS更新为了kubeDNS，而类似的另一个基于较新的DNS的服务发现项目是由CNCF（Cloud Native Computing Foundation）孵化的CoreDNS，它基于Go语言开发，通过串接一组实现DNS功能的插件的插件链进行工作。自Kubernetes 1.11版本起，CoreDNS取代kubeDNS成为默认的DNS附件。不过，Kubernetes依然支持使用环境变量进行服务发现。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/k8s-redis-sentinel.html":{"url":"origin/k8s-redis-sentinel.html","title":"K8S上redis主从哨兵模式问题的解决方案","keywords":"","body":"k8s上redis主从哨兵模式的问题 一、上下文 主从哨兵模式 master 节点扩容 slave 节点扩容 二、问题 k8s集群外客户端无法连接至redis 集群节点 master pod故障重建后IP地址更换，无法加入原先的集群 三、解决方案 1、固定Redis PODIP地址 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-11-02 10:44:34 "},"origin/kubernetes-tools.html":{"url":"origin/kubernetes-tools.html","title":"辅助工具","keywords":"","body":"Kubernetes辅助工具 一、VSCode的Kubernetes插件 二、Electron公司的Kube Forwarder 下载地址：http://kube-forwarder.pixelpoint.io Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/k8s-multi-clusters.html":{"url":"origin/k8s-multi-clusters.html","title":"Doing：K8S 多集群管理与网络互联","keywords":"","body":"K8S多集群管理 一、Karmada多集群管理方案 Karmada：https://github.com/karmada-io/karmada 文档：https://karmada.io/zh/docs/ 架构：Karmada 的总体架构如下所示： Karmada 控制平面包括以下组件： Karmada API Server Karmada Controller Manager Karmada Scheduler ETCD 存储了 karmada API 对象，API Server 是所有其他组件通讯的 REST 端点，Karmada Controller Manager 根据您通过 API 服务器创建的 API 对象执行操作。 Karmada Controller Manager 在管理面运行各种 Controller，这些 Controller 监视 karmada 对象，然后与成员集群的 API Server 通信以创建常规的 Kubernetes 资源。 Cluster Controller：将 Kubernetes 集群连接到 Karmada，通过创建集群对象来管理集群的生命周期。 Policy Controller：监视 PropagationPolicy 对象。当添加 PropagationPolicy 对象时，Controller 将选择与 resourceSelector 匹配的一组资源，并为每个单独的资源对象创建 ResourceBinding。 Binding Controller：监视 ResourceBinding 对象，并为每个带有单个资源清单的集群创建一个 Work 对象。 Execution Controller：监视 Work 对象。当创建 Work 对象时，Controller 将把资源分发到成员集群。 二、Submariner多集群网络互联方案 Github：https://github.com/submariner-io/submariner 文档：https://submariner.io/getting-started/ 架构： Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:45:13 "},"origin/vm-on-k8s.html":{"url":"origin/vm-on-k8s.html","title":"VM On K8S","keywords":"","body":"KubeVirt：VM on K8S 一：简介 Github：https://github.com/kubevirt/kubevirt 文档：https://kubevirt.io/user-guide/architecture/ 原理：使用 k8s的调度、网络、存储等功能在 k8s node节点运行、管理 KVM 虚拟机 暂不调研。需要 k8s 的节点开起虚拟化功能。目前测试 K8s 集群服务器在云服务商上。 二、安装 1、部署operator export VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases | grep tag_name | grep -v -- '-rc' | sort -r | head -1 | awk -F': ' '{print $2}' | sed 's/,//' | xargs) echo $VERSION kubectl create -f \"https://github.com/kubevirt/kubevirt/releases/download/$VERSION/kubevirt-operator.yaml\" namespace/kubevirt customresourcedefinition.apiextensions.k8s.io/kubevirts.kubevirt.io priorityclass.scheduling.k8s.io/kubevirt-cluster-critical clusterrole.rbac.authorization.k8s.io/kubevirt.io:operator serviceaccount/kubevirt-operator role.rbac.authorization.k8s.io/kubevirt-operator rolebinding.rbac.authorization.k8s.io/kubevirt-operator-rolebinding clusterrole.rbac.authorization.k8s.io/kubevirt-operator clusterrolebinding.rbac.authorization.k8s.io/kubevirt-operator deployment.apps/virt-operator 2、部署 CRD kubectl create -f \"https://github.com/kubevirt/kubevirt/releases/download/$VERSION/kubevirt-cr.yaml\" 7 pods 3 services 1 daemonset 3 deployment apps 3 replica sets 3、安装 virtctl VERSION=$(kubectl get kubevirt.kubevirt.io/kubevirt -n kubevirt -o=jsonpath=\"{.status.observedKubeVirtVersion}\") ARCH=$(uname -s | tr A-Z a-z)-$(uname -m | sed 's/x86_64/amd64/') || windows-amd64.exe echo ${ARCH} curl -L -o virtctl \"https://github.com/kubevirt/kubevirt/releases/download/$VERSION/virtctl-$VERSION-$ARCH\" chmod +x virtctl mv virtctl /usr/local/bin Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:30:04 "},"origin/cicd-summary.html":{"url":"origin/cicd-summary.html","title":"CICD优化总结","keywords":"","body":"CICD优化总结 一、Gitlab pipeline + k8s 1、gitlab代码仓库的命名 如果是要设计通配类型的CICD流程，那代码仓库的命名对于后续CICD中的脚本至关重要（可以通过Gitlab Runner中的环境变量获取当前仓库的名字、commit id、提交者信息等信息）。例如以下场景： 可以根据仓库名，作为该应用在k8s中资源标识，如Deployment的名字，Configmap的名字等等。 可以根据日期+Commit ID作为镜像的版本 config.sh #!/bin/sh if [ ! -n \"$1\" ]; then echo 'must input file name ' exit 1 else cat $1 | sed 's/\\$NAMESPACE'\"/$NAMESPACE/g\" | sed 's~\\$IMAGE_NAME'\"~$IMAGE_NAME~g\" | sed 's~\\$APPENV'\"~$APPENV~g\" | sed 's~\\$POD_NUM'\"~$POD_NUM~g\" | sed 's~\\$CI_PROJECT_NAME'\"~$CI_PROJECT_NAME~g\" | cat - fi exit 0 k8s-application.tpl.yaml apiVersion: apps/v1 kind: Deployment metadata: name: $CI_PROJECT_NAME namespace: $NAMESPACE annotations: kubernetes.io/change-cause: $IMAGE_NAME labels: app: $CI_PROJECT_NAME spec: selector: matchLabels: app: $CI_PROJECT_NAME replicas: $POD_NUM template: metadata: labels: app: $CI_PROJECT_NAME spec: containers: - name: app image: $IMAGE_NAME imagePullPolicy: IfNotPresent .... 使用上述脚本可以替换指定环境变量到k8s模板资源文件中，从而生成最终的部署资源文件 - ./config.sh ./k8s-application.tpl.yaml > k8s-application.yaml 2、K8S部署版本说明 Deployment或者statefulset添加kubernetes.io/change-cause的注解用以标注发布变更历史。可以使用镜像版本号作为说明，也可以时间戳等。 ... metadata: annotations: kubernetes.io/change-cause: $IMAGE_NAME ... $ kubectl rollout history deployment test-nginx REVISION CHANGE-CAUSE 1 harbor.curiouser.com/test-nginx/stg:20210908-a3das215 2 harbor.curiouser.com/test-nginx/stg:20210908-8020cdfh 3、验证部署状态并增加回退通知流程 在CD步骤中，直接k8s资源配置文件apply发布到k8s集群中，要检测是否已部署成功，pod个数、状态是否已到预期。不成功，要自动回滚到上一个版本，并发送钉钉通知 - kubectl -n $NAMESPACE rollout status --timeout=50s deployment/$CI_PROJECT_NAME || exit_code=$? - | if [ $exit_code -ne 0 ];then ROLLBACK_ID=$(kubectl -n $NAMESPACE rollout undo deployment/$CI_PROJECT_NAME -ojson | jq -r '.status.observedGeneration') ; curl -s https://oapi.dingtalk.com/robot/send?access_token=\"$PIPELINE_DINGDING_ROBOT_TOKEN\" -H 'Content-Type: application/json' -d '{\"msgtype\": \"markdown\",\"markdown\": {\"title\": \"Gitlab流水线部署失败\",\"text\": \"['$CI_PROJECT_NAME']('$CI_PROJECT_URL'/-/tree/'$CI_BUILD_REF_NAME')的'$APPENV'环境第['$CI_PIPELINE_ID']('$CI_PIPELINE_URL')号流水线'$CI_JOB_STAGE'阶段失败，已回滚至最近一个稳定版本'$ROLLBACK_ID'，请检查相关错误！\"},\"at\": {\"isAtAll\": true}}' > /dev/null; exit 1; fi 4、关键信息的隐藏保护 CICD脚本中禁止直接使用用户名密码 通过变量引用的用户名密码也要禁止显示，加以保护 引用的CICD基础镜像中也要禁止明文使用用户名密码 涉及到要保护的密码、密钥： 内网Harbor登录用户名密码或Token：CI脚本中上传构建好的应用镜像或者下载内部的基础应用运行镜像等。 内网Nexus登录用户名密码或Token：一般会在Maven管理的JAVA项目POM文件中会使用到 服务器主机登录SSH公私钥：一般 Gitlab API Token或用户名密码、部署SSH公私钥： 内网SonarQube服务端Token或密码 5、Gitlab的安全设置 ①禁止使用HTTP协议作为git访问协议，只允许SSH。管理员-设置-通用-可见性与访问控制-启用 Git 访问协议-选择\"only ssh\" ②禁止自动注册用户，必须有管理员创建邀请。管理员-设置-通用-注册限制-不勾选\"已启用注册功能\"和\"新的注册需要管理员批准\" ③设置gitlab runner只能在组项目使用，而组只能管理员才能创建项目 ④禁止用户创建仓库， Account and limit 中的 Default projects limit 设置为 0 ⑤禁止用户创建任何级别的仓库，Restricted visibility levels 全选 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-19 21:46:10 "},"origin/jenkins-api.html":{"url":"origin/jenkins-api.html","title":"Jenkins API","keywords":"","body":"Jenkins API 一、简介 Jenkins API支持以下3种格式： XML JSON并支持JSONP跨域访问 Python https://jenkins-host/api/ Jenkins API没有统一的入口，而是采用“…/api/” 的REST API样式，其中”…” 表示Jenkins资源的URL API类型 说明 JobsAPI 任务管理（任务信息、创建、修改） PluginManagerAPI 插件管理（插件信息、安装插件） QueueAPI 任务队列相关（队列状态） StatisticsAPI Jenkins统计信息 CrumbIssuerAPI 系统哈希值信息（用于防御CSRF攻击） SystemAPI Jenkins系统状态（版本、路径） Jenkins 使用 Baisc Auth 的权限验证方式，需要传入 username 和 api token 。 但在 Job 的远程触发中，可以设置用于远程触发的 token (在 Job 的配置页面设置)，这样在触发 Job 时就不需要传入 Basic Auth 了。 远程触发的 token 使用 urlencode 的方式放在请求的 body 中，其原始数据为： token= Basic Auth curl -X POST /view//job//build --user : Token curl -X POST /view//job//build --data-urlencode token= _class\": \"org.jenkinsci.plugins.workflow.job.WorkflowJob\" \"_class\": \"hudson.model.FreeStyleProject\" 二、API 1. 创建Job 2. 创建视图 3. 触发Jo构建 4. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-SharedLibraries.html":{"url":"origin/jenkins-SharedLibraries.html","title":"Jenkins共享库Shared Libraries","keywords":"","body":"Jenkins Pipeline共享库Shared Libraries 一、简介 随着Job的增多和pipeline的功能越来越复杂，Pipeline代码冗余度高。所以可以将一些公共的pipeline抽象做成模块代码，在各种项目pipeline之间共享核心实现，同时可以放到SVM中进行版本控制。以减少冗余并保证所有job在构建的时候会调用最新的共享库代码。这时就可以用到pipline的共享库Shared Libraries功能。 模块化 可重用性 二、共享库的目录结构 共享库根目录 |-- vars |-- test1.groovy |-- src |-- test2.groovy |-- resources vars: 依赖于Jenkins运行环境的Groovy脚本。其中的Groovy脚本被称之为全局变量。 src: 标准的Java源码目录结构,其中的Groovy脚本被称为类库(Library class)。该目录所有下的所有类都一次性静态被添加到类路径classpath下 resources: 目录允许从外部库中使用 libraryResource 步骤来加载有关的非 Groovy 文件。 目前，内部库不支持该特性 三、配置全局共享库 可在Jenkins中的Manage Jenkins –> Configure System　–> Global Pipeline Libraries 添加一个或多个全局的共享库，同时也可以在构建过程中的任何位置使用library step动作动态地配置引用共享库，详见动态引用共享库 四、引用共享库 1. 引用全局共享库 格式：@Library('my-shared-library-1@$Branch/Tag','my-shared-library-1@$Branch/Tag') _ #!groovy // 引用默认配置的共享库 @Library('demo-shared-library') _ // 引用指定分支、tag的共享库代码 @Library('demo-shared-library@1.0') _ // 引用多个指定分支tag的共享库 @Library('demo-shared-library@$Branch/Tag','demo-shared-library-test@$Branch/Tag') _ @Library('utils') import org.foo.Utilities @Library('utils') import static org.foo.Utilities.* 2. 动态引用共享库 2.7版本后的Shared Groovy Libraries插件，增加了一个library的setp,可以随时在构建过程中引用共享库 #!groovy library 'demo-shared-library@$BRANCH_NAME' library \"demo-shared-library@${params.LIB_VERSION}\" library('demo-shared-library').com.mycorp.pipeline.Utils.someStaticMethod() // 此时共享库的版本必须指定 library identifier: 'custom-lib@master', retriever: modernSCM( [$class: 'GitSCMSource', remote: 'git@git.mycorp.com:my-jenkins-utils.git', credentialsId: 'my-private-key']) 3. 调用第三方Java库 @Grab('org.apache.commons:commons-math3:3.4.1') import org.apache.commons.math3.primes.Primes 引用完的第三方Java库后会缓存在Jenkins Master节点的~/.groovy/grapes/ 目录下 五、全局变量和类库的编写规则和调用方法 1. /var下定义的全局变量 全局变量必须以全小写或驼峰（camelCased）命名以便于能够在流水线中正确的加载 /vars目录中的脚本根据需求以单例的方式实例化，这允许在单个.groovy` 文件中定义多个方法 /vars/*.groovy若实现call()方法，直接引用时默认执行其中的方法，该方法可以让全局变量以一种以类似于step的方式被调用 /vars/log.groovy #!groovy def call(String name = 'human') { echo \"Hello, ${name}.\" } def info(message) { echo \"INFO: ${message}\" } def warning(message) { echo \"WARNING: ${message}\" } Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ log() // 输出\"Hello, human.\" log.info 'Starting' // 输出\"INFO: Starting\" log.warning 'Nothing to do!' // 输出\"WARNING: Nothing to do!\" 从2017年9月下旬发布的声明式 1.2开始，可以在全局变量中直接定义声明式流水线 /vars/evenOrOdd.groovy #!groovy def call(int buildNumber) { if (buildNumber % 2 == 0) { pipeline { agent any stages { stage('Even Stage') { steps { echo \"The build number is even\" } } } } } else { pipeline { agent any stages { stage('Odd Stage') { steps { echo \"The build number is odd\" } } } } } } Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ evenOrOdd(currentBuild.getNumber()) 全局变量的传参 /vars/buildPlugin.groovy #!groovy def call(Map config) { node { git url: \"https://github.com/jenkinsci/${config.name}-plugin.git\" sh 'mvn install' mail to: '...', subject: \"${config.name} plugin build\", body: '...' } } Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ buildPlugin name: 'git' 声明式流水线不允许在script指令之外使用全局变量 Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ pipeline { agent none stage ('Example') { steps { script { log.info 'Starting' log.warning 'Nothing to do!' } } } } 2. /src下定义的类库 类库不能直接调用 sh或 git这样的步骤。 但是他们可以在封闭的类的范围之外实现方法，从而调用流水线步骤 /src/org/foo/Zot.groovy #!groovy package org.foo; def checkOutFrom(repo) { git url: \"git@github.com:jenkinsci/${repo}\" } return this Jenkinsfile #!groovy @Library('demo-shared-library@1.0') _ def z = new org.foo.Zot() z.checkOutFrom(repo) 类库中使用”class“声明父类 /src/org/foo/Utilities.groovy package org.foo class Utilities implements Serializable { def steps Utilities(steps) {this.steps = steps} def mvn(args) { steps.sh \"${steps.tool 'Maven'}/bin/mvn -o ${args}\" } } Jenkinsfile @Library('utils') import org.foo.Utilities def utils = new Utilities(this) node { utils.mvn 'clean package' } 类库中的方法访问流水线中的变量 /src/org/foo/Utilities.groovy package org.foo class Utilities { static def mvn(script, args) { script.sh \"${script.tool 'Maven'}/bin/mvn -s ${script.env.HOME}/jenkins.xml -o ${args}\" } } Jenkinsfile @Library('utils') import static org.foo.Utilities.* node { mvn this, 'clean package' } 六、Jenkins Pipeline生成器生成动态引用共享库的代码 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-cli-command.html":{"url":"origin/jenkins-cli-command.html","title":"Jenkins 命令行接口","keywords":"","body":"Jenkins命令行接口 一、简介 1、下载命令行Jar包 下载CLI的Jar包：http://Jenkins地址/jnlpJars/jenkins-cli.jar 2、使用语法 java -jar jenkins-cli.jar [-s URL] 子命令 [opts...] args... 3、参数 -s URL : the server URL (defaults to the JENKINS_URL env var) -http : use a plain CLI protocol over HTTP(S) (the default; mutually exclusive with -ssh) -webSocket : like -http but using WebSocket (works better with most reverse proxies) -ssh : use SSH protocol (requires -user; SSH port must be open on server, and user must have registered a public key) -i KEY : SSH private key file used for authentication (for use with -ssh) -noCertificateCheck : bypass HTTPS certificate check entirely. Use with caution -noKeyAuth : don't try to load the SSH authentication private key. Conflicts with -i -user : specify user (for use with -ssh) -strictHostKey : request strict host key checking (for use with -ssh) -logger FINE : enable detailed logging from the client -auth [ USER:SECRET | @FILE ] : specify username and either password or API token (or load from them both from a file); for use with -http. Passing credentials by file is recommended. See https://www.jenkins.io/redirect/cli-http-connection-mode for more info and options. -bearer [ TOKEN | @FILE ] : specify authentication using a bearer token (or load the token from file); for use with -http. Mutually exclusive with -auth. Passing credentials by file is recommended. 4、子命令 子命令 功能描述 add-job-to-view Adds jobs to view. apply-configuration Apply YAML configuration to instance build Builds a job, and optionally waits until its completion. cancel-quiet-down Cancel the effect of the \"quiet-down\" command. check-configuration Check YAML configuration to instance clear-queue Clears the build queue. connect-node Reconnect to a node(s) console Retrieves console output of a build. copy-job Copies a job. create-credentials-by-xml Create Credential by XML create-credentials-domain-by-xml Create Credentials Domain by XML create-job Creates a new job by reading stdin as a configuration XML file. create-node Creates a new node by reading stdin as a XML configuration. create-view Creates a new view by reading stdin as a XML configuration. declarative-linter Validate a Jenkinsfile containing a Declarative Pipeline delete-builds Deletes build record(s). delete-credentials Delete a Credential delete-credentials-domain Delete a Credentials Domain delete-job Deletes job(s). delete-node Deletes node(s) delete-view Deletes view(s). disable-job 禁用任务 disable-plugin Disable one or more installed plugins. disconnect-node Disconnects from a node. enable-job 启用任务 enable-plugin Enables one or more installed plugins transitively. export-configuration Export jenkins configuration as YAML get-credentials-as-xml Get a Credentials as XML (secrets redacted) get-credentials-domain-as-xml Get a Credentials Domain as XML get-job Dumps the job definition XML to stdout. get-node Dumps the node definition XML to stdout. get-view Dumps the view definition XML to stdout. groovy Executes the specified Groovy script. groovysh Runs an interactive groovy shell. help Lists all the available commands or a detailed description of single command. import-credentials-as-xml Import credentials as XML. The output of \"list-credentials-as-xml\" can be used as input here as is, the only needed change is to set the actual Secrets which are redacted in the output. install-plugin Installs a plugin either from a file, an URL, or from update center. keep-build 永久保留这次构建。 list-changes Dumps the changelog for the specified build(s). list-credentials Lists the Credentials in a specific Store list-credentials-as-xml Export credentials as XML. The output of this command can be used as input for \"import-credentials-as-xml\" as is, the only needed change is to set the actual Secrets which are redacted in the output. list-credentials-context-resolvers List Credentials Context Resolvers list-credentials-providers List Credentials Providers list-jobs Lists all jobs in a specific view or item group. list-plugins Outputs a list of installed plugins. mail Reads stdin and sends that out as an e-mail. offline-node Stop using a node for performing builds temporarily, until the next \"online-node\" command. online-node Resume using a node for performing builds, to cancel out the earlier \"offline-node\" command. quiet-down Quiet down Jenkins, in preparation for a restart. Don’t start any builds. reload-configuration Discard all the loaded data in memory and reload everything from file system. Useful when you modified config files directly on disk. reload-jcasc-configuration Reload JCasC YAML configuration reload-job Reload job(s) remove-job-from-view Removes jobs from view. replay-pipeline 从标准输入中获取的脚本并回放流水线执行 restart 重新启动Jenkins restart-from-stage Restart a completed Declarative Pipeline build from a given stage. safe-restart 安全地重新启动Jenkins safe-shutdown Puts Jenkins into the quiet mode, wait for existing builds to be completed, and then shut down Jenkins. session-id Outputs the session ID, which changes every time Jenkins restarts. set-build-description Sets the description of a build. set-build-display-name Sets the displayName of a build. shutdown 立刻关闭Jenkins stop-builds Stop all running builds for job(s) update-credentials-by-xml Update Credentials by XML update-credentials-domain-by-xml Update Credentials Domain by XML update-job Updates the job definition XML from stdin. The opposite of the get-job command. update-node Updates the node definition XML from stdin. The opposite of the get-node command. update-view Updates the view definition XML from stdin. The opposite of the get-view command. version Outputs the current version. wait-node-offline Wait for a node to become offline. wait-node-online Wait for a node to become online. who-am-i Reports your credential and permissions. 二、常用操作 1、导出Job为XML文件 java -jar jenkins-cli.jar -s http://jenkins地址/ -webSocket -auth 用户名:密码 get-job 任务名 > 任务名.xml 批量导出Job脚本 jenkins_host_url=http://jenkins地址 jenkins_user=用户名 jenkins_password=密码 wget $jenkins_host_url/jnlpJars/jenkins-cli.jar for i in `java -jar jenkins-cli.jar -s $jenkins_host_url -auth $jenkins_user:$jenkins_password list-jobs` ;do java -jar jenkins-cli.jar -s $jenkins_host_url -auth $jenkins_user:$jenkins_password get-job $i > $i.xml; done 2、从XML文件导入Job java -jar jenkins-cli.jar -s http://jenkins地址/ -webSocket -auth 用户名:密码 create-job 任务名 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/jenkins-声明式Declarative-pipeline语法.html":{"url":"origin/jenkins-声明式Declarative-pipeline语法.html","title":"声明式Declarative语法","keywords":"","body":"Jenkins声明式Declarative Pipeline 一、语法结构 Jenkins 2.5新加入的pipeline语法 声明式pipeline 基本语法和表达式遵循 groovy语法，但是有以下例外： 声明式pipeline 必须包含在固定格式的pipeline{}中 每个声明语句必须独立一行， 行尾无需使用分号 块(Blocks{}) 只能包含章节(Sections),指令（Directives）,步骤(Steps),或者赋值语句 属性引用语句被视为无参数方法调用。 如input() 一个声明式Pipeline中包含的元素 pipeline：声明这是一个声明式的pipeline脚本 agent：指定要执行该Pipeline的节点（job运行的slave或者master节点） stages：阶段集合，包裹所有的阶段（例如：打包，部署等各个阶段） stage：阶段，被stages包裹，一个stages可以有多个stage steps：步骤,为每个阶段的最小执行单元,被stage包裹 post：执行构建后的操作，根据构建结果来执行对应的操作 示例： pipeline{ // 指定pipeline在哪个slave节点上允许 agent { label 'jdk-maven' } // 指定pipeline运行时的一些配置 option { timeout(time: 1, unit: 'HOURS') } // 自定义的参数 parameters { string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') text(name: 'BIOGRAPHY', defaultValue: '', description: 'Enter some information about the person') booleanParam(name: 'TOGGLE', defaultValue: true, description: 'Toggle this value') choice(name: 'CHOICE', choices: ['One', 'Two', 'Three'], description: 'Pick something') password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'Enter a password') } // 自定义的环境变量 environment { Gitlab_Deploy_KEY = credentials('gitlab-jenkins-depolykey') } // 定义pipeline的阶段任务 stages { stage (\"阶段1任务：拉代码\") { steps { // 拉代码的具体命令 } } stage (\"阶段2任务：编译代码\") { steps { // 编译代码的具体命令 } } stage (\"阶段3任务：扫描代码\") { steps { // 拉代码的具体命令 } } stage (\"阶段4任务：打包代码\") { steps { // 打包代码的具体命令 } } stage (\"阶段5任务：构建推送Docker镜像\") { steps { // 构建推送Docker镜像的具体命令 } } stage (\"阶段6任务：部署镜像\") { steps { // 部署镜像的具体命令 } } } post { success { // 当pipeline构建状态为\"success\"时要执行的事情 } always { // 无论pipeline构建状态是什么都要执行的事情 } } } 二、章节Sections 1、agent（必须） 指定整个Pipeline或特定阶段是在Jenkins Master节点还是Jenkins Slave节点上运行。可在顶级pipeline块和每个stage块中使用（在顶层pipeline{}中是必须定义的 ，但在阶段Stage中是可选的） 参数（以下参数值在顶层pipeline{}和stage{}中都可使用）： any：在任何可用的节点上执行Pipeline或Stage none：当在顶层pipeline{}中应用时，将不会为整个Pipeline运行分配全局代理，并且每个stage部分将需要包含其自己的agent部分 label node docker dockerfile kubernetes 公用参数： label customWorkspace reuseNode args 2、post 定义在Pipeline运行或阶段结束时要运行的操作。具体取决于Pipeline的状态 支持pipeline运行状态: always：无论Pipeline运行的完成状态如何都要运行 changed：只有当前Pipeline运行的状态与先前完成的Pipeline的状态不同时，才能运行 fixed：整个pipeline或者stage相对于上一次失败或不稳定Pipeline的状态有改变。才能运行 regression： aborted：只有当前Pipeline处于“中止”状态时，才会运行，通常是由于Pipeline被手动中止（通常在具有灰色指示的Web UI 中表示） failure：仅当当前Pipeline处于“失败”状态时才运行（通常在Web UI中用红色指示表示） success：仅当当前Pipeline在“成功”状态时才运行（通常在具有蓝色或绿色指示的Web UI中表示） unstable：只有当前Pipeline在不稳定”状态，通常由测试失败，代码违例等引起，才能运行（通常在具有黄色指示的Web UI中表示） unsuccessful： cleanup：无论Pipeline或stage的状态如何，在跑完所有其他的post条件后运行此条件下 的post步骤。 3、stages（必须） 至少包含一个用于执行任务的stage指令 pipeline{ }中只能有一个stages{} 4、steps（必须） 在stage指令中至少包含一个用于执行命令的steps 三、Jenkins中的变量 变量的来源 Jenkins内置的环境变量 构建任务相关的变量 构建状态相关的变量 插件提供的环境变量 pipeline中environment指令定义的变量 脚本自定义的变量 变量的引用 $变量名 ${变量名} ${env.变量名} 变量的处理 ${变量名[0..7]} 变量名.take(8) ${变量名.replace(' and counting', '')} The issue here is caused by the way Jenkins interprets $var inside sh block: if you use \"double quotes\", $var in sh \"... $var ...\" will be interpreted as Jenkins variable; if you use 'single quotes', $var in sh '... $var ...' will be interpreted as shell variable. 参考 https://stackoverflow.com/questions/16943665/how-to-get-git-short-hash-in-to-a-variable-in-jenkins-running-on-windows-2008 https://stackoverflow.com/questions/44007034/conditional-environment-variables-in-jenkins-declarative-pipeline/53771302 四、指令Directives 1、Environment环境变量 environment{…},使用键值对来定义一些环境变量并赋值。它的作用范围，取决environment{…}所写的位置。写在顶层环境变量，可以让所有stage下的step共享这些变量；也可以单独定义在某一个stage下，只能供这个stage去调用变量，其他的stage不能共享这些变量。一般来说，我们基本上上定义全局环境变量，如果是局部环境变量，我们直接用def关键字声明就可以，没必要放environment{…}里面。 同时，environment{…}支持credentials() 方法来访问预先在Jenkins保存的凭据，并赋值给环境变量 credentials() 支持的凭据类型： Secret Text Secret File Username and password：使用变量名_USR and 变量名_PSW 来获取其中的用户名和Password pipeline { agent any stages { stage('Example Username/Password') { environment { SERVICE_CREDS = credentials('my-prefined-username-password') } steps { sh 'echo \"Service user is $SERVICE_CREDS_USR\"' sh 'echo \"Service password is $SERVICE_CREDS_PSW\"' sh 'curl -u $SERVICE_CREDS https://myservice.example.com' } } } } SSH with Private Key pipeline { agent any stages { stage('Example Username/Password') { environment { SSH_CREDS = credentials('my-prefined-ssh-creds') } steps { sh 'echo \"SSH private key is located at $SSH_CREDS\"' sh 'echo \"SSH user is $SSH_CREDS_USR\"' sh 'echo \"SSH passphrase is $SSH_CREDS_PSW\"' } } } } 2、Parameters参数 pipeline{ }中只能有一个parameters{} 参数定义格式 parameters { 参数类型(name: '参数名', defaultValue: '默认值', description: '描述') } 参数类型 string text boobleanParam choice password 参数调用格式：${params.参数名} 示例： pipeline { agent any parameters { string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') text(name: 'BIOGRAPHY', defaultValue: '', description: 'Enter some information about the person') booleanParam(name: 'TOGGLE', defaultValue: true, description: 'Toggle this value') choice(name: 'CHOICE', choices: ['One', 'Two', 'Three'], description: 'Pick something') password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'Enter a password') } stages { stage('Example') { steps { echo \"Hello ${params.PERSON}\" echo \"Biography: ${params.BIOGRAPHY}\" echo \"Toggle: ${params.TOGGLE}\" echo \"Choice: ${params.CHOICE}\" echo \"Password: ${params.PASSWORD}\" } } } } 3、Options选项 pipeline{ }中只能有一个options{} buildDiscarder checkoutToSubdirectory disableConcurrentBuilds disableResume newContainerPerStage overrideIndexTriggers preserveStashes quietPeriod retry skipDefaultCheckout skipStagesAfterUnstable timeout timestamps parallelsAlwaysFailFast 4、Triggers触发器 pipeline{ }中只能有一个triggers {} 触发器类型 cron pollSCM upstream Jenkins的Cron语法 5、Stage阶段(至少有一个) 包含在stages{}中 至少有一个 6、Tools工具 包含在pipeline{}或stage{} 支持的工具： Maven JDK Gradle 7、Input用户输入 8、When条件 内置条件： branch Execute the stage when the branch being built matches the branch pattern given, for example: when { branch 'master' }. Note that this only works on a multibranch Pipeline. buildingTag Execute the stage when the build is building a tag. Example: when { buildingTag() } changelog Execute the stage if the build’s SCM changelog contains a given regular expression pattern, for example: when { changelog '.*^\\[DEPENDENCY\\] .+$' } changeset Execute the stage if the build’s SCM changeset contains one or more files matching the given string or glob. Example: when { changeset \"*/.js\" } By default the path matching will be case insensitive, this can be turned off with the caseSensitive parameter, for example: when { changeset glob: \"ReadMe.*\", caseSensitive: true } changeRequest Executes the stage if the current build is for a \"change request\" (a.k.a. Pull Request on GitHub and Bitbucket, Merge Request on GitLab or Change in Gerrit etc.). When no parameters are passed the stage runs on every change request, for example: when { changeRequest() }. By adding a filter attribute with parameter to the change request, the stage can be made to run only on matching change requests. Possible attributes are id, target, branch, fork, url, title, author, authorDisplayName, and authorEmail. Each of these corresponds to a CHANGE_* environment variable, for example: when { changeRequest target: 'master' }. The optional parameter comparator may be added after an attribute to specify how any patterns are evaluated for a match: EQUALS for a simple string comparison (the default), GLOB for an ANT style path glob (same as for example changeset), or REGEXP for regular expression matching. Example: when { changeRequest authorEmail: \"[\\w_-.]+@example.com\", comparator: 'REGEXP' } environment Execute the stage when the specified environment variable is set to the given value, for example: when { environment name: 'DEPLOY_TO', value: 'production' } equals Execute the stage when the expected value is equal to the actual value, for example: when { equals expected: 2, actual: currentBuild.number } expression Execute the stage when the specified Groovy expression evaluates to true, for example: when { expression { return params.DEBUG_BUILD } } Note that when returning strings from your expressions they must be converted to booleans or return null to evaluate to false. Simply returning \"0\" or \"false\" will still evaluate to \"true\". tag Execute the stage if the TAG_NAME variable matches the given pattern. Example: when { tag \"release-*\" }. If an empty pattern is provided the stage will execute if the TAG_NAME variable exists (same as buildingTag()). The optional parameter comparator may be added after an attribute to specify how any patterns are evaluated for a match: EQUALS for a simple string comparison, GLOB (the default) for an ANT style path glob (same as for example changeset), or REGEXP for regular expression matching. For example: when { tag pattern: \"release-\\d+\", comparator: \"REGEXP\"} not Execute the stage when the nested condition is false. Must contain one condition. For example: when { not { branch 'master' } } allOf Execute the stage when all of the nested conditions are true. Must contain at least one condition. For example: when { allOf { branch 'master'; environment name: 'DEPLOY_TO', value: 'production' } } anyOf Execute the stage when at least one of the nested conditions is true. Must contain at least one condition. For example: when { anyOf { branch 'master'; branch 'staging' } } triggeredBy Execute the stage when the current build has been triggered by the param given. For example: when { triggeredBy 'SCMTrigger' } when { triggeredBy 'TimerTrigger' } when { triggeredBy 'UpstreamCause' } when { triggeredBy cause: \"UserIdCause\", detail: \"vlinde\" } 未完待整理更新 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-07-13 15:16:10 "},"origin/Jenkins-在Kubernetes上使用Kubernetes插件动态创建Slave节点.html":{"url":"origin/Jenkins-在Kubernetes上使用Kubernetes插件动态创建Slave节点.html","title":"Kubernetes Plugin","keywords":"","body":"Jenkins在Kubernetes上使用Kubernetes插件动态创建Slave节点 一、Context 插件GIthub地址：https://github.com/jenkinsci/kubernetes-plugin Jenkins 分布式架构是由一个 Master 和多个 Slave Node组成的分布式架构。在 Jenkins Master 上管理你的项目，可以把你的一些构建任务分担到不同的 Slave Node 上运行，Master 的性能就提高了。Master/Slave相当于Server和agent的概念。Master提供web接口让用户来管理job和slave，job可以运行在master本机或者被分配到slave上运行构建。一个master（jenkins服务所在机器）可以关联多个slave用来为不同的job或相同的job的不同配置来服务。 传统的 Jenkins Slave 一主多从式会存在一些痛点。比如： 主 Master 发生单点故障时，整个流程都不可用了； 每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致 管理起来非常不方便，维护起来也是比较费劲； 资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态； 资源有浪费，每台 Slave 可能是实体机或者 VM，当 Slave 处于空闲状态时，也不会完全释放掉资源。 而使用Kubernetes插件可以在Kubernetes上动态创建slave POD作为Slave节点。Jenkins Master 和 Slave 节点以 Docker Container 形式运行在 Kubernetes 集群的 Node 上，Master 运行在其中一个节点，并且将其配置数据存储到一个 Volume 上去，Slave 运行在各个节点上，并且它不是一直处于运行状态，它会按照需求动态的创建并自动删除。 这种方式的工作流程大致为：当 Jenkins Master 接收到 Build 请求时，会根据配置的 Label 动态创建一个运行在 Docker Container 中的 Jenkins Slave 并注册到 Master 上，当运行完 Job 后，这个 Slave 会被注销并且 Docker Container 也会自动删除，恢复到最初状态。这种方式带来的好处有很多： 服务高可用，当 Jenkins Master 出现故障时，Kubernetes 会自动创建一个新的 Jenkins Master 容器，并且将 Volume 分配给新创建的容器，保证数据不丢失，从而达到集群服务高可用。 动态伸缩，合理使用资源，每次运行 Job 时，会自动创建一个 Jenkins Slave，Job 完成后，Slave 自动注销并删除容器，资源自动释放，而且 Kubernetes 会根据每个资源的使用情况，动态分配 Slave 到空闲的节点上创建，降低出现因某节点资源利用率高，还排队等待在该节点的情况。 扩展性好，当 Kubernetes 集群的资源严重不足而导致 Job 排队等待时，可以很容易的添加一个 Kubernetes Node 到集群中，从而实现扩展。 二、Jenkins与Slave的连接方式 Jenkins的Master/Slave相当于Server和agent的概念。Master提供web接口让用户来管理job和slave，job可以运行在master本机或者被分配到slave上运行。一个master可以关联多个slave用来为不同的job或相同的job的不同配置来服务。当job被分配到slave上运行的时候，此时master和slave其实是建立的双向字节流的连接，其中连接方法主要有如下几种： SSH：Jenkins内置有ssh客户端实现，可以用来与远程的sshd通信，从而启动slave agent。这是对unix系统的slave最方便的方法，因为unix系统一般默认安装有sshd。在创建ssh连接的slave的时候，你需要提供slave的host名字，用户名和ssh证书。创建public/private keys，然后将public key拷贝到slave的~/.ssh/authorized_keys中，将private key 保存到master上某ppk文件中。jenkins将会自动地完成其他的配置工作，例如copy slave agent的binary，启动和停止slave。 Java web start（JNLP：Java Network Lancher Protocol）：jnlp连接方式有个好处就是不用master和slave之间能够ssh连接，只需要能够ping即可。并且如果slave的机器是windows的话，也是可以的这个其实是非常实用的 WMI+DCOM：对于Windows的Slave，Jenkins可以使用Windows2000及以后内置的远程管理功能（WMI+DCOM），你只需要提供对slave有管理员访问权限的用户名和密码，jenkins将远程地创建windows service然后远程地启动和停止他们。对于windows的系统，这是最方便的方法，但是此方法不允许运行有显示交互的GUI程序。 在Kubernetes上的Jenkins通过Kubernetes插件动态创建的Slave POD节点是通过JNLP的方式与Jenkins Master进行通信的！ 三、Jenkins Kubernetes插件的安装配置 安装 配置 四、定制Slave镜像 Slave镜像中安装的软件信息 工具 版本 说明 Oracel JDK 1.8.0_171 Maven编译打包时使用 Apache Maven 3.6.1 在Slave容器中使用MAVEN编译打包源代码 helm v2.13.1 helm客户端 git 1.8.3.1 git命令 docker client 1.13.1 Dockers客户端，用于在Slave容器中构建应用镜像 sonar-scanner 3.3.0.1492 用于扫描源代码 FROM centos:7.4.1708 ENV TZ=Asia/Shanghai \\ LANG=en_US.UTF-8 \\ JDK_VERSION=Oracle_1.8.0_171 \\ MAVEN_VERSION=Apache_3.6.1 \\ HOME=/home/jenkins \\ MAVEN_HOME=/opt/apache-maven-3.6.1 \\ JAVA_HOME=/opt/jdk1.8.0_171 \\ SONARSCANNER_HOME=/opt/sonar-scanner-3.3.0.1492-linux COPY jdk1.8.0_171 /opt/jdk1.8.0_171 COPY apache-maven-3.6.1 /opt/apache-maven-3.6.1 COPY helm /usr/bin/helm COPY sonar-scanner-3.3.0.1492-linux /opt/sonar-scanner-3.3.0.1492-linux RUN curl https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo \\ && yum makecache \\ && yum install -y git docker-ce-cli make \\ && yum clean all \\ && groupadd -g 1000 jenkins \\ && useradd -c \"Jenkins user\" -d /home/jenkins -u 1000 -g 0 -m jenkins \\ && mkdir /home/jenkins/.m2 \\ && chown -R 1000.0 /home/jenkins \\ && ln -s /opt/jdk1.8.0_171/bin/java /usr/bin/java \\ && ln -s /opt/apache-maven-3.6.1/bin/mvn /usr/bin/mvn \\ && ln -s /opt/sonar-scanner-3.3.0.1492-linux/bin/sonar-scanner /usr/bin/sonar-scanner USER jenkins WORKDIR /home/jenkins COPY dumb-init /usr/bin/dumb-init ADD run-jnlp-client /usr/bin/ ENTRYPOINT [\"/usr/bin/dumb-init\", \"--\", \"/usr/bin/run-jnlp-client\"] 五、Pipeline或者Job中使用验证 Job 创建一个自由风格的Job 点击构建后，会自动创建一个Slave POD，并通过JNLP协议与Jenkins Master的Agent端口5000进行通通信 Declarative Pipeline pipeline { agent { label 'maven' } stages { stage (\"代码编译\") { steps { configFileProvider([configFile(fileId: 'nexus-maven-settings', targetLocation: 'settings.xml')]){ sh 'mvn -s settings.xml compile' } } } stage(\"代码扫描\"){ steps{ sh \"sonar-scanner \\ -Dsonar.projectName=demo-springboot2 \\ -Dsonar.projectKey=demo-springboot2 \\ -Dsonar.sources=src \\ -Dsonar.host.url=http://sonarqube.apps.okd311.curiouser.com \\ -Dsonar.login=****** \\ -Dsonar.java.binaries=. \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.java.source=8 \\ -Dsonar.gitlab.project_id=1 \\ -Dsonar.issuesReport.html.enable=true \\ -Dsonar.gitlab.commit_sha=$GIT_COMMIT \\ -Dsonar.gitlab.ref_name=$GIT_BRANCH \\ -Dsonar.gitlab.user_token=***** \\ -Dsonar.gitlab.url=http://gitlab.apps.okd311.curiouser.com/ \\ -Dsonar.gitlab.ignore_certificate=true \\ -Dsonar.gitlab.comment_no_issue=true \\ -Dsonar.gitlab.max_global_issues=1000 \\ -Dsonar.gitlab.unique_issue_per_inline=true\" } } stage (\"代码打包\") { steps { sh \"mvn -s settings.xml package\" } } stage(\"上传制品\"){ steps{ script{ def pomfile = readMavenPom file: 'pom.xml' sh \"curl -sL -w 'Upload the jar to the repository status code: %{http_code}\\n' -u devops:**** \" + \"--upload-file target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging} \" + \"http://nexus.apps.okd311.curiouser.com/repository/jenkins-product-repo/${pomfile.artifactId}-${pomfile.version}-${env.GIT_COMMIT}.${pomfile.packaging}\" } } } stage(\"构建应用镜像\"){ steps{ sh 'docker login -p ********** -u unused docker-registry-default.apps.okd311.curiouser.com' sh 'make' } } } post { always { emailext attachLog: true, body: ''' 构建任务的完整日志详见见附件,Jenkins查看链接: $BUILD_URL''', subject: '$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } 参考博客： https://github.com/easzlab/kubeasz/blob/master/docs/guide/jenkins.md https://www.qikqiak.com/k8s-book/docs/36.Jenkins%20Slave.html https://jenkins.io/blog/2018/09/14/kubernetes-and-secret-agents/ https://www.jianshu.com/p/1440b5b4b980 https://www.cnblogs.com/guguli/p/7827435.html https://blog.csdn.net/felix_yujing/article/details/78725142 https://jicki.me/kubernetes/2018/02/08/kubernetes-jenkins/ https://testerhome.com/topics/17251 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-04-21 16:33:01 "},"origin/jenkins-pipeline-utility-steps.html":{"url":"origin/jenkins-pipeline-utility-steps.html","title":"Pipeline Utility Steps","keywords":"","body":"Jenkins Pipeline-Utility-steps插件 一、Pipeline-Utility-steps插件简介 Jekins中的Pipeline-Utility-steps插件能让你在pipeline的Step中直接使用它的API方法进行某些操作，例如查找文件，读取YAML/JSON/Properties文件、读取Maven工程POM文件等。这些方法有一个前提，任何文件都需要放在jenkins的workspace下，执行的job才能去找到文件。 Github地址：https://github.com/jenkinsci/pipeline-utility-steps-plugin 相关文档：https://jenkins.io/doc/pipeline/steps/pipeline-utility-steps/ 二、Pipeline-Utility-steps插件的方法 1、文件操作 findFiles 根据一些字符串规则去查找文件，如果有匹配的查找，返回是一个fille数组对象。（文档） 参数 excludes(可选，参数类型为String) glob(可选，参数类型为String) 示例 def files = findFiles(glob: '**/TEST-*.xml') echo \"\"\"${files[0].name} ${files[0].path} ${files[0].directory} ${files[0].length} ${files[0].lastModified}\"\"\" touch 创建文件（如果文件不存在的话）并设置时间戳. Returns a FileWrapper representing the file that was touched. (文档) 参数 file(参数类型为String)：The path to the file to touch. timestamp(可选，参数类型为long)：The timestamp to set (number of ms since the epoc), leave empty for current system time. sha1 计算指定文件的SHA1 (文档) 参数 file(参数类型为String): The path to the file to hash. tee 将输出重定向到文件 参数 file(参数类型为String) Zip Files zip：创建Zip文件. (文档) 参数 zipFile(参数类型为String): The name/path of the zip file to create. archive(可选，参数类型为boolean): If the zip file should be archived as an artifact of the current build. The file will still be kept in the workspace after archiving. dir(可选，参数类型为String): The path of the base directory to create the zip from. Leave empty to create from the current working directory. glob(可选，参数类型为String): Ant style pattern of files to include in the zip. Leave empty to include all files and directories. unzip：解压或读取Zip文件 (文档) 参数 zipFile(参数类型为String): The name/path of the zip file to extract. charset(可选，参数类型为String): Specify which Charset you wish to use eg. UTF-8 dir(可选，参数类型为String): The path of the base directory to extract the zip to. Leave empty to extract in the current working directory. glob(可选，参数类型为String): Ant style pattern of files to extract from the zip. Leave empty to include all files and directories. quiet(可选，参数类型为boolean): Suppress the verbose output that logs every single file that is dealt with. E.g. unzip zipFile: 'example.zip', quiet: true read(可选，参数类型为boolean): Read the content of the files into a Map instead of writing them to the workspace. The keys of the map will be the path of the files read. E.g. def v = unzip zipFile: 'example.zip', glob: '*.txt', read: true String version = v['version.txt'] test(可选，参数类型为boolean): Test the integrity of the archive instead of extracting it. When this parameter is enabled, all other parameters (except for zipFile) will be ignored. The step will return true or false depending on the result instead of throwing an exception. 2、配置文件操作 readProperties Reads a file in the current working directory or a String as a plain text Java Properties file. The returned object is a normal Map with String keys. The map can also be pre loaded with default values before reading/parsing the data. (文档) 参数 defaults (可选，Nested Choice of Objects): An Map containing default key/values. These are added to the resulting map first. file(可选，参数类型为String): path to a file in the workspace to read the properties from. These are added to the resulting map after the defaults and so will overwrite any key/value pairs already present. interpolate (可选，参数类型为boolean): Flag to indicate if the properties should be interpolated or not. In case of error or cycling dependencies, the original properties will be returned. text (可选，参数类型为String): An String containing properties formatted data. These are added to the resulting map after file and so will overwrite any key/value pairs already present. 示例 def d = [test: 'Default', something: 'Default', other: 'Default'] def props = readProperties defaults: d, file: 'dir/my.properties', text: 'other=Override' assert props['test'] == 'One' assert props['something'] == 'Default' assert props.something == 'Default' assert props.other == 'Override' def props = readProperties interpolate: true, file: 'test.properties' assert props.url = 'http://localhost' assert props.resource = 'README.txt' // if fullUrl is defined to ${url}/${resource} then it should evaluate to http://localhost/README.txt assert props.fullUrl = 'http://localhost/README.txt' readManifest Reads a Jar Manifest file or text and parses it into a set of Maps. The returned data structure has two properties: main for the main attributes, and entries containing each individual section (except for main). (文档) 参数 file(可选，参数类型为String。值只能是file或text，两者不能同时设置): path to a file to read. It could be a plain text, .jar, .war or .ear. In the latter cases the manifest will be extracted from the archive and then read. text(可选，参数类型为String。值只能是file或text，两者不能同时设置): text containing the manifest data. 示例 def man = readManifest file: 'target/my.jar' assert man.main['Version'] == '6.15.8' assert man.main['Application-Name'] == 'My App' assert man.entries['Section1']['Key1'] == 'value1-1' assert man.entries['Section2']['Key2'] == 'value2-2' readYaml Reads a file in the current working directory or a String as a plain text YAML file. It uses SnakeYAML as YAML processor. The returned objects are standard Java objects like List, Long, String, ...: bool: [true, false, on, off] int: 42 float: 3.14159 list: ['LITE', 'RES_ACID', 'SUS_DEXT'] map: {hp: 13, sp: 5}. (文档) 参数 file(可选，参数类型为String) text(可选，参数类型为String) 示例 // 读取单个YAML文件 def datas = readYaml text: \"\"\" something: 'my datas' size: 3 isEmpty: false \"\"\" assert datas.something == 'my datas' assert datas.size == 3 assert datas.isEmpty == false // 读取多个YAML文件 def datas = readYaml text: \"\"\" --- something: 'my first document' --- something: 'my second document' \"\"\" assert datas.size() == 2 assert datas[0].something == 'my first document' assert datas[1].something == 'my second document' // With file dir/my.yml containing something: 'my datas' : def datas = readYaml file: 'dir/my.yml', text: \"something: 'Override'\" assert datas.something == 'Override' writeYaml Write a YAML file from an object. (文档) 参数 file(参数类型为String): Mandatory path to a file in the workspace to write the YAML datas to. data: A Mandatory Object containing the data to be serialized. charset: Optionally specify the charset to use when writing the file. Defaults to UTF-8 if nothing else is specified. What charsets that are available depends on your Jenkins master system. The java specification tells us though that at least the following should be available: [ US-ASCII、ISO-8859-1、UTF-8、UTF-16BE、UTF-16LE、UTF-16] 示例 def amap = ['something': 'my datas', 'size': 3, 'isEmpty': false] writeYaml file: 'datas.yaml', data: amap def read = readYaml file: 'datas.yaml' assert read.something == 'my datas' assert read.size == 3 assert read.isEmpty == false readJSON Reads a file in the current working directory or a String as a plain text JSON file. The returned object is a normal Map with String keys or a List of primitives or Map. (文档) 参数 file(可选，参数类型为String。值只能是file或text，两者不能同时设置): Path to a file in the workspace from which to read the JSON data. Data could be access as an array or a map. text(可选，参数类型为String。值只能是file或text，两者不能同时设置): A string containing the JSON formatted data. Data could be access as an array or a map. 示例 def props = readJSON file: 'dir/input.json' assert props['attr1'] == 'One' assert props.attr1 == 'One' def props = readJSON text: '{ \"key\": \"value\" }' assert props['key'] == 'value' assert props.key == 'value' def props = readJSON text: '[ \"a\", \"b\"]' assert props[0] == 'a' assert props[1] == 'b' writeJSON：Write a JSON file in the current working directory. That for example was previously read by readJSON. (文档) 参数 file(可选，参数类型为String): Path to a file in the workspace to write to. json(Nested Choice of Objects): The JSON object to write. pretty (可选，参数类型为int): Prettify the output with this number of spaces added to each level of indentation. 示例 def input = readJSON file: 'myfile.json' //Do some manipulation writeJSON file: 'output.json', json: input //or pretty print it, indented with a configurable number of spaces writeJSON file: 'output.json', json: input, pretty: 4 readCSV Reads a file in the current working directory or a String as a plain text. A List of CSVRecord instances is returned. (文档) 参数 file(可选，参数类型为String。值只能是file或text，两者不能同时设置): Path to a file in the workspace from which to read the CSV data. Data is accessed as a List of String Array. text(可选，参数类型为String。值只能是file或text，两者不能同时设置): A string containing the CSV formatted data. Data is accessed as a List of String Arrays. format(可选，org.apache.commons.csv.CSVFormat) 示例 def records = readCSV file: 'dir/input.csv' assert records[0][0] == 'key' assert records[1][1] == 'b' def content = readCSV text: 'key,value\\na,b' assert records[0][0] == 'key' assert records[1][1] == 'b' // 进阶示例 def excelFormat = CSVFormat.EXCEL def records = readCSV file: 'dir/input.csv', format: excelFormat assert records[0][0] == 'key' assert records[1][1] == 'b' def content = readCSV text: 'key,value\\na,b', format: CSVFormat.DEFAULT.withHeader() assert records[1].get('key') == 'a' assert records[1].get('value') == 'b' writeCSV Write a CSV file in the current working directory. That for example was previously read by readCSV. See CSVPrinter for details.(文档) 参数 file(参数类型为String): Path to a file in the workspace to write to. records(java.lang.Iterable): The list of CSVRecord instances to write. format(可选，org.apache.commons.csv.CSVFormat):See CSVFormat for details. 示例 def records = [['key', 'value'], ['a', 'b']] writeCSV file: 'output.csv', records: records, format: CSVFormat.EXCEL 3、Maven项目 readMavenPom 读取Maven POM文件到一个Model数据结构中. (文档) 参数 file(可选，参数类型为String)：默认读取目前工作区下的POM.xml文件 示例 stage ('上传制品') { steps { script{ def pomfile = readMavenPom file: 'pom.xml' nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] } } } writeMavenPom Writes a Maven project file. That for example was previously read by readMavenPom. (文档) 参数 model(参数类型为org.apache.maven.model.Model): The Model object to write. file(可选，参数类型为String): Optional path to a file in the workspace to write to. If left empty the step will write to pom.xml in the current working directory. 示例 def pom = readMavenPom file: 'pom.xml' //Do some manipulation writeMavenPom model: pom Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-18 09:48:22 "},"origin/jenkins-Nexus-Platform的使用.html":{"url":"origin/jenkins-Nexus-Platform的使用.html","title":"Nexus Platform Plugin","keywords":"","body":"Preflight 官方插件文档：https://help.sonatype.com/integrations/nexus-and-continuous-integration/nexus-platform-plugin-for-jenkins 安装插件：Pipeline Utility Steps 功能： 一、安装 二、配置 系统管理--> 系统设置--> Sonatype Nexus 三、使用 上传构建后的制品到Nexus的Hosted类型仓库中 Job Declarative Pipeline ```bash stage ('上传制品') { steps { script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用Nexus Platform插件上传maven制品到Nexus的maven格式release仓库 nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] //拼接maven制品的搜索链接,该链接是以源代码POM文件中的maven制品坐标信息参数对nexus api进行搜索，返回的response会重定向到制品的下载链接 echo \"The Jar Format Asset of Maven have been pushed to Hosted Repository: Maven-Release. The Download URL of the Asset: http://nexus-nexus.apps.okd311.curiouser.com/service/rest/v1/search../assets/download?maven.groupId=${pomfile.groupId}&maven.artifactId=${pomfile.artifactId}&maven.baseVersion=${pomfile.version}&maven.extension=jar&maven.classifier\" } } } ``` 四、注意 如果Job再次构建，产生相同的Jar，上传信息还是一样的，Nexus的Release仓库需要设置为\"允许Redeploy\"。不然，仓库中已经相同版本信息的制品，会造成上传失败 参考链接 https://support.sonatype.com/hc/en-us/articles/115009108987-Jenkins-Publish-Using-Maven-Coordinates-from-the-pom-xml https://www.jianshu.com/p/29403ecf7fc2 https://stackoverflow.com/questions/37603619/extract-version-id-from-pom-in-a-jenkins-pipeline Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-配置SMTP邮箱服务.html":{"url":"origin/jenkins-配置SMTP邮箱服务.html","title":"Mail Plugin","keywords":"","body":"Jenkins配置SMTP邮箱服务 Prerequisite 自己邮箱运营商设置了开通SMTP服务 Jenkins 安装了Jenkins Mailers Plugin 一、Context Jenkins默认有个插件叫\"Mailer Plugin\"用来发送通知邮件。该插件使用的\"JavaMail \"来进行配置自定义个邮箱服务器 二、配置 系统管理-->系统设置 配置Jenkins的系统管理员邮箱地址 配置SMTP邮件服务器地址 三、使用 Job中 四、问题 当构建不成功时发送的邮件，内容包含构建的日志。 当初次构建成功时会发送邮件通知，当再次重复构建成功时，则不会发送邮件通知，得等到构建失败时才会再次发送通知邮件 功能太弱，可使用\"Mail Extension\"插件进行功能扩展。详见：jenkins-Mailer邮箱功能扩展插件Email-Extension 不知Jenkins的系统管理员邮箱时，发送会报错 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-Mailer邮箱功能扩展插件Email-Extension.html":{"url":"origin/jenkins-Mailer邮箱功能扩展插件Email-Extension.html","title":"Mail Extension","keywords":"","body":"Jenkins Mailer邮箱功能扩展插件Email-Extension 一、Context Jenkins自带的邮件插件功能太弱，有个邮箱扩展插件。 官方文档WIKI：https://wiki.jenkins.io/display/JENKINS/Email-ext+plugin 优势： 邮件格式改为HTML，更美观 使用模板来配置邮件内容 为不同的Job配置不一样的收件人 为不同的事件配置不一样的trigger 在Jenkins pipeline中集成发送邮件通知功能 二、插件安装配置 1、安装 2、配置 三、使用 1、Jobs中 2、Pipeline中 pipeline{ ...上文省略... post { always { emailext attachLog: true, body: ''' 构建任务的完整日志详见见附件,Jenkins查看链接: $BUILD_URL''', subject: '$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } 四、发送HTML格式的邮件 1、Pipeline中 Prerequisite 准备格式化好的HTML ${ENV, var=\"JOB_NAME\"}-第${BUILD_NUMBER}次构建日志 Jenkins构建信息邮件，请勿回复！ 构建信息 项目名称： ${PROJECT_NAME} 构建编号： ${BUILD_NUMBER} 构建状态： ${BUILD_STATUS} 构建人员： ${GITLABUSERNAME} 构建日志： 见附件 Jenkins构建页面： ${BUILD_URL} 变更代码： ${GITLABSOURCEREPOHOMEPAGE}/commit/${gitlabMergeRequestLastCommit} 单元测试报告： ${BUILD_URL}jacoco 测试报告 特别说明：Instructions指令覆盖，Branches分支覆盖，Cyclomatic Complexity非抽象方法计算圈复杂度，Lines行覆盖，Methods方法覆盖，Classes类覆盖 ${FILE,path=\"./target/site/jacoco/index.html\"} 使用pipeline语法生成器生成pipeline 压缩pipeline. (压缩HTML源代码的工具网站：http://tool.oschina.net/jscompress?type=2) pipeline{ ...上文省略... post { always { emailext attachLog: true, body: '''${ENV, var=\"JOB_NAME\"}-第${BUILD_NUMBER}次构建日志Jenkins构建信息邮件，请勿回复！构建信息项目名称： ${PROJECT_NAME}构建编号： ${BUILD_NUMBER}构建状态： ${BUILD_STATUS}构建人员： ${GITLABUSERNAME}构建日志： 见附件Jenkins构建页面：${BUILD_URL}变更代码：${GITLABSOURCEREPOHOMEPAGE}/commit/${gitlabMergeRequestLastCommit}单元测试报告：${BUILD_URL}jacoco测试报告特别说明：Instructions指令覆盖，Branches分支覆盖，Cyclomatic Complexity非抽象方法计算圈复杂度，Lines行覆盖，Methods方法覆盖，Classes类覆盖${FILE,path=\"./target/site/jacoco/index.html\"}''', mimeType: 'text/html', subject: '项目构建报告：$PROJECT_NAME的第$BUILD_NUMBER次构建$BUILD_STATUS !', to: '*******@163.com' } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-gitlab插件的使用.html":{"url":"origin/jenkins-gitlab插件的使用.html","title":"Gitlab","keywords":"","body":"Jenkins Gitlab插件的使用 一、Overviews Jenkins的Gitlab插件能接收gitlab仓库代码事件触发的Webhook来触发Jenkins Job/Pipeline的构建，并且能将构建状态同步到Gitlab中. 插件Github：https://github.com/jenkinsci/gitlab-plugin 利用此插件可实现如下效果： 二、安装 Jenkins -> 系统管理 ->插件管理->可用插件->搜索\"gitlab\" 插件下载地址：https://plugins.jenkins.io/gitlab-plugin 三、配置 1、Gitlab发送Webhook到Jenkins时的安全认证配置 Gitlab代码仓库配置web hook时需要Jenkins Gitlab插件的Token,来加密验证webhook的安全性 方式一：全局性的认证Token ① Jenkins创建新用户（授予Job/Build权限即可） ② 获取新用户的User ID和API Token ③ Gitlab配置web hook填写URL时，使用http://USERID:APITOKEN@JENKINS_URL/project/YOUR_JOB即可（Secret Token可忽略） 方式二(推荐)：单独项目Jenkins Job的认证Token 在Gitlab中配置Webhook时，将图中的Token填入即可 方式三(不推荐)：不使用认证 ① Manage Jenkins -> Configure System -> GitLab section -> 取消勾选 \"Enable authentication for '/project' end-point\" 2、Jenkins回写构建状态到Gitlab时需要的认证授权配置 Jenkins通过Gitlab API将构建状态回写到对应代码仓库时，需要有权限在代码仓库中创建评论，更改状态等 步骤一 ① Gitlab创建新用户（建议命名用户名时尽量见名知意，例如：Jenkins） ② 登陆新用户，获取Access Tokens,Token权限只要api即可(及时复制Token,只显示一次) ③ 在Jenkins中创建GitLab API token类型的凭据，在API token字段中保存上一步获取的Token ④ 在Jenkins Gitlab插件中配置gitlab Server相关信息 步骤二 对应代码仓库配置Jenkins Job时要将该Token所属的用户加入members中，授予Developers角色或有更高权限的角色 五、Jenkins Job中配置Webhook 1、配置Gitlab Buid Trigger 2、配置构建后Gitlab回写信息的动作 添加构建后动作\"Pushlish build status to Gitlab\" 之后会在Gitlab代码仓库的CI/CD-->Pipeline和Merge Request中看到构建状态图标 添加构建后动作\"Add note with build status on Gitlab merge request\" 之后会根据构建状态的不同在Gitlab的Merge Request中添加不同的comment评论 添加构建后动作\"Add Vote for build status on Gitlab merge request\" 之后会根据构建状态的不同在Gitlab的Merge Request中显示不同的投票 添加构建后动作\"Accept Gitlab merge request on success\" 该动作会在build构建成功后在Gitla上自动同意接受Merge Request 不添加构建后动作 Gitlab的事件只会触发Jenkins构建，不会回写任何信息到gitlab对应代码仓库中 五、Gitlab 代码仓库配置Webhook 详见：gitlab-配置代码仓库事件触发器Webhook 六、获取Webhook HTTP请求信息的变量 对于Gitlab发送过来的Webhook HTTP POST请求信息，可直接通过以下变量来获取。 gitlabBranch gitlabSourceBranch gitlabActionType gitlabUserName gitlabUserEmail gitlabSourceRepoHomepage gitlabSourceRepoName gitlabSourceNamespace gitlabSourceRepoURL gitlabSourceRepoSshUrl gitlabSourceRepoHttpUrl gitlabMergeRequestTitle gitlabMergeRequestDescription gitlabMergeRequestId gitlabMergeRequestIid gitlabMergeRequestState gitlabMergedByUser gitlabMergeRequestAssignee gitlabMergeRequestLastCommit gitlabMergeRequestTargetProjectId gitlabTargetBranch gitlabTargetRepoName gitlabTargetNamespace gitlabTargetRepoSshUrl gitlabTargetRepoHttpUrl gitlabBefore gitlabAfter gitlabTriggerPhrase Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/jenkins-generic-webhook-trigger插件.html":{"url":"origin/jenkins-generic-webhook-trigger插件.html","title":"Generic Webhook Trigger","keywords":"","body":"Jenkins Generic Webhook Trigger插件 之前写过在Jenkins中使用Gitlab插件接收gitlab仓库代码指定事件触发的webhook来触发Job或Pipeline的执行构建,详见:Jenkins的Gitlab插件。这种方式在某种场景下有些限制，无法满足一些功能性复杂的webhook触发（例如只接受特定tag,分支或Merge的webhook触发，例如创建通配的pipeline，gitlab各个仓库配置固定的Webhook请求地址等等场景）。所以可以使用Jenkins Generic Webhook Trigger插件监听包含自定义设置的HTTP请求来触发job/pipeline的构建！对比Gitlab插件，有以下可自定义的特性： 暴露出来的回调API URL统一， 不同的job/pipeline使用不同的Token或者指定特殊的请求参数进行区分 可使用不同的方式提取HTTP请求中的各种信息，然后通过环境变量的形式传递给job/pipeline使用 可设置白名单，只允许接收指定IP地址的Webhook请求 一、简介 Generic Webhook Trigger是一款Jenkins插件，简称GWT，安装后会暴露出来一个公共API，GWT插件接收到 JSON 或 XML 的 HTTP POST 请求后，根据我们配置的规则决定触发哪个Jenkins项目 安装的话在Jenkins的插件管理中心直接搜索安装即可，下载HPI文件手动安装, 插件下载地址 插件Github地址：https://github.com/jenkinsci/generic-webhook-trigger-plugin 支持以下系统发送的Webhook: Bitbucket Cloud Bitbucket Server GitHub GitLab Gogs and Gitea Assembla Jira 二、GenericTrigger 触发器设置 GenericTrigger 触发条件分为5部分： 从 HTTP POST 请求中提取数据 Token, GWT 插件用于标识Jenkins项目的唯一性 根据请求参数值判断是否触发Jenkins项目的执行 日志打印控制 Webhook 响应控制 1. 从 HTTP POST 请求中匹配数据到环境变量中 GWT 插件可以从一个HTTP POST请求的Body、URL参数、header中提取数据并赋予指定的环境变量，后续任务可以直接引用。 在Job中设置数据的匹配 在Pipeline中设置数据的匹配 genericRequestVariables：从URL参数中提取值 genericVariables： 从HTTP POST的body 中提取值 genericHeaderVariables：从HTTP header 中提取值（用法和genericRequestVariables一样） genericVariables: [ [key: 'ref', value: '$.ref'], [key: 'before',value: '$.before', expressionType: 'JSONPath', //Optional, defaults to JSONPath regexpFilter: '', //Optional, defaults to empty string defaultValue: '' //Optional, defaults to empty string ] ], genericRequestVariables: [ [key: 'requestWithNumber', regexpFilter: '[^0-9]'], [key: 'requestWithString', regexpFilter: ''] ], genericHeaderVariables: [ [key: 'headerWithNumber', regexpFilter: '[^0-9]'], [key: 'headerWithString', regexpFilter: ''] ] 2.过滤符合条件的Webhook参数才能触发构建 例如只允许gitlab仓库Pipeline分支commit事件、带有特殊前缀的tag才能触发的Webhook才能触发构建，此时就可以先从webhook请求体中获取对应的值，放到指定的环境变量中，在插件的Option filter中设置正则表达式进行过滤，符合的才能触发pipeline，不符合的不触发。 3.Token 参数 当多个Jenkins Jobs中使用该插件时，Webhook Trigger都是同一个相同的URL。如果你只想触发一个特定的工作，可以 使用令牌参数为不同的作业提供不同的令牌。 可以再header或post中添加一些请求参数，并仅在该参数具有特定值时使用regexp筛选器触发。 发送Webhook请求时可使用以下方式在HTTP POST请求中携带Token： 请求URL参数: curl -vs http://localhost:8080/jenkins/generic-webhook-trigger/invoke?token=abc123 token的请求header: curl -vs -H \"token: abc123\" http://localhost:8080/jenkins/generic-webhook-trigger/invoke Bearer类型Authorization的请求header: curl -vs -H \"Authorization: Bearer abc123\" http://localhost:8080/jenkins/generic-webhook-trigger/invoke 4. 日志打印控制 Silent response： 当为true，只返回http 200 状态码，不返回触发状态等信息。 Print post content： 是否在构建日志中打印webhook 请求的内容 Print contributed variables： 是否在构建日志中打印提取后的变量 5. 显示触发信息 三、设置白名单 可在Jenkins Generic Webhook Trigger的全局配置中，配置IP地址白名单，指定、验证请求的来源IP地址，IP地址格式可为CIDR 或 ranges。 1.2.3.4 2.2.3.0/24 3.2.1.1-3.2.1.10 2001:0db8:85a3:0000:0000:8a2e:0370:7334 2002:0db8:85a3:0000:0000:8a2e:0370:7334/127 同时还支持HMAC验证(HMAC百度百科)。 Jenkins --> Manage Jenkins --> Configure System Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-12-29 12:18:20 "},"origin/gitlab-install.html":{"url":"origin/gitlab-install.html","title":"部署与配置","keywords":"","body":"Gitlab的部署与配置 一、Docker Deb安装包下载地址：https://packages.gitlab.com/gitlab/ ARM 官方gitlab-ce暂没有ARM 架构的镜像,所以由第三方的镜像进行部署 参考：https://about.gitlab.com/handbook/engineering/development/enablement/distribution/maintenance/arm.html#why-dont-you-compile-arm32-bit-packages-on-arm64-for-speed GitHub：https://github.com/ulm0/gitlab DockerHub：https://hub.docker.com/r/yrzr/gitlab-ce-arm64v8 mkdir -p /data/gitlab/data /data/gitlab/logs /data/gitlab/config && \\ docker run -d \\ --hostname 192.168.1.8 \\ -e GITLAB_OMNIBUS_CONFIG=\"external_url 'http://192.168.1.1:38080';gitlab_rails['lfs_enabled'] = true; gitlab_rails['gitlab_shell_ssh_port'] = 30022 ; node_exporter['enable'] = true ;\" \\ -e RAILS_ENV=\"production\" \\ -e GITLAB_EMAIL_DISPLAY_NAME=\"Gitlab 13\" \\ -e GITLAB_EMAIL_FROM=\"*****@163.com\" \\ -e GITLAB_EMAIL_REPLY_TO=\"*****@163.com\" \\ -e GITLAB_EMAIL_SUBJECT_SUFFIX=\"Gitlab 13\" \\ -e GITLAB_ROOT_PASSWORD=\"*****\" \\ -p 38080:38080 \\ -p 30022:22 \\ --name gitlab \\ --restart always \\ --privileged \\ -v /data/gitlab/config:/etc/gitlab \\ -v /data/gitlab/logs:/var/log/gitlab \\ -v /data/gitlab/data:/var/opt/gitlab \\ ulm0/gitlab:13.2.6 手动构建新版本的arm gitlab docker镜像 git clone https://github.com/ulm0/gitlab.git gitlab-arm-docker cd gitlab-arm-docker echo \"13.8.1\" > VERSION make build 针对raspberry的deb包下载地址：https://packages.gitlab.com/gitlab/raspberry-pi2 二、包管理器 ARM架构(ubuntu) https://packages.gitlab.com/app/gitlab/gitlab-ce/search?q=&filter=debs&filter=debs&dist=ubuntu%2Fjammy apt-get install curl gnupg apt-transport-https curl -fsSL https://packages.gitlab.com/gitlab/gitlab-ce/gpgkey | gpg --dearmor > /usr/share/keyrings/gitlab_gitlab-ce-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/gitlab_gitlab-ce-archive-keyring.gpg] https://packages.gitlab.com/gitlab/gitlab-ce/ubuntu jammy main\" >> /etc/apt/sources.list.d/gitlab_gitlab-ce.list echo \"deb-src [signed-by=/usr/share/keyrings/gitlab_gitlab-ce-archive-keyring.gpg] https://packages.gitlab.com/gitlab/gitlab-ce/ubuntu jammy main\" >> /etc/apt/sources.list.d/gitlab_gitlab-ce.list apt update apt install gitlab-ce 安装路径：/opt/gitlab 配置文件：/etc/gitlab/gitlab.rb 启动命令：gitlab-ctl reconfigure Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-30 18:16:54 "},"origin/gitlab-配置SMTP邮件服务.html":{"url":"origin/gitlab-配置SMTP邮件服务.html","title":"配置SMTP邮件服务","keywords":"","body":"1. 修改/etc/gitlab/gitlab.rb ### Email Settings gitlab_rails['gitlab_email_enabled'] = true gitlab_rails['gitlab_email_from'] = '******@163.com' gitlab_rails['gitlab_email_display_name'] = 'Curiouser163SMTPServer' gitlab_rails['gitlab_email_reply_to'] = '*****@163.com' gitlab_rails['gitlab_email_subject_suffix'] = 'Gitlab' gitlab_rails['smtp_enable'] = true gitlab_rails['smtp_address'] = \"smtp.163.com\" gitlab_rails['smtp_port'] = 25 gitlab_rails['smtp_user_name'] = \"******@163.com\" gitlab_rails['smtp_password'] = \"******\" gitlab_rails['smtp_domain'] = \"163.com\" gitlab_rails['smtp_authentication'] = \"login\" gitlab_rails['smtp_enable_starttls_auto'] = false gitlab_rails['smtp_tls'] = false 注意： 当使用Docker镜像部署时，相关Gitlab参数可追加在环境变量“GITLAB_OMNIBUS_CONFIG”的值中进行配置。详见：https://docs.gitlab.com/omnibus/docker/ 及 gitlab的安装与配置 测试发送邮件 $ gitlab-rails console Loading production environment (Rails 4.2.10) # 查看发送邮件使用的方式。方式有smtp和sendmail两种 $ irb(main):001:0> ActionMailer::Base.delivery_method # 查看 SMTP 相关的配置 $ irb(main):001:0> ActionMailer::Base.smtp_settings # 发送测试邮件 $ irb(main):001:0> Notify.test_email('******@163.com','gitlab send mail test','gitlab test mail').deliver_now Notify#test_email: processed outbound mail in 539.6ms Sent mail to ******@163.com (397.8ms) Date: Thu, 04 Jul 2019 15:07:33 +0000 From: Curiouser163SMTPServer Reply-To: Curiouser163SMTPServer To:******@163.com Message-ID: Subject: gitlab send mail test Mime-Version: 1.0 Content-Type: text/html; charset=UTF-8 Content-Transfer-Encoding: 7bit Auto-Submitted: auto-generated X-Auto-Response-Suppress: All gitlab test mail=> #, >, >, , >, , , , , , > irb(main):002:0> 参考 https://docs.gitlab.com/omnibus/settings/smtp.html#smtp-settings Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-30 17:50:14 "},"origin/gitlab-backuprestore.html":{"url":"origin/gitlab-backuprestore.html","title":"代码仓库的备份与恢复","keywords":"","body":"Gitlab代码仓库的备份与恢复 一、Preface GItlab上的代码仓库需要进行定期的导出备份，并且可以随时进行恢复 二、API接口备份代码仓库数据 gitlab提供导出代码仓库指定资源的API接口 支持代码仓库导出的gitlab资源 不支持代码仓库导出的gitlab资源 Project and wiki repositories Build traces and artifacts Project uploads Container registry images Project configuration, including services CI variables Issues with comments, merge requests with diffs and comments, labels, milestones, snippets, and other project entities Webhooks Design Management files and data Any encrypted tokens LFS objects Merge Request Approvers Issue boards Push Rules Awards 1、通过API接口 ① 调用生成Export的接口 接口及参数 POST /projects/:id/export Attribute Type Required Description id integer/string yes The ID or URL-encoded path of the project owned by the authenticated user description string no Overrides the project description upload hash no Hash that contains the information to upload the exported project to a web server upload[url] string yes The URL to upload the project upload[http_method] string no The HTTP method to upload the exported project. Only PUT and POST methods allowed. Default is PUT curl --request POST --header \"PRIVATE-TOKEN: \" https://gitlab.example.com/api/v4/projects/1/export ②调用获取Export生成状态的接口 接口及参数 Attribute Type Required Description id integer/string yes The ID or URL-encoded path of the project owned by the authenticated user curl --header \"PRIVATE-TOKEN: \" https://gitlab.example.com/api/v4/projects/1/export ③下载Export 接口及参数 GET /projects/:id/export/download Attribute Type Required Description id integer/string yes The ID or URL-encoded path of the project owned by the authenticated user curl --header \"PRIVATE-TOKEN: \" --remote-header-name --remote-name https://gitlab.example.com/api/v4/projects/5/export/download ④执行脚本 #/bin/bash gitlab_url=http://gitlab.apps.okd311.curiouser.com/ gitlab_access_token=***** gitlab_api_version=\"api/v4\" for project_id in $(curl -s -XGET ''\"$gitlab_url\"''\"$gitlab_api_version\"'/projects?simple=true&order_by=id&sort=asc' -H 'private-token: '\"$gitlab_access_token\"'' | jq '.[].id') do curl -s -XPOST ''\"$gitlab_url\"''\"$gitlab_api_version\"'/projects/'\"$project_id\"'/export' -H 'private-token: '\"$gitlab_access_token\"'' > /dev/null if [[ `curl -s -XGET ''\"$gitlab_url\"''\"$gitlab_api_version\"'/projects/'\"$project_id\"'/export' -H 'private-token: '\"$gitlab_access_token\"'' | jq '.export_status' ` =~ \"finished\" ]];then curl -s -O -XGET ''\"$gitlab_url\"'/'\"$gitlab_api_version\"'/projects/'\"$project_id\"'/export/download' -H 'private-token: '\"$gitlab_access_token\"'' ; echo hahah ; else echo test; fi done 2、通过UI界面 ①UI路径 仓库页面-->Settings-->General-->Advanced-->Export project按钮 ②点击Export ③当Export生成后会发送带有Export下载链接的邮件给代码仓库的维护者 ④(可选)后续还可以在UI界面上点击下载Export 三、命令行备份gitlab实例数据 gitlab提供了相应的命令可以进行备份与恢复。 1、备份 ①命令 二进制方式或docker方式部署的 版本>= GitLab 12.2 sudo gitlab-backup create # 或者 docker exec -t gitlab-backup create 版本 gitlab-rake gitlab:backup:create # 或者 docker exec -t gitlab-rake gitlab:backup:create 源代码方式部署的 sudo -u git -H bundle exec rake gitlab:backup:create RAILS_ENV=production 使用helm部署在k8s中的 使用helm部署的，GitLab task runner容器中有个一个工具脚本可进行备份 kubectl exec -it backup-utility 生成的备份文件的信息 备份文件的路径：默认/var/opt/gitlab/backups，可在config/gitlab.yml配置文件设置backup_path进行控制 备份文件名规则：默认[TIMESTAMP]-版本_gitlab_backup.tar ②支持备份的gitlab实例数据 Database Attachments Git repositories data CI/CD job output logs CI/CD job artifacts LFS objects Container Registry images GitLab Pages content Snippets Group wikis ③命令行的备份策略参数 默认的备份策略实际上是使用Linux命令tar和gzip将数据从相应的数据位置流式传输到备份。在大多数情况下，这可以正常工作，但是在数据快速变化时可能会导致问题。例如： 当tar正在读取数据时更改数据时，错误文件可能会随着我们读取数据而发生更改，并导致备份过程失败。为了解决这个问题，8.17引入了一种称为副本的新备份策略。该策略在调用tar和gzip之前将数据文件复制到一个临时位置，从而避免了错误。副作用是备份过程最多占用额外的1X磁盘空间。该过程尽力在每个阶段清理临时文件，这样问题就不会复杂化，但是对于大型安装而言，这可能是一个相当大的变化。这就是为什么复制策略不是8.17中的默认策略 BACKUP：设置备份文件名，代替默认备份文件名规则中的时间戳字段，例如：[BACKUP的值]-版本_gitlab_backup.tar sudo gitlab-backup create BACKUP=dump # 版本 GZIP_RSYNCABLE：设置备份文件可被rsync传输 sudo gitlab-backup create BACKUP=dump GZIP_RSYNCABLE=yes # 版本 SKIP：设置备份时排除哪些gitlab资源，可选项有db(database)、uploads(attachments)、builds(CI job output logs)、artifacts(CI job artifacts)、lfs(LFS objects)、registry(Container Registry images)、pages(Pages content)、repositories(Git repositories data) sudo gitlab-backup create BACKUP=dump GZIP_RSYNCABLE=yes SKIP=builds,lfs # 版本 SKIP=tar：创建备份的最后一部分是生成包含所有部分的.tar文件。在某些情况下（例如，如果备份是由其他备份软件获取的），创建.tar文件可能会浪费精力，甚至直接有害，因此可以通过将tar添加到SKIP环境变量中来跳过此步骤。将tar添加到SKIP变量会将包含备份的文件和目录保留在用于中间文件的目录中。创建新备份时，这些文件将被覆盖，因此，应确保将它们复制到其他位置，因为系统上只能有一个备份。 GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY与GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY ： 设置导出代码仓库数据时使用多线程，GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY是设置同时最多可导出多少个代码仓库数据，默认为1。GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY 设置每个存储上同时备份的最大项目数。这样可以将存储库备份分散到各个存储中 sudo gitlab-backup create GITLAB_BACKUP_MAX_CONCURRENCY=4 GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY=1 # 源码安装时 sudo -u git -H bundle exec rake gitlab:backup:create GITLAB_BACKUP_MAX_CONCURRENCY=4 GITLAB_BACKUP_MAX_STORAGE_CONCURRENCY=1 CRON：在使用crontab进定时备份时，设置为1是为了不显示备份进度信息，减少备份期间相关输出 0 2 * * * /opt/gitlab/bin/gitlab-backup create CRON=1 # 版本 ④备份相关的配置 备份时的一些配置除了在命令行中通过参数进行配置，还可以在配置文件/etc/gitlab/gitlab.rb或/home/git/gitlab/config/gitlab.yml中进行配置 设置备份文件的权限 gitlab_rails['backup_archive_permissions'] = 0644 # 源码安装时 backup: archive_permissions: 0644 # Makes the backup archives world-readable 设置备份文件的保存期限 如果特意指定备份文件名前缀，使用默认时间戳为前缀的。在执行备份时，可将比早于backup_keep_time时间的备份文件进行自动清除 ## Limit backup lifetime to 7 days - 604800 seconds gitlab_rails['backup_keep_time'] = 604800 # 源码安装时 backup: ## Limit backup lifetime to 7 days - 604800 seconds keep_time: 604800 ⑤配置文件的备份 GitLab提供的备份Rake任务是不会备份配置文件的 手动备份 直接手动备份整个/etc/gitlab目录。如果不想备份整个目录的话，至少也要备份以下文件 /etc/gitlab/gitlab-secrets.json /etc/gitlab/gitlab.rb 源代码安装时 /home/git/gitlab/config/secrets.yml /home/git/gitlab/config/gitlab.yml 命令行备份 当版本 >= 12.3时，可以使用命令sudo gitlab-ctl backup-etc命令备份配置文件。备份文件路径在/etc/gitlab/config_backup/,备份文件及目录都是只有root可读写。 同时可以设置配置备份文件的路径sudo gitlab-ctl backup-etc 配置文件的备份文件名默认规则：gitlab_config_时间戳_日期.tar 会备份以下配置文件 /etc/gitlab/ /etc/gitlab/ssh_host_rsa_key.pub /etc/gitlab/ssh_host_ecdsa_key.pub /etc/gitlab/trusted-certs/ /etc/gitlab/ssh_host_rsa_key /etc/gitlab/ssh_host_ecdsa_key /etc/gitlab/ssh_host_ed25519_key.pub /etc/gitlab/ssh_host_ed25519_key /etc/gitlab/gitlab.rb /etc/gitlab/gitlab-secrets.json 可设置定时任务，15 04 * * 2-6 gitlab-ctl backup-etc && cd /etc/gitlab/config_backup && cp $(ls -t | head -n1) /data/gitlab/config-backups/ ⑥上传备份到外部云存储 可在/etc/gitlab/gitlab.rb中设置参数，让命令行执行的脚本将备份文件上传到外部云存储中，支持的云存储：AWS S3, Google Cloud Storage, Azure Blob storage, 其他S3类型供应商 设置参数将备份上传至云存储时，可在执行命令后添加DIRECTORY参数，将上传文件路径前添加路径，便于备份 sudo gitlab-backup create DIRECTORY=daily sudo gitlab-backup create DIRECTORY=weekly Amazon S3 gitlab_rails['backup_upload_connection'] = { 'provider' => 'AWS', 'region' => 'eu-west-1', 'aws_access_key_id' => 'AKIAKIAKI', 'aws_secret_access_key' => 'secret123' # If using an IAM Profile, don't configure aws_access_key_id & aws_secret_access_key # 'use_iam_profile' => true } gitlab_rails['backup_upload_remote_directory'] = 'my.s3.bucket' Digital Ocean Spaces gitlab_rails['backup_upload_connection'] = { 'provider' => 'AWS', 'region' => 'ams3', 'aws_access_key_id' => 'AKIAKIAKI', 'aws_secret_access_key' => 'secret123', 'endpoint' => 'https://ams3.digitaloceanspaces.com' } gitlab_rails['backup_upload_remote_directory'] = 'my.s3.bucket' 如果上传时出现400 Bad Request，是因为默认备份文件时加密的，而Digital Ocean Spaces不支持上传加密文件，注释或删除掉['gitlab_rails['backup_encryption'] Google Cloud Storage gitlab_rails['backup_upload_connection'] = { 'provider' => 'Google', 'google_storage_access_key_id' => 'Access Key', 'google_storage_secret_access_key' => 'Secret', ## If you have CNAME buckets (foo.example.com), you might run into SSL issues ## when uploading backups (\"hostname foo.example.com.storage.googleapis.com ## does not match the server certificate\"). In that case, uncomnent the following ## setting. See: https://github.com/fog/fog/issues/2834 #'path_style' => true } gitlab_rails['backup_upload_remote_directory'] = 'my.google.bucket' 源码安装时 backup: upload: connection: provider: 'Google' google_storage_access_key_id: 'Access Key' google_storage_secret_access_key: 'Secret' remote_directory: 'my.google.bucket' Azure Blob storage gitlab_rails['backup_upload_connection'] = { 'provider' => 'AzureRM', 'azure_storage_account_name' => '', 'azure_storage_access_key' => '', 'azure_storage_domain' => 'blob.core.windows.net', # Optional } gitlab_rails['backup_upload_remote_directory'] = '' 源码安装时 backup: upload: connection: provider: 'AzureRM' azure_storage_account_name: '' azure_storage_access_key: '' remote_directory: '' 2、恢复备份 ①确定要恢复的备份文件在指定的目录下，git用户并拥有相应读写权限 备份文件要放到配置文件/etc/gitlab/gitlab.rb 中，配置项gitlab_rails['backup_path’] (默认是/var/opt/gitlab/backups)或者backup: path: \"tmp/backups\"(默认是/home/git/gitlab/tmp/backups)指定的路径下 sudo cp /data/gitlab-backups/11493107454_2018_04_25_10.6.4-ce_gitlab_backup.tar /var/opt/gitlab/backups/ sudo chown git.git /var/opt/gitlab/backups/11493107454_2018_04_25_10.6.4-ce_gitlab_backup.tar ②停掉连接数据库的进程 sudo gitlab-ctl stop unicorn sudo gitlab-ctl stop puma sudo gitlab-ctl stop sidekiq # 源码安装时 sudo service gitlab stop # 验证gitlab状态 sudo gitlab-ctl status ③开始恢复指定的备份 sudo gitlab-backup restore BACKUP=11493107454_2018_04_25_10.6.4-ce # 版本 恢复备份时的一些参数 GITLAB_ASSUME_YES=1：恢复备份时，恢复脚本可能跳出一些提示，设置这个环境变量可以跳过这些提示 sudo GITLAB_ASSUME_YES=1 gitlab-backup restore # 源码安装时 sudo -u git -H GITLAB_ASSUME_YES=1 bundle exec rake gitlab:backup:restore RAILS_ENV=production ④恢复数据加密用的秘钥文件 恢复/etc/gitlab/gitlab-secrets.json或/home/git/gitlab/.secret(源码安装时)文件。该文件包含数据库加密密钥，CI / CD变量以及用于两重身份验证的变量。如果您无法连同应用程序数据备份一起还原此加密密钥文件，则启用了双重身份验证的用户以及GitLab Runner都将失去对GitLab服务器的访问权限。 ⑤启动 sudo gitlab-ctl reconfigure sudo gitlab-ctl restart # 源码安装时 sudo service gitlab restart ⑥验证 sudo gitlab-rake gitlab:check SANITIZE=true sudo gitlab-ctl status 当版本 >= 13.1,可执行sudo gitlab-rake gitlab:doctor:secrets检查数据数据库中的数据是否解密啦 3、问题 ①压缩备份文件时报错 sudo /opt/gitlab/bin/gitlab-backup create ... Dumping ... ... gzip: stdout: Input/output error Backup failed 解决方案： 检查磁盘空间是否够用 如果存储的路径是NFS挂载点，检查挂载选项的timeout是否始终，默认是600，可设置大一点 四、通过git clone代码导出代码仓库 shell脚本 #!/bin/bash # A script to backup GitLab repositories. GLAB_BACKUP_DIR=${GLAB_BACKUP_DIR-\"gitlab_backup\"} # where to place the backup files GLAB_TOKEN=${GLAB_TOKEN-\"YOUR_TOKEN\"} # the access token of the account GLAB_GITHOST=${GLAB_GITHOST-\"gitlab.com\"} # the GitLab hostname GLAB_PRUNE_OLD=${GLAB_PRUNE_OLD-true} # when `true`, old backups will be deleted GLAB_PRUNE_AFTER_N_DAYS=${GLAB_PRUNE_AFTER_N_DAYS-7} # the min age (in days) of backup files to delete GLAB_SILENT=${GLAB_SILENT-false} # when `true`, only show error messages GLAB_API=${GLAB_API-\"https://gitlab.com/api/v3\"} # base URI for the GitLab API GLAB_GIT_CLONE_CMD=\"git clone --quiet --mirror git@${GLAB_GITHOST}:\" # base command to use to clone GitLab repos TSTAMP=`date \"+%Y%m%d\"` # The function `check` will exit the script if the given command fails. function check { \"$@\" status=$? if [ $status -ne 0 ]; then echo \"ERROR: Encountered error (${status}) while running the following:\" >&2 echo \" $@\" >&2 echo \" (at line ${BASH_LINENO[0]} of file $0.)\" >&2 echo \" Aborting.\" >&2 exit $status fi } # The function `tgz` will create a gzipped tar archive of the specified file ($1) and then remove the original function tgz { check tar zcf $1.tar.gz $1 && check rm -rf $1 } $GLAB_SILENT || (echo \"\" && echo \"=== INITIALIZING ===\" && echo \"\") $GLAB_SILENT || echo \"Using backup directory $GLAB_BACKUP_DIR\" check mkdir -p $GLAB_BACKUP_DIR $GLAB_SILENT || echo -n \"Fetching list of repositories ...\" GLAB_PROJ_API=\"${GLAB_API}/projects?private_token=${GLAB_TOKEN}&per_page=100&simple=true\" echo ${GLAB_PROJ_API} REPOLIST=`check curl --silent ${GLAB_PROJ_API} | check perl -p -e \"s/,/\\n/g\" | check grep \"\\\"path_with_namespace\\\"\" | check awk -F':\"' '{print $2}' | check sed -e 's/\"}//g'` $GLAB_SILENT || echo \"found `echo $REPOLIST | wc -w` repositories.\" $GLAB_SILENT || (echo \"\" && echo \"=== BACKING UP ===\" && echo \"\") for REPO in $REPOLIST; do $GLAB_SILENT || echo \"Backing up ${REPO}\" check ${GLAB_GIT_CLONE_CMD}${REPO}.git ${GLAB_BACKUP_DIR}/${GLAB_ORG}-${REPO}-${TSTAMP}.git && tgz ${GLAB_BACKUP_DIR}/${GLAB_ORG}-${REPO}-${TSTAMP}.git done if $GLAB_PRUNE_OLD; then $GLAB_SILENT || (echo \"\" && echo \"=== PRUNING ===\" && echo \"\") $GLAB_SILENT || echo \"Pruning backup files ${GLAB_PRUNE_AFTER_N_DAYS} days old or older.\" $GLAB_SILENT || echo \"Found `find $GLAB_BACKUP_DIR -name '*.tar.gz' -mtime +$GLAB_PRUNE_AFTER_N_DAYS | wc -l` files to prune.\" find $GLAB_BACKUP_DIR -name '*.tar.gz' -mtime +$GLAB_PRUNE_AFTER_N_DAYS -exec rm -fv {} > /dev/null \\; fi $GLAB_SILENT || (echo \"\" && echo \"=== DONE ===\" && echo \"\") $GLAB_SILENT || (echo \"GitLab backup completed.\" && echo \"\") 参考 https://docs.gitlab.com/omnibus/settings/backups.html https://docs.gitlab.com/ee/api/project_import_export.html https://docs.gitlab.com/ee/user/project/settings/import_export.html https://gist.github.com/devopstaku/7b1b2594ce657957206f3ec5f262eadb Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-20 22:42:27 "},"origin/gitlab-upgrade.html":{"url":"origin/gitlab-upgrade.html","title":"版本升级","keywords":"","body":"Gitlab版本升级 一、简介 gitlab的版本规则是：主版本(Major).次版本(Minor).补丁版本(Patch) 主版本会在每年的五月22左右发布 次版本会在每月22号左右发布 补丁版本会不定时发布 版本升级说明 在同一个主要版本下，补丁版本和次要版本之间跳转是安全的。例如 12.7.5 -> 12.10.5 11.3.4 -> 11.11.1 12.0.4 -> 12.0.12 11.11.1 -> 11.11.8 而主版本升级，需要考虑版本是否可向后兼容与数据迁移，原则上先升级到当前主版本的最大次版本，然后在升级到下个主要版本最小次版本，依次往最新的版本进行升级 说明文档： https://docs.gitlab.com/ee/policy/maintenance.html https://docs.gitlab.com/ee/update/README.html#version-specific-upgrading-instructions 二、升级前准备 1、确定版本升级路径 可查看最新的版本升级路径文档：https://docs.gitlab.com/ee/update/index.html#upgrade-paths 8.11.Z --> 8.12.0 --> 8.17.7 --> 9.5.10 --> 10.8.7 --> 11.11.8 --> 12.0.12 --> 12.1.17 --> 12.10.14 --> 13.0.14 --> 13.1.11 -- > latest 13.Y.Z 当前版本 目标版本 支持的版本升级路径 14.6.2 15.1.0 14.6.2 -> 14.9.5 -> 14.10.5 -> 15.0.2 -> 15.1.0 14.6.2 15.0.0 14.6.2 -> 14.9.5 -> 14.10.5 -> 15.0.2 13.9.2 14.1.8 13.9.2 -> 13.12.15 -> 14.0.12 -> 14.1.8 12.9.2 13.4.3 12.9.2 -> 12.10.14 -> 13.0.14 -> 13.4.3 11.5.0 13.2.10 11.5.0 -> 11.11.8 -> 12.0.12 -> 12.1.17 -> 12.10.14 -> 13.0.14 -> 13.2.10 11.3.4 12.10.14 11.3.4 -> 11.11.8 -> 12.0.12 -> 12.1.17 -> 12.10.14 10.4.5 12.9.5 10.4.5 -> 10.8.7 -> 11.11.8 -> 12.0.12 -> 12.1.17 -> 12.9.5 9.2.6 12.2.5 9.2.6 -> 9.5.10 -> 10.8.7 -> 11.11.8 -> 12.0.12 -> 12.1.17 -> 12.2.5 8.13.4 11.3.4 8.13.4 -> 8.17.7 -> 9.5.10 -> 10.8.7 -> 11.3.4 已实践的版本升级路线： 11.6.1 ----> 11.11.0-ce.0(关键点) ----> 12.0.1-ce.0(关键点) ----> 12.8.0-ce.0 ----> 12.10.0(关键点) ----> 13.0.0-ce.0(关键点) ----> latest(13.8.3) 2、确定升级步骤 升级的步骤取决于Gitlab的部署方式 二进制包安装 参考文档：https://docs.gitlab.com/omnibus/update/ 源码编译安装 参考文档：https://docs.gitlab.com/ee/update/upgrading_from_source.html Docker方式安装 参考文档：https://docs.gitlab.com/omnibus/docker/README.html#update Helm方式安装 参考文档：https://docs.gitlab.com/charts/installation/upgrade.html#steps 三、Docker升级步骤 以升级14.5.3 -> 14.9.5 -> 14.10.5 -> 15.0.5 -> 15.1.4为例 /data/gitlab/docker-compose.yml web: image: 'gitlab/gitlab-ce:14.5.3-ce.0' restart: always container_name: gitlab hostname: 'gitlab.curiouser.com' environment: GITLAB_OMNIBUS_CONFIG: | external_url 'http://gitlab.curiouser.com' gitlab_rails['gitlab_shell_ssh_port'] = 30022 ports: - '80:80' - '30022:22' volumes: - '/data/gitlab/config:/etc/gitlab' - '/data/gitlab/logs:/var/log/gitlab' - '/data/gitlab/data:/var/opt/gitlab' 1、备份数据和镜像 备份docker-compose整个目录 mkdir /data/gitlab-backup-20220805 cp -r /data/gitlab/ /data/gitlab-backup-20220805 备份docker镜像 docker save -o /data/gitlab-backup-20220805/gitlab-ce-14-5-3-ce.tar gitlab/gitlab-ce:14.5.3-ce.0 2、修改docker-compose文件中的镜像版本 # 14.5.3 -> 14.9.5 sed -i \"s/image\\: \\'gitlab\\/gitlab-ce:14.5.3-ce.0\\'/image\\: \\'gitlab\\/gitlab-ce\\:14.9.5-ce.0\\'/g\" docker-compose.yml # 14.9.5 -> 14.10.5 sed -i \"s/image\\: \\'gitlab\\/gitlab-ce:14.9.5-ce.0\\'/image\\: \\'gitlab\\/gitlab-ce\\:14.10.5-ce.0\\'/g\" docker-compose.yml # 14.10.5 -> 15.0.5 sed -i \"s/image\\: \\'gitlab\\/gitlab-ce:14.10.5-ce.0\\'/image\\: \\'gitlab\\/gitlab-ce\\:15.0.5-ce.0\\'/g\" docker-compose.yml # 15.0.5 -> 15.1.4 sed -i \"s/image\\: \\'gitlab\\/gitlab-ce:15.0.5-ce.0\\'/image\\: \\'gitlab\\/gitlab-ce\\:15.1.4-ce.0\\'/g\" docker-compose.yml 3、重启容器 cd /data/gitlab docker-compose up -d && docker-compose logs -f 4、检查后台迁移任务是否完成 必须确认后台迁移任务均为0时，才可以开始后续的下一个版本的升级 方式一：在Web页面的管理中心–> 监控 –> 后台任务中查看队列中的迁移任务 方式二： GitLab EE > 14.0 gitlab-psql -c 'select job_class_name,table_name, column_name, job_arguments from batched_background_migrations where status <> 3;' GitLab EE=12.9 gitlab-rails runner -e production 'puts Gitlab::BackgroundMigration.remaining' 参考：https://docs.gitlab.com/ee/update/index.html#batched-background-migrations 5、重复2~4步升级至指定版本 每一次替换升级，都需要容器重启至可访问，后台迁移任务检查为0时方可操作下一个版本的升级 6、验证 使用账号密码看是否能正常登陆？ 本地使用git客户端看是否能正常拉取推送代码？ 测试各个环境的gitlab runner，提交代码看pipeline能否正常运行和部署？ 测试钉钉通知是否正常？ 验证SonarQube是否可以通过Gitlab认证进行登录？ 验证新增功能 7、启用新功能 # 进入容器 docker exec -it gitlab bash # 进入GitLab Rails 控制台 gitlab-rails console -e production # 执行命令以启动新功能 Feature.enable(:performance_analytics) 四、升级失败的处理 利用备份数据文件和镜像文件重新恢复最初状态 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-08 18:00:48 "},"origin/gitlab-package-registry.html":{"url":"origin/gitlab-package-registry.html","title":"Gitlab的制品仓库","keywords":"","body":"Gitlab的制品仓库 一、简介 二、配置 Personal Access Token CI Job Token Deploy Token 三、通用文件仓库 Gitlab Docs：https://docs.gitlab.com/ee/user/packages/generic_packages/ 开起 手动上传 curl --header \"PRIVATE-TOKEN: \" \\ --upload-file 要上传的文件 \\ \"http://gitlab地址/api/v4/projects/仓库ID/packages/generic/包名/版本/文件名\" Pipeline中上传 image: curlimages/curl:latest stages: - upload - download upload: stage: upload script: - 'curl --header \"JOB-TOKEN: $CI_JOB_TOKEN\" --upload-file path/to/file.txt \"${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/my_package/0.0.1/file.txt\"' download: stage: download script: - 'wget --header=\"JOB-TOKEN: $CI_JOB_TOKEN\" ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/packages/generic/my_package/0.0.1/file.txt' 制品命令 包名 只允许[a-zA-Z0-9][._-] 版本名 格式：X.Y.Z,包含[0-9][.]，符合/\\A\\d+\\.\\d+\\.\\d+\\z/正则表达式 文件名 只允许[a-zA-Z0-9][._-] 四、Docker Image仓库 1、配置 Gitlab Docs ：https://docs.gitlab.com/ee/administration/packages/container_registry.html 五、Maven Package仓库 Gitlab Docs：https://docs.gitlab.com/ee/user/packages/maven_repository/ settings.xml gitlab-maven Private-Token / Deploy-Token / Job-Token YOUR_PERSONAL_ACCESS_TOKEN / YOUR_DEPLOY_TOKEN / ${env.CI_JOB_TOKEN} pom.xml gitlab-maven https://gitlab.example.com/api/v4/projects/PROJECT_ID/packages/maven gitlab-maven https://gitlab.example.com/api/v4/projects/PROJECT_ID/packages/maven gitlab-maven https://gitlab.example.com/api/v4/projects/PROJECT_ID/packages/maven 在gitlab的pipeline中pom.xml文件 gitlab-maven ${env.CI_SERVER_URL}/api/v4/projects/${env.CI_PROJECT_ID}/packages/maven gitlab-maven ${env.CI_SERVER_URL}/api/v4/projects/${env.CI_PROJECT_ID}/packages/maven gitlab-maven ${env.CI_SERVER_URL}/api/v4/projects/${env.CI_PROJECT_ID}/packages/maven 发布命令 mvn deploy # 如果成功后会显示一下信息 Uploading to gitlab-maven: https://gitlab.example.com/api/v4/projects/PROJECT_ID/packages/maven/com/mycompany/mydepartment/my-project/1.0-SNAPSHOT/my-project-1.0-20200128.120857-1.jar Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/gitlab-配置代码仓库事件触发器Webhook.html":{"url":"origin/gitlab-配置代码仓库事件触发器Webhook.html","title":"Gitlab的Webhook","keywords":"","body":"Gitlab 代码仓库配置事件Webhoook触发器 一、Overviews Webhook与异步编程中\"订阅-发布模型\"非常类似，一端触发事件，一端监听执行。 WebHook就是一个接收HTTP POST（或GET，PUT，DELETE）的URL 通常来说，WebHook通过HTTP协议将请求数据发送到外部系统后，就不再处理响应数据啦！ 二、Gitlab事件触发器配置Webhook 当Gitlab中代码仓库发生了一些事件后，可设置触发器发送http通知到外部系统。通常配置Jenkins上的Webhook 配置Gitlab的网络限制 否则后续发送webhook请求时会报错\"Requests to the local network are not allowed\",原因详见：附录 配置代码代码仓库的Webhook 配置完可以模拟一些事件进行测试 查看详细的Webhook请求信息 附录：gitlab发送webhook请求时报错原因 If you have non-GitLab web services running on your GitLab server or within its local network, these may be vulnerable to exploitation via Webhooks. With Webhooks, you and your project maintainers and owners can set up URLs to be triggered when specific things happen to projects. Normally, these requests are sent to external web services specifically set up for this purpose, that process the request and its attached data in some appropriate way. Things get hairy, however, when a Webhook is set up with a URL that doesn't point to an external, but to an internal service, that may do something completely unintended when the webhook is triggered and the POST request is sent. Because Webhook requests are made by the GitLab server itself, these have complete access to everything running on the server (http://localhost:123) or within the server's local network (http://192.168.1.12:345), even if these services are otherwise protected and inaccessible from the outside world. If a web service does not require authentication, Webhooks can be used to trigger destructive commands by getting the GitLab server to make POST requests to endpoints like \"http://localhost:123/some-resource/delete\". To prevent this type of exploitation from happening, starting with GitLab 10.6, all Webhook requests to the current GitLab instance server address and/or in a private network will be forbidden by default. That means that all requests made to 127.0.0.1, ::1 and 0.0.0.0, as well as IPv4 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16 and IPv6 site-local (ffc0::/10) addresses won't be allowed. This behavior can be overridden by enabling the option \"Allow requests to the local network from hooks and services\" in the \"Outbound requests\" section inside the Admin area under Settings (/admin/application_settings): Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/gitlab-server-hook.html":{"url":"origin/gitlab-server-hook.html","title":"Gitlab的服务端git hook","keywords":"","body":"Gitlab的服务端git钩子 一、简介 官方文档：https://docs.gitlab.com/ee/administration/server_hooks.html Git除了客户端有各种钩子进行配置使用，远程存储库，例如gitlab也支持服务端的钩子。本地git客户端推送代码到远程仓库时，可以触发执行钩子脚本。 服务端git钩子类型 pre-receive：git push向仓库推送代码时被执行 post-receive：在成功推送后被调用，适合用于发送通知。这个脚本没有参数，但和pre-receive一样通过标准输入读取 update：在pre-receive之后被调用。它也是在实际更新前被调用的，但它可以分别被每个推送上来的引用分别调用。也就是说如果用户尝试推送到4个分支，update会被执行4次。不需要读取标准输入。事实上，它接受三个参数： 更新的引用名称 引用中存放的旧的对象名称 引用中存放的新的对象名称 支持配置的范围 单个仓库 全局所有的仓库 应用场景 保护仓库CICD脚本(例如：.gitlab-ci.yaml，Dockerfile)等非业务代码不被开发人员修改 检查MergeRequest的变更，或者触发其他动作 检查commit的提交者信息是否是gitlab中设置的用户名，便于溯源代码提交者信息 二、配置 1、创建钩子脚本 单个仓库钩子脚本 根据钩子类型需要在仓库存储路径下的custom_hooks(不存在就自行创建)文件下创建对应钩子类型名称的脚本文件 在项目经过Hashed过的项目路径下创建钩子脚本文件（项目hashed的路径参考附录一） mkdir /var/opt/gitlab/git-data/repositories/@hashed/项目hashed路径/custom_hooks touch /var/opt/gitlab/git-data/repositories/@hashed/项目hashed路径.git/custom_hooks/pre-receive chmod +x /var/opt/gitlab/git-data/repositories/@hashed/项目hashed路径.git/custom_hooks/pre-receive 全局仓库钩子脚本 全局构建脚本默认配置路径：Omnibus安装模式下是在 /opt/gitlab/embedded/service/gitlab-shell/hooks，源码安装模式下是在/home/git/gitlab-shell/hooks 根据钩子类型需要在钩子默认配置路径下创建对应钩子类型名称的文件夹(其他名字的会被忽略)。例如：pre-receive.d,post-receive.d,update.d mkdir -p /opt/gitlab/embedded/service/gitlab-shell/hooks/{pre-receive.d,post-receive.d,update.d} touch detect-modify-cicd-scripts-files.sh chmod +x detect-modify-cicd-scripts-files.sh 三、编写钩子脚本 脚本生效顺序： 内置的钩子脚本 单个仓库.git/custom_hooks/以钩子类型名称命名的脚本文件 单个仓库.git/custom_hooks/钩子类型名称.d/* /钩子类型名称.d/* 对于文件夹下的多个脚本则按字母顺序执行，以脚本非零值退出时中断执行。 脚本语言支持： shell ruby 1、检查commit是否修改受保护文件 脚本逻辑： 判断当前推送的分支是否为新分支？如果是新分支，n 使用git diff对比commit与当前仓库commit有变更的文件 判读变更文件是否是预设受保护的文件？是的话，追加到预置变量中进行记录 判断预置变量是否为空？不为空，代表本次commit有修改受保护文件。 再次判断当前commit的用户是否为预设允许修改受保护文件的用户？是的话，允许提交。不是的话， git push显示警告，然后发送钉钉通知 判断提交协议是否是web？是的话，则表示触发执行当前脚本的是Web界面上创建的MergeRequest请求，然后显示告警信息。 最后脚本返回状态1，然后gitlab就会拒绝当前commit的推送 #!/bin/bash new_branch_commitid_zero40=0000000000000000000000000000000000000000 protected_files=\".gitlab-ci.yml docker/Dockerfile docker/config.sh docker/k8s-application.tpl.yaml docker/k8s-cronjob.tpl.yaml\" approval_users=\"root curiouser\" DINGDING_ROBOT_TOKEN=\"钉钉机器人的Token\" changed_file=\"\" while read oldrev newrev refname; do if [ $oldrev -eq $new_branch_commitid_zero40 ] ; then commit_changed_file_name=$(git diff --name-only ${newrev}^!) else commit_changed_file_name=$(git diff --name-only $oldrev $newrev) fi for i in $commit_changed_file_name; do if $(echo $protected_files | grep -q $i); then changed_file+=\" $i\\n\" fi done if [[ ! $changed_file == \"\" ]] && [[ ${approval_users/$GL_USERNAME//} == $approval_users ]]; then echo -e \"\\033[31m########################################\\033[0m\" if [[ $GL_PROTOCOL == 'web' ]] ;then echo \"GL-HOOK-ERR: 本次MergeRequest修改了受保护文件:\" echo -e \"GL-HOOK-ERR: ${changed_file//\\\\n/ }\" echo \"GL-HOOK-ERR: 无法完成合并，请检查修改！\" fi echo -n -e \"\\033[31m${GL_USERNAME}, 请不要修改以下仓库保护文件: \\n${changed_file}本次推送${newrev:0:8}将被拒绝推送至远程仓库\\n########################################\\033[0m\" curl -s -H 'Content-Type: application/json' -d '{\"msgtype\": \"markdown\",\"markdown\": {\"title\": \"Gitlab\",\"text\": \"# Gitlab仓库保护文件修改通知\\n'$GL_USERNAME'正在尝试将以下保护文件修改推送至'$GL_PROJECT_PATH'仓库中: \\n > '\"$changed_file\"'\\n#### 推送'\"${newrev:0:8}\"'已拒绝 \"},\"at\": {\"isAtAll\": true}}' https://oapi.dingtalk.com/robot/send?access_token=$DINGDING_ROBOT_TOKEN >/dev/null exit 1 fi done 验证测试 ①当在git本地修改了受保护文件推送至远程仓库时，会显示提示信息，并拒绝推送 ②当Web界面创建MergeRequest合并某个分支到目标分支时，修改了目标分支，会显示提示信息，并拒绝合并 2、检查commit用户名及邮箱的设置 #!/usr/bin/env bash # 访问 GitLab API 所需的 token, 需使用 admin 用户生成, scope 选择 read_user 即可 # https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html TOKEN=\"GitLab admin user read_user TOKEN\" # GitLab 服务的访问地址, 因为该脚本是放置在 GitLab 服务中, 所以使用本机地址即可 GITLAB_URL=http://127.0.0.1:30000 # 使用 python 提取 json 中的 name 和 email 代码 get_name_and_email=\"import sys, json; try: obj=json.load(sys.stdin); print(obj[0]['name']+':'+obj[0]['email']) except: print('error')\" # 访问 GitLab Users API 获取用户显示名称与 email # GL_USERNAME 为 GitLab 的 username, 一般为英文, 而 commit 中的 username 我们一般设置为中文 # API 返回的数据为 json 格式, 通过 python 代码进行提取显示名称与 email # 因为显示名称为中文, 为了解决乱码问题, 配置 PYTHONIOENCODING='UTF-8' # python 返回的格式为 name:email # https://docs.gitlab.com/ee/api/users.html#list-users GITLAB_NAME_EMAIL=`curl -s --header \"Private-Token: ${TOKEN}\" \"${GITLAB_URL}/api/v4/users?username=${GL_USERNAME}\" | PYTHONIOENCODING='UTF-8' python3 -c \"${get_name_and_email}\"` if [ \"${GITLAB_NAME_EMAIL}\" == \"error\" ]; then echo \"Push 异常: GitLab 获取用户信息异常, 请通知管理员进行排查\" exit 1 fi echo $GL_USERNAME $GITLAB_NAME_EMAIL # 截取显示名称 GITLAB_USER_NAME=${GITLAB_NAME_EMAIL%:*} # 截取 email GITLAB_USER_EMAIL=${GITLAB_NAME_EMAIL#*:} zero_commit=\"0000000000000000000000000000000000000000\" excludeExisting=\"--not --all\" while read oldrev newrev refname; do # branch or tag get deleted if [ \"$newrev\" = \"$zero_commit\" ]; then continue fi # Check for new branch or tag if [ \"$oldrev\" = \"$zero_commit\" ]; then span=`git rev-list $newrev $excludeExisting` else span=`git rev-list $oldrev..$newrev $excludeExisting` fi for COMMIT in $span; do AUTHOR_USER=`git log --format=%an -n 1 ${COMMIT}` AUTHOR_EMAIL=`git log --format=%ae -n 1 ${COMMIT}` # COMMIT_USER=`git log --format=%cn -n 1 ${COMMIT}` # COMMIT_EMAIL=`git log --format=%ce -n 1 ${COMMIT}` # 进行 username 与 email 校验 # 在 GitHub 的示例脚本中启用了 AUTHOR_USER 与 AUTHOR_EMAIL 校验, 但是使用时可能存在 author 与 committer 不是同一个人的情况, 故注释校验 AUTHOR_USER 与 AUTHOR_EMAIL 的代码 # 如果自己公司实际使用时不存在这种情况, 可以取消注释 # author 与 committer 区别 https://stackoverflow.com/q/6755824 if [[ ${AUTHOR_USER} != ${GITLAB_USER_NAME} ]]; then echo -e \"\\033[31mPush 异常: ${COMMIT:0:8} 的 author (${AUTHOR_USER}) 不是 GitLab 中的中文名 (${GITLAB_USER_NAME})\\033[0m\" exit 20 fi # if [[ ${COMMIT_USER} != ${GITLAB_USER_NAME} ]]; then # echo -e \"\\033[31mPush 异常: ${COMMIT:0:8} 的 committer (${COMMIT_USER}) 不是 GitLab 中的中文名 (${GITLAB_USER_NAME})\\033[0m\" # exit 30 # fi if [[ ${AUTHOR_EMAIL} != ${GITLAB_USER_EMAIL} ]]; then echo -e \"\\033[31mPush 异常: ${COMMIT:0:8} 的 author 的邮箱 (${AUTHOR_EMAIL}) 不是 GitLab 中的邮箱 (${GITLAB_USER_EMAIL})\\033[0m\" exit 40 fi # if [[ ${COMMIT_EMAIL} != ${GITLAB_USER_EMAIL} ]]; then # echo -e \"\\033[31mPush 异常: ${COMMIT:0:8} 的 committer 的邮箱 (${COMMIT_EMAIL}) 不是 GitLab 中的邮箱 (${GITLAB_USER_EMAIL})\\033[0m\" # exit 50 # fi done done exit 0 四、脚本可引用的环境变量 环境变量 描述 GL_ID 用户ID。格式：key-ID GL_PROJECT_PATH 仓库路径，GitLab版本 >= 13.2 GL_PROTOCOL 推送协议（http/ssh/web），GitLab版本 >= 13.2 GL_REPOSITORY 仓库ID，project- GL_USERNAME 用户名称 pre-receive 和 post-receive server 钩子脚本可引用的变量 环境变量 描述 GIT_ALTERNATE_OBJECT_DIRECTORIES 仓库底层存储文件路径。（可选，路径包含Hash过的路径） GIT_OBJECT_DIRECTORY 仓库底层存储文件路径。（路径包含Hash过的路径） GIT_PUSH_OPTION_COUNT Number of push options. See Git pre-receive documentation. GIT_PUSH_OPTION_ Value of push options where i is from 0 to GIT_PUSH_OPTION_COUNT - 1. See Git pre-receive documentation. 附录 1、显示仓库文件Hash之后的存储路径 在gitlab-rails控制台中使用命令显示 /var/opt/gitlab/git-data/repositories/ # 进入gitlab-rails控制台 gitlab-rails console => Project.find(项目ID).disk_path \"@hashed/6f/4b/6f4b6612125fb3a0daecd2799dfd6c9c299424fd920f9b308110a2c1fbd8f443\" => Project.find_by_full_path('组/仓库').disk_path \"@hashed/6f/4b/6f4b6612125fb3a0daecd2799dfd6c9c299424fd920f9b308110a2c1fbd8f443\" 拼凑 项目仓库文件Hash之后的存储路径是Project_ID进行SHA256加密过的字符串构成。例如 /var/opt/gitlab/git-data/repositories/@hashed/ |--Project_ID的SHA256加密字符串前两位/ |--Project_ID的SHA256加密字符串第三位和第四位 |--Project_ID的SHA256加密字符串.git |--Project_ID的SHA256加密字符串.wiki.git 参考 https://docs.gitlab.com/ee/administration/server_hooks.html https://blog.csdn.net/ygdlx521/article/details/115751703 https://git-scm.com/book/tr/v2/Customizing-Git-An-Example-Git-Enforced-Policy https://ghthou.github.io/2020/11/07/GitLab-%E4%BD%BF%E7%94%A8-Server-Hooks-%E6%A0%A1%E9%AA%8C-Commit-%E7%94%A8%E6%88%B7%E5%90%8D%E4%B8%8E%E9%82%AE%E7%AE%B1/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/gitlab-runner.html":{"url":"origin/gitlab-runner.html","title":"Gitlab Runner","keywords":"","body":"Gitlab Runner 一、简介 Gitlab Runner是Gitlab Pipeline中各阶段Job任务中脚本执行的承载者。 二、原理 1、Runner说明 所有runner注册到gitlab时都要选择一个执行器，而执行器决定了pipeline中job任务的运行环境 支持的runner执行器： SSH Shell Parallels VirtualBox Docker Docker Machine (auto-scaling) Kubernetes Custom runner类型 Shared runners 共享runner：所有仓库可以使用 Group runners 组runner：所有仓库可以使用 Specific runners 特殊runner：个别指定的仓库可以使用 2、Runner执行器工作流 执行器为kubernetes的runner的工作流 三、注册Runner 1、在Gitlab中获取Runner的注册信息 2、Runner注册流程 Runtime platform arch=amd64 os=linux pid=48 revision=2ebc4dc4 version=13.9.0 Running in system-mode. Enter the GitLab instance URL (for example, https://gitlab.com/): # 输入gitlab实例的地址 Enter the registration token: # 输入向gitlab实例注册的Token Enter a description for the runner: [759510becaba]: # 输入对当前gitlab runner的描述信息 Enter tags for the runner (comma-separated): # 输入当前gitlab runner的标签 Registering runner... succeeded runner=Db2NbZ2M Enter an executor: docker-ssh+machine, kubernetes, custom, docker, parallels, virtualbox, docker+machine, docker-ssh, shell, ssh: # 输入当前gitlab runner的执行器类型 docker Enter the default Docker image (for example, ruby:2.6): # 输入当前gitlab runner的执行器默认使用的docker镜像 alpine:latest Runner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded! 四、Runner部署及配置 1、Docker 部署runner docker run -d \\ --name gitlab-runner \\ --restart always \\ -v /srv/gitlab-runner/config:/etc/gitlab-runner \\ -v /var/run/docker.sock:/var/run/docker.sock \\ gitlab/gitlab-runner:latest 注册runner docker exec -it gitlab-runner gitlab-runner register \\ --non-interactive \\ --url # gitlab地址 \\ --registration-token # gitlab的runner注册Token \\ --executor # 执行器 \\ --description # \"runner的详细描述\" \\ --docker-image # 默认镜像 \\ --docker-privileged \\ --docker-pull-policy # \"镜像拉取策略\" \\ --tag-list # \"runner 标签\" 2、二进制 部署runner # Linux x86-64 / Linux x86 / Linux arm / Linux arm64 / Linux s390x arch=[ amd64,386,arm,arm64,s390x ] curl -L -o /usr/local/bin/gitlab-runner \"https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-linux-$arch\" && \\ chmod +x /usr/local/bin/gitlab-runner && \\ useradd --comment 'GitLab Runner' --create-home gitlab-runner --shell /bin/bash && \\ gitlab-runner install --user=gitlab-runner --working-directory=/home/gitlab-runner && \\ gitlab-runner start 注册 runner gitlab-runner register \\ --non-interactive \\ --url # gitlab地址 \\ --registration-token # gitlab的runner注册Token \\ --executor # 执行器 \\ --description # \"runner的详细描述\" \\ --docker-image # 默认镜像 \\ --docker-privileged \\ --docker-pull-policy # \"镜像拉取策略\" \\ --tag-list # \"runner 标签\" 3、Kubernetes helm repo add gitlab https://charts.gitlab.io helm update 编写helm charts values.yaml配置文件 helm install gitab-runner --namespace gitlab -f values.yaml gitlab/gitlab-runner 五、Runner与Pipeline的流程图 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/gitlab-pipeline.html":{"url":"origin/gitlab-pipeline.html","title":"Gitlab Pipeline","keywords":"","body":"Gitlab CI/CD Pipeline 一、简介 Stages 所有 Stages 会按照顺序运行，即当一个 Stage 完成后，下一个 Stage 才会开始。 只有当所有 Stages 完成后，该构建任务 (Pipeline) 才会成功 如果任何一个 Stage 失败，那么后面的 Stages 不会执行，该构建任务 (Pipeline) 失败 有.pre、.post和default Jobs：构建任务，表示某个 Stage 里面执行的工作 相同 Stage 中的 Jobs 会并行执行 相同 Stage 中的 Jobs 都执行成功时，该 Stage 才会成功 如果任何一个 Job 失败，那么该 Stage 失败，即该构建任务 (Pipeline) 失败 二、Stage定义及语法 三、Job定义及语法 变量variable 脚本script image Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/gitlab-cicd-mr-scan-mr-approval.html":{"url":"origin/gitlab-cicd-mr-scan-mr-approval.html","title":"流水线：MR触发代码扫描+MR审批+合并触发CICD","keywords":"","body":"Gitlab CICD：自动扫描MR代码+MR CodeReview审批 一、简介 ​ 代码静态扫描能避免代码技术层面大部分的问题，而人为codereview能从业务层面避免部分的代码业务逻辑问题。两者结合起来，代码质量能得到有效提升。那在以gitlab pipeline为基础的CICD中如何实现？ ​ 集成代码自动化扫描，只要在Gitlab Pipeline中添加一个stage阶段任务即可实现。但在实际使用过程中，遇到了扫描频繁导致SonarQube服务器后台扫描任务崩溃的问题。那该如何控制扫描频率和开发流程相得益彰呢？同时，扫描完后的结果如何在gitlab中呈现，而不用反复在SonarQube服务端切换页面？ ​ 自动化静态代码扫描之后，人为再介入codereview，会节省人工成本。但在什么时候介入会比较合适呢？同时，在Gitlab Pipeline CICD过程中，合并代码往往是运维人员。而codereview任务是技术负责人处理的，那如何实现两者之间的审批流呢？对于公共项目，多个项目负责人该如何实现审批流？ 二、CICD流水线详解 master、release、dev分支分别对应部署生产环境、stg环境、测试环境 个人从master分支创建feature分支或hotfix分支，开发完成后，创建MR合并feature分支到dev、release分支，对应开发流程到集成测试、stg测试等阶段 stg测试环境相对于集成测试环境，代码变更没有那么频繁。如果代码扫描放在集成测试环境的MR触发流水线中，会造成频繁扫描，SonarQube服务会吃不消。所以放在stg测试环境对应release分支合并触发的流水线中会比较合适。 直推master、release、dev分支：直接触发构建部署流水线 再次推送MR源分支：依旧会触发代码扫描流水线 三、相关配置 1、Gitlab设置 设置合并必须要等到流水线成功 设置合并请求批准 2、文件结构 .....项目文件.... └── docker ├── Dockerfile ├── config.sh └── k8s-application.tpl.yaml └── .gitlab-ci.yml └── .dockerignore └── .gitignore 3、.gitlab-ci.yml image: golang-builder:alpine-with-dockerctl variables: DOCKER_HOST: tcp://localhost:2375 DOCKER_DAEMON: | { \"registry-mirrors\": [\"https://1231.mirror.aliyuncs.com\"], \"insecure-registries\":[\"hub.curiouser.com\"] } services: - name: docker:20.04-dind command: - /bin/sh - -c - | mkdir -p /etc/docker || exit echo \"$DOCKER_DAEMON\" > /etc/docker/daemon.json || exit dockerd-entrypoint.sh || exit stages: - scan-code - build - deploy ############################################################################## # sonarscanner-cli scan code # ############################################################################## scan-code: stage: scan-code image: name: sonarscanner-cli:4.3.0.2102 entrypoint: [\"\"] variables: GIT_DEPTH: 0 script: - sonar-scanner -Dsonar.host.url=http://sonarqube.curiouser.com -Dsonar.login=$SONARQUBE_SERVER_TOKEN -Dsonar.qualitygate.wait=true -Dsonar.qualitygate.timeout=600 -Dsonar.projectName=$CI_PROJECT_NAME -Dsonar.projectKey=$CI_PROJECT_NAME -Dsonar.projectVersion=$CI_COMMIT_SHA -Dsonar.gitlab.project_id=$CI_PROJECT_ID -Dsonar.gitlab.commit_sha=$CI_COMMIT_SHA -Dsonar.gitlab.ref_name=$CI_COMMIT_REF_NAME -Dsonar.links.scm=$CI_PROJECT_URL -Dsonar.links.ci=$CI_PIPELINE_URL rules: - if: $CI_PIPELINE_SOURCE == 'merge_request_event' && $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == 'release' tags: - k8s-offline-runner ##################################################################### # Kubernetes build scripts # ##################################################################### .k8s_build: &k8s_build - export RELEASE_NUM=\"$(date +%Y%m%d)-$(echo $CI_COMMIT_SHA | cut -c1-8)\" - export IMAGE_NAME=\"hub.curiouser.com/$CI_PROJECT_NAME/$APPENV:$RELEASE_NUM\" - docker login hub.curiouser.com -u ci -p $HARBOR_TOKEN - docker build -t \"hub.curiouser.com/$CI_PROJECT_NAME/$APPENV:$RELEASE_NUM\" . -f docker/Dockerfile - docker push \"hub.curiouser.com/$CI_PROJECT_NAME/$APPENV:$RELEASE_NUM\" ###################################################################### # Kubernetes deploy scripts # ###################################################################### .k8s_deploy: &k8s_deploy - export RELEASE_NUM=\"$(date +%Y%m%d)-$(echo $CI_COMMIT_SHA | cut -c1-8)\" - export IMAGE_NAME=\"hub.curiouser.com/$CI_PROJECT_NAME/$APPENV:$RELEASE_NUM\" - ./docker/config.sh docker/k8s-application.tpl.yaml > k8s-application.yaml - cat k8s-application.yaml - kubectl apply -f k8s-application.yaml - kubectl -n $NAMESPACE rollout status --timeout=260s deployment/$CI_PROJECT_NAME || exit_code=$? - | if [ $exit_code -ne 0 ];then ROLLBACK_ID=$(kubectl -n $NAMESPACE rollout undo deployment/$CI_PROJECT_NAME -ojson | jq -r '.status.observedGeneration') ; curl -s https://oapi.dingtalk.com/robot/send?access_token=\"$PIPELINE_DINGDING_ROBOT_TOKEN\" -H 'Content-Type: application/json' -d '{\"msgtype\": \"markdown\",\"markdown\": {\"title\": \"Gitlab流水线部署失败\",\"text\": \"['$CI_PROJECT_NAME']('$CI_PROJECT_URL'/-/tree/'$CI_BUILD_REF_NAME')的'$APPENV'环境第['$CI_PIPELINE_ID']('$CI_PIPELINE_URL')号流水线'$CI_JOB_STAGE'阶段失败，已回滚至最近一个稳定版本'$ROLLBACK_ID'，请检查相关错误！\"},\"at\": {\"isAtAll\": true}}' > /dev/null; exit 1; fi ############################################################################## # Test environment build and deploy CI/CD stage scripts # ############################################################################## build-test: stage: build variables: APPENV: test script: *k8s_build only: - develop tags: - k8s-offline-runner deploy-test: stage: deploy image: kubeconfig-offline:1.17.0 variables: NAMESPACE: app-test APPENV: test POD_NUM: 1 script: *k8s_deploy only: - develop tags: - k8s-offline-runner ############################################################################## # Stg environment build and deploy CI/CD stage scripts # ############################################################################## build-stg: stage: build variables: APPENV: stg script: *k8s_build only: - release tags: - k8s-offline-runner deploy-stg: stage: deploy image: kubeconfig-offline:1.17.0 variables: NAMESPACE: app-stg APPENV: stg POD_NUM: 1 script: *k8s_deploy only: - release tags: - k8s-offline-runner ############################################################################## # Production environment build and deploy CI/CD stage scripts # ############################################################################## build-prd: stage: build variables: APPENV: prod script: *k8s_build only: - tags except: - release - develop tags: - k8s-online-runner deploy-prd: stage: deploy image: kubeconfig-online:1.17.0 variables: NAMESPACE: app-prod APPENV: prod POD_NUM: 2 script: *k8s_deploy only: - tags except: - release - develop when: manual tags: - k8s-online-runner Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-07-27 11:05:29 "},"origin/gitlab-kickout-apache-wget.html":{"url":"origin/gitlab-kickout-apache-wget.html","title":"解耦代码仓库路径下的容器化和k8s部署资源等非业务代码文件","keywords":"","body":"剥离Gitlab代码仓库中非业务代码文件 一、简介 ​ 不管使用 Jenkins、Drone CI还是 Gitlab Pipeline做 CICD 自动化，代码仓库的 根目录 总是会有一些流水线相关的配置文件。如果使用的 Git Flow不注意这个问题，会因为部署环境的不一致，这些文件在各个分支中会不一致。之后一个代码合并可就会代码冲突、合并错误，造成代码部署不到环境中。同时这些文件，对于开发者无关紧要，但是对于devops来说却是关键。所有要把这些配置文件从业务代码仓库中剥离出来，与业务代码解耦。做到开发无感知，同时又方便灵活配置。 ​ Gitlab 从 12.6 版本开始，pipeline支持读取使用外部存储的.gitlab-ci.yml。这样就可将Gitlab 所有项目业务代码仓库中的容器化配置、k8s部署配置等非业务文件剥离出来，按照目录层级方式进行区分，放在一个静态文件服务器中，或者作为一个单独的应用代码仓库，放在到 Gitlab中，通过 Pipeline 脚本流程部署到 k8s 中的 Apache2服务容器中。配置好访问地址或域名，在其他业务代码仓库cicd脚本流程中需要的地方使用wget进行下载自己的 CICD 脚本文件或容器化配置。 统一存储管理的优点： 可以快速集中修改 不与业务代码耦合，防止各分支中这些文件差异引起的代码冲突。 所有文件进行git版本管理，变更有历史追溯 统一存储管理防单点故障及安全措施 跟普通业务代码应用仓库一样进行容器化部署 无状态，使用K8s部署多个实例 设置用户名密码访问下载，在Gitlab 保存密码（密码设置成保密隐藏变量） 限制 IP 地址段访问下载 二、流水线配置文件中心的部署 以下示例操作时，是将流水线配置文件代码仓库部署到 k8s中 apace2容器中，统称为：流水线配置文件中心 1. 目录层级 -- docker |-- config.sh |-- Dockerfile |-- k8s-application.tpl.yaml -- .gitlab-ci.yml -- application -- app1 |-- .gitlab-ci.yml |-- test |-- Dockerfile |-- k8s-deploymen.tpl.yaml |-- stg |-- Dockerfile |-- k8s-deploymen.tpl.yaml |-- prod |-- Dockerfile |-- k8s-deploymen.tpl.yaml -- app2 |-- .gitlab-ci.yml |-- test |-- Dockerfile |-- k8s-deploymen.tpl.yaml |-- stg |-- Dockerfile |-- k8s-deploymen.tpl.yaml |-- prod |-- Dockerfile |-- k8s-deploymen.tpl.yaml .... 2. docker/k8s-application.tpl.yaml 模板中的变量会在该仓库的Gitlab CICD 过程中提供。然后使用 config.sh替换该模板文件中的预置变量，生成最终能应用到 k8s 集群的文件 CI_PROJECT_NAME：是该仓库的仓库名 IMAGE_NAME： CICD 脚本命令拼成的镜像名 POD_NUM： 在 k8s中该应用的部署个数 apiVersion: apps/v1 kind: Deployment metadata: name: $CI_PROJECT_NAME namespace: tools labels: app: $CI_PROJECT_NAME annotations: kubernetes.io/change-cause: $IMAGE_NAME spec: selector: matchLabels: app: $CI_PROJECT_NAME replicas: $POD_NUM template: metadata: labels: app: $CI_PROJECT_NAME spec: imagePullSecrets: - name: registry-pull-secret volumes: - name: apache2-80-conf configMap: name: $CI_PROJECT_NAME items: - key: apache2-80.conf path: 000-default.conf - name: apache2-80-auth-file configMap: name: $CI_PROJECT_NAME items: - key: apache2-80-auth-file path: .htpasswd containers: - name: app image: $IMAGE_NAME imagePullPolicy: IfNotPresent env: - name: K8S_POD_UID valueFrom: fieldRef: fieldPath: metadata.uid volumeMounts: - name: apache2-80-conf mountPath: /etc/apache2/sites-enabled/000-default.conf subPath: 000-default.conf - name: apache2-80-auth-file mountPath: /etc/apache2/.htpasswd subPath: .htpasswd ports: - containerPort: 80 protocol: TCP name: web livenessProbe: failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 tcpSocket: port: 80 timeoutSeconds: 2 readinessProbe: failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 2 tcpSocket: port: 80 resources: limits: cpu: \"500m\" memory: 500Mi requests: cpu: \"100m\" memory: 100Mi configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: app-docker-k8s-config namespace: tools data: # http base auth基础认证密码文件内容 apache2-80-auth-file: | cicd-user:\"该处内容填充htpasswd工具生成的密码本\" # Apache的配置文件，配置访问下载文件需要进行http基础认证 apache2-80.conf: | ServerAdmin localhost DocumentRoot /var/www/html ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined AuthType Basic AuthName \"Restricted Content\" AuthUserFile /etc/apache2/.htpasswd Require valid-user 3. docker/Dockerfile FROM apache2:latest COPY applications /var/www/html RUN echo \"$CI_COMMIT_SHORT_SHA\" >/var/www/html/index.html 4. docker/config.sh 该脚本用于替换 Gitlab Runner 中的变量到模板文件预置的变量Key中 #!/bin/sh if [ ! -n \"$1\" ]; then echo 'must input file name ' exit 1 else cat $1 | sed 's/\\$NAMESPACE'\"/$NAMESPACE/g\" | sed 's~\\$IMAGE_NAME'\"~$IMAGE_NAME~g\" | sed 's~\\$APPENV'\"~$APPENV~g\" | sed 's~\\$POD_NUM'\"~$POD_NUM~g\" | sed 's~\\$CI_PROJECT_NAME'\"~$CI_PROJECT_NAME~g\" | cat - fi exit 0 5. .gitlab-ci.yml 该应用的部署自动化Gitlab CICD脚本，脚本中的部分变量是在 Gitlab 通用 CICD 变量中保存的（设置的是值隐藏保密模式：会在输出时以星号代替） HARBOR_TOKEN：保存的是Harbor 拉取镜像账号的 Token PIPELINE_DINGDING_ROBOT_TOKEN：保存的是该应用部署失败时，发送钉钉通知机器人的Webhook Token image: alpine:with-dockerctl variables: DOCKER_HOST: tcp://localhost:2375 DOCKER_DAEMON: | { \"registry-mirrors\": [\"https://12345.mirror.aliyuncs.com\"], \"insecure-registries\": [\"harbor.curiouser.com\"] } services: - name: docker:18.09-dind command: - /bin/sh - -c - | mkdir -p /etc/docker || exit echo \"$DOCKER_DAEMON\" > /etc/docker/daemon.json || exit dockerd-entrypoint.sh || exit stages: - build - deploy ##################################################################################### ### 构建阶段脚本 ## ##################################################################################### .k8s_build: &k8s_build - export RELEASE_NUM=\"$(date +%Y%m%d)-$(echo $CI_COMMIT_SHA | cut -c1-8)\" - export IMAGE_NAME=\"harbor.curiouser.com/$CI_PROJECT_NAME/$APPENV:$RELEASE_NUM\" - docker login harbor.curiouser.com -u gitlab-ci -p $HARBOR_TOKEN - docker build -t \"harbor.curiouser.com/$CI_PROJECT_NAME/$APPENV:$RELEASE_NUM\" . -f docker/Dockerfile - docker push \"harbor.curiouser.com/$CI_PROJECT_NAME/$APPENV:$RELEASE_NUM\" ##################################################################################### ### 部署阶段脚本 ## ##################################################################################### .k8s_deploy: &k8s_deploy - export RELEASE_NUM=\"$(date +%Y%m%d)-$(echo $CI_COMMIT_SHA | cut -c1-8)\" - export IMAGE_NAME=harbor.curiouser.com/$CI_PROJECT_NAME/$APPENV:$RELEASE_NUM - cd docker && ./config.sh ./k8s-application.tpl.yaml > k8s-application.yaml - cat k8s-application.yaml - kubectl apply -f k8s-application.yaml - kubectl -n $NAMESPACE rollout status --timeout=100s deployment/$CI_PROJECT_NAME || exit_code=$? - | if [ $exit_code -ne 0 ];then apk add curl jq --no-cache > /dev/null; ROLLBACK_ID=$(kubectl -n $NAMESPACE rollout undo deployment/$CI_PROJECT_NAME -ojson | jq -r '.status.observedGeneration') ; curl -s https://oapi.dingtalk.com/robot/send?access_token=\"$PIPELINE_DINGDING_ROBOT_TOKEN\" -H 'Content-Type: application/json' -d '{\"msgtype\": \"markdown\",\"markdown\": {\"title\": \"Gitlab流水线部署失败\",\"text\": \"['$CI_PROJECT_NAME']('$CI_PROJECT_URL'/-/tree/'$CI_BUILD_REF_NAME')的'$APPENV'环境第['$CI_PIPELINE_ID']('$CI_PIPELINE_URL')号流水线'$CI_JOB_STAGE'阶段失败，已回滚至最近一个稳定版本'$ROLLBACK_ID'，请检查相关错误！\"},\"at\": {\"isAtAll\": true}}' > /dev/null; exit 1; fi ##################################################################################### # CICD 触发条件 # # 在 master 分支代码有代码表更时触发，将构建、部署阶段脚本任务分配至 stg-k8s-runner 上运行 # # 同时设置了一系列环境变量，设置了最后部署阶段要手动触发 # ##################################################################################### build-prd: stage: build variables: APPENV: prod retry: max: 2 when: - always script: *k8s_build only: - master tags: - stg-k8s-runner deploy-prd: stage: deploy image: harbor.curiouser.com/kubectl-stg:latest variables: NAMESPACE: tools APPENV: prod POD_NUM: 2 script: *k8s_deploy only: - master when: manual tags: - stg-k8s-runner 6. 配置域名（可选） 在 k8s中的设置一个 Ingress 域名，域名绑定内网IP地址至内网DNS ，确保 CICD 过程中 Pipeline 所在环境（容器或服务器） 能访问到。方法有很多种，配置Ingress域名只是一种方法。 三、如何使用？ 其他代码仓库中如何在自己的 CICD 过程中从流水线配置文件中心下载自己的配置文件 1、gitlab 仓库 设置 http://gitlab-ci:$APP_K8S_CONFIG_CENTER@app-k8s-config-center.curiouser.com/$CI_PROJECT_NAME/.gitlab-ci.yml 2、Gitlab pipeline配置文件 自动化配置文件中心代码仓库中的：applicatlion/app1/.gitlab-ci.yml image: php:7.4-alpine-with-dockerctl variables: DOCKER_HOST: tcp://localhost:2375 DOCKER_DAEMON: | { \"registry-mirrors\": [\"https://12345.mirror.aliyuncs.com\"], \"insecure-registries\":[\"harbor.curiouser.com\"] } services: - name: docker:18.09-dind command: - /bin/sh - -c - | mkdir -p /etc/docker || exit echo \"$DOCKER_DAEMON\" > /etc/docker/daemon.json || exit dockerd-entrypoint.sh || exit stages: - scan-code - build - deploy scan-code: stage: scan-code image: name: harbor.curiouser.com/sonarscanner-cli:4.3.0.2102 entrypoint: [\"\"] variables: GIT_DEPTH: 0 script: - sonar-scanner -Dsonar.host.url=http://sonarqube.curiouser.com -Dsonar.login=$SONARSCANNER_TOKEN -Dsonar.qualitygate.wait=true -Dsonar.qualitygate.timeout=500 -Dsonar.projectName=$CI_PROJECT_NAME -Dsonar.projectKey=$CI_PROJECT_NAME -Dsonar.projectVersion=$CI_COMMIT_SHA -Dsonar.gitlab.project_id=$CI_PROJECT_ID -Dsonar.gitlab.commit_sha=$CI_COMMIT_SHORT_SHA -Dsonar.gitlab.ref_name=$CI_COMMIT_REF_NAME -Dsonar.links.scm=$CI_PROJECT_URL -Dsonar.links.ci=$CI_PIPELINE_URL rules: - if: $CI_PIPELINE_SOURCE == 'merge_request_event' && $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == 'release' tags: - stg-k8s-runner ############################### K8s build scripts ########################################################### .k8s_build: &k8s_build_scripts - sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories && apk update && apk add --no-cache wget git - wget -q -nv -np -r -l=1 -nd --http-user=gitlab-ci --http-password=$APP_K8S_CONFIG_CENTER -R \"index.html*\" -P docker http://app-k8s-config-center.curiouser.com/$CI_PROJECT_NAME/$APPENV/ - export RELEASE_NUM=\"$(date +%Y%m%d)-$(echo $CI_COMMIT_SHA | cut -c1-8)\" - composer install --ignore-platform-reqs --no-dev --no-interaction -o - docker login harbor.curiouser.com -u gitlab-ci -p $HARBOR_TOKEN - docker build -t \"harbor.curiouser.com/$CI_PROJECT_NAME/$APPENV:$RELEASE_NUM\" . -f docker/Dockerfile - docker push \"harbor.curiouser.com/$CI_PROJECT_NAME/$APPENV:$RELEASE_NUM\" ############################### K8s Deploy scripts ########################################################### .k8s_deploy: &k8s_deploy_scripts - export RELEASE_NUM=\"$(date +%Y%m%d)-$(echo $CI_COMMIT_SHA | cut -c1-8)\" - export IMAGE_NAME=harbor.curiouser.com/$CI_PROJECT_NAME/$APPENV:$RELEASE_NUM - sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories && apk update && apk add --no-cache wget git - wget -q -nv -np -r -l=1 -nd --http-user=gitlab-ci --http-password=$APP_K8S_CONFIG_CENTER -R \"index.html*\" -P docker http://app-k8s-config-center.curiouser.com/$CI_PROJECT_NAME/$APPENV/ - chmod +x docker/config.sh && cd docker && ./config.sh ./k8s-deployment.tpl.yaml > k8s-application.yaml - ./config.sh ./k8s-cronjob.tpl.yaml > k8s-cronjob.yaml - cat k8s-application.yaml - kubectl apply -f k8s-application.yaml - kubectl -n $NAMESPACE rollout status --timeout=280s deployment/$CI_PROJECT_NAME || exit_code=$? - | if [ $exit_code -ne 0 ];then apk add curl jq --no-cache > /dev/null; ROLLBACK_ID=$(kubectl -n $NAMESPACE rollout undo deployment/$CI_PROJECT_NAME -ojson | jq -r '.status.observedGeneration') ; curl -s https://oapi.dingtalk.com/robot/send?access_token=\"$PIPELINE_DINGDING_ROBOT_TOKEN\" -H 'Content-Type: application/json' -d '{\"msgtype\": \"markdown\",\"markdown\": {\"title\": \"Gitlab流水线部署失败\",\"text\": \"['$CI_PROJECT_NAME']('$CI_PROJECT_URL'/-/tree/'$CI_BUILD_REF_NAME')的'$APPENV'环境第['$CI_PIPELINE_ID']('$CI_PIPELINE_URL')号流水线'$CI_JOB_STAGE'阶段失败，已回滚至最近一个稳定版本'$ROLLBACK_ID'，请检查相关错误！\"},\"at\": {\"isAtAll\": true}}' > /dev/null; exit 1; fi ##################################### 测试环境构建部署触发条件 ##################################### test_build: stage: build variables: APPENV: test script: *k8s_build_scripts only: - develop tags: - stg-k8s-runner test_deploy: stage: deploy image: harbor.curiouser.com/kubectl-stg:latest variables: NAMESPACE: application-test APPENV: test POD_NUM: 1 script: *k8s_deploy_scripts only: - develop tags: - stg-k8s-runner ##################################### 测试环境构建部署触发条件 ##################################### ##################################### STG环境构建部署触发条件 ##################################### staging_build: stage: build variables: APPENV: stg script: *k8s_build_scripts only: - release tags: - stg-k8s-runner staging_deploy: stage: deploy image: harbor.curiouser.com/kubectl-stg:latest variables: NAMESPACE: application-stg APPENV: stg POD_NUM: 1 script: *k8s_deploy_scripts only: - release tags: - stg-k8s-runner ##################################### STG环境构建部署触发条件 ##################################### ##################################### 生产环境构建部署触发条件 ##################################### prod_build: stage: build variables: APPENV: prod script: *k8s_build_scripts only: - tags except: - release - develop tags: - prod-k8s-runner prod_deploy: stage: deploy image: harbor.curiouser.com/kubectl-prod:latest variables: NAMESPACE: application-prd APPENV: prod POD_NUM: 3 script: *k8s_deploy_scripts when: manual only: - tags except: - release - develop tags: - prod-k8s-runner ##################################### 生产环境构建部署触发条件 ##################################### 四、疑问 1. 所有配置文件都放到 Gitlab 仓库里，为什么不直接使用，还要放到Apache2中？ Gitlab Pipeline 支持git 协议拉取其他仓库中的流水线配置文件，但是使用 git获取，就会拉取流水线配置文件中心中所有的项目流水线文件(使用 git的sparsecheckout比较麻烦)。不安全，也不够快。同时也无法完美解决谁有权限拉取的问题。 2. 为什么用Apache2，而不用 Nginx ？为什么用 wget，不用 curl ? 当时同时测试了Nginx和 Apache2。但是忘了 Nginx 是因为什么原因没有用。有在测试时遇到问题的或者我想起了，评论区见。 curl命令无法配置成下载整个文件夹 五、附录 1、wget命令参数详解 wget -q -nv -np -r -l=1 -nd --http-user=cicd-user --http-password=$APP_K8S_CONFIG_CENTER_TOKEN -R \"index.html*\" -P docker http://app-k8s-config-center.curiouser.com/test-project/test/ # -q 静默下载，不显示输出 # -nv, --no-verbose 关闭详尽输出，但不进入安静模式。 # -np, --no-parent 不追溯至父目录。 # -r 指定递归下载 # -l 最大递归深度( inf 或 0 代表无限制，即全部下载)。 # -nd, --no-directories 不创建服务器上文件所在的目录路径 # -P 指定文件存储目录 # -R 指定不下载的文件列表,逗号分隔 # --http-user=USER 设置 http 用户名 # --http-password=PASS 设置 http 密码 2、Gitlab CICD Pipeline详解 参考本人个人博客：https://gitbook.curiouser.top/origin/gitlab-runner.html?h=gitlab%20 参考 https://docs.gitlab.com/ee/ci/pipelines/settings.html 声明 创作不易，转载请附上原文出处链接及本声明。 本文章同时会发布至个人博客站点。 如果本文章能帮到你，请至个人博客GitHub 仓库点个 Star，或者 Buy me a Coffee。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-03-16 09:47:27 "},"origin/gitlab-api.html":{"url":"origin/gitlab-api.html","title":"Gitlab Restful API","keywords":"","body":"Gitlab Restful API 一、简介 GItlab拥有Restful的API接口来操作其中的资源对象。 Gitlab Restful API Docs：https://docs.gitlab.com/ee/api/README.html#status-codes 1、接口请求方式 Endpoint：http://gitlab地址/api/api版本/资源对象/操作URI?参数1&参数2 目前接口版本为V4，V3在11.0版本后已经移除 接口需要认证，支持的验证方式： OAuth2 tokens curl --header \"Authorization: Bearer OAUTH-TOKEN\" \"https://gitlab.example.com/api/v4/projects\" # 或者 curl \"https://gitlab.example.com/api/v4/projects?access_token=OAUTH-TOKEN\" Personal access tokens curl --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/projects\" # 或者 curl \"https://gitlab.example.com/api/v4/projects?private_token=\" # 或者 curl --header \"Authorization: Bearer \" \"https://gitlab.example.com/api/v4/projects\" Project access tokens curl --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/projects\" # 或者 curl \"https://gitlab.example.com/api/v4/projects?private_token=\" # 或者 curl --header \"Authorization: Bearer \" \"https://gitlab.example.com/api/v4/projects\" Session cookie GitLab CI/CD job token 接口返回的是JSON数据格式（在Linux下可以使用jq进行处理） curl --header \"PRIVATE-TOKEN: \" \\ \"https://gitlab.example.com/api/v4/projects\" | jq -r '.[].id' 如果某些资源对象的URL操作中包含特殊字符，需要做URL编码处理 例如：资源对象路径包含的/可以使用%2F进行处理 GET /api/v4/projects/1/repository/files/src%2FREADME.md?ref=master GET /api/v4/projects/1/branches/my%2Fbranch/commits GET /api/v4/projects/1/repository/tags/my%2Ftag 请求资源对象参数中的数组和hash 数组 curl --request POST --header \"PRIVATE-TOKEN: \" \\ -d \"import_sources[]=github\" \\ -d \"import_sources[]=bitbucket\" \\ \"https://gitlab.example.com/api/v4/some_endpoint\" Hash curl --request POST --header \"PRIVATE-TOKEN: \" \\ --form \"namespace=email\" \\ --form \"path=impapi\" \\ --form \"file=@/path/to/somefile.txt\" --form \"override_params[visibility]=private\" \\ --form \"override_params[some_other_param]=some_value\" \\ \"https://gitlab.example.com/api/v4/projects/import\" Hash数组 curl --globoff --request POST --header \"PRIVATE-TOKEN: \" \\ \"https://gitlab.example.com/api/v4/projects/169/pipeline?ref=master&variables[][key]=VAR1&variables[][value]=hello&variables[][key]=VAR2&variables[][value]=world\" curl --request POST --header \"PRIVATE-TOKEN: \" \\ --header \"Content-Type: application/json\" \\ --data '{ \"ref\": \"master\", \"variables\": [ {\"key\": \"VAR1\", \"value\": \"hello\"}, {\"key\": \"VAR2\", \"value\": \"world\"} ] }' \\ \"https://gitlab.example.com/api/v4/projects/169/pipeline\" 请求分页 对于一些有大量返回数据的资源请求，gitlab为了保持性能，是有默认分页操作的。支持的分页方式 基于Offset的分页(所有接口都支持) 参数 | Parameter | Description | | :--------- | :------------------------------------------ | | page | 页数，默认为1 | | per_page | 一页显示的对象个数 (默认: 20,最大: 100) | curl --request PUT --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/namespaces?per_page=50\" 分页返回的Header curl --head --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/projects/9/issues/8/notes?per_page=3&page=2\" HTTP/2 200 OK cache-control: no-cache content-length: 1103 content-type: application/json date: Mon, 18 Jan 2016 09:43:18 GMT link: ; rel=\"prev\", ; rel=\"next\", ; rel=\"first\", ; rel=\"last\" status: 200 OK vary: Origin x-next-page: 3 x-page: 2 x-per-page: 3 x-prev-page: 1 x-request-id: 732ad4ee-9870-4866-a199-a9db0cde3c86 x-runtime: 0.108688 x-total: 8 x-total-pages: 3 | Header | Description | | :-------------- | :------------- | | x-next-page | 下一页的索引 | | x-page | 当前页的索引 | | x-per-page | 每页的对象个数 | | X-prev-page | 上一页的索引 | | x-total | 所有的对象格式 | | x-total-pages | 所有页的个数 | 基于Keyset的分页(仅支持部分接口 ) 参数 | Parameter | Description | | :----------- | :------------------------------------------ | | pagination | keyset (开启分页使用keyset). | | per_page | 一页显示的对象个数 (默认: 20,最大: 100) | 分页返回的Header curl --request GET --header \"PRIVATE-TOKEN: \" \"https://gitlab.example.com/api/v4/projects?pagination=keyset&per_page=50&order_by=id&sort=asc\" HTTP/1.1 200 OK ... Links: ; rel=\"next\" Link: ; rel=\"next\" Status: 200 OK ... 接口返回状态码 | 状态返回码 | 描述 | | :----------------------- | :----------------------------------------------------------- | | 200 OK | GET,PUT或者DELETE请求已处理完成，已返回JSON结构的数据 | | 204 No Content | 接口已处理完成了请求，但是没有什么数据可返回的 | | 201 Created | POST 请求已处理完成，已返回JSON结构的数据 | | 304 Not Modified | 自上次请求以来，该资源尚未修改。 | | 400 Bad Request | 请求参数不完整 | | 401 Unauthorized | 用户未认证，请求需要认证Token | | 403 Forbidden | 请求中认证Token没有相应的权限对请求中的对象进行操作 | | 404 Not Found | 请求操作的资源没发现。可能是参数不对，没有匹配到准确的对象 | | 405 Method Not Allowed | 请求中的资源对象不支持该请求方法 | | 409 Conflict | 请求的资源操作跟已有的现有对象有冲突。 | | 412 | 请求的资源操作被拒绝啦。如果尝试删除资源时提供了If-Unmodified-Since标头，则可能会发生这种情况。 | | 422 Unprocessable | The entity couldn’t be processed. | | 429 Too Many Requests | 请求被限制了！ | | 500 Server Error | 在服务端处理请求过程中出现了错误 | 二、Project API DQL 1、列出所有仓库 Endpoint GET /projects 文档 https://docs.gitlab.com/ee/api/projects.html#list-all-projects 参数 参数 类型 是否必须 描述 archived boolean No 是否列出archived状态的 id_after integer No Limit results to projects with IDs greater than the specified ID. id_before integer No Limit results to projects with IDs less than the specified ID. last_activity_after datetime No Limit results to projects with last_activity after specified time. Format: ISO 8601 YYYY-MM-DDTHH:MM:SSZ last_activity_before datetime No Limit results to projects with last_activity before specified time. Format: ISO 8601 YYYY-MM-DDTHH:MM:SSZ membership boolean No Limit by projects that the current user is a member of. min_access_level integer No Limit by current user minimal access level. order_by string No Return projects ordered by id, name, path, created_at, updated_at, or last_activity_at fields. repository_size, storage_size, packages_size or wiki_size fields are only allowed for admins. Default is created_at. owned boolean No Limit by projects explicitly owned by the current user. repository_checksum_failed boolean No Limit projects where the repository checksum calculation has failed (Introduced in GitLab Premium 11.2). repository_storage string No Limit results to projects stored on repository_storage. (admins only) search_namespaces boolean No Include ancestor namespaces when matching search criteria. Default is false. search string No Return list of projects matching the search criteria. simple boolean No Return only limited fields for each project. This is a no-op without authentication as then only simple fields are returned. sort string No Return projects sorted in asc or desc order. Default is desc. starred boolean No Limit by projects starred by the current user. statistics boolean No Include project statistics. visibility string No Limit by visibility public, internal, or private. wiki_checksum_failed boolean No Limit projects where the wiki checksum calculation has failed (Introduced in GitLab Premium 11.2). with_custom_attributes boolean No Include custom attributes in response. (admins only) with_issues_enabled boolean No Limit by enabled issues feature. with_merge_requests_enabled boolean No Limit by enabled merge requests feature. with_programming_language string No Limit by projects which use the given programming language. 示例 GET /projects?custom_attributes[key]=value&custom_attributes[other_key]=other_value Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/drone-install-basics.html":{"url":"origin/drone-install-basics.html","title":"部署与配置","keywords":"","body":"Drone部署及配置 一、简介 二、部署 1、对接gitlab 生成随机secret openssl rand -hex 16 # 611f45c7*****b715da0cb17c91 配置Gitlab 在gitlab的管理中心添加集成应用，获取应用客户端的ID和Secret。权限授予api和read_api docker-compose integrated-gitlab-docker-compose.yaml drone: image: 'drone/drone:2.12.1' restart: always container_name: drone environment: TZ: Asia/Shanghai # gitlab地址 DRONE_GITLAB_SERVER: http://192.168.1.7:30000 DRONE_GITLAB_CLIENT_ID: 876442ae9a*****0cd35a2 DRONE_GITLAB_CLIENT_SECRET: 053d4a*****1dd3b81bece # 指定与runner通信的随机secret DRONE_RPC_SECRET: 611f45c7*****b715da0cb17c91 DRONE_SERVER_HOST: 192.168.1.7:3180 DRONE_SERVER_PROTO: http # 指定gitlab的用户为admin角色 DRONE_USER_CREATE: username:root,admin:true ports: - '3180:80' volumes: - '/data/drone/data:/data' runner-ssh: image: 'drone/drone-runner-ssh:linux-amd64' restart: always container_name: drone-runner-ssh environment: TZ: Asia/Shanghai DRONE_RPC_PROTO: http DRONE_RPC_HOST: 192.168.1.7:3180 DRONE_RPC_SECRET: 611f45c7*****b715da0cb17c91 volumes: - '/data/drone/runner-data/ssh:/drone/src' runner-docker: image: 'drone/drone-runner-docker:1.8.2' restart: always container_name: drone-runner-docker environment: TZ: Asia/Shanghai DRONE_RPC_PROTO: http DRONE_RPC_HOST: 192.168.1.7:3180 DRONE_RPC_SECRET: 611f45c7*****b715da0cb17c91 DRONE_RUNNER_CAPACITY: 2 DRONE_RUNNER_NAME: runner-docekr volumes: - '/var/run/docker.sock:/var/run/docker.sock' - '/data/drone/runner-data/docker:/drone/src' docker-compose -f integrated-gitlab-docker-compose.yaml up -d 参考：https://docs.drone.io/server/provider/gitlab/ 2、对接Gogs 生成随机secret openssl rand -hex 16 # 8433b8***d0bf5 docker-compose integrated-gogs-docker-compose.yaml drone: image: 'drone/drone:2.12.1' restart: always container_name: drone environment: TZ: Asia/Shanghai DRONE_AGENTS_ENABLED: \"true\" DRONE_GOGS_SERVER: http://192.168.1.7:10880 DRONE_RPC_SECRET: 8433b8***d0bf5 DRONE_SERVER_HOST: 192.168.1.7:3180 DRONE_SERVER_PROTO: http DRONE_USER_CREATE: username:root,admin:true ports: - '3180:80' volumes: - '/data/drone/integrated-gogs-data:/data' runner-ssh: image: 'drone/drone-runner-ssh:linux-amd64' restart: always container_name: drone-runner-ssh environment: TZ: Asia/Shanghai DRONE_RPC_PROTO: http DRONE_RPC_HOST: 192.168.1.7:3180 DRONE_RPC_SECRET: 8433b8***d0bf5 volumes: - '/data/drone/runner-data/ssh:/drone/src' runner-docker: image: 'drone/drone-runner-docker:1.8.2' restart: always container_name: drone-runner-docker environment: TZ: Asia/Shanghai DRONE_RPC_PROTO: http DRONE_RPC_HOST: 192.168.1.7:3180 DRONE_RPC_SECRET: 8433b8***d0bf5 DRONE_RUNNER_CAPACITY: 2 DRONE_RUNNER_NAME: runner-docekr volumes: - '/var/run/docker.sock:/var/run/docker.sock' - '/data/drone/runner-data/docker:/drone/src' 参考：https://docs.drone.io/server/provider/gogs/ 三、Pipeline --- kind: pipeline type: docker name: default workspace: path: /drone/src steps: - name: build image: golang volumes: - name: cache path: /tmp/cache commands: - export GOPROXY=https://goproxy.io,direct - export GOCACHE=/tmp/cache - go build trigger: branch: - master volumes: - name: cache host: path: /data/drone/runner-data/docker/go-cache 参考： https://blog.csdn.net/uisoul/article/details/113554242 https://docs.drone.io/server/provider/gitlab/ https://docs.drone.io/server/provider/gogs/ https://blog.csdn.net/qq_35425070/article/details/106822191 https://blog.csdn.net/qq_35425070/article/details/106822146 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-09-01 14:28:45 "},"origin/nexus-简介.html":{"url":"origin/nexus-简介.html","title":"Nexus","keywords":"","body":"一、Nexus简介 Nexus是Sonatype公司出的一款目前最为流程的构件仓库管理软件，主要用于局域网内部的构件管理，代理访问外部仓库等。例如对于公司私有的Java制品Jar包，可上传至Nexus的Maven类型仓库中进行集中管理；代理访问阿里云Maven仓库，缓存加速获取互联网上的Java制品。Nexus使用Lucene提供了强大的构件搜索功能，拥有丰富的RestFul API接口用于管理控制，支持WebDAV和LDAP安全身份认证，基于RBAC的权限访问控制等功能。市面上同类产品有Apache的Archiva和JFrog的Artifatory。 Nexus分为免费开源版OSS和收费商业版Professional 二、仓库类型 Proxy 类型仓库主要用于代理缓存访问外网上其他公开的仓库，将每次从代理仓库拉取的制品缓存到nexus文件系统中，下次再拉取相同版本制品时就不需再次从外网拉取，起到代理访问缓存的功能 Hosted 类型的仓库主要用于存放各个项目组产出的、用于共享、不能放到公网上、私有的制品。有两种版本策略，一种是Snapshots版本策略类型的，对于相同版本制品的上传，nexus会自动追加时间戳加以区分；一种是Release版本策略类型的，对于相同的制品，要明确版本，不能存放相同版本。可以理解为snapshots仓库存放一些内容变更频繁的制品，这样不管上传还是使用时不用频繁变更版本号就能拉取到最新版本。而release仓库存放一些内容稳定变更少的制品，使用时指定好版本就行，无需经常变动 Group 类型仓库主要用于组合其他仓库，统一对外使用方式。可设置组仓库组合其他仓库的顺序。例如组合顺序为先拉取maven格式aliyun代理仓库中的制品，如果其中没有想要的制品，再去拉取maven格式Central代理仓库中的制品。如果还没有，就去maven格式hosted类型仓库中拉取，直到遍历完所有的组合仓库。同时，拉取使用时不需要配置那么多的仓库地址，只需要配置group仓库地址就行 三、官方高可用方案 官方收费版的高可用方案 四、Kubernetes部署 --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: nexus3 name: nexus3 namespace: nexus3 spec: replicas: 1 selector: matchLabels: app: nexus3 strategy: type: Recreate template: metadata: labels: app: nexus3 spec: imagePullSecrets: - name: harbor-secrets initContainers: - name: init-scheduler image: busybox:latest imagePullPolicy: IfNotPresent command: ['sh', '-c', 'chmod -R 777 /nexus-data'] volumeMounts: - name: nexus3-data mountPath: /nexus-data containers: - env: - name: CONTEXT_PATH value: / - name: TZ value: 'Asia/Shanghai' image: docker.io/sonatype/nexus3:3.15.2 imagePullPolicy: IfNotPresent readinessProbe: failureThreshold: 1 initialDelaySeconds: 100 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 8081 timeoutSeconds: 1 livenessProbe: failureThreshold: 1 initialDelaySeconds: 100 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 8081 timeoutSeconds: 1 name: nexus3 ports: - containerPort: 8081 protocol: TCP resources: limits: memory: 4096Mi requests: memory: 2048Mi terminationMessagePath: /dev/termination-log volumeMounts: - mountPath: /nexus-data name: nexus3-data dnsPolicy: ClusterFirst restartPolicy: Always securityContext: {} terminationGracePeriodSeconds: 30 volumes: - name: nexus3-data persistentVolumeClaim: claimName: nexus3-pv --- apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: volume.beta.kubernetes.io/storage-class: cephfs labels: app: nexus3 name: nexus3-pv namespace: nexus3 spec: accessModes: - ReadWriteMany resources: requests: storage: 1000Gi --- apiVersion: v1 kind: Service metadata: labels: app: nexus3 name: nexus3 namespace: nexus3 spec: ports: - name: 8081-tcp port: 8081 protocol: TCP targetPort: 8081 selector: app: nexus3 sessionAffinity: None type: ClusterIP --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nexus3 namespace: nexus3 spec: rules: - host: nexus.curiouser.com http: paths: - path: / backend: serviceName: nexus3 servicePort: 8081 五、常见仓库配置 YUM格式制品 Group类型仓库 yum yum-ustc yum-ansible yum-cloudera5 Proxy类型仓库 yum-ansible：https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ yum-ustc： http://mirrors.ustc.edu.cn/ yum-cloudera5：https://archive.cloudera.com/cdh5/ Hosted类型仓库 yum-hosted Maven格式制品 Group类型仓库 maven maven-aliyun maven-central maven-releases maven-snapshots Proxy类型仓库 maven-central：https://repo1.maven.org/maven2/ maven-aliyun：http://maven.aliyun.com/nexus/content/groups/public Hosted类型仓库 maven-snapshots maven-releases NPM格式制品 Group类型仓库 npm npm-taobao npm-cnpm npm-hosted Proxy类型仓库 npm-taobao：http://registry.npm.taobao.org/ npm-cnpm：http://registry.cnpmjs.org/ Hosted类型仓库 npm-hosted（上传权限需修改realms) Docker格式制品 Group类型仓库 docker（设置http-port:8082） Proxy类型仓库 docker-io： https://registry-1.docker.io Hosted类型仓库 docker-hosted（设置http-port:8083） Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-使用OrientDB Console在DB层面修改配置.html":{"url":"origin/nexus-使用OrientDB Console在DB层面修改配置.html","title":"使用OrientDB Console在DB层面修改配置","keywords":"","body":"Nexus使用OrientDB Console在DB层面修改配置 一、Context 在配置Nexus对接LDAP过程中，原先的用户（admin和原来创建的）也是可以登录的，为了测试只让LDAP用户登录，将安全域中的Local Authorizing Realm和Local Authenticating Realm给去掉了，导致admin用户登陆不上，LDAP上的用户能登陆，但没有管理Nexus的权限。换句话说，这个Nexus成了僵尸。如果Nexus的admin密码忘了,情形也类似。都需要修改Nexus数据库中的用户数据才能修改相关配置（Nexus是将用户、配置等相关信息存放在OrientDB 数据库中，而不是在配置文件中，所以改密码或者在没有权限的情况下改配置都是要操作OrientDB 中的数据） 相关信息 Nexus是在Openshift(Kubernetes也一样)中容器化部署的 Nexus版本：3.15.2 Nexus的数据目录是单独使用NFS类型的PV挂载的 openshift上Nexus容器的执行用户没有权限操作除持久化目录之外的目录。（以root用户起的Nexus进程可直接使用Nexus容器中的”/opt/nexus/lib/support/nexus-orient-console.jar“连接OrientDB ） 问题解决思路 将Nexus的PV数据目录挂载到物理节点一个临时目录下 使用Nexus相同版本安装包中的OrientDB Console连接到临时数据目录中的DB，然后修改相关信息 Note: 如果容器中执行Nexus进程的用户和物理节点上的用户不一样导致权限不够的话，可暂时将临时数据目录中的所有文件权限设置为777，修改完再改回来 再将Nexus的PV数据目录挂载到openshift上Nexus容器中，重启Nexus 二、Preflight Nexus的PV数据目录已挂载到/roor/test 下载相同版本的Nexus到/opt/目录下 /roor/test下的所有文件及文件夹权限临时修改为777 三、操作 1. 使用官方的OrientDB Console连接到数据目录中的DB java -jar /opt/nexus-3.15.2-01/lib/support/nexus-orient-console.jar 此时会进入OrientDB 控制台 # OrientDB console v.2.2.36 (build d3beb772c02098ceaea89779a7afd4b7305d3788, branch 2.2.x) https://www.orientdb.com # Type 'help' to display all the supported commands. orientdb> #输入exit退出 2. 连接临时数据目录中的DB orientdb> connect plocal:/root/test/db/security admin admin 此时会连到一个security的Database 3. 重置Realm 如果从活动列表中删除了缺省安全域，则缺省管理员用户(即使用户名密码正确)也无法进行身份验证 orientdb> delete from realm Note: 重置后。默认的安全域将被激活，任何自定义的安全域都将被删除。后续再UI界面进行添加 4.(可选) 重置admin用户密码为默认值\"admin123\" orientdb> update user SET password=\"$shiro1$SHA-512$1024$NE+wqQq/TmjZMvfI7ENh/g==$V4yPw8T64UQ6GfJfxYq2hLsVrBY8D1v+bktfOxGdt4b/9BthpWPNUy/CBk6V9iA0nHpzYzJFWO8v/tZFtES8CA==\" UPSERT WHERE id=\"admin\" 5.(可选) 重置admin为管理员 如果admin用户不在是“nx-admin”管理员角色 orientdb> select * from user_role_mapping where userID = \"admin\" orientdb> update user_role_mapping set roles = [\"nx-admin\"] where userID = \"admin\" orientdb> select status from user where id = \"admin\" orientdb> update user set status=\"active\" upsert where id=\"admin\" 6. 退出OrientDB 控制台，修改回临时数据目录所有文件的原始权限，重新将数据目录挂载到容器上，然后重启Nexus Bazinga，admin用户能正常，又重新夺回Nexus的管理权限，所有的仓库配置和数据没有丢失，重新将LDAP的安全域加回去，一切恢复原样。 附录：Nexus使用的OrientDB 1. What is the OrientDB Console Nexus 3 uses several OrientDB databases. In very specific circumstances, these databases can be manipulated as advised by Sonatype support. This article describes how to open a special command-line interface for connecting to and working with the databases used by Nexus. This interface is known as the OrientDB Console. Caution: Using the console incorrectly can cause irreparable harm to the databases. 2. Launching the OrientDB Console on Nexus 3.2.1 and Newer Nexus 3.2.1+ includes a single jar executable which can launch the OrientDB console. As the operating system user account that typically owns the Nexus Repository Manager process, start a terminal session on the host where Nexus is installed. Change directories to your application directory. Launch the console using the same version of Java executable that Nexus is using : Unix java -jar ./lib/support/nexus-orient-console.jar Windows java -jar lib\\support\\nexus-orient-console.jar Mac .install4j/jre.bundle/Contents/Home/jre/bin/java -jar ./lib/support/nexus-orient-console.jar You should be presented with a command-line interface such as this, ready to accept your commands: # OrientDB console v.2.2.16 www.orientdb.com #Type 'help' to display all the supported commands. orientdb> # When you are done your commands, type exit to quit the console. The nexus-orient-console.jar sets up the correct classpath to successfully launch the console. Launching the jar from another location is not supported 参考连接 https://support.sonatype.com/hc/en-us/articles/115002930827-Accessing-the-OrientDB-Console https://support.sonatype.com/hc/en-us/articles/213467158-How-to-reset-a-forgotten-admin-password-in-Nexus-3-x Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-设置SMTP邮件服务.html":{"url":"origin/nexus-设置SMTP邮件服务.html","title":"设置SMTP邮件服务","keywords":"","body":" Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-maven仓库的配置与使用.html":{"url":"origin/nexus-maven仓库的配置与使用.html","title":"Maven","keywords":"","body":"Maven仓库的配置与使用 一、Overview 有了Maven仓库之后，当Maven需要下载构件时，直接请求Nexus，Maven仓库上存在则下载到本地仓库；Maven仓库上不存在的话，Nexus请求外部的远程仓库，将构件下载到Maven仓库，再提供给本地仓库下载。 Maven格式制品仓库配置 Group类型仓库 maven maven-aliyun maven-central maven-releases maven-snapshots Proxy类型仓库 maven-central：https://repo1.maven.org/maven2/ maven-aliyun：http://maven.aliyun.com/nexus/content/groups/public Hosted类型仓库 maven-snapshots maven-releases maven格式的仓库有两种典型的使用场景，一个是顺序拉取group组仓库中组合仓库里的制品，一个是上传制品到hosted类型的仓库中，例如maven-snapshots仓库。而maven配置nexus中的仓库有三个地方，配置最终生效的顺序为全局配置文件--->用户配置文件---->POM文件： maven的全局配置文件settings.xml中,该配置文件在maven安装目录conf文件夹下 maven的用户配置文件settings.xml中,该配置文件在用户目录.m2文件夹下 项目的POM文件 二、代理仓库的使用 在Maven用户配置文件setting.xml中添加maven格式group仓库的地址 .....上文省略...... curiouser-maven microservices microservices用户的密码 curiouser-maven * The Maven repository of curiouser http://nexus-ip地址:8081/repository/maven/ .....下文省略...... 三、发布制品到Maven的Hosted仓库 1、mvn deploy 在Maven用户配置文件setting.xml中添加snapshot、release仓库的id .....上文省略...... maven-releases microservices microservices用户的密码 maven-snapshots microservices microservices用户的密码 .....下文省略...... 在项目POM.xml文件中添加Snapshots仓库或Release仓库的地址 .....上文省略...... maven-releases User Project Release http://nexus-ip地址:8081/repository/maven-releases/ maven-snapshots User Project SNAPSHOTS http://nexus-ip地址:8081/repository/maven-snapshots/ .....下文省略...... 然后mvn deploy 2、Curl手动上传 curl -v -u microservices:microservices用户的密码 --upload-file springboot2-0.0.0.jar http://nexus-ip地址:8081/repository/maven-releases/com/curiouser/demoeverything/springboot2/0.0.0/springboot2-0.0.0.jar curl -v -u microservices:microservices用户的密码 --upload-file pom.xml http://nexus-ip地址:8081/repository/maven-releases/com/curiouser/demoeverything/springboot2/0.0.0/springboot2-0.0.0.pom 3、mvn命令手动上传 前提是Maven用户配置文件setting.xml中已经添加了snapshot、release仓库的id Linux mvn deploy:deploy-file \\ #要上传的Jar包路径 \\ -Dfile=/root/websocket-server-9.4.11.v20180605.jar \\ #Jar包Maven坐标的GroupID \\ -DgroupId=org.eclipse.jetty.websocket \\ #Jar包Maven坐标的ArtifactID \\ -DartifactId=websocket-server \\ #Jar包Maven坐标的Version \\ -Dversion=9.4 \\ #要上传到仓库的制品类型，该值还可以是pom -Dpackaging=jar \\ #maven私服hosted类型仓库的地址 -Durl=http://nexus-ip地址:8081/repository/maven-releases/ \\ #maven私服hosted类型仓库的repositoryid -DrepositoryId=maven-releases Windows mvn deploy:deploy-file ^ -Dfile=D:\\websocket-server-9.4.11.v20180605.jar ^ -DgroupId=org.eclipse.jetty.websocket ^ -DartifactId=websocket-server ^ -Dversion=9.4 ^ -Dpackaging=jar ^ -Durl=http://nexus-ip地址:8081/repository/maven-releases/ ^ -DrepositoryId=maven-releases 4、UI界面上传 5、使用Postman上传 注意事项 Jar包上传只能上传hosted类型的仓库中，Proxy和Group类型的无法上传。同时，注意有些仓库设置禁止了上传 hosted类型的Snapshot仓库默认设置的上传版本控制为只能上传以“-snapshot”结尾版本的制品。所以版本为非“-snapshot”结尾的Jar包无法上传到snapshot仓库中 如果Nexus禁止匿名用户访问时，匿名用户是被禁止拉取maven仓库的Jar包。所以可以创建一个对maven仓库类型有读取权限的用户，在settings.xml文件进行配置。 附录：如何在maven的配置文件settings.xml中使用加密过的用户密码 在Maven的settings.xml中，往往要配置访问远程库所在的服务器的username/password。但是明文的密码总是显得那么扎眼，必欲除之而后快。Apache Maven项目提供了便捷的密码加密机制，该机制的最近更新时间为2018-03-06。该机制目前只支持在命令行下的操作，如生成密码的密文。此外，用户还需要在${user.home}/.m2目录下配置settings-security.xml文件，其中包含： 用以加密其他密码的master password（此处也是密文） 或指向另一个保密文件的完整路径 在该加密机制中有两个概念，一个是master password，即用以加密其他密码的密码，另一个就是实际使用的服务器访问密码password。master password的密文配置在settings-security.xml文件中，而服务器访问密码password的密文就可以大大方方地配置在settings.xml中。具体用法如下： 生成Master password的密文 mvn --encrypt-master-password 根据提示输入Master password: 就可以生成密文{iENT44//TgwH46wJQ0Go3et0u9PRZivf7LcAA9mY4LA=} 配置${user.home}/.m2/settings-security.xml文件(如果没有就手动创建) {iENT44//TgwH46wJQ0Go3et0u9PRZivf7LcAA9mY4LA=} # 如果settings-security.xml文件被保存到U盘，则配置${user.home}/.m2/settings-security.xml文件如下： /my_u_volume/my_path/settings-security.xml 加密访问服务器的密码 mvn --encrypt-password 根据提示输入Password: ​ 就可以生成密文{rZhmW6UmQw0HhRTeqSBchuMAgAoH6owP/hJjV3a/9Eg=} 配置settings.xml文件 my.server myfoo add_any_comment or \\{\\{rZhmW6UmQw0HhRTeqSBchuMAgAoH6owP/hJjV3a/9Eg=\\}\\} add_any_comment 参考链接 https://blog.csdn.net/taiyangdao/article/details/79500507 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-npm仓库的配置与使用.html":{"url":"origin/nexus-npm仓库的配置与使用.html","title":"NPM","keywords":"","body":"NPM仓库的配置与使用 一、npm仓库的配置信息 Group类型仓库 npm-taobao npm-cnpm npm-hosted Proxy类型仓库 npm-taobao：http://registry.npm.taobao.org/ npm-cnpm：http://registry.cnpmjs.org/ Hosted类型仓库 npm-hosted 二、使用NPM仓库 npm config set registry http://nexus-ip-address:8081/repository/npm/ 三、发布制品到NPM的Hosted仓库 当使用 npm login 或npm adduser 等NPM客户端使用Token进行登录认证到Nexus的NPM仓库时，Nexus默认仅支持Local Authenticating Realm ,认证不了NPM相关token。所以配置Nexus添加npm的认证域。 echo \"hello\" >> test npm init # package name: (test) sadsada # version: (1.0.0) # description: # entry point: (index.js) test # test command: # git repository: # keywords: # author: # license: (ISC) # About to write to /root/test/package.json: # { # \"name\": \"sadsada\", # \"version\": \"1.0.0\", # \"description\": \"\", # \"main\": \"f\", # \"scripts\": { # \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" # }, # \"author\": \"\", # \"license\": \"ISC\" # } npm login -registry http://nexus-ip-address:8081/repository/npm-hosted/ # Username: admin #Password: ***** # Email: (this IS public) asdad@sada.com npm publish -registry http://nexus-ip-address:8081/repository/npm-hosted/ 四、使用nrm工具切换npm仓库 Github地址：https://github.com/Pana/nrm 帮助快速切换npm仓库源。默认已经配置了npm、yarn、taobao、cnpm、nj、npmMirror、edunpm等常见的仓库源。 1. 安装 npm install nrm -g 2. 命令详解 $ nrm -h Usage: nrm [options] [command] Options: -V, --version output the version number -h, --help output usage information Commands: ls List all the registries current Show current registry name use Change registry to registry add [home] Add one custom registry set-auth [options] [value] Set authorize information for a custom registry with a base64 encoded string or username and pasword set-email Set email for a custom registry set-hosted-repo Set hosted npm repository for a custom registry to publish packages del Delete one custom registry home [browser] Open the homepage of registry with optional browser publish [options] [|] Publish package to current registry if current registry is a custom registry. if you're not using custom registry, this command will run npm publish directly test [registry] Show response time for specific or all registries help Print this help 3. 常用命令 查看默认支持的npm 仓库 $ nrm ls * npm -------- https://registry.npmjs.org/ yarn ------- https://registry.yarnpkg.com/ cnpm ------- http://r.cnpmjs.org/ taobao ----- https://registry.npm.taobao.org/ nj --------- https://registry.nodejitsu.com/ npmMirror -- https://skimdb.npmjs.com/registry/ edunpm ----- http://registry.enpmjs.org/ # \"*\"编注的仓库代表当前使用的仓库 添加私有的npm仓库 nrm add okd-nexus http://nexus-ip-address:8081/repository/npm-hosted/ 切换npm仓库 nrm use 仓库名 删除仓库 nrm del 仓库名 测试仓库速度 nrm test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-yum仓库的配置与使用.html":{"url":"origin/nexus-yum仓库的配置与使用.html","title":"YUM","keywords":"","body":"YUM仓库的配置与使用 一、Overviews 之前搭建内网YUM源仓库都是使用HTTP和createrepo做的，每次还要手动去同步外网镜像源最新的RPM包，繁琐耗时。最近发现新版的Nexus已经有可以做YUM源的功能，能像Maven仓库那样，没有相关资源的时候去外网拉，有的话就不去外网啦。即节省存储，又加快速度，还能自动更新RPM版本。 Group类型仓库 yum yum-ustc yum-ansible yum-cloudera5 Proxy类型仓库 yum-ustc：http://mirrors.ustc.edu.cn/ yum-tuna：https://mirrors.tuna.tsinghua.edu.cn/ yum-163：http://mirrors.163.com/ yum-ansible：https://releases.ansible.com/ansible/rpm/releasepel-7-x86_64/ yum-cloudera5：https://archive.cloudera.com/cdh5/ Hosted类型仓库 yum-hosted（\"Repodata Depth\"设置为“1”） 二、手动上传RPM包到Hosted仓库 上传RPM包到Hosted仓库，相关的RPM元信息文件夹repodata会自动生成，不再需要createrepo生成啦（如果没有自动生成，点击仓库的Rebuild Index，然后稍等即可） $ ls -l | grep ^[^d] | awk '{print $9}' mysql-community-client-5.7.19-1.el7.x86_64.rpm mysql-community-common-5.7.19-1.el7.x86_64.rpm mysql-community-devel-5.7.19-1.el7.x86_64.rpm mysql-community-embedded-5.7.19-1.el7.x86_64.rpm mysql-community-embedded-compat-5.7.19-1.el7.x86_64.rpm mysql-community-embedded-devel-5.7.19-1.el7.x86_64.rpm mysql-community-libs-5.7.19-1.el7.x86_64.rpm mysql-community-libs-compat-5.7.19-1.el7.x86_64.rpm mysql-community-minimal-debuginfo-5.7.19-1.el7.x86_64.rpm mysql-community-server-5.7.19-1.el7.x86_64.rpm mysql-community-server-minimal-5.7.19-1.el7.x86_64.rpm mysql-community-test-5.7.19-1.el7.x86_64.rpm #一次只能上传一个RPM文件， #为了一个Host仓库存放不同软件的不同版本，上传时需要指定上传到Hosted仓库的哪个目录下，以文件夹名来区分当前上传的RPM包是哪个版本的。 $ for i in `ls *rpm` ;do curl -v --user 'admin:admin123' --upload-file $i http://nexus-ip地址:8081/repository/yum-nexus-MySQL/mysql5.7.19/;done 三、YUM仓库的使用 [centos] name=Nexus Yum baseurl=http://nexus-ip地址:8081/repository/yum-nexus/centos/$releasever/os/$basearch/ enabled=1 gpgcheck=0 [centos-extras] name=Nexus Yum Extras baseurl=http://nexus-ip地址:8081/repository/yum-nexus/centos/$releasever/extras/$basearch/ enabled=1 gpgcheck=0 [epel] name=Nexus Epel baseurl=http://nexus-ip地址:8081/repository/yum-nexus/epel/$releasever/$basearch/ enabled=1 gpgcheck=0 [openshift] name=Nexus openshift 3.11 baseurl=http://nexus-ip地址:8081/repository/yum-nexus/centos/$releasever/paas/$basearch/openshift-origin311/ enabled=1 gpgcheck=0 [mysql5.7] name=Nexus MySQL 5.7.19 baseurl=http://nexus-ip地址:8081/repository/yum-nexus/mysql5.7.19 enabled=1 gpgcheck=0 注意 不要代理多个外网的YUM源。例如同时代理缓存网易的163镜像源和清华大学的镜像源，这些外网镜像源中的RPM都大差无几，配置多个的话，使用group去代理拉RPM的时候，会优先选择group中靠前的外网代理镜像源。有时（偶尔其中一个外网镜像源不能用）会出现相同的RPM存在于每一个proxy仓库中，浪费存储。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-14 18:11:45 "},"origin/nexus-composer.html":{"url":"origin/nexus-composer.html","title":"Composer","keywords":"","body":"Nexus支持Composer仓库 一、简介 Nexus 3 默认暂不支持PHP Composer仓库，但是可以通过第三方插件支持 插件Github地址：https://github.com/sonatype-nexus-community/nexus-repository-composer Group类型仓库 composer-public composer-packagist composer-hosted Proxy类型仓库 composer-packagist：https://packagist.org/ Hosted类型仓库 composer-hosted 二、编译、安装插件 git clone https://github.com/sonatype-nexus-community/nexus-repository-composer.git mvn clean package # 编译成功后，生成的Jar包在nexus-repository-composer/target下 将插件jar包放到/system/org/sonatype/nexus/plugins/nexus-repository-composer/0.0.2/nexus-repository-composer-0.0.2.jar 编辑/system/org/sonatype/nexus/assemblies/nexus-core-feature/3.x.y/nexus-core-feature-3.x.y-features.xml，添加 nexus-repository-rubygems + nexus-repository-composer nexus-repository-gitlfs + + org.sonatype.nexus.plugins:nexus-repository-composer + mvn:org.sonatype.nexus.plugins/nexus-repository-composer/0.0.2 + 重启Nexus，看是否支持创建Composer仓库了 三、仓库配置 四、使用 1、安装Composer curl -sS https://getcomposer.org/installer | php mv composer.phar /usr/local/bin/composer composer --version 2、Composer配置使用私有源 方式一：配置Composer全局使用私有源 ①手动 composer config -g repo.packagist composer http://Neuxs-IP:8081/repository/composer-public/ # Composer默认使用ssl连接代理源，使用私有仓库源时，使用的HTTP，需要关闭SSL composer config -g -- disable-tls true composer config -g -- secure-http false ②使用crm工具 #安装 composer global require slince/composer-registry-manager ^2.0 # 添加公司内部的私有源 composer repo:add synology-nxus http://Neuxs-IP:8081/repository/composer-public/ # 查看所有的私有源 composer repo:ls # 切换到私有源 composer repo:use synology-nxus 方式二：配置项目级别配置使用私有源 编写项目根目录下的composer.json { \"packagist.org\": false , \"repositories\": { \"packagist\": { \"type\": \"composer\", \"url\": \"http://Neuxs-IP:8081/repository/composer-public/\" } }, \"config\": { \"secure-http\": false }, \"require\": { \"monolog/monolog\": \"1.0.*\", \"pugx/shortid-php\":\"v0.5.1\" } } 3、安装依赖 composer会根据当前路径下composer.json中写的下载依赖 composer install # 依赖会被下载当前目录下的vendor文件夹中 手动添加依赖 composer require monolog/monolog 4、上传包到Hosted类型的仓库中 编辑composer.json，添加项目包的信息，像名字，描述，版本号，维护者等信息。 { \"version\": \"1.0\", \"name\": \"php-composer-test\", \"description\": \"this is a demo composer repo\", \"authors\": [{ \"name\": \"curiouser\", \"email\": \"12345678@qq.com\" }], \"packagist.org\": false , \"repositories\": { \"packagist\": { \"type\": \"composer\", \"url\": \"http://Neuxs-IP:8081/repository/composer-public/\" } }, \"config\": { \"secure-http\": false }, \"require\": { \"monolog/monolog\": \"1.0.*\", \"pugx/shortid-php\":\"v0.5.1\" } } 归档项目 composer archive --format=zip 上传归档项目包 curl -v --user 'user:pass' --upload-file example.zip http://Neuxs-IP:8081/repository/composer-hosted/packages/upload/项目名/组件名/版本号 查看项目包是否已上传到Nexus中 下载引用上传的包 composer require curiouser/test:1.0 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/nexus-pypi.html":{"url":"origin/nexus-pypi.html","title":"Pypi","keywords":"","body":"Nexus Pypi仓库的使用 一、简介 Nexus针对Python制品仓库的管理与使用 官方文档：https://help.sonatype.com/repomanager3/formats/pypi-repositories Group类型仓库 pypi-public Proxy类型仓库 pypi-aliyun：http://mirrors.aliyun.com/pypi/ pupi-python：https://pypi.python.org/ Hosted类型仓库 pypi-hosted 二、客户端使用 1、pip3配置仓库源 ①全局配置 编辑 ~/.pip/pip.conf [global] # 用户名密码已配置在URL中。如果密码中包含特殊字符，使用特殊字符对应ASCII码的16进制进行代替。例如：密码中包含的“#”的ASCII为35,对应的16进制表示%23。密码中包含的“/”的ASCII为47,对应的16进制表示%2F。 index-url = http://pypi:******@nexus-ip:8081/repository/pypi-public/simple trusted-host = 192.168.150.88 ②临时配置 pip3 install flask \\ -i http://pypi:******@nexus-ip:8081/repository/pypi-public/simple \\ --trusted-host 192.168.150.88 注意 使用pip3 config list 可查看pip配置 官方已不再推荐支持 easy_install ，详见一下链接 https://setuptools.readthedocs.io/en/latest/easy_install.html https://packaging.python.org/discussions/pip-vs-easy-install/ 2、使用twine上传python制品到私有仓库 ①安装twine pip3 install twine \\ -i http://pypi:******@nexus-ip:8081/repository/pypi-public/simple \\ --trusted-host 192.168.150.88 ②编写setup.py import setuptools import os import requests # 将README.md中的描述文字作为制品的详细描述 with open(\"README.md\", \"r\") as fh: long_description = fh.read() # 将requirements.txt中依赖模块的版本信息作为制品的依赖描述 if os.path.exists(\"requirements.txt\"): install_requires = io.open(\"requirements.txt\").read().split(\"\\n\") else: install_requires = [] setuptools.setup( # 项目命名 name=\"demotest\", # 版本 version=\"0.0.1\", # 作者 author=\"curiouser\", # 作者邮箱 author_email=\"*******@163.com\", # 项目制品包的简要描述 description=\"test\", # 项目制品包的详细描述 long_description=long_description, # 制品包详细描述的格式 long_description_content_type=\"text/markdown\", # 项目代码仓库地址 url=\"https://github.com/test\", packages=setuptools.find_packages(), classifiers=[ \"Programming Language :: Python :: 3\", \"License :: OSI Approved :: MIT License\", \"Operating System :: OS Independent\", ], install_requires = install_requires, # 是否打包文件夹内的所有数据 include_package_data=true, package_data = { # If any package contains *.txt or *.rst files, include them: 'chinesename': ['source/*.txt', \"source/*.json\"], }, # 如果需要支持脚本方法运行，可以配置入口点 entry_points={ 'console_scripts': [ 'chinesename = chinesename.run:main' ] } ) 编写README.md setuptools官方文档：https://packaging.python.org/guides/distributing-packages-using-setuptools/ setup.py推荐规则：https://github.com/pypa/sampleproject/blob/master/setup.py ③打包项目 python3 setup.py sdist bdist_wheel # 打完包，会在当前目录下的dist目录下产生源文件tar.gz，分发文件.whl两个项目包 ④上传项目包到hosted仓库 # 检测包 twine check dist/* # 上传包 twine upload dist/* \\ --repository-url http://nexus-ip:8081/repository/pypi-hosted/ ⑤验证 ⑥下载使用 sudo pip3 install demotest==0.0.1 # 或者 sudo pip3 install demotest==0.0.1 \\ -i http://pypi:******@nexus-ip:8081/repository/pypi-public/simple \\ --trusted-host 192.168.150.88 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/nexus-helm.html":{"url":"origin/nexus-helm.html","title":"Helm","keywords":"","body":"Nexus Helm的配置与使用 一、简介 Nexus从3.21.1开始正式支持Helm charts仓库的管理。之前一直是使用第三方插件的形式支持的。 目前暂时支持Hosted和Proxy类型的Helm仓库 二、配置 Proxy helm-google https://kubernetes-charts-incubator.storage.googleapis.com/ Hosted helm-hosted 三、使用 1、Helm客户端添加Nexus上Proxy类型的远程charts仓库 添加Nexus上的Helm代理类型的仓库 helm repo add 远程Charts仓库别名 http://nexus-ip:8081/repository/helm-google/ --username admin --password=\"*****\" 查看添加的远程仓库 helm repo list 搜索远程仓库 helm search repo 从远程仓库中下载charts到本地，并解压 helm pull 远程Charts仓库别名/chart名 --untar 从远程仓库中下载指定版本的charts到本地 helm fetch 远程Charts仓库别名/chart名 --version 1.3.0 2、上传Helm charts到Nexus上Hosted类型的仓库 curl -u 用户名:密码 http://nexus-ip:8081/repository/helm-hosted/ --upload-file sentry-kubernetes-0.2.3.tgz 3、手动在Nexus UI界面上传.gz格式的Chart到Helm Hosted仓库 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/nexus-数据的备份恢复.html":{"url":"origin/nexus-数据的备份恢复.html","title":"数据备份恢复","keywords":"","body":"Nexus的数据备份与恢复 Nexus的备份分为两个部分。一个是元信息和配置信息数据库的备份，一个是Blob存储的备份 一、配置DB的备份操作 二、Blob存储的备份操作 Nexus的blob stores可以简单的理解为一个文件夹，存放着各种制品的原始文件，例如原始的Java制品Jar包，POM文件，War包等文件。Blob的类型可以是文件系统的一个文件夹，也可以是S3对象存储的一个存储Buckets。（默认是文件系统类型，仅支持S3对象存储）一个blob存储可以被一个或者多个仓库组使用 三、Kubernetes上Nexus的数据备份 由于nexus在Kubernetes上的实例使用了CephFS类型的PVC存储作为Volume挂载在到数据目录下。所以，只要将Nexus容器持久化卷的PV对应Ceph路径挂载到某个目录下，拷贝其中的数据库DB目录和Blob目录进行数据备份。 mkdir test nexus3-k8s-2019-5-6-bak mount -t ceph jk1:/pvc-volumes/kubernetes/kubernetes-dynamic-pvc-25c75aeb-6f2b-11e9-ac5b-9209ee33fdea test -o name=admin,secret=AQCxFKtcDGd1AhAAvytw5KlaeuApSi1a3G2iwA== cp -r test/db test/blob nexus3-k8s-2019-5-6-bak umount test 四、数据恢复操作 数据恢复时使用的版本与旧版本的差异仅限于小版本号 暂停旧的Nexus POD 挂载旧的Nexus CephFS PV 到本地目录 将备份提拷贝新的CephFS PV中 修改备份目录的权限 重启新的Nexus POD 五、备份策略优化 hosted类型的仓库可使用单独的Blob存储，备份时只备份该Blob。Proxy类型的仓库可不用备份。 可添加执行脚本类型的定时任务做备份，将新增Blob同步到其他Nexus实例中 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-api.html":{"url":"origin/nexus-api.html","title":"API","keywords":"","body":"Nexus API 一、Context 官网API文档：https://help.sonatype.com/repomanager3/rest-and-integration-api 二、Search API Search API用于搜索component和asset。 GET /service/rest/v1/search 1、Search Components 例如在maven-central仓库中搜索\"group=org.osgi\"的component $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/search?repository=maven-central&group=org.osgi' { \"items\" : [ { \"id\" : \"bWF2ZW4tY2VudHJhbDoyZTQ3ZGRhMGYxYjU1NWUwNzE1OWRjOWY5ZGQzZmVmNA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"group\" : \"org.osgi\", \"name\" : \"org.osgi.core\", \"version\" : \"4.3.1\", \"assets\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDplMDE4OGVkMDcyOGZhNjhmNDExNzU2OGU1MjQ2NjZiYg\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"80bfafcf783988442b3a58318face1d2132db33d\", \"md5\" : \"87ee0258b79dc852626b91818316b9c3\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDpkMDY0ODA0YThlZDVhZDZlNjhmZGU5MWNmM2NiZTgzMw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"5458ffe2ba049e76c29f2df2dc3ffccddf8b839e\", \"md5\" : \"8053bbc1b55d51f5abae005625209d08\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDo2NTRiYjdkMGE1OTIxMzg1OWZhMTVkMzNmYWU1ZmY3OA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"79391fc69dd72ad1fd983d01b4572f93f644882b\", \"md5\" : \"3d87a59bcdb4b131d9a63e87e0ed924a\" } } ] } ], \"continuationToken\" : null } 2、Search Assets 例如在maven-central仓库中搜索\"group=org.osgi\"的assets $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/search../assets?repository=maven-central&group=org.osgi' { \"items\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1-sources.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDplMDE4OGVkMDcyOGZhNjhmNDExNzU2OGU1MjQ2NjZiYg\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"80bfafcf783988442b3a58318face1d2132db33d\", \"md5\" : \"87ee0258b79dc852626b91818316b9c3\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDpkMDY0ODA0YThlZDVhZDZlNjhmZGU5MWNmM2NiZTgzMw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"5458ffe2ba049e76c29f2df2dc3ffccddf8b839e\", \"md5\" : \"8053bbc1b55d51f5abae005625209d08\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"path\" : \"org/osgi/org.osgi.core/4.3.1/org.osgi.core-4.3.1.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDo2NTRiYjdkMGE1OTIxMzg1OWZhMTVkMzNmYWU1ZmY3OA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"79391fc69dd72ad1fd983d01b4572f93f644882b\", \"md5\" : \"3d87a59bcdb4b131d9a63e87e0ed924a\" } } ], \"continuationToken\" : null } 3、Search and Download Asset 用于搜索一个资产，然后将请求重定向到该资产的downloadUrl GET /service/rest/v1/search../assets/download 例如获取一个maven坐标为\"groupId=com.curiosuer，artifactId=SpringBoot2，version=0.0.0\"Jar包的下载链接 $ curl -v -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/search../assets/download?maven.groupId=com.curiosuer&maven.artifactId=SpringBoot2&maven.baseVersion=0.0.0&maven.extension=jar&maven.classifier' 浏览器中 三、Repositories API 1、List Repositories 获取用户能访问到的Repository仓库列表 GET /service/rest/v1/repositories $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/repositories' [ { \"name\" : \"YUM-Hosted\", \"format\" : \"yum\", \"type\" : \"hosted\", \"url\" : \"http://localhost:8081/repository/YUM-Hosted\" }, ... ] 此endpoint返回所有存储库，并且不允许分页。 注意，存储库的顺序在多个查询之间是一致的，并且不是按字母顺序排列的。 四、Assets API 1、List Assets 列出指定Repository仓库中包含的Assets GET /service/rest/v1../assets 例如列出maven-central仓库中的Assets $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1../assets?repository=Maven-Releases' { \"items\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/Maven-Releases/com/curiosuer/SpringBoot2/0.0.0/SpringBoot2-0.0.0.jar\", \"path\" : \"com/curiosuer/SpringBoot2/0.0.0/SpringBoot2-0.0.0.jar\", \"id\" : \"TWF2ZW4tUmVsZWFzZXM6MzZlM2RlYzhkZTUyOGM5YmRkYTdhZTNjZjlmYjFiNTY\", \"repository\" : \"Maven-Releases\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"c1ab61e9f407cbabaa8f3b377a76afa1f8afa4f1\", \"md5\" : \"5474cd7fc95a581eb6b6a3319c8aa6ba\" }, ... ], \"continuationToken\" : \"3f5cae01760233b6506547dc7be10e0b\" } 该endpoint使用分页策略，如果需要，可以使用该策略遍历所有资产。 注意，资产的顺序在多个查询之间是一致的，并且不是按字母顺序排列的。 2、Get Asset GET /service/rest/v1../assets/{id} This endpoint allows us to get the details of an individual asset. $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1../assets/bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MjRiOTEwMmMwMmNiYmU4YQ' { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/sonatype/nexus/buildsupport/nexus-buildsupport-metrics/2.9.1-02/nexus-buildsupport-metrics-2.9.1-02.pom\", \"path\" : \"org/sonatype/nexus/buildsupport/nexus-buildsupport-metrics/2.9.1-02/nexus-buildsupport-metrics-2.9.1-02.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MjRiOTEwMmMwMmNiYmU4YQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"a3bf672b3ea844575acba3b84790e76ed86a7c66\", \"md5\" : \"49e439c814c3098450dc4bbee952463f\" }} 3、Delete Asset DELETE /service/rest/v1../assets/{id} This endpoint can be used to delete an individual asset. $ curl -u admin:admin123 -X DELETE 'http://localhost:8081/service/rest/v1../assets/bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MjRiOTEwMmMwMmNiYmU4YQ' HTTP/1.1 204 No Content Date: Fri, 19 Jan 2018 20:41:47 GMT ... 五、Components API 1、List Components 遍历仓库中的Components GET /service/rest/v1/components $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/components?repository=Maven-Central' { \"items\" : [ { \"id\" : \"bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjMyNjhmMjIwZTQ1ZDdkZQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"group\" : \"com.google.guava\", \"name\" : \"guava\", \"version\" : \"21.0\", \"assets\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.jar\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.jar\", \"id\" : \"bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2MzA4OThiZjZmZTFkOTE2NA\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"3a3d111be1be1b745edfa7d91678a12d7ed38709\", \"md5\" : \"ddc91fd850fa6177c91aab5d4e4d1fa6\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.jar.sha1\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.jar.sha1\", \"id\" : \"bWF2ZW4tY2VudHJhbDpmODk4YjM5MDNjYjk5YzU5MDc0MDFlYzRjNjVlNjU5OQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"a1ff60cb911e1f64801c03d03702044d10c9bdd3\", \"md5\" : \"e34b8695ede1677ba262411d757ea980\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.pom\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDpkMDY0ODA0YThlZDVhZDZlOWJjNDgzOGE1MzM2OGZlZg\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"fe4fa08a8c0897f9896c7e278fb397ede4a2feed\", \"md5\" : \"5c10f97af2ce9db54fa6c2ea6997a8d7\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/com/google/guava/guava/21.0/guava-21.0.pom.sha1\", \"path\" : \"com/google/guava/guava/21.0/guava-21.0.pom.sha1\", \"id\" : \"bWF2ZW4tY2VudHJhbDplMDE4OGVkMDcyOGZhNjhmZDA3NDdkNjlhZDNmZjI5Nw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"992b43ab7b3a061be47767e910cab58180325abc\", \"md5\" : \"33aed29aa0bb4e03ea7854066a5b4738\" } } ] }, ... ], \"continuationToken\" : \"88491cd1d185dd136f143f20c4e7d50c\" } 该endpoint使用分页策略，如果需要，可以使用该策略遍历所有资产。 注意，资产的顺序在多个查询之间是一致的，并且不是按字母顺序排列的。 2、Get Component 获取仓库中component的详细信息 GET /service/rest/v1/components/{id} $ curl -u admin:admin123 -X GET 'http://localhost:8081/service/rest/v1/components/bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjYwYmUwMjE1MjI2NGUwZQ' { \"id\" : \"bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjYwYmUwMjE1MjI2NGUwZQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"group\" : \"org.apache.httpcomponents\", \"name\" : \"httpcomponents-client\", \"version\" : \"4.3.5\", \"assets\" : [ { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom\", \"path\" : \"org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom\", \"id\" : \"bWF2ZW4tY2VudHJhbDozZjVjYWUwMTc2MDIzM2I2YTFhOGUxOGQxZmFkOGM3Mw\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"95d80a44673358a5dcbcc2f510770b9f93fe5eba\", \"md5\" : \"f4769c4e60799ede664414c26c6c5c9d\" } }, { \"downloadUrl\" : \"http://localhost:8081/repository/maven-central/org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom.sha1\", \"path\" : \"org/apache/httpcomponents/httpcomponents-client/4.3.5/httpcomponents-client-4.3.5.pom.sha1\", \"id\" : \"bWF2ZW4tY2VudHJhbDpmODk4YjM5MDNjYjk5YzU5ZDU3YjFlYjE0MzM1ZTcwMQ\", \"repository\" : \"maven-central\", \"format\" : \"maven2\", \"checksum\" : { \"sha1\" : \"6b98f5cef5d7102f8f45215bdcf48dc843d060af\", \"md5\" : \"f3b3ac640853fcb887621d13029a1747\" } } ] } 3、Upload Component 上传Component到指定仓库中，一些格式的仓库允许上传的Component中包含多个Assets。 endpoint的上传参数取决于要上传Component到那个仓库的格式 POST /service/rest/v1/components $ curl -v -u admin:admin123 \\ -X POST 'http://localhost:8081/service/rest/v1/components?repository=maven-releases' \\ -F maven2.groupId=com.google.guava \\ -F maven2.artifactId=guava \\ -F maven2.version=24.0-jre \\ -F maven2.asset1=@guava-24.0-jre.jar \\ -F maven2.asset1.extension=jar \\ -F maven2.asset2=@guava-24.0-jre-sources.jar \\ -F maven2.asset2.classifier=sources \\ -F maven2.asset2.extension=jar HTTP/1.1 204 No Content Date: Fri, 19 Jan 2018 20:26:13 GMT ... ①Maven Maven format allows multiple assets to be uploaded as part of a single component. To upload multiple assets just follow the information from a table describing the given format and replace assetN with multiple instances of it (e.g. asset1, asset2, etc.): Field name Field type Required? Description maven2.groupId String Yes, unless a POM asset is included in the upload Group ID of the component maven2.artifactId String Yes, unless a POM asset is included in the upload Artifact ID of the component maven2.version String Yes, unless a POM asset is included in the upload Version of the component maven2.generate-pom Boolean No Whether the Nexus Repository Manager should generate a POM file based on above component coordinates provided maven2.packaging String No Define component packaging (e.g. jar, ear) maven2.assetN File Yes, at least one Binary asset maven2.assetN.extension String Yes Extension of the corresponding assetN asset maven2.assetN.classifier String No Classifier of the corresponding assetN asset Examples：Uploading a jar and Automatically Creating a pom File $ curl -v -u admin:admin123 \\ -F \"maven2.generate-pom=true\" \\ -F \"maven2.groupId=com.example\" \\ -F \"maven2.artifactId=commercial-product\" \\ -F \"maven2.packaging=jar\" \\ -F \"version=1.0.0\" \\ -F \"maven2.asset1=@/absolute/path/to/the/local/file/product.jar;type=application/java-archive\" \\ -F \"maven2.asset1.extension=jar\" \\ \"http://localhost:8081/service/rest/v1/components?repository=maven-third-party\" Upload a POM and associated JAR File $ curl -v -u admin:admin123 \\ -F \"maven2.generate-pom=false\" \\ -F \"maven2.asset1=@/absolute/path/to/the/local/file/pom.xml\" \\ -F \"maven2.asset1.extension=pom\" \\ -F \"maven2.asset2=@/absolute/path/to/the/local/file/product-1.0.0.jar;type=application/java-archive\" \\ -F \"maven2.asset2.extension=jar\" \\ \"http://localhost:8081/service/rest/v1/components?repository=maven-releases\" ②Raw Raw supports multiple assets within a single component. Field name Field type Required? Description raw.directory String Yes Destination for upload files (e.g. /path/to/files) raw.assetN File Yes, at least one Binary asset raw.assetN.filename String Yes Filename to be used for the corresponding assetN asset ③PyPI Field name Field type Required? Description pypi.asset File Yes Binary asset ④RubyGems Field name Field type Required? Description rubygems.asset File Yes Binary asset ⑤NuGet Field name Field type Required? Description nuget.asset File Yes Binary asset ⑥NPM Field name Field type Required? Description npm.asset File Yes Binary asset 4、Delete Component：删除仓库中的component DELETE /service/rest/v1/components/{id} $ curl -u admin:admin123 -X DELETE 'http://localhost:8081/service/rest/v1/components/bWF2ZW4tY2VudHJhbDo4ODQ5MWNkMWQxODVkZDEzNjYwYmUwMjE1MjI2NGUwZQ' HTTP/1.1 204 No Content Date: Fri, 19 Jan 2018 20:26:13 GMT ... 六、Metrics API 示例 https://raw.githubusercontent.com/OpenShiftDemos/nexus/master/scripts/nexus-functions ################################################################# # Functions for Managing Sonatype Nexus # # # # Authors: # # - Jorge Morales https://github.com/jorgemoralespou # # - Siamak Sadeghianfar https://github.com/siamaksade # # # ################################################################# ​ # # add_nexus2_repo [repo-id] [repo-url] [nexus-username] [nexus-password] [nexus-url] # ​ function add_nexus2_repo() { local _REPO_ID=$1 local _REPO_URL=$2 local _NEXUS_USER=$3 local _NEXUS_PWD=$4 local _NEXUS_URL=$5 ​ read -r -d '' _REPO_JSON Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nexus-使用jenkins插件上传CI流程制品到Nexus仓库.html":{"url":"origin/nexus-使用jenkins插件上传CI流程制品到Nexus仓库.html","title":"Jenkins相关插件","keywords":"","body":"使用jenkins插件上传CI流程制品到Nexus仓库 一、Overviews 现在Nexus各个格式仓库中的制品大多数都是在Jenkins的持续集成CI流水线中生成的，每次流水线构建都需要其制品上传到Nexus中进行管理。Nexus针对Jenkins有Nexus Platform的插件来简化上传步骤，该插件主要用来上传Maven格式制品到Hosted类型的仓库中。同时，Jenkins CI Pipeline中除了可以使用该插件来上传Maven制品到Maven格式仓库，原始Curl也是可以的。 插件Github：https://github.com/jenkinsci/nexus-platform-plugin 二、Jenkins使用Nexus Platform上传maven格式制品 1、安装 2、配置 系统管理--> 系统设置--> Sonatype Nexus 3、Jenkins Job 4、Jenkins Pipeline .....上文省略...... stage ('上传制品') { steps { script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用Nexus Platform插件上传maven制品到Nexus的maven格式release仓库 nexusPublisher nexusInstanceId: 'curiouser-okd-nexus', \\ nexusRepositoryId: 'Maven-Releases', \\ packages: [[$class: 'MavenPackage', \\ mavenAssetList: [[classifier: '', extension: '', \\ filePath: \"target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging}\"]], \\ mavenCoordinate: [artifactId: \"${pomfile.artifactId}\", \\ groupId: \"${pomfile.groupId}\", \\ packaging: \"${pomfile.packaging}\", \\ version: \"${pomfile.version}\"]]] //拼接maven制品的搜索链接,该链接是以源代码POM文件中的maven制品坐标信息参数对nexus api进行搜索，返回的response会重定向到制品的下载链接 echo \"The Jar Format Asset of Maven have been pushed to Hosted Repository: Maven-Release. The Download URL of the Asset: http://Nexus-IP地址:8081/service/rest/v1/search../assets/download?maven.groupId=${pomfile.groupId}&maven.artifactId=${pomfile.artifactId}&maven.baseVersion=${pomfile.version}&maven.extension=jar&maven.classifier\" } } } .....下文省略...... 5、Jenkins使用Curl命令手动上传maven制品到Nexus仓库中 stage(\"上传制品\"){ steps{ script{ //读取源代码中的POM文件，获取生成制品的maven坐标信息（Jenkins需要安装pipeline-utility-steps插件） def pomfile = readMavenPom file: 'pom.xml' //使用curl命令通过Nexus API接口上传制品到RAW仓库。下载URL既是上传URL sh \"curl -sL -w 'Upload the jar to the repository status code: %{http_code}\\n' -u admin:****** \" + \"--upload-file target/${pomfile.artifactId}-${pomfile.version}.${pomfile.packaging} \" + \"http://Nexus-IP地址:8081/repository/jenkins-product-repository/${pomfile.artifactId}-${pomfile.version}-${params.BUILD_VERSION}-${params.BUILD_ID}.${pomfile.packaging}\" } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/SonarQube静态代码扫描分析简介.html":{"url":"origin/SonarQube静态代码扫描分析简介.html","title":"SonarQube静态代码扫描分析","keywords":"","body":"一、静态代码分析 Why：在软件开发过程中，开发团队往往要花费大量的时间和精力发现并修改代码缺陷。而 发现BUG越晚，修复的成本越大 缺陷引入的大部分是在编码阶段，但发现更多的是在单元测试、集成测试、功能测试阶段 30% 至 70% 的代码逻辑设计和编码缺陷是可以通过静态代码分析来发现和修复的 How：在编码阶段，可以通过以下手段发现源代码问题，从源头及时规避，保证代码质量 静态代码扫描工具 Code Review What：Code Review往往要求大量的时间消耗和相关知识的积累，因此对于软件开发团队来说，使用静态代码分析工具自动化执行代码检查和分析，能够极大地提高软件可靠性并节省软件开发和测试成本。 帮助程序开发人员自动执行静态代码分析，快速定位代码隐藏错误和缺陷 助代码设计人员更专注于分析和解决代码设计缺陷 显著减少在代码逐行检查上花费的时间，提高软件可靠性并节省软件开发和测试成本 常见的一些静态分析工具 Checkstyle：SourceForge 的开源项目，通过检查对代码编码格式，命名约定，Javadoc，类设计等方面进行代码规范和风格的检查，从而有效约束开发人员更好地遵循代码编写规范 FindBugs：由马里兰大学提供的一款开源 Java 静态代码分析工具。FindBugs 通过检查类文件或 JAR 文件，将字节码与一组缺陷模式进行对比从而发现代码缺陷，完成静态代码分析 PMD：由 DARPA 在 SourceForge 上发布的开源 Java 代码静态分析工具。PMD 通过其内置的编码规则对 Java 代码进行静态检查，主要包括对潜在的 bug，未使用的代码，重复的代码，循环体创建新对象等问题的检验 二、Sonar简介 Sonar是一个用于代码质量管理的开源平台，可以从 七个维度检测代码质量 Sonar可以通过PMD、CheckStyle、Findbugs等代码规则检测工具来检测你的代码，帮助你发现代码的漏洞，Bug，异味等信息。 Sonar最大的特点就是插件化，可以根据不同的场景需求进行插件化安装，可以同时可以检测Python、C++等多种语言。 Sonar客户端可以采用IDE插件、Sonar-Scanner插件、Ant插件和Maven插件等多种方式，并通过各种不同的分析机制对项目源代码进行分析和扫描，并把分析扫描后的结果上传到sonar的数据库，通过sonar web界面对分析结果进行管理 Sonar的架构体系 Project：是需要被分析的源码 SonarQube Scanner：用于执行代码分析的工具，SonarQube Scanner分析完毕之后，会将结果上报到指定的SonarQube Server。 SonarQube Server：显示分析结果的Web Server，在SonarQube Scanner第一次将一个工程的分析结果上报给SonarQube Server后，Server上会自动创建一个工程显示分析的结果，可以在Server上设置代码质量管理相关的各种配置，如设置代码检查规则（Rule）和质量门限（Quality Gate）等。SonarQube Server包含三个子进程（web服务（界面管理）、搜索服务、计算引擎服务（写入数据库）） SonarQube Database：保存SonarQube服务端的权限配置，插件配置，项目快照，项目视图等 三、自动化扫描分析源代码的流程 官方推荐的自动化扫描流程 自动化静态代码扫描流程 本地开发：JetBrains Intellij IDEA 、Eclipse安装阿里巴巴的代码检查规范插件，可在编写代码时提示规范信息；安装使用sonarlint插件在本地运行代码扫描 Gitlab：Gitlab代码仓库可设置事件监听器，例如PUSH事件、Merge Request事件等。发送Web-hook到外部系统 Jenkins：Jenkins中可安装Gitlab插件，用于设置特定的Web-hook后端监听器来触发当前任务。 Jenkins Pipeline：在Jenkins Pipeline中获取Web-hook信息来拉取代码，然后编译、执行Sonar Scanner扫描源代码文件或二进制文件，最后将扫描的结果发送SnarQube进行存储、展示、管理等操作 SonarQube： 四、SonarQube服务端配置 1. 配置代码规则插件 2. 配置全局参数 3. 管理扫描结果 4. 质量门禁 五、Sonar体系中的配置参数生效优先级 UI界面中的全局参数配置 项目UI界面中的参数配置 项目分析客户端全局配置文件中的参数 例如sonar scanner的全局配置文件/opt/sonarscanner/conf/sonar.properties中的参数 例如sonar scanner Maven插件在settings.xml中配置的参数 项目分析客户端命令行运行时配置的参数，例如sonar-scanner二进制命令行运行时以“-D”开头的配置参数 六、扫描器 当SonarQube服务端搭建配置好了，Sonar提供了各种插件形式的Sonar Scanner扫描器供你选择来扫描你的源代码。 SonarScanner：二进制客户端 SonarScanner for Maven：Maven插件 SonarScanner for Jenkins：Jenkins插件 SonarScanner for Gradle：Gradle插件 SonarScanner for Ant：Ant插件 SonarScanner扫描参数 官方文档说明 参数 描述 默认值 是否必要 sonar.host.url SonarQube服务端地址 http://localhost:9000 是 sonar.projectKey 项目的唯一标识。以字母,-,_,:,至少有一个非数字 对于Maven插件的话,默认值是 :其他形式插件不提供默认值 是 sonar.projectName 在SonarQube Web UI上面显示的项目名 对于Maven插件形式,默认值是其他形式插件不提供默认值 否 sonar.projectVersion 项目的扫描版本 对于Maven插件形式,默认值是其他形式插件不提供默认值 否 sonar.login 发送扫描结果到SonarQube时的认证方式之一。值类型可为用户生成的认证Token，用户名 是 sonar.password 当sonar.login值类型为认证Token时，则不填 是 sonar.ws.timeout 等待服务端响应的最大秒数 60 否 sonar.projectDescription 项目描述。用于在项目Web UI中显示项目的描述 对于Maven插件形式,默认值是 否 sonar.links.homepage 项目地址。用于在项目Web UI中显示项目访问链接 对于Maven插件形式,默认值是 否 sonar.links.issue 项目代码Issue管理地址。用于在项目Web UI中显示Issue管理链接 对于Maven插件形式,默认值是 否 sonar.links.scm 项目源代码仓库地址。用于在项目Web UI中显示源代码仓库链接 for Maven projects 否 sonar.sources 以逗号分割的main源代码文件夹路径 否 sonar.tests 以逗号分割的测试源代码文件夹路径 否 sonar.sourceEncoding 源代码文件的编码格式，例如：UTF-8, MacRoman, Shift_JIS 系统的编码格式 否 sonar.externalIssuesReportPaths 否 sonar.projectDate 格式： yyyy-MM-dd, 例如: 2010-12-01 Current date 否 sonar.projectBaseDir 针对多模块项目时，指定要扫描源代码的目录路径 否 sonar.working.directory 指定Sonarscanner的工作空间。必须是不存在的，相对路径。针对MSBuild的插件，此参数不兼容 ~/.scannerwork 否 sonar.scm.provider 否 sonar.scm.forceReloadAll 否 sonar.scm.exclusions.disabled 否 sonar.scm.revision 否 sonar.buildString 100 否 sonar.analysis.[yourKey] 10 否 sonar.log.level 控制Sonarscanner输出日志的级别 INFO 否 sonar.verbose 输出更多Sonarscanner客户端和Sonarqube服务的扫描信息 false 否 sonar.showProfiling 否 sonar.scanner.dumpToFile 输出扫描期间所有的配置参数到文件中 否 sonar.scanner.metadataFilePath 指定report-task.txt文件的生成路径 等于sonar.working.directory的值 否 1. SonarScanner 下载地址：https://docs.sonarqube.org/latest/analysis/scan/sonarscanner/ 配置文件 全局配置文件路径：$安装目录/conf/sonar-scanner.properties 项目配置文件路径：$项目根目录/sonar-project.properties CLI命令参数 $ sonar-scanner usage: sonar-scanner [options] 参数: -D,--define Define property -h,--help Display help information -v,--version Display version information -X,--debug Produce execution debug output If you need more debug information you can add one of the following to your command line: -X, --verbose, or -Dsonar.verbose=true. 2. SonarScanner for Maven 官方文档：https://docs.sonarqube.org/latest/analysis/scan/sonarscanner-for-maven/ 注意： 从maven-sonar-plugin 3.4.0.905开始，不再支持SonarQube 从maven-sonar-plugin 3.1开始，不再支持Maven 全局参数 详见SonarQube服务端配置 配置Maven的setting.xml org.sonarsource.scanner.maven sonar true http://myserver:9000 f6eedc3d8bfa850a15f2ffcd 在项目pom.xml中配置项目扫描参数 ....上文省略.... com.curiosuer springboot2 0.0.1 用于演示Spring Boot2的一些功能 Curiouser-Demo-SpringBoot2-${gitlabBranch} http://springboot2-demo.apps.okd311.curiouser.com/swagger-ui.html ${gitlabSourceRepoHomepage}/commit/${gitlabMergeRequestLastCommit} ${gitlabSourceRepoHomepage}/issues ${BUILD_URL} UTF-8 UTF-8 1.8 指定的值 --> ${gitlabMergeRequestLastCommit} src/main/ 1 ${gitlabMergeRequestLastCommit} ${gitlabMergeRequestLastCommit} reuseReports jacoco ....下文省略.... 注意：pom.xml中有些参数的值是可以在Jenkins CI流水线中通过环境变量获取的。 执行扫描命令 mvn test sonar:sonar -Dspring.profiles.active=local 默认参数 sonar.projectKey ==> POM中的: sonar.projectName ==> POM中的 sonar.projectVersion ==> POM中的 sonar.projectDescription ==> POM中的 sonar.links.homepage ==> POM中的 sonar.links.ci ==> POM中的 sonar.links.issue ==> POM中的 sonar.links.scm ==> POM中的 七、扫描结果解析 附录 1、Sonar检查代码质量的七个维度 复杂度分布（complexity）：代码复杂度过高将难以理解、难以维护 重复代码（duplications）：程序中包含大量复制粘度的代码是质量低下的表现 单元测试（unit tests）：统计并展示单元测试覆盖率 编码规范（coding rules）：通过Findbugs、PMD、CheckStyle等规范代码编写 注释（commments）：少了可读性差，多了看起来费劲 潜在的Bug（potential bugs）：通过Findbugs、PMD、CheckStyle等检测潜在bug 结构与设计（architecture & design）：依赖i、耦合等 2、常见检查分析工具的内置规范 Checkstyle：分析源代码文件 Javadoc 注释：检查类及方法的 Javadoc 注释 命名约定：检查命名是否符合命名规范 标题：检查文件是否以某些行开头 Import 语句：检查 Import 语句是否符合定义规范 代码块大小，即检查类、方法等代码块的行数 空白：检查空白符，如 tab，回车符等 修饰符：修饰符号的检查，如修饰符的定义顺序 块：检查是否有空块或无效块 代码问题：检查重复代码，条件判断，魔数等问题 类设计：检查类的定义是否符合规范，如构造函数的定义等问题 FindBugs：分析字节码文件 Bad practice 坏的实践：常见代码错误，用于静态代码检查时进行缺陷模式匹配 Correctness 可能导致错误的代码，如空指针引用等 国际化相关问题：如错误的字符串转换 可能受到的恶意攻击，如访问权限修饰符的定义等 多线程的正确性：如多线程编程时常见的同步，线程调度问题。 运行时性能问题：如由变量定义，方法调用导致的代码低效问题。 PMD：：分析源代码文件 可能的 Bugs：检查潜在代码错误，如空 try/catch/finally/switch 语句 未使用代码（Dead code）：检查未使用的变量，参数，方法 复杂的表达式：检查不必要的 if 语句，可被 while 替代的 for 循环 重复的代码：检查重复的代码 循环体创建新对象：检查在循环体内实例化新对象 资源关闭：检查 Connect，Result，Statement 等资源使用之后是否被关闭掉 Jtest 可能的错误：如内存破坏、内存泄露、指针错误、库错误、逻辑错误和算法错误等 未使用代码：检查未使用的变量，参数，方法 初始化错误：内存分配错误、变量初始化错误、变量定义冲突 命名约定：检查命名是否符合命名规范 Javadoc 注释：检查类及方法的 Javadoc 注释 线程和同步：检验多线程编程时常见的同步，线程调度问题 国际化问题： 垃圾回收：检查变量及 JDBC 资源是否存在内存泄露隐患 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-22 12:08:49 "},"origin/sonarscanner-将扫描结果以comment的形式回写到gitlab.html":{"url":"origin/sonarscanner-将扫描结果以comment的形式回写到gitlab.html","title":"SonarScanner-将扫描结果以comment的形式回写到gitlab","keywords":"","body":"SonarScanner使用Sonarqube的Gitlab插件将扫描结果以gitlab comment的形式回写到Gitlab 一、Context 在Jenkins中做CI过程中,有一个步骤是代码编译完,使用sonar scanner扫描代码,检查静态代码中的语法错误等,然后将扫描结果发送到sonarqube,供项目经理查看代码质量. sonarqube可以安装插件gitlab,让sonarscanner扫描完代码,将结果以gitlab注释的方式回写到提交的commit中.方便开发人员排查代码. 以下操作过程各组件的版本 sonarqube: 7.3 (build 15553) sonarscanner: 3.3.0.1492 sonarqube gitlab插件: 4.0.0 gitlab: 10.8.4 ce jenkins: 2.150.2 Jenkins CI流水线是在使用Jenkins Slave(Kubernetes插件动态生成Slave POD)节点中来运行的,所以Sonarscanner,Maven等工具都是在Kubernetes Jenkins Slave镜像中已经安装好的. 二、操作 1、安装sonar-gitlab-plugin插件 插件Github:https://github.com/gabrie-allaigre/sonar-gitlab-plugin/ 2、生成用户访问Token 该Token用于客户端调用SonarQube API上传扫描代码时使用 3、gitlab创建sonarscanner的用户,并生成AccessKey 在sonarqube 服务端分析生成结果后，会使用该用户的身份在对应代码下以评论的形式显示扫描结果。所以需要该用户的AccessKey有访问Gitlab API的权限。 4、在gitlab中将sonarqube加入到对应项目仓库的Members中 设置sonarqube用户在该仓库的角色起码是开发者 5、Sonarqube中编辑gitlab插件的全局配置 扫描项目时，扫描参数生效优先级如下： UI界面中的全局参数配置 项目UI界面中的参数配置 项目分析客户端全局配置文件中的参数（例如sonar scanner的全局配置文件/opt/sonarscanner/conf/sonar.properties中的参数） 项目分析客户端命令行中配置的参数 所以可以在UI界面全局配置中配置一些通用、不经常变动的、由管理员控制的参数。例如：gitlab插件的通用配置、gitlab地址等参数 6、Jenkins Pipeline中使用sonarscanner扫描代码 stage(\"代码扫描\"){ steps{ sh \"sonar-scanner \\ -Dsonar.host.url=http://sonarqube.apps.okd311.curiouser.com \\ -Dsonar.login=6a6fa6f1702ae42f8d0a0fe14166d9a2 \\ -Dsonar.projectName=demo-springboot2-$GITLABSOURCEBRANCH \\ -Dsonar.projectKey=demo-springboot2-$GITLABSOURCEBRANCH \\ -Dsonar.projectVersion=$GIT_COMMIT \\ -Dsonar.sourceEncoding=UTF-8 \\ -Dsonar.sources=src/main \\ -Dsonar.test=src/test \\ -Dsonar.java.binaries=target/classes \\ -Dsonar.java.test.binaries='target/test-classes/*/*.class' \\ -Dsonar.java.source=8 \\ -Dsonar.gitlab.project_id=1 \\ -Dsonar.gitlab.commit_sha=$GIT_COMMIT \\ -Dsonar.gitlab.ref_name=$GIT_BRANCH \\ -Dsonar.java.coveragePlugin=jacoco \\ -Dsonar.dynamicAnalysis=reuseReports \" } } 三、效果 四、Soanarscanner Gitlab插件参数详解 Variable Comment Type Version sonar.gitlab.url GitLab url Administration, Variable >= 1.6.6 sonar.gitlab.max_global_issues Maximum number of anomalies to be displayed in the global comment Administration, Variable >= 1.6.6 sonar.gitlab.user_token Token of the user who can make reports on the project, either global or per project Administration, Project, Variable >= 1.6.6 sonar.gitlab.project_id Project ID in GitLab or internal id or namespace + name or namespace + path or url http or ssh url or url or web Project, Variable >= 1.6.6 sonar.gitlab.commit_sha SHA of the commit comment Variable >= 1.6.6 sonar.gitlab.ref Branch name or reference of the commit Variable sonar.gitlab.ref_name Branch name or reference of the commit Variable >= 1.6.6 sonar.gitlab.max_blocker_issues_gate Max blocker issue for build failed (default 0). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_critical_issues_gate Max critical issues for build failed (default 0). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_major_issues_gate Max major issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_minor_issues_gate Max minor issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.max_info_issues_gate Max info issues for build failed (default -1 no fail). Note: only for preview mode Project, Variable >= 2.0.0 sonar.gitlab.ignore_certificate Ignore Certificate for access GitLab, use for auto-signing cert (default false) Administration, Variable >= 2.0.0 sonar.gitlab.comment_no_issue Add a comment even when there is no new issue (default false) Administration, Variable >= 2.0.0 sonar.gitlab.disable_inline_comments Disable issue reporting as inline comments (default false) Administration, Variable >= 2.0.0 sonar.gitlab.only_issue_from_commit_file Show issue for commit file only (default false) Variable >= 2.0.0 sonar.gitlab.only_issue_from_commit_line Show issue for commit line only (default false) Variable >= 2.1.0 sonar.gitlab.build_init_state State that should be the first when build commit status update is called (default pending) Administration, Variable >= 2.0.0 sonar.gitlab.disable_global_comment Disable global comment, report only inline (default false) Administration, Variable >= 2.0.0 sonar.gitlab.failure_notification_mode Notification is in current build (exit-code) or in commit status (commit-status) (default commit-status) Administration, Variable >= 2.0.0 sonar.gitlab.global_template Template for global comment in commit Administration, Variable >= 2.0.0 sonar.gitlab.ping_user Ping the user who made an issue by @ mentioning. Only for default comment (default false) Administration, Variable >= 2.0.0 sonar.gitlab.unique_issue_per_inline Unique issue per inline comment (default false) Administration, Variable >= 2.0.0 sonar.gitlab.prefix_directory Add prefix when create link for GitLab Variable >= 2.1.0 sonar.gitlab.api_version GitLab API version (default v4 or v3) Administration, Variable >= 2.1.0 sonar.gitlab.all_issues All issues new and old (default false, only new) Administration, Variable >= 2.1.0 sonar.gitlab.json_mode Create a json report in root for GitLab EE (codeclimate.json or gl-sast-report.json) Project, Variable >= 3.0.0 sonar.gitlab.query_max_retry Max retry for wait finish analyse for publish mode Administration, Variable >= 3.0.0 sonar.gitlab.query_wait Max retry for wait finish analyse for publish mode Administration, Variable >= 3.0.0 sonar.gitlab.quality_gate_fail_mode Quality gate fail mode: error, warn or none (default error) Administration, Variable >= 3.0.0 sonar.gitlab.issue_filter Filter on issue, if MAJOR then show only MAJOR, CRITICAL and BLOCKER (default INFO) Administration, Variable >= 3.0.0 sonar.gitlab.load_rules Load rules for all issues (default false) Administration, Variable >= 3.0.0 sonar.gitlab.disable_proxy Disable proxy if system contains proxy config (default false) Administration, Variable >= 4.0.0 sonar.gitlab.merge_request_discussion Allows to post the comments as discussions (default false) Project, Variable >= 4.0.0 sonar.gitlab.ci_merge_request_iid The IID of the merge request if it’s pipelines for merge requests Project, Variable >= 4.0.0 五、问题 1、当项目是私有仓库时 2、获取项目仓库的ProjectID 3、gitlab插件4.0.0无法兼容Sonarqube 7.6-community至7.9-community的版本 报错如下！插件GIthub的原始Issue：https://github.com/gabrie-allaigre/sonar-gitlab-plugin/issues/213 [ERROR] Failed to execute goalorg.sonarsource.scanner.maven:sonar-maven-plugin:3.6.0.1398:sonar(default-cli) on project egsdloen-bc-facade:com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJob hasunsatisfied dependency 'classcom.talanlabs.sonar.plugins.gitlab.ReporterBuilder' for constructor'public com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJo(com.talanlabs.sonar.plugins.gitlab.GitLabPluginConfigurationcom.talanlabs.sonar.plugins.gitlab.SonarFacadecom.talanlabs.sonar.plugins.gitlab.CommitFacadecom.talanlabs.sonar.plugins.gitlab.ReporterBuilder)' fromorg.sonar.core.platformComponentContainer$ExtendedDefaultPicoContainer@7615666e:512[Immutable:org.sonar.core.platform.ComponentContainer$ExtendedDefaultPicoContaner@364adb24:56 [Help 1] org.apache.maven.lifecycle.LifecycleExecutionException: Failed toexecute goalorg.sonarsource.scanner.maven:sonar-maven-plugin:3.6.0.1398:sonar(default-cli) on project egsdloen-bc-facade:com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJob hasunsatisfied dependency 'classcom.talanlabs.sonar.plugins.gitlab.ReporterBuilder' for constructor'public com.talanlabs.sonar.plugins.gitlab.CommitPublishPostJo(com.talanlabs.sonar.plugins.gitlab.GitLabPluginConfigurationcom.talanlabs.sonar.plugins.gitlab.SonarFacadecom.talanlabs.sonar.plugins.gitlab.CommitFacadecom.talanlabs.sonar.plugins.gitlab.ReporterBuilder)' fromorg.sonar.core.platformComponentContainer$ExtendedDefaultPicoContainer@7615666e:512[Immutable:org.sonar.core.platform.ComponentContainer$ExtendedDefaultPicoContaner@364adb24:56 原因 解决方案 已经修改编译好的插件Jar包：https://github.com/gabrie-allaigre/sonar-gitlab-plugin/releases/download/4.1.0-SNAPSHOT/sonar-gitlab-plugin-4.1.0-SNAPSHOT.jar 参考链接 https://gitlab.com/gitlab-org/gitlab-ce/issues/28342 https://www.cnblogs.com/amyzhu/p/8988519.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-01-22 11:58:01 "},"origin/sonarqube-gitlab-auth.html":{"url":"origin/sonarqube-gitlab-auth.html","title":"Sonarqube使用Gitlab登录","keywords":"","body":"Sonarqube对接Gitlab认证 一、操作 1、Sonarqube安装Gitlab Auth插件 2、Gitlab Application配置 3、Sonarqube配置Gitlab认证插件 4、认证 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/sonarqube-ide-scanner.html":{"url":"origin/sonarqube-ide-scanner.html","title":"IDE本地插件扫描检查","keywords":"","body":"SonarQube本地IDE插件扫描 一、简介 SonarQube扫描代码的步骤是：不管是使用Maven插件还是通用的SonarScanner命令行工具，都是将项目的代码发送到SonarQube服务端进行处理的。由于SonarQube底层组件会用到ElasticSearch服务，如果服务器性能不够好或同时多个项目同时进行了扫描，会造成SonarQube服务端的后台处理任务加重，耗时严重，降低CICD流水线中集成扫描步骤的效率。所以有以下可建议的措施： 提高服务器性能 减少扫描无用的代码（例如服务端客户端进行exclude配置，只扫关注程度高的代码） 在合适的CICD环节再加入代码，增加代码扫描结果的重视程度 例如在发布到STG环境的CI流水线中加入代码扫描。如果是加在发布到生产环境的CI流水线中，会影响发版上线计划，开发人员来不及修改。如果是加在发布到测试环境的CI流水线中，会因为代码修改频繁，降低代码自动集成效果。而STG环境代码更新没那么频繁，离上线计划时间还有缓存进行修改代码。 开发人员在本地开发时先扫描一遍，提前规避掉大部分的代码错误 所以以Jetbrains公司的系列IDE IntelliJIDEA为例进行本地代码扫描。 二、IDEA安装配置插件 在IDE的插件管理中心安装名为\"SonarQube Community Plugin\"的插件，然后重启IDE。 插件GitHub地址：https://github.com/sonar-intellij-plugin/sonar-intellij-plugin 1、插件中心安装插件 2、配置插件添加sonarqueb服务端地址 扫描添加SonarQube服务端已有的项目 三、扫描代码（以扫描单个源代码文件为例） 扫描前会先从服务端下载服务端项目中配置的代码规则 会得到以下扫描结果 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/openldap-basic.html":{"url":"origin/openldap-basic.html","title":"OpenLDAP安装部署-查询-管理","keywords":"","body":"OpenLDAP 一、简介 LDAP 具有两个国家标准，分别是X.500 和LDAP。OpenLDAP 是基于X.500 标准的，而且去除了X.500 复杂的功能并且可以根据自我需求定制额外扩展功能，但与X.500 也有不同之处，例如OpenLDAP 支持TCP/IP 协议等，目前TCP/IP 是Internet 上访问互联网的协议。 二、基础概念 DN（Distinguished Name，区别名称）：DN 是条目在 LDAP 中的唯一标识符，它表示了条目在 LDAP 树中的位置。DN 的格式类似于文件系统中的路径 Base DN：Base DN 是在进行 LDAP 搜索时指定的起始点，搜索将从该点开始递归地向下进行。例如，如果 Base DN 设置为 dc=example,dc=com，那么搜索将从这个节点开始。 二、安装 1、 二进制 Ubuntu / Debian apt update && DEBIAN_FRONTEND=noninteractive apt install -y slapd ldap-utils gosa-schema https://www.linuxquestions.org/questions/linux-server-73/script-install-slapd-with-admin-ldap-password-4175426002/ https://superuser.com/questions/1786748/how-to-skip-the-password-typing-when-installing-library 2、Docker/k8s https://github.com/osixia/docker-openldap 三、初始化配置LDAP /etc/ldap/slapd.d/: 根据slapd.conf配置信息生成的文件 /etc/ldap/schema/: OpenLDAP的schema存放目录 /var/lib/ldap/: OpenLDAP数据目录 日志级别 Level Keyword Description -1 any enable all debugging 0 no debugging 1 (0x1 trace) trace function calls 2 (0x2 packets) debug packet handling 4 (0x4 args) heavy trace debugging 8 (0x8 conns) connection management 16 (0x10 BER) print out packets sent and received 32 (0x20 filter) search filter processing 64 (0x40 config) configuration processing 128 (0x80 ACL) access control list processing 256 (0x100 stats) stats log connections/operations/results 512 (0x200 stats2) stats log entries sent 1024 (0x400 shell) print communication with shell backends 2048 (0x800 parse) print entry parsing debugging 16384 (0x4000 sync) syncrepl consumer processing 32768 (0x8000 none) only messages that get logged whatever log level is set basedn='dc=devops,dc=test,dc=top' \\ rootuserdn='cn=admin,'$basedn \\ ldap_admin_passw=`specialchars='-_=#' && \\ astring=$(echo $(date +%s)$RANDOM | md5sum | base64 | tr -dc A-Za-z0-9 | head -c 16 ) && \\ randomchar=${specialchars:$RANDOM % ${#specialchars}:1} && \\ randompos=$(( $RANDOM % ( ${#astring} + 1 ) )) && \\ echo ${astring:0:$randompos}${randomchar}${astring:$randompos}` && \\ echo \"==============ldap 管理员admin密码: \"$ldap_admin_passw\"==============\" && \\ ldap_admin_passw_sha=`slappasswd -h {SHA} -s $ldap_admin_passw` && \\ cp /usr/share/slapd/slapd.init.ldif /etc/ldap/slapd.init.ldif && \\ sed -i -e \"/nis.ldif/a include: file:///etc/ldap/schema/gosa/rfc2307bis.ldif\" \\ -e \"/nis.ldif/d\" \\ -e \"s|olcRootPW:.*|olcRootPW: $ldap_admin_passw_sha|g\" \\ -e \"s/olcRootDN:.*/olcRootDN: $rootuserdn/g\" \\ -e \"s/olcSuffix:.*/olcSuffix: $basedn/g\" \\ /etc/ldap/slapd.init.ldif && \\ slapadd -F \"/etc/ldap/slapd.d/\" -b \"cn=config\" -l /etc/ldap/slapd.init.ldif && \\ chown -R openldap:openldap /etc/ldap/slapd.d && \\ systemctl restart slapd && \\ ldapsearch -LLL -Y external -H ldapi:/// -b cn=schema,cn=config -s one dn && \\ ldapwhoami -x -D \"$rootuserdn\" -w $ldap_admin_passw 四、创建组织结构 ldapadd -x -D \"$rootuserdn\" -w $ldap_admin_passw 五、开启Overlay功能 Overlays是OpenLDAP的一种软件功能模块，它通过给其他功能提供一些钩子(Hook)来实现类似backend的功能。memberof和refint都是一种Overlay。 1、memberOf与Refint功能模块 MemberOf Member是连接Group和Entry的桥梁。 如果某个组中通过 member 属性新增了一个用户，OpenLDAP 便会自动在该用户上创建一个 memberOf 属性，其值为该组的 dn。遗憾的是，OpenLDAP 默认并不启用这个特性，因此我们需要通过相关的配置开启它。 Refint Refint全称是Referential Integrity，可以理解为“引用一致性”，它用于支持动态更新。在配置memberof的时候，会有以下一条配置olcMemberOfRefInt: TRUE。该行配置会启用Referential Integrity。当配置refint之后，那么修改用户信息后。用户对应组的属性会被动态更新。 2、加载配置模块 ldapadd -x -D \"$rootuserdn\" -w $ldap_admin_passw # cn=module 的名称请查看ls -l /etc/ldap/slapd.d/cn\\=config/ |grep module 如果没有直接写module ，Docker中默认为module{0}; # olcDatabase={1}mdb 请确认ls -l /etc/ldap/slapd.d/cn=config/ |grep olcDatabase 的名称，docker中默认的为{1}mdb,Centos rpm 安装默认为{2}hdb。 bash -c 'cat >/etc/ldap/01-memberOf.ldif /etc/ldap/02-refint.ldif /etc/ldap/03-index.ldif # 用于检索 olcDatabase={1}mdb,cn=config 的 DN，即数据库配置的 DN。 ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b olcDatabase={1}mdb,cn=config dn # 检索 memberof overlay 的 DN，但在 OpenLDAP 中，overlay的DN通常不是一个单独的节点，而是与数据库关联的属性 ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b \"olcOverlay={0}memberof,olcDatabase={1}mdb,cn=config\" dn # 查看 memberof overlay 的配置 ldapsearch -Q -LLL -Y EXTERNAL -H ldapi:/// -b \"olcOverlay={0}memberof,olcDatabase={1}mdb,cn=config\" 3、测试memberof与Refint的功能 ①创建测试用户与用户组测试数据 ldapadd -x -D \"cn=admin,dc=devops,dc=test,dc=top\" -w $ldap_admin_passw ② 测试查询 ldapsearch -x -LLL -H ldap://localhost -b \"cn=testuser2,dc=devops,dc=test,dc=top\" memberOf ldapsearch -x -H ldap:/// -b \"cn=test,ou=users,dc=devops,dc=test,dc=top\" -D \"cn=admin,dc=devops,dc=test,dc=top\" -w $ldap_admin_passw memberOf 五、导入 RFC2307Bis 功能差异： NIS 模式是为了向后兼容 UNIX 系统中使用的 NIS 服务而设计的。它定义了一组属性和对象类，用于存储与用户、组、主机等相关的信息。 RFC2307bis 模式是为了与 RFC 2307 中定义的 LDAP 网络信息服务（NIS）模式保持兼容，并添加了一些额外的属性和对象类，以便更好地支持 UNIX 环境中的用户和组信息的存储和检索。 属性和对象类的差异： NIS 模式包含了一些与 NIS 相关的属性和对象类，如 nisNetgroup、nisMap、nisObject 等。 RFC2307bis 模式在 NIS 模式的基础上扩展了属性和对象类，如添加了 posixAccount、posixGroup、shadowAccount 等，以更好地支持 UNIX 环境中的用户和组管理。 应用场景： NIS 模式通常用于那些仍在使用 NIS 服务的旧系统或需要与 NIS 服务进行交互的系统。 RFC2307bis 模式更适用于需要更现代化的 UNIX 用户和组管理功能的系统，它提供了更多的属性和对象类，使得用户和组信息的存储更加灵活和完善。 总的来说，RFC2307bis 模式可以看作是对 NIS 模式的扩展和改进，提供了更多功能和更好的兼容性，因此在现代化的 UNIX 环境中更常见和推荐使用。 cp /etc/ldap/add/rfc2307bis.* /etc/ldap/schema/ mkdir -p /tmp/schema slaptest -f /etc/ldap/add/rfc2307bis.conf -F /tmp/schema 查询已导入的 schema ldapsearch -LLL -Y external -H ldapi:/// -b cn=schema,cn=config -s one dn 测试 $ ldapadd -W -H ldap:/// -D 'cn=Manager,dc=example,dc=com' https://bubblesorted.raab.link/content/replace-nis-rfc2307-rfc2307bis-schema-openldap https://unofficialaciguide.com/2019/07/31/ldap-schemas-for-aci-administrators-rfc2307-vs-rfc2307bis/ 六、常用命令 ① 查看openLDAP的版本 bashslapd -VV ② 查看已启用的模块 ldapsearch -LLL -Y EXTERNAL -H ldapi:/// -b cn=config | grep -i module slapcat -n 0 | grep olcModuleLoad ③ 查询数据库配置 ldapsearch -LLL -Y EXTERNAL -H ldapi:/// -b cn=config olcDatabase | grep mdb # 输出 # SASL/EXTERNAL authentication started # SASL username: gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth # SASL SSF: 0 # dn: olcDatabase={1}mdb,cn=config # olcDatabase: {1}mdb ④ 查看module模块路径 ldapsearch -LLL -Y EXTERNAL -H ldapi:/// -b cn=config | grep -i module # Centos: /usr/lib64/openldap # Ubuntu: # memberof: /usr/lib/ldap/memberof.la # refint: /usr/lib/ldap/refint.la ⑤ 测试配置文件 slaptest -u ⑥ 导入基本的Schema ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/cosine.ldif ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/nis.ldif ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/inetorgperson.ldif ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/ppolicy.ldif ⑦ 生成密码 slappasswd -h {SHA} -s 密码 六、用户管理 1、新增只读用户 basedn='dc=devops,dc=test,dc=top' \\ rootuserdn='cn=admin,'$basedn \\ LDAP_READONLY_USER_PW=`specialchars='-_=#' && \\ astring=$(echo $(date +%s)$RANDOM | md5sum | base64 | tr -dc A-Za-z0-9 | head -c 16 ) && \\ randomchar=${specialchars:$RANDOM % ${#specialchars}:1} && \\ randompos=$(( $RANDOM % ( ${#astring} + 1 ) )) && \\ echo ${astring:0:$randompos}${randomchar}${astring:$randompos}` && \\ echo \"==============ldap 只读用户名密码: \"$ldap_admin_passw\"==============\" && \\ LDAP_READONLY_USER_PW_SHA=`slappasswd -h {SHA} -s $LDAP_READONLY_USER_PW` ldapadd -x -D $rootuserdn -w $ldap_admin_passw 2、创建用户 dn: cn=uservpn,dc=bufx,dc=gov,dc=cn cn: uservpn sn: vpn userPassword: {SSHA}abZPY3cWgMMtnNqtFyuX64xq7Mm0TNFd objectClass: inetOrgPerson objectClass: organizationalPerson 十一、卸载备份 systemctl stop slapd rm -rf /etc/ldap/slapd.d/* /etc/ldap/slapd.init.ldif /var/lib/ldap/* /etc/ldap/0*.ldif 参考 https://www.cnblogs.com/somata/p/OPENLDAPServerConfigAndPostManagement.html https://cloud.tencent.com/developer/article/1932586 https://www.brianshowalter.com/blog/installing-configuring-openldap https://www.adimian.com/blog/how-to-enable-memberof-using-openldap/ https://www.openldap.org/doc/admin25/guide.html https://www.openldap.org/software/release/announce_lts.html https://www.cnblogs.com/eagle6688/p/16990393.html https://tylersguides.com/guides/openldap-memberof-overlay/ https://lework.github.io/2019/07/18/ldap/ https://mayanbin.com/posts/enable-memberof-in-openldap https://github.com/osixia/docker-openldap/blob/master/image/service/slapd/assets/config/bootstrap/ldif/03-memberOf.ldif https://blog.csdn.net/qq_23191379/article/details/106867730 https://blog.361way.com/ldap-adduser-grants/2825.html https://blog.csdn.net/xyy511/article/details/90521086 https://blog.buubiu.com/open-ldap-%E5%88%9B%E5%BB%BA%E5%8F%AA%E8%AF%BB%E7%94%A8%E6%88%B7/ https://www.cnblogs.com/husbandmen/p/13307381.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-21 15:33:27 "},"origin/go-ldap-admin.html":{"url":"origin/go-ldap-admin.html","title":"go-ldap-admin","keywords":"","body":"Go LDAP Admin 一、简介 基于Go+Vue实现的openLDAP后台管理项目 Github：https://github.com/eryajf/go-ldap-admin 官网：http://ldapdoc.eryajf.net/ 提供在线体验地址如下： 分类 地址 用户名 密码 go-ldap-admin http://demo-go-ldap-admin.eryajf.net admin 123456 phpLdapAdmin http://demo-go-ldap-admin.eryajf.net:8091/ cn=admin,dc=eryajf,dc=net 123456 二、安装部署 1、docker git clone https://github.com/eryajf/go-ldap-admin.git cd docs/docker-compose docker-compose up -d 本地进行访问：http://localhost:8090，用户名/密码：`admin`/`123456` 如果想要访问PhpLdapAdmin，则可访问：http://localhost:8091，用户名/密码：`cn=admin,dc=eryajf,dc=net`/`123456` Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-04-16 09:45:15 "},"origin/ldap-Jenkins对接LDAP.html":{"url":"origin/ldap-Jenkins对接LDAP.html","title":"Jenkins","keywords":"","body":"一. Context OpenLDAP的条目组织形式 二. Jenkins配置 1. Jenkins安装LDAP插件 安装插件有两种方法： 方法一：后台插件管理里直接安装 优点：简单方便，不需要考虑插件依赖问题 缺点：因为网络等各种问题安装不成功 安装方法：登录Jenkins --> 系统管理 --> 插件管理 --> 可选插件 --> 搜索LDAP --> 选中 --> 直接安装 --> 安装完成重启 方法二：官网下载安装文件后台上传 优点：一定可以安装成功的 缺点：麻烦，要去官网找插件并解决依赖 安装方法：官网下载插件 --> 登录Jenkins --> 系统管理 --> 插件管理 --> 高级 --> 上传插件 --> 选择文件 --> 上传 --> 安装完成后重启 LDAP插件下载地址：https://updates.jenkins.io/download/plugins/ldap/ 2. 登录Jenkins --> 系统管理 --> 全局安全配置 root DN：这里的root DN只是指搜索的根，并非LDAP服务器的root dn。由于LDAP数据库的数据组织结构类似一颗大树，而搜索是递归执行的，理论上，我们如果从子节点（而不是根节点）开始搜索，因为缩小了搜索范围那么就可以获得更高的性能。这里的root DN指的就是这个子节点的DN，当然也可以不填，表示从LDAP的根节点开始搜索 User search base：这个配置也是为了缩小LDAP搜索的范围，例如Jenkins系统只允许ou为Admin下的用户才能登陆，那么你这里可以填写ou=Admin，这是一个相对的值，相对于上边的root DN，例如你上边的root DN填写的是dc=domain,dc=com，那么user search base这里填写了ou=Admin，那么登陆用户去LDAP搜索时就只会搜索ou=Admin,dc=domain,dc=com下的用户了 User search filter：这个配置定义登陆的“用户名”对应LDAP中的哪个字段，如果你想用LDAP中的uid作为用户名来登录，那么这里可以配置为uid={0}（{0}会自动的替换为用户提交的用户名），如果你想用LDAP中的mail作为用户名来登录，那么这里就需要改为mail={0}。在测试的时候如果提示你user xxx does not exist，而你确定密码输入正确时，就要考虑下输入的用户名是不是这里定义的这个值了 Group search base：参考上边User search base解释 Group search filter：这个配置允许你将过滤器限制为所需的objectClass来提高搜索性能，也就是说可以只搜索用户属性中包含某个objectClass的用户，这就要求你对你的LDAP足够了解，一般我们也不配置 Group membership：没配置，没有详细研究 Manager DN：这个配置在你的LDAP服务器不允许匿名访问的情况下用来做认证（详细的认证过程参考文章LDAP落地实战（二）：SVN集成OpenLDAP认证中关于LDAP服务器认证过程的讲解），通常DN为cn=admin,dc=domain,dc=com这样 Manager Password：上边配置dn的密码 Display Name LDAP attribute：配置用户的显示名称，一般为显示名称就配置为uid，如果你想显示其他字段属性也可以这里配置，例如mail Email Address LDAP attribute：配置用户Email对应的字段属性，一般没有修改过的话都是mail，除非你用其他的字段属性来标识用户邮箱，这里可以配置 3. 登录验证 参考链接 https://mp.weixin.qq.com/s/S5ozDJSh4yTSfP_glNoiOQ https://plugins.jenkins.io/ldap https://wiki.jenkins.io/display/JENKINS/LDAP+Plugin#LDAPPlugin-Groupmembership https://www.cnblogs.com/zhaojiedi1992/p/zhaojiedi_liunx_52_ldap_for_jenkins.html https://blog.csdn.net/wanglei_storage/article/details/52935312 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ldap-SonarQube对接LDAP.html":{"url":"origin/ldap-SonarQube对接LDAP.html","title":"SonarQube","keywords":"","body":"一、Context OpenLDAP的条目组织形式 Sonaeqube官方文档的操作步骤 二、操作 1、Sonarqube安装LDAP插件 配置--> 应用市场 2、修改配置文件/opt/sonarqube/conf/sonar.properties 如果sonarqube的部署实例是使用Dockers的话，则可通过环境变量的方式注入以下配置 sonar.security.realm=LDAP sonar.forceAuthentication=true ldap.authentication=simple ldap.url=ldap://openldap-service.openldap.svc:389 ldap.bindDn=cn=admin,dc=curiouser,dc=com ldap.bindPassword=****** # User Configuration ldap.user.baseDn=ou=employee,dc=curiouser,dc=com ldap.user.request=(&(memberOf=cn=sonarqube,ou=applications,dc=curiouser,dc=com)(cn={0})) ldap.user.realNameAttribute=sn ldap.user.emailAttribute=mail 相关配置 Property Description Default value Required Example sonar.security.realm Set this to LDAP authenticate first against the external sytem. If the external system is not reachable or if the user is not defined in the external system, authentication will be performed against SonarQube's internal database. none Yes LDAP (only possible value) sonar.authenticator.downcase Set to true when connecting to a LDAP server using a case-insensitive setup. false No ldap.url URL of the LDAP server. If you are using ldaps, you should install the server certificate into the Java truststore. none Yes ldap://localhost:10389 ldap.bindDn The username of an LDAP user to connect (or bind) with. Leave this blank for anonymous access to the LDAP directory. none No cn=sonar,ou=users,o=mycompany ldap.bindPassword The password of the user to connect with. Leave this blank for anonymous access to the LDAP directory. none No secret ldap.authentication Possible values: simple, CRAM-MD5, DIGEST-MD5, GSSAPI. See the tutorial on authentication mechanisms simple No ldap.realm See Digest-MD5 Authentication, CRAM-MD5 Authentication none No example.org ldap.contextFactoryClass Context factory class. com.sun.jndi.ldap.LdapCtxFactory No ldap.StartTLS Enable use of StartTLS false No ldap.followReferrals Follow referrals or not. See Referrals in the JNDI true 用户配置 Property Description Default value Required Example ldap.user.baseDn Distinguished Name (DN) of the root node in LDAP from which to search for users. None Yes cn=users,dc=example,dc=org ldap.user.request LDAP user request. (&(objectClass=inetOrgPerson)(uid={login})) No (&(objectClass=user)(sAMAccountName={login})) ldap.user.realNameAttribute Attribute in LDAP defining the user’s real name. cn No ldap.user.emailAttribute Attribute in LDAP defining the user’s email. mail No Group Mapping Only groups are supported (not roles). Only static groups are supported (not dynamic groups). For the delegation of authorization, groups must be first defined in SonarQube. Then, the following properties must be defined to allow SonarQube to automatically synchronize the relationships between users and groups. Property Description Default value Required Example for Active Directory ldap.group.baseDn Distinguished Name (DN) of the root node in LDAP from which to search for groups. none No cn=groups,dc=example,dc=org ldap.group.request LDAP group request. (&(objectClass=groupOfUniqueNames)(uniqueMember={dn})) No (&(objectClass=group)(member={dn})) ldap.group.idAttribute Property used to specifiy the attribute to be used for returning the list of user groups in the compatibility mode. cn No sAMAccountName 重启Sonarqube，启动过程中如果出现以下日志，则证明LDAP连接成功 INFO org.sonar.INFO Security realm: LDAP ... INFO o.s.p.l.LdapContextFactory Test LDAP connection: OK 3、登录验证 4、权限控制 将admin用户的管理员权限删除，赋予另一个用户 参考链接 https://hub.docker.com/_/sonarqube?tab=description https://docs.sonarqube.org/latest/instance-administration/delegated-auth/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ldap-Gitlab对接LDAP.html":{"url":"origin/ldap-Gitlab对接LDAP.html","title":"Gitlab","keywords":"","body":"一、Context OpenLDAP的条目组织形式 二、配置 1. 修改/etc/gitlab/gitlab.rb ..................省略............................. gitlab_rails['ldap_enabled'] = true ###! **remember to close this block with 'EOS' below** gitlab_rails['ldap_servers'] = YAML.load 三、测试登录 四、注意 当用户第一次使用LDAP登录GitLab时，如果其LDAP电子邮件地址是现有GitLab用户的电子邮件地址时，那么LDAP DN用户将与现有gitlab用户相关联。如果在GitLab的数据库中没有找到LDAP电子邮件属性，就会创建一个新用户。 换句话说，如果现有的GitLab用户希望自己启用LDAP登录，那么他们应该检查他们的GitLab电子邮件地址是否匹配LDAP电子邮件地址，然后通过他们的LDAP凭证登录GitLab。 https://docs.gitlab.com/ee/administration/auth/ldap.html#enabling-ldap-sign-in-for-existing-gitlab-users 参考链接 https://blog.csdn.net/tongdao/article/details/52538365 https://docs.gitlab.com/ee/administration/auth/ldap.html#configuration https://docs.gitlab.com/ee/administration/auth/how_to_configure_ldap_gitlab_ce/index.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ldap-Nexus对接LDAP.html":{"url":"origin/ldap-Nexus对接LDAP.html","title":"Nexus","keywords":"","body":"Preflight Nexus 3 OpenLDAP 3.15.2-01 1.2.4 一、Context OpenLDAP的条目组织形式 二、Nexus设置 1. Nexus开启认证Realm 2. 配置LDAP Name：Enter a unique name for the new configuration. LDAP server address：Enter Protocol, Hostname, and Port of your LDAP server. Protocol：Valid values in this drop-down are ldap and ldaps that correspond to the Lightweight Directory Access Protocol and the Lightweight Directory Access Protocol over SSL. Hostname：The hostname or IP address of the LDAP server. Port：The port on which the LDAP server is listening. Port 389 is the default port for the ldap protocol, and port 636 is the default port for the ldaps. Search base：The search base further qualifies the connection to the LDAP server. The search base usually corresponds to the domain name of an organization. For example, the search base could be dc=example,dc=com. Note: If the values in your search base contain spaces, escape them with \"%20\", as in \"dc=example%20corp,dc=com\" You can configure one of four authentication methods to be used when connecting to the LDAP Server with the Authentication method drop-down. Simple Authentication：Simple authentication consists of a Username and Password. Simple authentication is not recommended for production deployments not using the secure ldaps protocol as it sends a clear-text password over the network. Anonymous Authentication：The anonymous authentication uses the server address and search base without further authentication. Digest-MD5：This is an improvement on the CRAM-MD5 authentication method. For more information, see RFC-2831. CRAM-MD5：The Challenge-Response Authentication Method (CRAM) is based on the HMAC-MD5 MAC algorithm. In this authentication method, the server sends a challenge string to the client. The client responds with a username followed by a Hex digest that the server compares to an expected value. For more information, see RFC-2195.For a full discussion of LDAP authentication approaches, see RFC-2829 and RFC-2251. SASL Realm：The Simple Authentication and Security Layer (SASL) realm used to connect to the LDAP server. It is only available if the authentication method is Digest-MD5 or CRAM-MD5. Username or DN：Username or DN (Distinguished Name) of an LDAP user with read access to all necessary users and groups. It is used to connect to the LDAP server. Password：Password for the Username or DN configured above. Base DN：Corresponds to the collection of distinguished names used as the base for user entries. This DN is relative to the Search Base. For example, if your users are all contained in ou=users,dc=sonatype,dc=com and you specified a Search Base of dc=sonatype,dc=com, you use a value of ou=users. User subtree：Check the box if True. Uncheck if False. Values are true if there is a tree below the Base DN that can contain user entries and false if all users are contain within the specified Base DN. For example, if all users are in ou=users,dc=sonatype,dc=com this field should be False. If users can appear in organizational units within organizational units such as ou=development,ou=users,dc=sonatype,dc=com, this field should be True . Object class：This value is a standard object class defined in RFC-2798. It specifies the object class for users. Common values are inetOrgPerson, person, user, or posixAccount. User filter：This allows you to configure a filter to limit the search for user records. It can be used as a performance improvement. User ID attribute：This is the attribute of the object class specified above, that supplies the identifier for the user from the LDAP server. The repository manager uses this attribute as the User ID value. Real name attribute：This is the attribute of the Object class that supplies the real name of the user. The repository manager uses this attribute when it needs to display the real name of a user similar to usage of the internal First name and Last name attributes. Email attribute：This is the attribute of the Object class that supplies the email address of the user. The repository manager uses this attribute for the Email attribute of the user. It is used for email notifications of the user. Password attribute：It can be used to configure the Object class, which supplies the password (\"userPassword\"). If this field is blank the user will be authenticated against a bind with the LDAP server. The password attribute is optional. When not configured authentication will occur as a bind to the LDAP server. Otherwise this is the attribute of the Object class that supplies the password of the user. The repository manager uses this attribute when it is authenticating a user against an LDAP server. Group Base DN：This field is similar to the Base DN field described for User Element Mapping, but applies to groups instead of users. For example, if your groups were defined under ou=groups,dc=sonatype,dc=com, this field would have a value of ou=groups. Group subtree：This field is similar to the User subtree field described for User Element Mapping, but configures groups instead of users. If all groups are defined under the entry defined in Base DN, set the field to false. If a group can be defined in a tree of organizational units under the Base DN, set the field to true. Group object class：This value in this field is a standard object class defined in RFC-2307. The class is simply a collection of references to unique entries in an LDAP directory and can be used to associate user entries with a group. Examples are groupOfUniqueNames, posixGroup or custom values. Group ID attribute：Specifies the attribute of the object class that specifies the group identifier. If the value of this field corresponds to the ID of a role, members of this group will have the corresponding privileges. Group member attribute：Specifies the attribute of the object class which specifies a member of a group. An example value is uniqueMember. Group member format：This field captures the format of the Group Member Attribute, and is used by the repository manager to extract a username from this attribute. An example values is ${dn} . 3. 分配Nexus管理员的角色\"nx-admin\"给LDAP上的一个用户，作为nexus新的管理员。然后将admin用户禁用。 参考链接 https://help.sonatype.com/repomanager3/security/ldap Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ldap-Grafana对接LDAP.html":{"url":"origin/ldap-Grafana对接LDAP.html","title":"Grafana","keywords":"","body":"一、Context OpenLDAP的条目组织形式 二、操作 1、修改/etc/grafana/grafana.ini .............省略............. [auth.ldap] enabled = true config_file = /etc/grafana/ldap.toml allow_sign_up = true .............省略............. 2、修改/etc/grafana/ldap.toml .............省略............. # To troubleshoot and get more log info enable ldap debug logging in grafana.ini # [log] # filters = ldap:debug [[servers]] # Ldap server host (specify multiple hosts space separated) host = \"openldap-service.openldap.svc\" # Default port is 389 or 636 if use_ssl = true port = 389 # Set to true if ldap server supports TLS use_ssl = false # Set to true if connect ldap server with STARTTLS pattern (create connection in insecure, then upgrade to secure connection with TLS) start_tls = false # set to true if you want to skip ssl cert validation ssl_skip_verify = false # set to the path to your root CA certificate or leave unset to use system defaults # root_ca_cert = \"/path/to/certificate.crt\" # Authentication against LDAP servers requiring client certificates # client_cert = \"/path/to/client.crt\" # client_key = \"/path/to/client.key\" # Search user bind dn bind_dn = \"cn=admin,dc=curiouser,dc=com\" # Search user bind password，If the password contains # or ; you have to wrap it with triple quotes. Ex \"\"\"#password;\"\"\" bind_password = '*********' # User search filter, for example \"(cn=%s)\" or \"(sAMAccountName=%s)\" or \"(uid=%s)\" search_filter = \"(&(memberOf=cn=grafana,ou=applications,dc=curiouser,dc=com))\" # An array of base dns to search through search_base_dns = [\"ou=employee,dc=curiouser,dc=com\"] ## For Posix or LDAP setups that does not support member_of attribute you can define the below settings。Please check grafana LDAP docs for examples # group_search_filter = \"(&(objectClass=posixGroup)(memberUid=%s))\" # group_search_base_dns = [\"ou=groups,dc=grafana,dc=org\"] # group_search_filter_user_attribute = \"uid\" # Specify names of the ldap attributes your ldap uses [servers.attributes] name = \"sn\" username = \"cn\" member_of = \"memberOf\" email = \"mail\" # Map ldap groups to grafana org roles [[servers.group_mappings]] #group_dn = \"cn=admins,dc=grafana,dc=org\" #org_role = \"Admin\" # To make user an instance admin (Grafana Admin) uncomment line below # grafana_admin = true # The Grafana organization database id, optional, if left out the default org (id 1) will be used # org_id = 1 [[servers.group_mappings]] group_dn = \"cn=users,dc=grafana,dc=org\" org_role = \"Editor\" [[servers.group_mappings]] # If you want to match all (or no ldap groups) then you can use wildcard group_dn = \"*\" #org_role = \"Viewer\" .............省略............. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ldap-jira接LDAP.html":{"url":"origin/ldap-jira接LDAP.html","title":"Jira","keywords":"","body":"Jira对接LDAP 一、上下文 二、Jira配置LDAP 三、测试LDAP用户 四、同步用户 五、将LDAP用户添加至管理员用户组 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/apollo-deplopy-setup.html":{"url":"origin/apollo-deplopy-setup.html","title":"Apollo简介部署与配置","keywords":"","body":"Apollo简介、部署与配置 一、简介 GitHub：https://github.com/apolloconfig/apollo 文档：https://www.apolloconfig.com/#/zh/README 二、源代码构建 三、部署 1、二进制 2、Docker Allinone(docker) 数据库初始化文件初始化SQL文件,请先导入数据库中。 使用其他MySQL服务 docker run --restart=always -d --name apollo \\ --link mysql5.7:apollo-db \\ -p 8070:8070 \\ -p 8080:8080 \\ -p 8090:8090 \\ -e JAVA_OPTS=\"-Xms100m -Xmx1000m -Xmn100m -Xss256k -XX:MetaspaceSize=10m -XX:MaxMetaspaceSize=250m\" \\ -e APOLLO_CONFIG_DB_USERNAME=apollo \\ -e APOLLO_CONFIG_DB_PASSWORD=apollo \\ -e APOLLO_PORTAL_DB_USERNAME=apollo \\ -e APOLLO_PORTAL_DB_PASSWORD=apollo \\ nobodyiam/apollo-quick-start Allinone(docker-compose) 数据库初始化文件初始化SQL文件请放置在docker-compose.yml同级目录下 version: '2' services: apollo-quick-start: image: nobodyiam/apollo-quick-start container_name: apollo-quick-start depends_on: - apollo-db ports: - \"8080:8080\" - \"8090:8090\" - \"8070:8070\" links: - apollo-db apollo-db: image: mysql:5.7 container_name: apollo-db environment: TZ: Asia/Shanghai MYSQL_ALLOW_EMPTY_PASSWORD: 'yes' depends_on: - apollo-dbdata ports: - \"13306:3306\" volumes: - ./sql:/docker-entrypoint-initdb.d volumes_from: - apollo-dbdata 注1：数据库用户名是root，密码留空。 注2：Apollo UI访问地址：http://127.0.0.1:8070/ 。用户名apollo，密码admin 3、K8S Helm helm repo add apollo https://charts.apolloconfig.com helm repo update helm install -n test apollo-service \\ --set configdb.host=1.2.3.4 \\ --set configdb.userName=apollo \\ --set configdb.password=apollo \\ --set configdb.service.enabled=true \\ --set configdb.connectionStringProperties=\"characterEncoding=utf8&useSSL=false\" \\ --set configService.replicaCount=1 \\ --set adminService.replicaCount=1 \\ apollo/apollo-service --debug --version 0.5.0 helm install -n tool apollo-portal \\ --set portaldb.host=1.2.3.4 \\ --set portaldb.userName=apollo \\ --set portaldb.password=apollo \\ --set portaldb.service.enabled=true \\ --set config.envs=\"dev\\,stg\\,pro\" \\ --set config.metaServers.test=http://apollo-service-test-apollo-configservice.test.svc:8080 \\ --set config.metaServers.stg=http://apollo-service-stg-apollo-configservice.stg.svc:8080 \\ --set replicaCount=1 \\ apollo/apollo-portal --debug --version 0.5.0 四、配置 1、开启Webhook通知 从 1.8.0 版本开始，apollo 增加了 webhook 支持，从而可以在配置发布时触发 webhook 并告知配置发布的信息。 ① Apollo配置 配置项统一存储在ApolloPortalDB.ServerConfig表中，也可以通过管理员工具 - 系统参数页面进行配置，修改完一分钟实时生效。 webhook.supported.envs：开启 webhook 的环境列表，多个环境以英文逗号分隔，例如DEV,FAT,UAT,PRO config.release.webhook.service.url：webhook 通知的 url 地址，需要接收 HTTP POST 请求。如有多个地址，以英文逗号分隔，例如：http://www.xxx.com/webhook1,http://www.xxx.com/webhook2 ② webhook的POST请求体样本 http://www.xxx.com/webhook1?env=DEV { \"id\": 11, \"appId\": \"test\", \"clusterName\": \"default\", \"namespaceName\": \"application\", \"branchName\": \"default\", \"operator\": \"apollo\", \"releaseId\": 11, \"releaseTitle\": \"20220610174331-release\", \"releaseComment\": \"\", \"releaseTime\": \"2022-06-10T17:43:33.000+0800\", \"releaseTimeFormatted\": \"now\", \"configuration\": [ { \"firstEntity\": \"dingding_token\", \"secondEntity\": \"****\" }, { \"firstEntity\": \"dingding_secret\", \"secondEntity\": \"**\" }, { \"firstEntity\": \"test\", \"secondEntity\": \"66\" } ], \"isReleaseAbandoned\": false, \"previousReleaseId\": 10, \"operation\": 0, \"operationContext\": { \"isEmergencyPublish\": false } } ③使用Go gin写一个Webhook请求后端，将通知转发到钉钉机器人群中 文档：https://www.apolloconfig.com/#/zh/development/portal-how-to-enable-webhook-notification 五、Apollo升级 0、简介 升级原因 版本更新带来的新增功能和bug修复 修复已知版本的安全漏洞问题 已知升级修复问题 1.8.0 升级至 1.8.2，可修复XStream XStream 1、升级路径 升级说明在release页面：https://github.com/apolloconfig/apollo/releases 1.8.0 –> 1.8.1 –> 1.8.2 没有数据库层面的变更 2、升级步骤 先升级apollo-configservice 再升级apollo-adminservice 再升级apollo-portal Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-08 13:28:48 "},"origin/apollo-application.html":{"url":"origin/apollo-application.html","title":"应用接入Apollo","keywords":"","body":"应用集成Apollo 一、Java 二、PHP 三、Golang Apollo Go SDK: https://github.com/apolloconfig/agollo Apollo Go SDK文档：https://pkg.go.dev/github.com/zouyx/agollo?utm_source=godoc 设置系统环境变量 export APP_NAME=test-app export APP_ENV=dev export APOLLO_ADDR=http://127.0.0.1:8080 export APP_NS=application export APOLLO_SECRET= package main import ( \"fmt\" \"github.com/apolloconfig/agollo/v4\" \"github.com/apolloconfig/agollo/v4/env/config\" ) var ( key1 string key2 string appConfig = &config.AppConfig{ AppID: os.Getenv(\"APP_NAME\"), Cluster: os.Getenv(\"APP_ENV\"), IP: os.Getenv(\"APOLLO_ADDR\"), NamespaceName: os.Getenv(\"APP_NS\"), IsBackupConfig: true, Secret: os.Getenv(\"APOLLO_SECRET\"), } ) func init() { client, _ := agollo.StartWithConfig(func() (*config.AppConfig, error) { return appConfig, nil }) fmt.Println(\"初始化Apollo配置成功\") key1 = client.GetStringValue(\"key1\", \"\") key2 = client.GetStringValue(\"key2\", \"\") } func main() { fmt.Println(\"main\") fmt.Println(key1) fmt.Println(key2) } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-06-09 12:07:30 "},"origin/jira-部署.html":{"url":"origin/jira-部署.html","title":"Jira的部署","keywords":"","body":"Jira的安装与部署 一、简介 镜像：https://hub.docker.com/r/atlassian/jira-software 第三方镜像：https://hub.docker.com/r/blacklabelops/jira 破解文件：atlassian-extras-3.2.jar 什么是JIRA? JIRA 是目前比较流行的基于Java架构的管理系统，由于Atlassian公司对很多开源项目实行免费提供缺陷跟踪服务，因此在开源领域，其认知度比其他的产品要高得多，而且易用性也好一些。同时，开源则是其另一特色，在用户购买其软件的同时，也就将源代码也购置进来，方便做二次开发。JIRA功能全面，界面友好，安装简单，配置灵活，权限管理以及可扩展性方面都十分出色。 JIRA的主要功能 问题追踪和管理：用它管理项目，跟踪任务、bug、需求，通过jira的邮件通知功能进行协作通知，在实际工作中使工作效率提高很多 问题跟进情况的分析报告：可以随时了解问题和项目的进展情况 项目类别管理功能：可以将相关的项目分组管理 组件/模块负责人功能：可以将项目的不同组件/模块指派相应的负责人，来处理所负责的组件的Issues 项目email地址功能：每个项目可以有不同的email（该项目的通知邮件从该地址发出） 无限制的工作流：可以创建多个工作流为不同的项目使用 JIRA的主要特点 JIRA的优点 用它管理项目，跟踪任务、bug，通过JIRA的邮件通知功能进行协作通知，在实际工作中使工作效率提高很多，效果非常不错！安全性、可扩展性方面发挥到了极致！ JIRA不仅仅是一个缺陷跟踪系统，通过Jira，可以整合客户、开发人员、测试人员，各人各司其职，信息很快得到交流和反馈，让大家感到软件开发在顺利快速的进行，朝意想的目标迈进。eclipse和IDEA下的Jira插件，主要为开发人员服务，实时将信息反馈给开发人员，开发人员同时迅速地将修复的结果信息反馈到跟踪系统中，最后通过持续集成，软件迅速地完成了更新，这些方便便捷的操作会极大地鼓舞软件开发中的各方人员，甚至包括客户，及时响应，相信是每一个客户都会欣赏的。 跟同类软件产品TestTracker、ClearQuest、TestDirector相比，JIRA的性价比最好！ JIRA的缺点 对于测试需求、测试用例等都没有提供直接的方式进行管理。不过可以利用JIRA的Issue Type的可定制性,来进行需求和测试用例方面的管理,也可以与Testlink集成。 相关版本 JIRA 4.0版本之后，不再按照功能区分版本。取消了以前的标准版，专业版，企业版之分；取而代之的是按照用户数量来划分：25、50、100、无限制用户。 所有的版本都具有之前企业版的功能！JIRA不限制创建项目数和Issue的数量，购买之后可以永久使用；并且一年内免费更新版本。 二、安装 0. 拉去镜像 docker pull docker.io/atlassian/jira-software:8.2.0 1. 部署PostgreSQL 省略 2. 创建破解文件的ConfigMap oc create configmap crack-jar --from-file=atlassian-extras-3.2.jar --from-literal=text=./atlassian-extras-3.2.jar 3. 创建其他资源 创建PVC apiVersion: v1 kind: PersistentVolumeClaim metadata: name: jira-data namespace: jira spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi 创建ServiceAccount oc create serviceaccount jira 创建RBAC相关资源 4. OKD部署Deployment声明文件 apiVersion: apps.openshift.io/v1 kind: DeploymentConfig metadata: labels: app: jira name: jira namespace: jira spec: replicas: 1 selector: app: jira deploymentconfig: jira strategy: type: Recreate template: metadata: labels: app: jira deploymentconfig: jira spec: containers: - env: - name: JVM_MINIMUM_MEMORY value: 800m - name: JVM_MAXIMUM_MEMORY value: 1024m - name: TZ value: Asia/Shanghai image: docker.io/atlassian/jira-software:8.2.0 imagePullPolicy: IfNotPresent name: jira ports: - containerPort: 8080 protocol: TCP resources: limits: cpu: '1' memory: 1500Mi requests: cpu: 500m memory: 500Mi readinessProbe: failureThreshold: 3 initialDelaySeconds: 40 periodSeconds: 30 successThreshold: 1 tcpSocket: port: 8080 timeoutSeconds: 2 livenessProbe: failureThreshold: 3 initialDelaySeconds: 40 periodSeconds: 30 successThreshold: 1 tcpSocket: port: 8080 timeoutSeconds: 1 volumeMounts: - mountPath: /var/atlassian/application-data/jira name: jira-data - mountPath: /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/atlassian-extras-3.2.jar name: crack-jar readOnly: true subPath: atlassian-extras-3.2.jar dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: jira serviceAccountName: jira terminationGracePeriodSeconds: 30 volumes: - name: jira-data persistentVolumeClaim: claimName: jira-data - configMap: defaultMode: 420 name: crack-jar name: crack-jar 5. 创建Service并创建HTTP访问Route oc expose dc jira --port=8080 oc expose service jira --name=jira --port=8080 --hostname=jira.apps.okd311.curiouser.com 6. 页面配置 配置页面语言-->选择手动配置 配置数据库 设置应用程序的属性 申请试用License 7. 配置管理员用户 8. 配置SMTP邮箱通知 9. 查看许可证 三、配置LDAP 见链接: Jira接LDAP 四、项目示例 项目类型 创建示例Scrum敏捷项目 项目的发布 五、问题 1. Unable to create and acquire lock file for jira.home directory '/var/atlassian/application-data/jira 解决：删除jira_home目录下的lock文件（.jira-home.lock），是一个隐藏文件，然后重启jira服务即可。 2. Unable to clean the cache directory: /var/atlassian/application-data/jira/plugins/.osgi-plugins/feli 解决：先停止jira服务，然后删除$JIRA_HOME/plugins/.osgi-plugins/felix/，然后启动jira服务即可 3. There is/are [1] thread(s) in total that are monitored by this Valve and may be stuck. 解决方案：等等就好了 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-02-19 17:45:44 "},"origin/redmine-backuprestore.html":{"url":"origin/redmine-backuprestore.html","title":"redmine的备份与恢复","keywords":"","body":"Redmine的备份与恢复 一、简介 要备份的数据类型 数据库中的数据 用户上传的附件 二、备份 Redmine backups should include: Database Attachments (stored in the files directory under the installation directory by default) 备份数据库数据 MySQL The mysqldump command can be used to backup the contents of your MySQL database to a text file. For example: /usr/bin/mysqldump -u -p -h > /path/to/backup/db/redmine.sql You can find ,, , and in the file config/database.yml. `` may not be required depending on your installation of the database. PostgreSQL The pg_dump command can be used to backup the contents of a PostgreSQL database to a text file. Here is an example: /usr/bin/pg_dump -U -h -Fc --file=redmine.sqlc You can find ,, and in the file `config/database.yml`. may not be required depending on your installation of the database. The pg_dump command will prompt you to enter the password when necessary. SQLite SQLite databases are all contained in a single file, so you can back them up by copying the file to another location. You can determine the file name of SQLite database by looking at config/database.yml. 备份用户上传的附件 All file uploads are stored in attachments_storage_path (defaults to the files/ directory). You can copy the contents of this directory to another location to easily back it up. WARNING: attachments_storage_path may point to a different directory other than files/. Be sure to check the setting in config/configuration.yml to avoid making a useless backup. 备份脚本 Here is a simple shell script that can be used for daily backups (assuming you're using a MySQL database): # Database /usr/bin/mysqldump -u -p | gzip > /path/to/backup/db/redmine_`date +%Y-%m-%d`.gz # Attachments rsync -a /path/to/redmine/files /path/to/backup/files 三、恢复 恢复数据库 MySQL For example if you have a gziped dump file with the name 2018-07-30.gz, then the database can be restored with the following command: gunzip -c 2018-07-30.gz | mysql -u --password Enter password: PostgreSQL When the option -Fc of the command pg_dump is used like it is at the above example then you need to use the command pg_restore: pg_restore -U -h -d redmine.sqlc otherwise a text file can be restored with psql: psql SQLite Copy the database file from the backup location. 参考 https://www.redmine.org/boards/2/topics/2442?r=18660 https://www.redmine.org/projects/redmine/wiki/RedmineBackupRestore Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/harbor.html":{"url":"origin/harbor.html","title":"Harbor","keywords":"","body":"Harbor：镜像管理工具 一、简介 二、安装部署 三、权限配置 开发者 个人开发者 能拉取镜像 能推送镜像 机器人 ci-robot： CI构建中 能拉取镜像 能推送镜像 cd-robot：k8s节点启动容器 能拉取镜像 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-07-01 15:14:40 "},"origin/vault.html":{"url":"origin/vault.html","title":"Vault","keywords":"","body":"Vault 一、简介 文档 Docker镜像：https://hub.docker.com/search?q=vault&type=image 中文文档：https://lonegunmanb.github.io/essential-vault/ 服务架构 后端存储（Storage Backend）：后端存储负责将密文数据存储到可靠的持久存储上。Vault 并不假设后端存储上的数据不会被盗取，而只是假设该存储是持久、可靠的。Vault 只会向后端存储写入加密过的数据。后端存储应在 Vault 服务启动前被妥善配置。 屏障(Barrier)：屏障是由加密算法所组成的钢筋混凝土防护工事。所有在 Vault 服务与后端存储之间流动的数据都会经过屏障处理。屏障确保了 Vault 写入后端存储的一切数据都是加密的，而从后端存储读取的数据都会经由屏障解密交由 Vault 服务使用。由于屏障的存在，Vault 服务启动后，必须进行“解封”（Unseal）处理，获得与后端存储数据相应的主密钥（Master Key）后才能正常工作。 机密引擎（Secret Engine）：机密引擎负责管理各种机密数据。比如“KV“引擎，就是一种简单的保存静态机密数据的机密引擎。某些机密引擎可提供被查询时动态创建机密的动态机密功能，这使得 Vault 可以提供细颗粒度权限配置的一次一密的临时机密。例如为运维与开发配置不同的策略，并对应不同的 AWS 权限，相关人员每次读取相关机密时，由 Vault 动态创建一组拥有有限有效期和预设 AWS 权限的 Access Key 和 Secret Key，并确保在有效期过后由 Vault 在 AWS 上自动删除该Key。 审计设备（Audit Device）：审计设备负责管理审计日志。进出 Vault 的每一个请求和响应都会被记录在预设的审计设备上。该部件为 Vault 与多种不同的审计日志存储的集成提供了一种简单的方式。 身份验证方法（Auth Method）：身份验证方法被用来认证连接到 Vault 服务的用户或是应用程序的身份信息。一旦验证通过，身份验证组件会返回一组当前身份适用的策略信息。Vault 接受一个通过认证的用户，并返回一个可供将来使用的客户端令牌。举个例子，使用 userpass 认证方式，用户通过提供用户名与密码来进行认证。如果使用 github 认证方式，用户通过 Github 令牌来通过 Vault 的认证。 客户端令牌（Client Token）：一个客户端令牌（又称“Vault Token“）类似于网站的会话 Cookie。一旦用户通过认证，Vault 返回一个客户端令牌。该令牌可以被 Vault 用来识别客户端身份并使用相应的访问控制权限约束客户端权限。该令牌通过 HTTP Header 传递。 机密（Secret）：机密指的是所有由 Vault 返回的包含机密信息或者密码学原材料（Cryptographic Material）的信息。并不是所有由 Vault 返回的信息都是机密，例如系统配置、服务状态信息、策略配置等就不属于机密范畴。机密都有对应的租约（Lease），这代表客户端不可以假设机密可以无限期地被使用。Vault 会在租约到期后吊销相关机密，Vault 管理员也可以在租约到期之前人工吊销机密。Vault 服务与客户端之间的这种契约是至关重要的，它允许实现全自动的机密凭据和策略的改变。 服务器（Server）：Vault 依赖于一个长期运行服务实例的服务器。Vault 服务器对外提供了一组 API 用以与客户端互动，以及管理与各种机密引擎的互动，访问控制权限体系的执行，以及机密租约管理和吊销。服务器的架构设计解除了客户端与机密信息、权限策略的耦合，使得管理员可以轻松实现对审计日志的中央化管控。 Vault 不信任后端存储，所以只会向后端存储写入加密后的密文数据。当 Vault 服务启动时，必须配置一个后端存储，使得 Vault 重启后仍然可以读取到数据。HTTP API 同样也必须在 Vault 服务启动时被启动，才能使得客户端可以与 Vault 服务交互。 当 Vault 启动后，Vault 处于“封印（Sealed）”状态。在执行任意操作之前，首先要对 Vault 执行“解封（Unseal）”操作。当 Vault 服务初始化时会生成一个加密密钥，所有写入后端存储的数据都会用该密钥加密。该密钥由一个主密钥（Master Key）保护。默认情况下，Vault 使用 Shamir 算法将主密钥拆分成5份，需要至少 3 份才能重建主密钥 用户名生成模板语法 二、安装部署 1、k8s Helm helm repo add hashicorp https://helm.releases.hashicorp.com helm repo update StandAlone模式 部署过程中，vault处于seal模式，此时pod健康状态是不正常的，需要进入到容器中执行vault operator init生成key和登录Token后，登录UI界面输入key(或执行vault operator unseal key1,vault operator unseal key2,vault operator unseal key3)可将vault服务进行unseal,使其pod健康状态恢复正常 helm upgrade --install vault --namespace tools hashicorp/vault \\ --set \"injector.enabled=false\" \\ --set \"ui.enabled=true\" \\ --set \"server.ingress.enabled=true\" \\ --set \"server.ingress.hosts[0].host=vault.curiouser.com\" dev模式 dev模式部署的话。vault服务是不用进行unseal处理的，可以直接使用 helm upgrade --install vault --namespace tools hashicorp/vault \\ --set \"injector.enabled=false\" \\ --set \"ui.enabled=true\" \\ --set \"server.dev.enabled=true\" \\ --set \"server.ingress.enabled=true\" \\ --set \"server.ingress.hosts[0].host=vault.curiouser.com\" 2、包管理器 Ubuntu/Debian # 添加HashiCorp GPG key curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - # 添加HashiCorp官方仓库 apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\" # 更新apt并安装 apt-get update && sudo apt-get install vault CentOS/RHEL yum install -y yum-utils yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo yum -y install vault MacOS brew tap hashicorp/tap brew install hashicorp/tap/vault # 升级： brew upgrade hashicorp/tap/vault # 启动Dev模式 vault server -dev 三、MySQL账号管理 1、创建engine 2、创建数据库连接 3、创建角色 4、生成token Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-07-08 11:08:30 "},"origin/alfred.html":{"url":"origin/alfred.html","title":"Alfred Workflow","keywords":"","body":"Alfred 一、Workflow简介 二、Workflow配置 1、组件 触发组件Trigger 快捷键Hotkey：设置键盘组合键进行快速触发 关键词Keyword：设置Alfred关键词，在Alfred调用框中输入关键字进行快速触发 输入组件Input 脚本Script filter List filter 2、变量引用 引用上一个输出： {query} 引用Workflow设置的变量：{var:变量名} 脚本中引用： og.getenv(\"变量名\") echo '{ \"items\": [ { \"title\": \"标题\", \"subtitle\": \"子标题\", \"valid\": true, \"arg\": \"传递给下个工作流的参数\", \"uid\": 10, \"quicklookurl\": \"ccc\" } ] }' 三、Workflow开发 pip3 install --target=. Alfred-Workflow # 或者 pip3 download --no-deps --no-binary :all: Alfred-Workflow Your Workflow/ info.plist icon.png workflow/ __init__.py background.py notify.py Notify.tgz update.py version web.py workflow.py yourscript.py .... import sys from workflow import Workflow, ICON_WEB, web API_KEY = 'your-pinboard-api-key' def main(wf): url = 'https://api.pinboard.in/v1/posts/recent' params = dict(auth_token=API_KEY, count=20, format='json') r = web.get(url, params) r.raise_for_status() for post in r.json()['posts']: wf.add_item(post['description'], post['href'], arg=post['href'], uid=post['hash'], valid=True, icon=ICON_WEB) wf.send_feedback() if __name__ == u\"__main__\": wf = Workflow() sys.exit(wf.run(main)) 9、实现CheckBox复选框功能 Script Filter组件通过添加使用Call External组件进行循环调用，再加上condition条件组件可实现CheckBox复选框功能 参考：https://www.alfredforum.com/topic/17529-checkbox-logic-workflow/#comment-90456 其他信息 缓存目录：~/Library/Caches/com.runningwithcrayons.Alfred/Workflow Data/ 参考 Workflow官网文档 Workflow API文档 https://pypi.org/project/Alfred-Workflow/ http://www.saitjr.com/others/alfred-script-filter-json-format.html https://www.alfredapp.com/help/workflows/inputs/script-filter/json/ https://www.alfredapp.com/help/workflows/inputs/script-filter/ https://www.alfredforum.com/topic/17529-checkbox-logic-workflow/#comment-90456 http://www.deanishe.net/alfred-workflow/api/index.html#workflow-variables http://www.deanishe.net/alfred-workflow/guide/variables.html#variables-run-script Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:34:31 "},"origin/vscode-on-k8s.html":{"url":"origin/vscode-on-k8s.html","title":"Web IDE: VSCode","keywords":"","body":"Web IDE - VSCode - Code-Server 一、简介 文档：https://coder.com/docs/code-server/latest Github：https://github.com/coder/code-server Dockerhub：https://hub.docker.com/r/codercom/code-server 二、部署 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: developer-A namespace: ide spec: accessModes: - ReadWriteMany resources: requests: storage: 20Gi storageClassName: nfs-client --- apiVersion: v1 kind: Service metadata: name: developer-A-svc namespace: ide spec: ports: - port: 80 targetPort: 8080 selector: code-server: developer-A --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: developer-A-vscode-server namespace: ide spec: rules: - host: developer-A.curiouser.com http: paths: - backend: serviceName: developer-A-svc servicePort: 80 path: / --- apiVersion: apps/v1 kind: Deployment metadata: labels: code-server: developer-A name: developer-A namespace: ide spec: selector: matchLabels: code-server: developer-A template: metadata: labels: code-server: developer-A spec: volumes: - name: user-workspace-data persistentVolumeClaim: claimName: developer-A hostname: developer-A containers: - image: hub.curiouser.com/vscode-server/backend:4.16.1-ubuntu imagePullPolicy: IfNotPresent name: vscode ports: - containerPort: 8080 env: - name: HASHED_PASSWORD value: \"4972cdcd065d9df443a8422c5a899d49be5b7b1e123ca9ff0663dbc8f461bf674\" volumeMounts: - name: user-workspace-data mountPath: /home/coder resources: limits: cpu: \"1500m\" memory: 4000Mi requests: cpu: \"100m\" memory: 100Mi livenessProbe: failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 8080 timeoutSeconds: 2 readinessProbe: failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 60 successThreshold: 1 tcpSocket: port: 8080 timeoutSeconds: 2 三、定制镜像 固定开发环境 统一运行时软件版本、系统版本、系统工具 配置文件化、统一配置、后期自由扩展 Dockerfile FROM codercom/code-server:4.16.1-ubuntu ENV LANG=en_US.UTF-8 \\ LANGUAGE=en_US.UTF-8 \\ TZ=Asia/Shanghai RUN sudo sed -i -e 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' -e 's/security.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list && \\ sudo apt update && \\ sudo apt install -y pip zsh vim jq telnet && \\ sudo apt-get clean && \\ sudo rm -rf /tmp/* /var/lib/apt/lists/* /var/tmp/* ENTRYPOINT [ \"/usr/bin/entrypoint.sh\",\"--disable-telemetry\",\"--disable-getting-started-override\",\"--disable-file-downloads\",\"--bind-addr\",\"0.0.0.0:8080\",\".\"] Makefile IMAGE_BASE_PUSH = hub.curiouser.com/vscode-server IMAGE_NAME = backend IMAGE_VERSION = 4.16.1-ubuntu all: build push build: docker build --rm -f Dockerfile -t ${IMAGE_BASE_PUSH}/${IMAGE_NAME}:${IMAGE_VERSION} . push: docker push ${IMAGE_BASE_PUSH}/${IMAGE_NAME}:${IMAGE_VERSION} 四、网络访问 1、VSCode内应用的访问 https://coder.com/docs/code-server/latest/guide#accessing-web-services Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-08-17 09:39:29 "},"origin/dolphinscheduler.html":{"url":"origin/dolphinscheduler.html","title":"DolphinScheduler","keywords":"","body":"DolphinScheduler 一、简介 文档：https://dolphinscheduler.apache.org/en-us/docs/3.1.8 Github：https://github.com/apache/dolphinscheduler 依赖 postgresql/mysql zookeeper 二、helm部署至k8s 1、安装Postgresql helm upgrade --install postgresql-for-dolphinscheduler \\ --namespace tools \\ --version 13.1.5 \\ bitnami/postgresql \\ --set global.postgresql.auth.postgresPassword=***postgres用户的密码*** \\ --set global.postgresql.auth.username=***另外创建的用户名*** \\ --set global.postgresql.auth.password=***另外创建的用户密码*** \\ --set global.postgresql.auth.database=***另外创建的Database*** 2、下载charts dolphinscheduler_version=3.1.8 && \\ curl -s https://dlcdn.apache.org/dolphinscheduler/$dolphinscheduler_version/apache-dolphinscheduler-$dolphinscheduler_version-src.tar.gz | tar -zx -C . apache-dolphinscheduler-$dolphinscheduler_version-src/deploy/kubernetes/dolphinscheduler && \\ cd apache-dolphinscheduler-$dolphinscheduler_version-src/deploy/kubernetes/dolphinscheduler helm repo add bitnami https://charts.bitnami.com/bitnami helm dependency update . 3、推送所需镜像到镜像仓库 for i in {master,tools,alert-server};do docker pull dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler-$i:3.1.8 && \\ docker tag dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler-$i:3.1.8 hub.test.com/dolphinscheduler/dolphinscheduler-$i:3.1.8 && \\ docker push hub.test.com/dolphinscheduler/dolphinscheduler-$i:3.1.8 done 4、定制镜像 定制镜像是为了支持 Python 3 、Pip3 需要在dolphinscheduler-worker服务容器中安装 Python 3 在数据源集成中支持连接MySQL、Oracle 需要在dolphinscheduler-worker, dolphinscheduler-api服务容器中的/opt/dolphinscheduler/libs/路径下放置对应的驱动 jar ①定制dolphinscheduler-api镜像 Dockerfile FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler-api:3.1.8 RUN wget -q https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.16/mysql-connector-java-8.0.16.jar -O /opt/dolphinscheduler/libs/mysql-connector-java-8.0.16.jar && \\ sed -i -e 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' -e 's/security.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list && \\ apt-get update && \\ rm -rf /var/lib/apt/lists/* docker build --rm -f Dockerfile -t hub.test.com/dolphinscheduler/dolphinscheduler-api:3.1.8 . docker push hub.test.com/dolphinscheduler/dolphinscheduler-api:3.1.8 ②定制dolphinscheduler-worker镜像 Dockerfile FROM dolphinscheduler.docker.scarf.sh/apache/dolphinscheduler-worker:3.1.8 RUN wget -q https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.16/mysql-connector-java-8.0.16.jar -O /opt/dolphinscheduler/libs/mysql-connector-java-8.0.16.jar && \\ sed -i -e 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' -e 's/security.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list && \\ apt-get update && \\ apt-get install -y --no-install-recommends python3 python3-pip && \\ pip3 install --no-cache-dir -i https://mirrors.aliyun.com/pypi/simple/ pandas PyMySQL SQLAlchemy xlwt xlsxwriter pypinyin eventlet && \\ rm -rf /var/lib/apt/lists/* docker build --rm -f Dockerfile -t hub.test.com/dolphinscheduler/dolphinscheduler-worker:3.1.8 . docker push hub.test.com/dolphinscheduler/dolphinscheduler-worker:3.1.8 5、部署 helm upgrade --install dolphinscheduler . -n tools \\ --set image.registry=\"harbor.test.com/dolphinscheduler\" \\ --set image.pullSecret=pull-harbor \\ --set postgresql.enabled=false \\ --set common.sharedStoragePersistence.enabled=true \\ --set common.sharedStoragePersistence.storageClassName=local-nfs-client \\ --set common.fsFileResourcePersistence.enabled=true \\ --set common.fsFileResourcePersistence.storageClassName=local-nfs-client \\ --set common.configmap.FS_DEFAULT_FS=\"file:///\" \\ --set externalDatabase.host=postgresql-for-dolphinscheduler.tools.svc \\ --set externalDatabase.username=***pg的dolphinscheduler使用的用户名*** \\ --set externalDatabase.password=***pg的dolphinscheduler使用的用户密码*** \\ --set master.persistentVolumeClaim.enabled=true \\ --set master.persistentVolumeClaim.storageClassName=local-nfs-client \\ --set worker.persistentVolumeClaim.enabled=true \\ --set worker.persistentVolumeClaim.dataPersistentVolume.enabled=true \\ --set worker.persistentVolumeClaim.dataPersistentVolume.storageClassName=local-nfs-client \\ --set worker.persistentVolumeClaim.logsPersistentVolume.enabled=true \\ --set worker.persistentVolumeClaim.logsPersistentVolume.storageClassName=local-nfs-client \\ --set api.persistentVolumeClaim.enabled=true \\ --set api.persistentVolumeClaim.storageClassName=local-nfs-client \\ --set ingress.enabled=true \\ --set ingress.host=\"dolphinscheduler.test.com\" 为了解决部署后无法创建租户的问题，修改部署后的 Configmap: dolphinscheduler-configs。然后重启 dolphinscheduler-master、dolphinscheduler-api、dolphinscheduler-worker data: common_properties: |- ... resource.hdfs.fs.defaultFS=file:/// ... 问题具体原因参考： https://dolphinscheduler.apache.org/en-us/docs/3.1.8/guide/installation/kubernetes#:~:text=Support%20Matrix-,FAQ,-Appendix%2DConfiguration https://blog.csdn.net/orchidofocean/article/details/132272168 三、配置 1、配置K8S集群 ①创建 K8S系统账号 权限 不能创建 NS 可以创建 job、pod kubectl create role dolphinscheduler-excutor --verb=create,get,list,watch ---resource-name=jobs --resource=batch kubectl create rolebinding dolphinscheduler-excutor-binding --clusterrole=dolphinscheduler-excutor --user=dolphinscheduler-excutor --namespace=dolphinscheduler-workspace cat serviceaccount/dolphinscheduler-excutor clusterrole.rbac.authorization.k8s.io/dolphinscheduler-excutor-role rolebinding.rbac.authorization.k8s.io/dolphinscheduler-excutor clusterrolebinding.rbac.authorization.k8s.io/dolphinscheduler-excutor Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:28:08 "},"origin/logging-日志系统数据在个组件中的流转格式.html":{"url":"origin/logging-日志系统数据在个组件中的流转格式.html","title":"日志系统数据在个组件中的流转格式","keywords":"","body":"1、原始日志 2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\"business\":\"curiouser\",\"currentTime\":\"2019-09-24 09:12:39.052\",\"data\":\"{\"args\": {\"AuthQueryDTO\": {\"clientId\":\"ppush-platform\",\"clientSecret\":\"Jygv8V4TerC5rDxO\"},},\"result\": {\"expireTime\":-1,\"token\":\"77ff1cd2d1985b6d2d99bd54453bbc5f\",\"type\":\"1\"}}\",\"datatype\":0,\"interface1\":\"com.curiouser.auth.center.controller.ClientApiController\",\"level\":\"INFO\",\"method\":\"serverAuth\",\"module\":\"curiouser-auth-center\",\"reqTime\":8,\"requestId\":\"req-bf1bcc406dfa4d35b9062e06fbad78cd\",\"thread\":\"XNIO-1 task-14\",\"urlPath\":\"/client/server/token\"} This is a test log ! hahaha {\"datatype\":0,\"business\":\"alert\",\"module\":\"alert-rule\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"level\":\"WARN \",\"method\":\"isConnectionAlive\",\"thread\":\"XNIO-1 task-20\",\"requestId\":\"req-498fe711243b444e9b73ed6d5dc20a20\",\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\"} 2、经过Filebeat处理 {\"@timestamp\":\"2019-09-24T11:02:47.692Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.2.0\"},\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"log\":{\"offset\":0,\"file\":{\"path\":\"/root/logs/test.log\"}},\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouser\\\",\\\"currentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.center.controller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"-auth-center\\\",\\\"reqTime\\\":8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"fields\":{\"ENV\":\"dev\",\"CANARY\":\"sit0\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"PROJECTNAME\":\"test\",\"CLUSTER\":\"cluster_dev\"}} {\"@timestamp\":\"2019-09-24T11:02:47.692Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.2.0\"},\"log\":{\"offset\":645,\"file\":{\"path\":\"/root/logs/test.log\"}},\"message\":\"This is a test log ! hahaha\",\"fields\":{\"CLUSTER\":\"cluster_dev\",\"ENV\":\"dev\",\"CANARY\":\"sit0\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"PROJECTNAME\":\"test\"},\"host\":{\"name\":\"allinone.tools.curiouser.com\"}} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"req2fe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 3、经过logstash处理 （内容没变，字段顺序变了，消息顺序变了） {\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\",\"ENV\":\"dev\",\"CANARY\":\"sit0\"},\"message\":\"This is a test log ! hahaha\",\"@version\":\"1\",\"@timestamp\":\"2019-09-24T11:24:50.336Z\",\"log\":{\"offset\":645,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"host\":{\"name\":\"allinone.tools.curiouser.com\"}} {\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\",\"ENV\":\"dev\",\"CANARY\":\"sit0\"},\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com..framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouserentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.cenoller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-center\\\",\\\"r8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"@version\":\"1\",\"@timestamp\":\"2019-09-24T11:24:50.332Z\",\"log\":{\"offset\":0,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"host\":{\"name\":\"allinone.tools.curiouser.com\"}} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"reqfe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 4、经过logstash-producer-->Kafka处理 (内容没变，字段顺序变了，消息顺序变了) {\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com..framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouserentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.cencuriouseroller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-cencuriousereqTime\\\":8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"@version\":\"1\",\"fields\":{\"CANARY\":\"sit0\",\"ENV\":\"dev\",\"TEMPLATE\":2019082110,\"PROJECTNAME\":\"test\",\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\"},\"log\":{\"file\":{\"path\":\"/root/logs/test.log\"},\"offset\":0},\"tags\":[\"beats_input_codec_plain_applied\"]} {\"message\":\"This is a test log ! hahaha\",\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"@version\":\"1\",\"fields\":{\"CANARY\":\"sit0\",\"ENV\":\"dev\",\"TEMPLATE\":2019082110,\"PROJECTNAME\":\"test\",\"NAMESPACE\":\"test\",\"CLUSTER\":\"cluster_dev\"},\"log\":{\"file\":{\"path\":\"/root/logs/test.log\"},\"offset\":645},\"tags\":[\"beats_input_codec_plain_applied\"]} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"reqfe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 5、经过Kafka-->logstash-consumer处理 (内容没变，字段顺序变了，消息顺序变了) {\"@version\":\"1\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"log\":{\"offset\":0,\"file\":{\"path\":\"/root/logs/test.log\"}},\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"CLUSTER\":\"cluster_dev\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"ENV\":\"dev\"},\"message\":\"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouser\\\",\\\"currentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.center.controller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-center\\\",\\\"reqTime\\\":8,\\\"requestId\\\":\\\"req-bf1bcc406dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\",\"tags\":[\"beats_input_codec_plain_applied\"]} {\"@version\":\"1\",\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"log\":{\"offset\":645,\"file\":{\"path\":\"/root/logs/test.log\"}},\"@timestamp\":\"2019-09-24T11:34:29.741Z\",\"fields\":{\"PROJECTNAME\":\"test\",\"TEMPLATE\":2019082110,\"CLUSTER\":\"cluster_dev\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"ENV\":\"dev\"},\"message\":\"This is a test log ! hahaha\",\"tags\":[\"beats_input_codec_plain_applied\"]} {\"host\":{\"name\":\"allinone.tools.curiouser.com\"},\"message\":\"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\",\"@timestamp\":\"2019-09-24T12:58:49.062Z\",\"log\":{\"offset\":675,\"file\":{\"path\":\"/root/logs/test.log\"}},\"tags\":[\"beats_input_codec_plain_applied\"],\"@version\":\"1\",\"fields\":{\"PROJECTNAME\":\"test\",\"CANARY\":\"sit0\",\"NAMESPACE\":\"test\",\"TEMPLATE\":2019082110,\"ENV\":\"dev\",\"CLUSTER\":\"cluster_dev\"},\"data\":{\"interface\":\"com.zaxxer.hikari.pool.PoolBase\",\"method\":\"isConnectionAlive\",\"currentTime\":\"2019-09-24 20:50:00,056\",\"thread\":\"XNIO-1 task-20\",\"module\":\"alert-rule\",\"level\":\"WARN \",\"business\":\"alert\",\"requestId\":\"reqfe711243b444e9b73ed6d5dc20a20\",\"data\":\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\",\"datatype\":0}} 经过\"原始日志+Filebeat--->logstash-producer--->Kafka--->logstash-consumer--->elasticsearch\"流程处理过的日志（将logstash-consumer发送的日志数据放在”_source“字段下，同时） # =========================================================================== { \"_index\": \"test-test-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"s_ZaY20BY8hT6jLicmK4\", \"_version\": 1, \"_score\": null, \"_source\": { \"message\": \"{\\\"datatype\\\":0,\\\"business\\\":\\\"alert\\\",\\\"module\\\":\\\"alert-rule\\\",\\\"currentTime\\\":\\\"2019-09-24 20:50:00,056\\\",\\\"level\\\":\\\"WARN \\\",\\\"method\\\":\\\"isConnectionAlive\\\",\\\"thread\\\":\\\"XNIO-1 task-20\\\",\\\"requestId\\\":\\\"req-498fe711243b444e9b73ed6d5dc20a20\\\",\\\"interface\\\":\\\"com.zaxxer.hikari.pool.PoolBase\\\",\\\"data\\\":\\\"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\\\"}\", \"log\": { \"file\": { \"path\": \"/root/logs/test.log\" }, \"offset\": 675 }, \"@version\": \"1\", \"data\": { \"business\": \"alert\", \"interface\": \"com.zaxxer.hikari.pool.PoolBase\", \"method\": \"isConnectionAlive\", \"requestId\": \"reqfe711243b444e9b73ed6d5dc20a20\", \"data\": \"HikariPool-1 - Failed to validate connection com.mysql.cj.jdbc.ConnectionImpl@6683d7 (No operations allowed after connection closed.). Possibly consider using a shorter maxLifetime value.\", \"datatype\": 0, \"thread\": \"XNIO-1 task-20\", \"level\": \"WARN \", \"module\": \"alert-rule\", \"currentTime\": \"2019-09-24 20:50:00,056\" }, \"tags\": [ \"beats_input_codec_plain_applied\" ], \"@timestamp\": \"2019-09-24T12:58:49.062Z\", \"host\": { \"name\": \"allinone.tools.curiouser.com\" }, \"fields\": { \"PROJECTNAME\": \"test\", \"CANARY\": \"sit0\", \"NAMESPACE\": \"test\", \"TEMPLATE\": 2019082110, \"ENV\": \"dev\", \"CLUSTER\": \"cluster_dev\" } }, \"fields\": { \"@timestamp\": [ \"2019-09-24T12:58:49.062Z\" ] }, \"sort\": [ 1569329929062 ] } # =========================================================================== { \"_index\": \"test-test-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"1fUgY20BY8hT6jLiJAfF\", \"_version\": 1, \"_score\": null, \"_source\": { \"message\": \"This is a test log ! hahaha\", \"tags\": [ \"beats_input_codec_plain_applied\" ], \"host\": { \"name\": \"allinone.tools.curiouser.com\" }, \"@timestamp\": \"2019-09-24T11:34:29.741Z\", \"fields\": { \"CANARY\": \"sit0\", \"NAMESPACE\": \"test\", \"CLUSTER\": \"cluster_dev\", \"TEMPLATE\": 2019082110, \"ENV\": \"dev\", \"PROJECTNAME\": \"test\" }, \"@version\": \"1\", \"log\": { \"file\": { \"path\": \"/root/logs/test.log\" }, \"offset\": 645 } }, \"fields\": { \"@timestamp\": [ \"2019-09-24T11:34:29.741Z\" ] }, \"sort\": [ 1569324869741 ] } # =========================================================================== { \"_index\": \"test-test-2019.09.24\", \"_type\": \"_doc\", \"_id\": \"DOlaY20B_ehr23pid9GM\", \"_version\": 1, \"_score\": null, \"_source\": { \"message\": \"2019-09-24 18:11:55,439 INFO : [XNIO-1 task-14] : com.curiouser.framework.common.aspect.ControllerAspect#around {\\\"business\\\":\\\"curiouser\\\",\\\"currentTime\\\":\\\"2019-09-24 09:12:39.052\\\",\\\"data\\\":\\\"{\\\"args\\\": {\\\"AuthQueryDTO\\\": {\\\"clientId\\\":\\\"ppush-platform\\\",\\\"clientSecret\\\":\\\"Jygv8V4TerC5rDxO\\\"},},\\\"result\\\": {\\\"expireTime\\\":-1,\\\"token\\\":\\\"77ff1cd2d1985b6d2d99bd54453bbc5f\\\",\\\"type\\\":\\\"1\\\"}}\\\",\\\"datatype\\\":0,\\\"interface1\\\":\\\"com.curiouser.auth.center.controller.ClientApiController\\\",\\\"level\\\":\\\"INFO\\\",\\\"method\\\":\\\"serverAuth\\\",\\\"module\\\":\\\"curiouser-auth-center\\\",\\\"reqTime\\\":8,\\\"requestId\\\":\\\"req-bf106dfa4d35b9062e06fbad78cd\\\",\\\"thread\\\":\\\"XNIO-1 task-14\\\",\\\"urlPath\\\":\\\"/client/server/token\\\"}\", \"log\": { \"file\": { \"path\": \"/root/logs/test.log\" }, \"offset\": 0 }, \"@version\": \"1\", \"tags\": [ \"beats_input_codec_plain_applied\" ], \"@timestamp\": \"2019-09-24T12:58:49.062Z\", \"host\": { \"name\": \"allinone.tools.curiouser.com\" }, \"fields\": { \"PROJECTNAME\": \"test\", \"CANARY\": \"sit0\", \"NAMESPACE\": \"test\", \"ENV\": \"dev\", \"TEMPLATE\": 2019082110, \"CLUSTER\": \"cluster_dev\" } }, \"fields\": { \"@timestamp\": [ \"2019-09-24T12:58:49.062Z\" ] }, \"sort\": [ 1569329929062 ] } 附录 1、filebeat配置 filebeat.inputs: - type: log enabled: true paths: - /root/logs/test.log exclude_files: [\"/root/logs/_filebeat\", \".gz$\"] recursive_glob.enabled: true setup.template.settings: index.number_of_shards: 3 processors: - decode_json_fields: fields: [\"message\"] process_array: false max_depth: 1 target: \"data\" overwrite_keys: false - drop_fields: fields: [\"agent\", \"tags\", \"input\", \"ecs\"] fields: NAMESPACE: \"test\" PROJECTNAME: \"test\" CLUSTER: cluster_dev ENV: dev CANARY: sit0 TEMPLATE: 2019082110 output.logstash: hosts: [\"localhost:5044\"] #output.file: # path: \"/root/logs/output\" # filename: filebeat.log 2、logstash_producer配置 input { beats { id => \"logstash_producer_input_beats\" port => 5044 } } output { #file{ # path => \"/root/logs/output/logstah-producer.log\" #} kafka { id => \"logstash_producer_output_kafka\" codec => json topic_id => \"logs\" bootstrap_servers => \"localhost:9092\" compression_type => \"snappy\" } } 3、logstash_consumer配置 input { kafka { id => \"logstash_consumer_input_kafka\" bootstrap_servers => \"localhost:9092\" topics => \"logs\" group_id => \"applications_logs_group\" codec => \"json\" auto_offset_reset => \"earliest\" } } output { #file{ # path => \"/root/logs/output/logstah-consumer.log\" #} elasticsearch { id => \"logstash_consumer_output_elasticsearch\" hosts => [\"localhost:9092\"] index=>\"%{[fields][NAMESPACE]}-%{[fields][PROJECTNAME]}-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true user => \"logstash-pipeline\" password => \"logstash-pipeline\" } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-07-27 18:15:11 "},"origin/elk-install.html":{"url":"origin/elk-install.html","title":"ELK系列安装部署","keywords":"","body":"部署ELK 一、Docker 部署 1、Elasticsearch 镜像信息 Docker Hub：https://hub.docker.com/_/elasticsearch 官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html 数据目录：/usr/share/elasticsearch/data docker run -d \\ --name elasticsearch \\ -p 9200:9200 \\ -e TZ=Asia/Shanghai \\ -e \"cluster.name=docker-desktop\" \\ -e \"bootstrap.memory_lock=true\" \\ -e \"discovery.type=single-node\" \\ -e ES_JAVA_OPTS=\"-Xms2g -Xmx2g\" \\ -e \"xpack.monitoring.collection.enabled=true\" \\ -e \"xpack.security.authc.api_key.enabled=true\" \\ -e \"xpack.security.enabled=true\" \\ -e ELASTIC_PASSWORD=Curiouser \\ elasticsearch:7.10.1 Docker Compose version: '2.2' services: es01: image: docker.elastic.co/elasticsearch/elasticsearch:7.5.2 container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - discovery.seed_hosts=es02,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data01:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - elastic es02: image: docker.elastic.co/elasticsearch/elasticsearch:7.5.2 container_name: es02 environment: - node.name=es02 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es03 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data02:/usr/share/elasticsearch/data networks: - elastic es03: image: docker.elastic.co/elasticsearch/elasticsearch:7.5.2 container_name: es03 environment: - node.name=es03 - cluster.name=es-docker-cluster - discovery.seed_hosts=es01,es02 - cluster.initial_master_nodes=es01,es02,es03 - bootstrap.memory_lock=true - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - data03:/usr/share/elasticsearch/data networks: - elastic volumes: data01: driver: local data02: driver: local data03: driver: local networks: elastic: driver: bridge Ansible二进制脚本部署GitHub地址：https://github.com/elastic/ansible-elasticsearch 2、Kibana docker run -d \\ --name kibana \\ --link elasticsearch:elasticsearch \\ -p 5601:5601 \\ -e TZ=Asia/Shanghai \\ -e ELASTICSEARCH_USERNAME=elastic \\ -e ELASTICSEARCH_PASSWORD=Curiouser \\ -e I18N_LOCALE=zh-CN \\ -e XPACK_SECURITY_ENABLED=TRUE \\ -e XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=ZjdlNDE1ZjJiM2M4ZGI0MjdkZDRlYzQ0 \\ -e XPACK_SECURITY_ENABLED=true \\ -e XPACK_SECURITY_AUTHC_API_KEY_ENABLED=true \\ kibana:7.10.1 3、Logstash docker run -d \\ --name logstash \\ --link elasticsearch:elasticsearch \\ -p 9600:9600 \\ -p 5044:5044 \\ -e TZ=Asia/Shanghai \\ -e XPACK_MONITORING_ELASTICSEARCH_USERNAME=elastic \\ -e XPACK_MONITORING_ELASTICSEARCH_PASSWORD=Curiouser \\ -e MONITORING_ENABLED=true \\ -v ~/logstash.conf:/usr/share/logstash/pipeline/logstash.conf \\ logstash:7.5.1 二、Helm 部署 1、部署单个组件 helm repo add elastic https://helm.elastic.co && \\ helm repo update && \\ helm upgrade --install \\ --version 7.17.3 \\ --namespace logging \\ elasticsearch-logging elastic/elasticsearch \\ --set cluster-name='elasticsearch-logging' \\ --set tests.enabled=false \\ --set replicas=1 \\ --set minimumMasterNodes=1 2、安装CRD部署各组件 官方文档：https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-overview.html ECK(Elastic Clound Kubernetes)支持的组件版本: Kubernetes 1.24-1.28 OpenShift 4.9-4.13 Google Kubernetes Engine (GKE), Azure Kubernetes Service (AKS), and Amazon Elastic Kubernetes Service (EKS) Helm: 3.2.0+ Elasticsearch, Kibana, APM Server: 6.8+, 7.1+, 8+ Enterprise Search: 7.7+, 8+ Beats: 7.0+, 8+ Elastic Agent: 7.10+ (standalone), 7.14+ (Fleet), 8+ Elastic Maps Server: 7.11+, 8+ Logstash: 8.7+ ①安装ECS Operator helm repo add elastic https://helm.elastic.co helm repo update helm upgrade --install --atomic \\ elastic-operator elastic/eck-operator \\ -n kube-system \\ --set managedNamespaces='{logging}' \\ --set telemetry.disabled=true 安装的 CRD： agents.agent.k8s.elastic.co apmservers.apm.k8s.elastic.co beats.beat.k8s.elastic.co elasticmapsservers.maps.k8s.elastic.co elasticsearchautoscalers.autoscaling.k8s.elastic.co elasticsearches.elasticsearch.k8s.elastic.co enterprisesearches.enterprisesearch.k8s.elastic.co kibanas.kibana.k8s.elastic.co logstashes.logstash.k8s.elastic.co stackconfigpolicies.stackconfigpolicy.k8s.elastic.co 创建的k8s资源 rbac资源 serviceaccount：elastic-operator secret：elastic-webhook-server-cert\" clusterrole：elastic-operator、elastic-operator-view、elastic-operator-edit clusterrolebinding：elastic-operator configmap：elastic-operator sts：elastic-operator service ：elastic-webhook-server admissionregistration：elastic-webhook.k8s.elastic.co ②使用CRD部署各组件 cat ③验证 查看es检查状态 kubectl exec -it logging-es-es-0 -c elasticsearch -- /bin/bash -c 'BASIC_AUTH_PSW=`cat $PROBE_PASSWORD_PATH` curl -XGET -s -k -u \"$PROBE_USERNAME:$BASIC_AUTH_PSW\" \"https://127.0.0.1:9200/\"' 访问kibana：http://kibana.test.com 获取kibana的elastic账号密码： es_instance=`kubectl get elasticsearch |grep -v NAME |awk '{print $1}'` kubectl get secrets ${es_instance}-es-elastic-user 使用 curl 查看 Kibana 登录页服务状态 ingress_controller_node=`kubectl -n kube-system get pod -l app.kubernetes.io/name=traefik -ojson | jq -r '.items[0] | .status.hostIP'` curl -s --noproxy \"kibana.test.com\" --resolve kibana.test.com:80:$ingress_controller_node http://kibana.test.com/login Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:51:34 "},"origin/kafka-origin.html":{"url":"origin/kafka-origin.html","title":"原理","keywords":"","body":"kafka原理 一、数据存储的目录结构 例如一个kafka集群有五个Broker节点，创建一个有2个分区1个副本的Topic主题， Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/logging-kafka基础知识.html":{"url":"origin/logging-kafka基础知识.html","title":"基础知识","keywords":"","body":"一、Kafka 整体架构分为producer、broker、consumer三部分，3.0版本之前依赖zookeeper做集群管理，3.0版本之后通过KRaft进行集群管理。 consumer有消费者组概念，同一个组内不同消费者负责消费不同的partation，一个分区只能由一个组内消费者消费；消费者组之间互不影响 集群中的broker会选举出一个leader作为Controller负责管理整个集群中所有分区和副本的状态 每个topic由多个partation组成，partation为真实存储数据的地方，每个partation以文件夹的形式存储在文件系统中。每个对应的partation数据目录下存储*.index，*log ，*timeindex三个文件 每个partation都有对应的副本，分散在不同的broker中来实现分布式存储。 整体使用主写主读架构，通过partation分布不同的broker上，尽量保证每个broker既有replicas分区拉数据也有leader分区生产数据，实现负载 kafka为了保证数据安全性，在producer写入数据时会通过副本机制对当前数据进行复制备份，其他分区副本通过拉取的方式进行数据同步，依赖多副本机制进行故障转移。 HW: 高水位，标识consumer可见的offset，取所有ISR中最小的那个，只有所有的副本都同步完成HW才会增加，消费者只能消费到HW之后的数据 LEO: 每个partation的log最后一条message位置 AR: 所有的分区副本集合 ISR: 同步的分区集合队列，属于AR的一个子集，ISR中如果同步慢了或挂起会被t出ISR队列。 OSR：从同步队列中被提出的分区集合、 当partation leader挂掉后由Controller在ISR集合中顺序查找出第一个选举新leader 对于一个 Topic 的并发量限制在于有多少个 Partition, 就能支撑多少的并发 二、Kafka 与 Zookeeper 1、Zookeeper在Kafka集群分布式消息中的作用 1.1、选举Controller Kafka是高可用的分布式消息系统，首先要解决的就是资源协调分配和多副本状态维护的问题。解决这些问题通常就是两种思路，一是依靠Zookeeper来协调，二是设定一个中心节点，让这个中心节点来协调。如果依靠Zookeeper来协调，会存在大量的竞争条件，对Zookeeper的访问压力增大，而且如果Zookeeper出现了问题（比如网络抖动），系统很容易出现紊乱。Kafka采用的是第二种思路，即选举一个中心节点来进行资源协调与多副本状态维护，这个中心节点被称作Controller（一个特殊的Broker），这个选举过程依靠Zookeeper来完成。 Broker启动时，会竞争创建临时\"/controller\"。如果创建成功，则成为Controller，并把Broker的id等信息写入这个节点。同时会全程监控\"/controller\"的数据变化，如果旧的Controller挂掉，则开启新一轮的竞争过程。 1.2、注册Broker Kafka要进行资源协调，第一件需要知道的事情就是各个Broker的存活状态，这个问题利用Zookeeper可以很容易做到。 假设某个Broker，id为0，它启动时，会创建\"/brokers/ids/0\"临时节点，并把端口等信息写进去。Controller会监控\"/brokers/ids\"的节点变化，以实时感知各broker的状态，进行资源协调。 1.3、协调topic的创建、调整与销毁 在Kafka这个多副本分区的消息系统里，创建一个topic，至少需要以下3个步骤： 持久化topic的多副本分区信息 为每个分区挑选一个副本leader 将上述信息发送给对应的Broker，以完成实际的日志文件创建过程 Controller的存在，可以很容易完成上面的b和c步骤，但a步骤不行，如果Controller挂掉，则这些信息会不可用。Kafka把这些信息保存在Zookeeper中，依靠其高可用特性来保证这些信息的高可用。假设某个topic名字为mytopic，创建时，其分区信息保存在\"/brokers/topics/mytopic\"中。Controller全程监控\"/brokers/topics\"的孩子节点变动，实时感知这些信息，以完成后续步骤。 创建完成之后，后续往往会有分区调整和topic删除等需求。普通青年可能会觉得这两个问题很简单，给Controller发个相关请求就可以了。事实远非如此！ 拿分区调整来说，假设某分区有三个副本，分别位于Broker-1、Broker-2和Broker-3，leader为1，现在扩容增加了Broker-4、Broker-5、Broker-6，为了平衡机器间压力，需要将副本1 2 3移到4 5 6，至少经历以下步骤： 修改该分区的副本信息为1 2 3 4 5 6，leader为1 等待4 5 6副本追赶1 2 3的进度直至大家都同步(in sync) 从4 5 6中挑选一个新的副本leader，假设为4 修改该分区的副本信息为4 5 6，leader为4 以上每个步骤都有可能失败，如何才能保证这次调整顺利进行呢？ 首先，我们不能直接修改该分区的副本信息为 4 5 6，原因很简单，需要等待4 5 6的追赶过程以便产生新leader。其次，操作未完全成功的命令需要保存下来，如果操作过程中，Controller挂掉，则新的Controller可以从头开始直至成功。Kafka怎么做的呢？ 通常是Admin控制台）把调整命令写入\"/admin/reassign_partitions\"节点 Controller监控\"/admin/reassign_partitions\"，拿到调整命令，执行上述步骤 如果操作成功则删除该节点；如果Controller挂掉，新的Controller还会拿到这个命令并从头开始执行 当然，这里一次只能有一个调整命令，但一个调整命令可以同时调整多个topic的多个分区。 在这个过程中，Zookeeper的作用是：持久化操作命令并实时通知操作者，是不是只有Zookeeper可以做这个事情呢，不是，但Zookeeper可以做得很好，保证命令高可用。 类似的操作还有topic删除，副本的leader变更等，都是沿用上面的套路。 1.4. 保存topic级别和client级别的配置信息 Broker的集群中有全局配置信息，但如果想针对某个topic或者某个client进行配置呢，Kafka把这些信息保存在Zookeeper中，各个Broker实时监控以更新。 1.5、脑裂问题 脑裂问题是指，在一个设有中心节点的系统中，出现了两个中心节点。两个中心同时传达命令，自然会造成系统的紊乱。 Kafka利用Zookeeper所做的第一件也是至关重要的一件事情是选举Controller，那么自然就有疑问，有没有可能产生两个Controller呢？ 首先，Zookeeper也是有leader的，它有没有可能产生两个leader呢？答案是不会。 quorum机制可以保证，不可能同时存在两个leader获得大多数支持。假设某个leader假死，其余的followers选举出了一个新的leader。这时，旧的leader复活并且仍然认为自己是leader，这个时候它向其他followers发出写请求也是会被拒绝的。因为每当新leader产生时，会生成一个epoch，这个epoch是递增的，followers如果确认了新的leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。那有没有follower不知道新的leader存在呢，有可能，但肯定不是大多数，否则新leader无法产生。Zookeeper的写也遵循quorum机制，因此，得不到大多数支持的写是无效的，旧leader即使各种认为自己是leader，依然没有什么作用。 Kafka的Controller也采用了epoch，具体机制如下: 所有Broker监控\"/controller\"，节点被删除则开启新一轮选举，节点变化则获取新的epoch Controller会注册SessionExpiredListener，一旦因为网络问题导致Session失效，则自动丧失Controller身份，重新参与选举 收到Controller的请求，如果其epoch小于现在已知的controller_epoch，则直接拒绝 理论上来说，如果Controller的SessionExpired处理成功，则可以避免双leader，但假设SessionExpire处理意外失效的情况：旧Controller假死，新的Controller创建。旧Controller复活，SessionExpired处理意外失效，仍然认为自己是leader。 这时虽然有两个leader，但没有关系，leader只会发信息给存活的broker（仍然与Zookeeper在Session内的），而这些存活的broker则肯定能感知到新leader的存在，旧leader的请求会被拒绝。 1.6、如果Zookeeper挂了会怎样 每个Broker有一个metaDataCache，缓存有topic和partition的基本信息，可以正常的生产和消费信息，但不能进行topic的创建、调整和删除等操作。 此外，Broker会不断重试连接。 1.7、Zookeeper用量估计 假设Broker数目为B，topic数目为T，所有topic总partition数目为P，Client数目为C，以下数值均为峰值： qps: 100以内 连接数: B watcher数目：3 * B + 2 * T + 6 Zookeeper节点数（叶子节点）: B + P + T + C + 8 2、kafka注册到zookeeper中的数据存储结构 Zookeeper路径的创建者与监听者 路径 创建者 监听者 类型 /controller 各个broker竞争创建 所有broker全程监控data change 临时节点 /controller_epoch controller 无 永久节点 /brokers/ids broker启动时检查并确保存在 controller全程监控child change 永久节点 /brokers/ids/{id} id对应的broker 无 临时节点 /brokers/topics broker启动时检查确保存在 controller全程监控child change 永久节点 /brokers/topics/{topic} controller收到创建请求，或者broker启用自动创建topic时，或admin工具 controller全程监控data change 永久节点 /brokers/topics/{topic}/{partition}/state partiton的leader partition reassign时，controller临时监控data change 永久节点 /config/changes broker启动时检查并确保存在 所有broker全程监控child change 永久节点 /config/topics broker启动时检查并确保存在 无 永久节点 /config/clients broker启动时检查并确保存在 无 永久节点 /brokers/seqid broker启动时检查并确保存在 待确认 永久节点 /admin/delete_topics broker启动时检查并确保存在 controller全程监控child change 永久节点 /isr_change_notification broker启动时检查并确保存在 controller全程监控child change 永久节点 /admin/reassign_partitions admin 工具 controller全程监控data change 永久节点，reassign结束后会删除 /admin/preferred_replica_election admin 工具 controller全程监控data change 永久节点，replica election结束后会删除 2.1、Topic注册信息 /brokers/topics/[topic] 存储某个topic的partitions所有分配信息 { \"version\": \"版本编号目前固定为数字1\", \"partitions\": { \"partitionId编号\": [ 同步副本组brokerId列表 ], \"partitionId编号\": [ 同步副本组brokerId列表 ], ....... } } 2.2、Partition状态信息 /brokers/topics/[topic]/partitions/[partition-Id]/state Schema: { \"controller_epoch\": 表示kafka集群中的中央控制器选举次数, \"leader\": 表示该partition选举leader的brokerId, \"version\": 版本编号默认为1, \"leader_epoch\": 该partition leader选举次数, \"isr\": [同步副本组brokerId列表] } 2.3、Broker注册信息 /brokers/ids/[0...N] 每个broker的配置文件中都需要指定一个数字类型的id(全局不可重复),此节点为临时znode(EPHEMERAL) Schema: { \"jmx_port\": jmx端口号, \"timestamp\": kafka broker初始启动时的时间戳, \"host\": 主机名或ip地址, \"version\": 版本编号默认为1, \"port\": kafka broker的服务端端口号,由server.properties中参数port确定 } 2.4、Controller epoch /controller_epoch -> int (epoch) 此值为一个数字,kafka集群中第一个broker第一次启动时为1，以后只要集群中center controller中央控制器所在broker变更或挂掉，就会重新选举新的center controller，每次center controller变更controller_epoch值就会 + 1 2.5、Controller注册信息 /controller -> int (broker id of the controller) 存储center controller中央控制器所在kafka broker的信息 { \"version\": 版本编号默认为1, \"brokerid\": kafka集群中broker唯一编号, \"timestamp\": kafka broker中央控制器变更时的时间戳 } 2.6、Consumer注册信息 /consumers/[groupId]/ids/[consumerIdString] 每个consumer都有一个唯一的ID(consumerId可以通过配置文件指定,也可以由系统生成),此id用来标记消费者信息.这是一个临时的znode,此节点的值为请看consumerIdString产生规则,即表示此consumer目前所消费的topic + partitions列表. Schema: { \"version\": 版本编号默认为1, \"subscription\": { //订阅topic列表 \"topic名称\": consumer中topic消费者线程数 }, \"pattern\": \"static\", \"timestamp\": \"consumer启动时的时间戳\" } 2.7、Consumer offset /consumers/[groupId]/offsets/[topic]/[partitionId] -> long (offset) 用来跟踪每个consumer目前所消费的partition中最大的offset.此znode为持久节点,可以看出offset跟group_id有关,以表明当消费者组(consumer group)中一个消费者失效,重新触发balance,其他consumer可以继续消费 2.8、admin管理信息 三、Kafka中的消费者与消费者组 从0.9版本开始，Consumer默认将offset保存在Kafka一个内置的名字叫_consumeroffsets的topic中。默认是无法读取的，可以通过设置consumer.properties中的exclude.internal.topics=false来读取。 1、消费者组里面的消费者消费Topic Partition的消息时流程 每个consumer客户端被创建时,会向zookeeper注册自己的信息.主要是为了\"负载均衡\" 同一个Consumer Group中的Consumers，Kafka将相应Topic中的每个消息只发送给其中一个Consumer。 Consumer Group中的每个Consumer读取Topic的一个或多个Partitions，并且是唯一的Consumer； 一个Consumer group的多个consumer的所有线程依次有序地消费一个topic的所有partitions,如果Consumer group中所有consumer总线程大于partitions数量，则会出现空闲情况 举例说明： kafka集群中创建一个topic为report-log，4个partitions 索引编号为0,1,2,3。假如有目前有三个消费者node（注意：一个consumer中一个消费线程可以消费一个或多个partition） 如果每个consumer创建一个consumer thread线程,各个node消费情况如下，node1消费索引编号为0,1分区，node2费索引编号为2,node3费索引编号为3 如果每个consumer创建2个consumer thread线程，各个node消费情况如下(是从consumer node先后启动状态来确定的)，node1消费索引编号为0,1分区；node2费索引编号为2,3；node3为空闲状态 总结：从以上可知，Consumer Group中各个consumer是根据先后启动的顺序有序消费一个topic的所有partitions的。如果Consumer Group中所有consumer的总线程数大于partitions数量，则可能consumer thread或consumer会出现空闲状态。 2、Consumer均衡算法 当一个group中,有consumer加入或者离开时,会触发partitions均衡(均衡的最终目的,是提升topic的并发消费能力) 假如topic1,具有如下partitions: P0,P1,P2,P3 加入group中,有如下consumer: C0,C1 首先根据partition索引号对partitions进行排序，假设排序: P0,P1,P2,P3 根据(consumer.id + '-'+ thread序号)对消费者进行排序,假设排序: C0,C1 计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整) 然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i M),P((i + 1) M -1)] 3、Consumer启动流程 首先进行\"Consumer Id注册\"; 然后在\"Consumer id 注册\"节点下注册一个watch用来监听当前group中其他consumer的\"退出\"和\"加入\";只要此znode path下节点列表变更,都会触发此group下consumer的负载均衡.(比如一个consumer失效,那么其他consumer接管partitions). 在\"Broker id 注册\"节点下,注册一个watch用来监听broker的存活情况;如果broker列表变更,将会触发所有的groups下的consumer重新balance. 四、kafka如何保证数据不丢失 Producer保证发送数据不丢，生产者发送消息有三种模式，发完即忘、同步和异步，可以通过设置同步或异步的方式获取响应结果，失败做重试来保证消息在发送阶段不丢(broker接受produer数据做了幂等性保证) Broker保证接收数据保证不丢失，当生产者向leader发数据时通过request.required.acks参数设置数据可靠性的级别。 1（默认）： producer在ISR中的leader已成功收到的数据并得到确认后发送下一条message。如果leader宕机了，则会丢失数据。 0：producer无需等待来自broker的确认而继续发送下一批消息。这种情况下数据传输效率最高，但是数据可靠性确是最低的。 -1或者all：producer需要等待ISR中的所有follower都确认接收到数据后才算一次发送完成，可靠性最高。通过设置ack=1，broker内部做副本同步保证broker内部数据不丢失。 Consumer保证消费数据不丢失，默认情况下，当消费者消费到消息后，会自动提交offse。但是如果消费者消费出错，没有进入真正的业务处理，那么就可能会导致这条消息消费失败，从而丢失。可以通过开启手动提交位移，等待业务正常处理完成后，再提交offset。 五、kafka的版本 Kafka版本规则 在Kafka 1.0.0之前基本遵循4位版本号，比如Kafka 0.8.2.0、Kafka 0.11.0.3等。而从1.0.0开始Kafka就告别了4位版本号，遵循 Major.Minor.Patch 的版本规则，其中Major表示大版本，通常是一些重大改变，因此彼此之间功能可能会不兼容；Minor表示小版本，通常是一些新功能的增加；最后Patch表示修订版，主要为修复一些重点Bug而发布的版本。比如Kafka 2.1.1，大版本就是2，小版本是1，Patch版本为1，是为修复Bug发布的第1个版本。 Kafka版本演进 Kafka总共发布了7个大版本，分别是0.7.x、0.8.x、0.9.x、0.10.x、0.11.x、1.x及2.x版本。截止目前，最新版本是Kafka 2.6.0.，也是最新稳定版本 kafka的offset保存位置分为两种情况 0.9.0.0版本之前默认保存在zookeeper当中 ，0.9.0.0版本之后保存在broker对应的topic当中 参考链接 http://blog.csdn.net/lizhitao/article/details/23744675 https://www.jianshu.com/p/5bef1f9f74cd https://juejin.cn/post/7142685653174583332 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-09-13 16:57:59 "},"origin/logging-kafka常用操作.html":{"url":"origin/logging-kafka常用操作.html","title":"kafka常用操作","keywords":"","body":"Apache Kafka常用操作 一、Topic管理 1. 列出所有Topic kafka-topics.sh --zookeeper 127.0.0.1:2181 --list kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --list 2. 创建一个topic kafka-topics.sh --create --zookeeper 127.0.0.1:2181 --replication-factor 2 --partitions 3 --topic Test #--replication-factor参数指定Topic的数据副本个数 #--partitions参数指定Topic的分区个数 3. 删除Topic kafka-topics.sh --delete --zookeeper 127.0.0.1:2181 --topic Test 或者 #只会删除zookeeper中的元数据，消息文件须手动删除 kafka-run-class.sh kafka.admin.DeleteTopicCommand --zookeeper 127.0.0.1:2181 --topic Test 4. 查看Topic的详细信息 kafka-topics.sh --describe --zookeeper 127.0.0.1:2181 --topic Test Topic:Test PartitionCount:2 ReplicationFactor:1 Configs: Topic: Test Partition: 0 Leader: 2 Replicas: 2 Isr: 2 Topic: Test Partition: 1 Leader: 3 Replicas: 3 Isr: 3 #第一行，列出了topic的名称，分区数(PartitionCount),副本数(ReplicationFactor)以及其他的配置(Config.s) #Leader:1 表示为做为读写的broker的节点编号 #Replicas:表示该topic的每个分区在那些borker中保存 #Isr:表示当前有效的broker, Isr是Replicas的子集 5. 增加Topic分区个数（只能增加扩容） kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic Test --partitions 2 6. 给Topic增加配置 kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic Test --config flush.messages=1 7. 删除Topic的配置 kafka-topics.sh --zookeeper 127.0.0.1:2181 --alter --topic Test --delete-config flush.messages=1 8. 查看消费者组 kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --list 9. 查看Topic各个分区的消息偏移量最大（小）值 kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list 127.0.0.1:9092 --time -1 --topic Test # time为-1时表示最大值，time为-2时表示最小值 10. 查看Topic中指定consumer组内消息消费的offset kafka的offset保存位置分为两种情况 0.9.0.0版本之前默认保存在zookeeper当中 ，0.9.0.0版本之后保存在broker对应的topic当中 kafka-consumer-offset-checker.sh --zookeeper 127.0.0.1:2181 --group logstash-group --topic Test GROUP TOPIC PID OFFSET LOGSIZE LAG Ower 消费者组 话题id 分区id 当前已消费的条数 总条数 未消费的条数 所有者 console-consumer-98995 Test 0 112 318084 317972 none console-consumer-98995 Test 1 -1 318088 unknown none 方式二： $ kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --describe --offsets --group Group-Name TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID basic_log_k8s 1 127509 333334 205825 logstash-0-490058d7-154f-4111-b514-57de254ecae8 /192.168.3.72 logstash-0 basic_log_k8s 0 127317 333333 206016 logstash-0-2bd85bcc-282e-41a0-a9c3-6d0dbefd547f /192.168.0.40 logstash-0 11. 修改指定消费者分组对应topic的offset 第一种情况offset信息保存在topic中 $ bin/kafka-consumer-groups.sh --bootstrap-server 127.0.0.1:9092 --group test-consumer-group --topic test --execute --reset-offsets --to-offset 10000 #参数解析： #--bootstrap-server 代表你的kafka集群 你的offset保存在topic中 #--group 代表你的消费者分组 #--topic 代表你消费的主题 #--execute 代表支持复位偏移 #--reset-offsets 代表要进行偏移操作 #--to-offset 代表你要偏移到哪个位置 是long类型数值，只能比前面查询出来的小 #还有其他的--to- ** 方式可以自己验证 本人验证过--to-datetime 没有成功 第二种方式offset信息保存在zookeeper当中 $ bin/kafka-consumer-groups.sh --zookeeper kafka_zk1:2181 --group test-consumer-group --topic test --execute --reset-offsets --to-offset 10000 12. 修改topic副本因子数 官方文档：https://kafka.apache.org/21/documentation.html#replication ① 先查看Topic的信息 $ kafka-topics.sh --zookeeper localhost:2181 --topic test --describe Topic:test PartitionCount:2 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 2 Replicas: 2 Isr: 2 Topic: test Partition: 1 Leader: 3 Replicas: 3 Isr: 3 ② 准备JSON文件 { \"version\": 1, \"partitions\": [ { \"topic\": \"test\", \"partition\": 0, \"replicas\": [2, 1, 3] }, { \"topic\": \"test\", \"partition\": 1, \"replicas\": [3, 2, 1] }] } ③ kafka-reassign-partitions命令增加topic分区副本数 $ kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file replication.json --execute Current partition replica assignment {\"version\":1,\"partitions\":[{\"topic\":\"test\",\"partition\":0,\"replicas\":[2]},{\"topic\":\"test\",\"partition\":1,\"replicas\":[3]}]} Save this to use as the --reassignment-json-file option during rollback Successfully started reassignment of partitions {\"version\":1,\"partitions\":[{\"topic\":\"test\",\"partition\":0,\"replicas\":[2,1,3]},{\"topic\":\"test\",\"partition\":1,\"replicas\":[3,2,1]}]} ④ 使用verify参数来检查副本数据是否复制分配完成 $ kafka-reassign-partitions.sh --zookeeper www.iteblog.com:2181 --reassignment-json-file replication.json --verify Status of partition reassignment: Reassignment of partition [test,0] is still in progress Reassignment of partition [test,1] is still in progress $ kafka-reassign-partitions.sh --zookeeper www.iteblog.com:2181 --reassignment-json-file replication.json --verify Status of partition reassignment: Reassignment of partition [test,0] completed successfully Reassignment of partition [test,1] completed successfully $ kafka-topics.sh --zookeeper localhost:2181 --topic test --describe Topic:iteblog PartitionCount:2 ReplicationFactor:3 Configs: Topic: test Partition: 0 Leader: 2 Replicas: 2,1,3 Isr: 2,1,3 Topic: test Partition: 1 Leader: 3 Replicas: 3,2,1 Isr: 3,2,1 13. 均衡Topic分区到新增Broker节点 重新分配官方文档地址：http://kafka.apache.org/documentation/#basic_ops_cluster_expansion 翻译官方文档中文地址：http://orchome.com/36 参考文章：https://blog.csdn.net/forrest_ou/article/details/79141391 ① 确定要重启分配分区的主题，新建topics-to-move.json json文件 { \"topics\": [ {\"topic\": \"foo1\"}, {\"topic\": \"foo2\"} ], \"version\":1 } // foo1 foo2 为要重新分配的主题 ② 使用 bin/kafka-reassign-partitions.sh重新分配工具生成分配规则的json语句分配到 5，6机器 kafka-reassign-partitions.sh --zookeeper localhost:2181 --topics-to-move-json-file topics-to-move.json --broker-list \"5,6\" –generate ③ 有分配规则的json语句输出到控制台，复制到新建的json文件expand-cluster-reassignment.json中，例如： {\"version\":1, \"partitions\":[{\"topic\":\"foo1\",\"partition\":0,\"replicas\":[5,6]}, {\"topic\":\"foo1\",\"partition\":1,\"replicas\":[5,6]}, {\"topic\":\"foo1\",\"partition\":2,\"replicas\":[5,6]}, {\"topic\":\"foo2\",\"partition\":0,\"replicas\":[5,6]}, {\"topic\":\"foo2\",\"partition\":1,\"replicas\":[5,6]}, {\"topic\":\"foo2\",\"partition\":2,\"replicas\":[5,6]}] } //描述分配之后分区的分布情况 ④ 执行命令，开始分区重新分配 kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json –execute ⑤ 验证是否完成 kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file expand-cluster-reassignment.json –verify //当输出全部都是completed successfully表明移动已经完成. 注意 kafka新建主题时的分区分配策略：随机选取第一个分区节点，然后往后依次增加。例如第一个分区选取为1，第二个分区就 是2，第三个分区就是3. 1，2，3是brokerid。不会负载均衡，所以要手动重新分配分区操作，尽量均衡。 在生产的同时进行数据迁移会出现重复数据。所以迁移的时候避免重复生产数据，应该停止迁移主题的生产。同时消费不会，同时消费之后出现短暂的leader报错，会自动恢复。 新增了broker节点，如果有主题的分区在新增加的节点上，生产和消费的客户端都应该在hosts配置文件中增加新增的broker节点，否则无法生产消费，但是也不报错。 可以不需要第一步和第二步，自己手动新建分配的json文件 14. 查询Topic不可用的分区 kafka-topics.sh --describe --unavailable-partitions --zookeeper localhost:2181 二、其他 1. 自带测试生产者 kafka-console-producer.sh --broker-list 127.0.0.1:9092 --topic Test 2. 自带测试消费者 kafka-console-consumer.sh --zookeeper 127.0.0.1:2181 --from-beginning --topic Test 3. 自带性能测试 位于bin/kafka-producer-perf-test.sh.主要参数有以下: messages 生产者发送总的消息数量 message-size 每条消息大小（单位为b） batch-size 每次批量发送消息的数量 topics 生产者发送的topic threads 生产者使用几个线程同时发送 例如 kafka-producer-perf-test.sh --messages 100000 --message-size 1000 --batch-size 10000 --topics test --threads 4 --broker-list 127.0.0.1:9092 start.time, end.time, compression, message.size, batch.size, total.data.sent.in.MB, MB.sec, total.data.sent.in.nMsg, nMsg.sec 2015-10-15 18:56:27:542, 2015-10-15 18:56:30:880, 0, 1000, 10000, 95.37, 28.5702, 100000, 29958.0587 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-19 18:39:55 "},"origin/zookeeper.html":{"url":"origin/zookeeper.html","title":"Zookeeper常用操作","keywords":"","body":"Zookeeper常用操作 一、Zookeeper日志 1、日志简介 zookeeper服务器会产生三类日志： 事务日志 事务日志指zookeeper在正常运行过程中，针对所有的更新操作，在返回客户端“更新成功”的响应前，zookeeper会保证已经将本次更新操作的事务日志已经写到磁盘上，只有这样，整个更新操作才会生效 事务日志文件的命名规则为log.*，文件大小为64MB，*表示写入该日志的第一个事务的ID，十六进制表示 事务日志文件为二进制格式，只能通过zookeeper自带的jar包读取。 将libs中的slf4j-api-1.6.1.jar文件和zookeeper根目录下的zookeeper-3.4.9.jar文件复制到临时文件夹tmplibs中，然后执行以下命令 java -classpath .:slf4j-api-1.6.1.jar:zookeeper-3.4.6.jar org.apache.zookeeper.server.LogFormatter /data/zookeeper/data/version-2/log.684e26700 |more 快照日志 zookeeper的数据在内存中是以树形结构进行存储的，而快照就是每隔一段时间就会把整个DataTree的数据序列化后存储在磁盘中，这就是快照日志文件。 快照文件的命名规则为snapshot.*，其中*表示zookeeper触发快照的那个瞬间，提交的最后一个事务的ID 事务日志文件为二进制格式，只能通过zookeeper自带的jar包读取。 将libs中的slf4j-api-1.6.1.jar文件和zookeeper根目录下的zookeeper-3.4.9.jar文件复制到临时文件夹tmplibs中，然后执行以下命令 java -classpath .:slf4j-api-1.6.1.jar:zookeeper-3.4.6.jar org.apache.zookeeper.server.SnapshotFormatter /data/zookeeper/data/version-2/snapshot.684e26700 |more log4j日志 当zookeeper集群进行频繁的数据读写操作时，会产生大量的事务日志信息，将两类日志分开存储会提高系统性能，建议将事务日志（dataLogDir）与快照日志（dataLog）单独配置（在没有dataLogDir配置项的时候，zookeeper默认将事务日志文件和快照日志文件都存储在dataDir对应的目录下） 2、清理方式 ①自动清理 在zookeeper 3.4.0以后，zookeeper提供了自动清理snapshot和事务日志功能，通过配置zoo.cfg下的autopurge.snapRetainCount和autopurge.purgeInterval这两个参数实现日志文件的定时清理。 autopurge.snapRetainCount这个参数指定了需要保留的文件数目，默认保留3个； autopurge.purgeInterval这个参数指定了清理频率，单位是小时，需要填写一个1或者更大的数据，默认0表示不开启自动清理功能。 ②手动清理 /opt/zookeeper/bin/zkCleanup.sh 数据目录 -n 保留日志个数 ③自定义清理脚本 #!/bin/bash #snapshot file dir dataDir=/data/zookeeper/data/version-2 #tran log dir dataLogDir=/data/zookeeper/data/version-2 logDir=/data/zookeeper/logs #Leave 60 files count=60 count=$[$count+1] ls -t $dataLogDir/log.* | tail -n +$count | xargs rm -f ls -t $dataDir/snapshot.* | tail -n +$count | xargs rm -f ls -t $logDir/zookeeper.log.* | tail -n +$count | xargs rm -f 参考： https://www.kancloud.cn/ningjing_home/ceph/710759 https://ningyu1.github.io/site/post/89-zookeeper-cleanlog/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/kafka-client-connect-procedure-tools.html":{"url":"origin/kafka-client-connect-procedure-tools.html","title":"kafka连接调试脚本","keywords":"","body":"kafka连接测试工具 一、简介 为了测试kafka客户端连接k8s上Kafka Bootstrap返回的信息，有一个Python脚本可显示Broker地址，并产生测试数据验证生产消费是否正常 GItHub：https://github.com/rmoff/kafka-listeners/blob/master/python/python_kafka_test_client.py 二、脚本使用 Python代码 from confluent_kafka.admin import AdminClient from confluent_kafka import Consumer from confluent_kafka import Producer from sys import argv from datetime import datetime topic='test_topic' def Produce(source_data): print('\\n') p = Producer({'bootstrap.servers': bootstrap_server}) def delivery_report(err, msg): \"\"\" Called once for each message produced to indicate delivery result. Triggered by poll() or flush(). \"\"\" if err is not None: print('❌ Message delivery failed: {}'.format(err)) else: print('✅ 📬 Message delivered: \"{}\" to {} [partition {}]'.format(msg.value().decode('utf-8'),msg.topic(), msg.partition())) for data in source_data: p.poll(0) p.produce(topic, data.encode('utf-8'), callback=delivery_report) r=p.flush(timeout=5) if r>0: print('❌ Message delivery failed ({} message(s) still remain, did we timeout sending perhaps?)\\n'.format(r)) def Consume(): print('\\n') c = Consumer({ 'bootstrap.servers': bootstrap_server, 'group.id': 'rmoff', 'auto.offset.reset': 'earliest' }) c.subscribe([topic]) try: msgs = c.consume(num_messages=1,timeout=30) if len(msgs)==0: print(\"❌ No message(s) consumed (maybe we timed out waiting?)\\n\") else: for msg in msgs: print('✅ 💌 Message received: \"{}\" from topic {}\\n'.format(msg.value().decode('utf-8'),msg.topic())) except Exception as e: print(\"❌ Consumer error: {}\\n\".format(e)) c.close() try: bs=argv[1] print('\\n🥾 bootstrap server: {}'.format(bs)) bootstrap_server=bs except: # no bs X-D bootstrap_server='localhost:9092' print('⚠️ No bootstrap server defined, defaulting to {}\\n'.format(bootstrap_server)) a = AdminClient({'bootstrap.servers': bootstrap_server}) try: md=a.list_topics(timeout=10) print(\"\"\" ✅ Connected to bootstrap server(%s) and it returned metadata for brokers as follows: %s --------------------- ℹ️ This step just confirms that the bootstrap connection was successful. ℹ️ For the consumer to work your client will also need to be able to resolve the broker(s) returned in the metadata above. ℹ️ If the host(s) shown are not accessible from where your client is running you need to change your advertised.listener configuration on the Kafka broker(s). \"\"\" % (bootstrap_server,md.brokers)) try: Produce(['foo / ' + datetime.now().strftime('%Y-%m-%d %H:%M:%S')]) Consume() except: print(\"❌ (uncaught exception in produce/consume)\") except Exception as e: print(\"\"\" ❌ Failed to connect to bootstrap server. 👉 %s ℹ️ Check that Kafka is running, and that the bootstrap server you've provided (%s) is reachable from your client \"\"\" % (e,bootstrap_server)) 安装脚本依赖 python3 -m pip install confluent_kafka 测试命令 python3 python_kafka_test_client.py localhost:9092 测试输出，显示了kafka bootstrap返回给客户端的broker连接地址 🥾 bootstrap server: localhost:9092 ✅ Connected to bootstrap server(localhost:9092) and it returned metadata for brokers as follows: {0: BrokerMetadata(0, curiouser:9092)} --------------------- ℹ️ This step just confirms that the bootstrap connection was successful. ℹ️ For the consumer to work your client will also need to be able to resolve the broker(s) returned in the metadata above. ℹ️ If the host(s) shown are not accessible from where your client is running you need to change your advertised.listener configuration on the Kafka broker(s). ✅ 📬 Message delivered: \"foo / 2020-12-23 18:19:24\" to test_topic [partition 0] ✅ 💌 Message received: \"foo / 2020-12-23 18:19:24\" from topic test_topic 参考： https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-06-21 13:10:35 "},"origin/zk-kafka-ui.html":{"url":"origin/zk-kafka-ui.html","title":"Zookeeper和kafka的WebUI工具","keywords":"","body":"ZK与Kafka的WebUI工具 一、ZK的WebUI管理工具 功能： 权限隔离，使用admin用户登录后方可操作zk 连接数据不保存在服务端，而是存在浏览器cookie中 可对ZK进行增删改查 GitHub地址：https://github.com/qiuxiafei/zk-web Docker镜像地址：https://hub.docker.com/r/tobilg/zookeeper-webui Docker部署 docker run -d \\ -p 8080:8080 \\ -e USER=admin \\ -e PASSWORD=12356789 \\ --name zookeeper-web-ui \\ -t tobilg/zookeeper-webui k8s部署 kubectl -n tools run zookeeper-web-ui --restart='Always' --env=\"USER=admin\" --env=\"PASSWORD=12356789\" --image tobilg/zookeeper-webui kubectl -n tools expose deployment zookeeper-web-ui --port=80 --target-port=8080 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/kafka-shell-kaf.html":{"url":"origin/kafka-shell-kaf.html","title":"命令行Kafka工具Kaf","keywords":"","body":"kafka命令行工具kaf 一、简介 如果是使用kafka原生bin目录下的二进制命令的话，每一次命令要打好多参数，参数还无法自动补全，甚是麻烦，而GitHub中有个项目，可以像docker、kubectl命令一样，快速操作kafka。 GitHub地址：https://github.com/birdayz/kaf 二、安装配置 1、安装 Go go get -u github.com/birdayz/kaf/cmd/kaf 二进制 直接在GitHub的releases页面下载对应操作系统的二进制文件到可执行路径下 MacOS brew tap birdayz/kaf brew install kaf 2、配置 ①命令行参数 Kafka Command Line utility for cluster management Usage: kaf [command] Available Commands: completion Generate bash completion script for bash or zsh config Handle kaf configuration consume Consume messages group Display information about consumer groups. groups List groups help Help about any command node Describe and List nodes nodes List nodes in a cluster produce Produce record. Reads data from stdin. query Query topic by key topic Create and describe topics. topics List topics Flags: -b, --brokers strings Comma separated list of broker ip:port pairs -c, --cluster string set a temporary current cluster --config string config file (default is $HOME/.kaf/config) -h, --help help for kaf --schema-registry string URL to a Confluent schema registry. Used for attempting to decode Avro-encoded messages -v, --verbose Whether to turn on sarama logging Use \"kaf [command] --help\" for more information about a command. ②命令行补全 Bash Linux kaf completion bash > /etc/bash_completion.d/kaf Bash MacOS kaf completion bash > /usr/local/etc/bash_completion.d/kaf Zsh kaf completion zsh > \"${fpath[1]}/_kaf\" Fish kaf completion fish > ~/.config/fish/completions/kaf.fish Powershell Invoke-Expression (@(kaf completion powershell) -replace \" ''\\)$\",\" ' ')\" -join \"`n\") 3、使用 ①配置kafka连接 kaf config add-cluster local -b localhost:9092 连接配置会写在~/.kaf/config文件中 ②选择对应的kafka连接 kaf config select-cluster ③列出kafka broker节点的详细信息 kaf node ls ④列出所有的Topic及其分区、副本信息 kaf topics ⑤列出指定Topic的详细信息 kaf topics describe test_topic ⑥列出所有的消费者组 kaf groups ⑦列出指定消费者组的详细信息 kafa group describe dispatcher ⑧从标准输入写消息到指定Topic echo test | kaf produce test_topic ⑨消费指定Topic中的消息 kaf consume test_topic -f Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-09-01 10:44:33 "},"origin/filebeat-简介安装配置.html":{"url":"origin/filebeat-简介安装配置.html","title":"filebeat简介安装配置","keywords":"","body":"Filebeat的简介、安装、配置、Pipeline 一. 简介 Filebeat由两个主要组件组成： Inputs： 负责管理harvester并找到所有要读取的文件来源。如果输入类型为日志，则查找器将查找路径匹配的所有文件，并为每个文件启动一个harvester。每个Inputs都在自己的Go协程中运行 每个prospector类型可以定义多次 Harvesters： 一个harvester负责读取一个单个文件的内容，每个文件启动一个harvester。harvester逐行读取每个文件（一行一行地读取每个文件），并把这些内容发送到输出。在harvester正在读取文件内容的时候，文件被删除或者重命名了，那么Filebeat会续读这个文件。这就有一个问题了，就是只要负责这个文件的harvester没用关闭，那么磁盘空间就不会释放。默认情况下，Filebeat保存文件打开的状态直到close_inactive到达。 关闭harvester会产生以下结果： 如果在harvester仍在读取文件时文件被删除，则关闭文件句柄，释放底层资源。 文件的采集只会在scan_frequency过后重新开始 如果在harvester关闭的情况下移动或移除文件，则不会继续处理文件 二. 安装 默认的安装文件路径 Type Description Default Location Config Option home Home of the Filebeat installation. path.home bin The location for the binary files. {path.home}/bin config The location for configuration files. {path.home} path.config data The location for persistent data files. {path.home}/data path.data logs The location for the logs created by Filebeat. {path.home}/logs path.logs YUM/RPM [elastic-7.x] name=Elastic repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md yum install filebeat-7.4.0 RPM下载地址：https://www.elastic.co/cn/downloads/beats/filebeat yum localinstall -y filebeat-7*.rpm 安装文件路径 Type Description Location home Home of the Filebeat installation. /usr/share/filebeat bin The location for the binary files. /usr/share/filebeat/bin config The location for configuration files. /etc/filebeat data The location for persistent data files. /var/lib/filebeat logs The location for the logs created by Filebeat. /var/log/filebeat 二进制文件 zip, tar.gz, tgz 压缩格式的二进制安装包，下载地址：https://www.elastic.co/cn/downloads/beats/filebeat 安装文件路径 Type Description Location home Home of the Filebeat installation. {extract.path} bin The location for the binary files. {extract.path} config The location for configuration files. {extract.path} data The location for persistent data files. {extract.path}/data logs The location for the logs created by Filebeat. {extract.path}/logs Filebeat命令行启动 /usr/share/filebeat/bin/filebeat Commands SUBCOMMAND [FLAGS] Commands 描述 export 导出配置到控制台，包括index template, ILM policy, dashboard help 显示帮助文档 keystore 管理secrets keystore. modules 管理配置Modules run Runs Filebeat. This command is used by default if you start Filebeat without specifying a command. setup 设置初始环境。包括index template, ILM policy, write alias, Kibana dashboards (when available), machine learning jobs (when available). test 测试配置文件 version 显示版本信息 Global Flags 描述 -E \"SETTING_NAME=VALUE\" 覆盖配置文件中的配置项 --M \"VAR_NAME=VALUE\" 覆盖Module配置文件的中配置项 -c FILE 指定filebeat的配置文件路径。路径要相对于`path.config -d SELECTORS -e --path.config --path.data --path.home --path.logs --strict.perms 示例： /usr/share/filebeat/bin/filebeat --modules mysql -M \"mysql.slowlog.var.paths=[/root/slow.log]\" -e /usr/share/filebeat/bin/filebeat -e -E output.console.pretty=true --modules mysql -M \"mysql.slowlog.var.paths=[\"/root/mysql-slow-sql-log/mysql-slowsql.log\"]\" -M \"mysql.error.enabled=false\" -E output.elasticsearch.enabled=false SystemD启动 systemctl enable filebeat systemctl start filebeat systemctl stop filebeat systemctl status filebeat journalctl -u filebeat.service systemctl daemon-reload systemctl restart filebeat Filebeat的SystemD配置文件 $ /usr/lib/systemd/system/filebeat.service [Unit] Description=Filebeat sends log files to Logstash or directly to Elasticsearch. Documentation=https://www.elastic.co/products/beats/filebeat Wants=network-online.target After=network-online.target [Service] Environment=\"BEAT_LOG_OPTS=-e\" Environment=\"BEAT_CONFIG_OPTS=-c /etc/filebeat/filebeat.yml\" Environment=\"BEAT_PATH_OPTS=-path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat\" ExecStart=/usr/share/filebeat/bin/filebeat $BEAT_LOG_OPTS $BEAT_CONFIG_OPTS $BEAT_PATH_OPTS Restart=always [Install] WantedBy=multi-user.target Variable Description Default value BEAT_LOG_OPTS Log options -e BEAT_CONFIG_OPTS Flags for configuration file path -c /etc/filebeat/filebeat.yml BEAT_PATH_OPTS Other paths -path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat 三. Docker镜像 docker pull docker.elastic.co/beats/filebeat:7.4.0 docker pull filebeat:7.4.0 镜像中的安装文件路径 Type Description Location home Home of the Filebeat installation. /usr/share/filebeat bin The location for the binary files. /usr/share/filebeat config The location for configuration files. /usr/share/filebeat data The location for persistent data files. /usr/share/filebeat/data logs The location for the logs created by Filebeat. /usr/share/filebeat/logs Kubernetes部署 默认部署到kube-system命名空间 部署类型是Daemonset，会部署到每一个Node上 每个Node上的/var/lib/docker/containers目录会挂载到filebeat容器中 默认Filebeat会将日志吐到kube-system命名空间下的elasticsearch中，如果需要指定吐到其他elasticsearch中，修改环境变量 - name: ELASTICSEARCH_HOST value: elasticsearch - name: ELASTICSEARCH_PORT value: \"9200\" - name: ELASTICSEARCH_USERNAME value: elastic - name: ELASTICSEARCH_PASSWORD value: changeme curl -L -O https://raw.githubusercontent.com/elastic/beats/7.4/deploy/kubernetes/filebeat-kubernetes.yaml kubectl create -f filebeat-kubernetes.yaml kubectl --namespace=kube-system get ds/filebeat OKD部署 curl -L -O https://raw.githubusercontent.com/elastic/beats/7.4/deploy/kubernetes/filebeat-kubernetes.yaml 修改部署文件 securityContext: runAsUser: 0 privileged: true oc adm policy add-scc-to-user privileged system:serviceaccount:kube-system:filebeat 四. 配置 Filebeat的配置文件路径：/etc/filebeat/filebeat.yml 配置语法为YAML 配置项 描述 示例 processors.* Processors配置 processors:- include_fields: fields: [\"cpu\"]- drop_fields: fields: [\"cpu.user\", \"cpu.system\"] filebeat.modules: Module配置 filebeat.modules:- module: mysql error: enabled: true filebeat.inputs: Input配置 filebeat.inputs:- type: log enabled: false paths: - /var/log/*.log output.*: Output配置 output.console: enabled: true path.* 组件产生文件的位置配置 path.home: /usr/share/filebeatpath.data: ${path.home}/datapath.logs: ${path.home}/logs setup.template.* Template配置 logging.* 日志配置 logging.level: infologging.to_stderr: falselogging.to_files: true monitoring.* X-Pack监控配置 monitoring.enabled: falsemonitoring.elasticsearch.hosts: [\"localhost:9200\"] http.* HTTP Endpoint配置 http.enabled: falsehttp.port: 5066http.host: localhost filebeat.autodiscover.* Filebeat自动发现配置 通用配置 全局配置项 queue.* 缓存队列设置 全局配置项 配置项 默认值 描述 示例 registry.path ${path.data}/registry 注册表文件的根路径 filebeat.registry.path: registry registry.file_permissions 0600 注册表文件的权限。Window下该配置项无效 filebeat.registry.file_permissions: 0600 registry.flush 0s filebeat.registry.flush: 5s registry.migrate_file filebeat.registry.migrate_file: /path/to/old/registry_file config_dir filebeat.config_dir: path/to/configs shutdown_timeout 5s filebeat.shutdown_timeout: 5s 通用配置项 配置项 默认值 描述 示例 name name: \"my-shipper\" tags tags: [\"service-X\", \"web-tier\"] fields fields: {project: \"myproject\", instance-id: \"57452459\"} fields_under_root 如果该选项设置为true，则新增fields会放在根路径下，而不是放在fields路径下。自定义的field会覆盖filebeat默认的field。 fields_under_root: true processors 该配置项可配置以下Processors，详见 max_procs 配置示例 # Modules配置项 filebeat.modules: - module: system # 通用配置项 fields: level: debug review: 1 fields_under_root: false # Processors配置项 processors: - decode_json_fields: # Input配置项 filebeat.inputs: - type: log # Output配置项 output.elasticsearch: output.logstash: 五. Input插件类型 Input类型 类型 描述 配置示例 Log 从日志文件中读取每一行 filebeat.inputs: - type: log paths: - /var/log/messages - /var/log/*.log Stdin filebeat.inputs: - type: stdin Container filebeat.inputs: - type: container paths: - '/var/lib/docker/containers//.log' Kafka filebeat.inputs: - type: kafka hosts: - kafka-broker-1:9092 - kafka-broker-2:9092 topics: [\"my-topic\"] group_id: \"filebeat\" Redis filebeat.inputs: - type: redis hosts: [\"localhost:6379\"] password: \"${redis_pwd}\" UDP filebeat.inputs: - type: udp max_message_size: 10KiB host: \"localhost:8080\" Docker filebeat.inputs: - type: docker containers.ids: - 'e067b58476dc57d6986dd347' TCP filebeat.inputs: - type: tcp max_message_size: 10MiB host: \"localhost:9000\" Syslog filebeat.inputs: - type: syslog protocol.udp: host: \"localhost:9000\" s3 filebeat.inputs: - type: s3 queue_url: https://test.amazonaws.com/12/test access_key_id: my-access-key secret_access_key: my-secret-access-key NetFlow Google Pub/Sub 六. Output插件类型 类型 描述 配置样例 Elasticsearch output.elasticsearch: hosts: [\"https://localhost:9200\"] protocol: \"https\" index: \"filebeat-%{[agent.version]}-%{+yyyy.MM.dd}\" ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] ssl.certificate: \"/etc/pki/client/cert.pem\" ssl.key: \"/etc/pki/client/cert.key\" username: \"filebeat_internal\" password: \"YOUR_PASSWORD\" Logstash output.logstash: hosts: [\"127.0.0.1:5044\"] Kafka output.kafka: hosts: [\"kafka1:9092\", \"kafka2:9092\", \"kafka3:9092\"] topic: '%{[fields.log_topic]}' partition.round_robin: reachable_only: false required_acks: 1 compression: gzip max_message_bytes: 1000000 Redis output.redis: hosts: [\"localhost\"] password: \"my_password\" key: \"filebeat\" db: 0 timeout: 5 File output.file: path: \"/tmp/filebeat\" filename: filebeat #rotate_every_kb: 10000 #number_of_files: 7 #permissions: 0600 Console output.console: pretty: true Cloud 七. Processors插件 配置语法 processors: - if: then: - : - : ... else: - : - : 可以再Input中添加Processor - type: processors: - : when: 条件语法 equals equals: http.response.code: 200 contains contains: status: \"Specific error\" regexp regexp: system.process.name: \"foo.*\" range：The condition supports lt, lte, gt and gte. The condition accepts only integer or float values. range: http.response.code: gte: 400 network network: source.ip: private destination.ip: '192.168.1.0/24' destination.ip: ['192.168.1.0/24', '10.0.0.0/8', loopback] has_fields has_fields: ['http.response.code'] or or: - - - ... ----------------------------- or: - equals: http.response.code: 304 - equals: http.response.code: 404 and and: - - - ... ----------------------------- and: - equals: http.response.code: 200 - equals: status: OK ----------------------------- or: - - and: - - not not: -------------- not: equals: status: OK 支持的Processors 类型 作用 配置样例 add_cloud_metadata add_docker_metadata processors: - add_docker_metadata: host: \"unix:///var/run/docker.sock\" add_fields processors:- add_fields: target: project fields: name: myproject id: '574734885120952459' add_host_metadata processors: - add_host_metadata: netinfo.enabled: false cache.ttl: 5m geo: name: nyc-dc1-rack1 location: 40.7128, -74.0060 continent_name: North America country_iso_code: US region_name: New York region_iso_code: NY city_name: New York add_kubernetes_metadata processors: - add_kubernetes_metadata: host: kube_config: ~/.kube/config default_indexers.enabled: false default_matchers.enabled: false indexers: - ip_port: matchers: - fields: lookup_fields: [\"metricset.host\"] add_labels processors:- add_labels: labels: number: 1 with.dots: test nested: with.dots: nested array: - do - re - with.field: mi add_locale processors:- add_locale: ~processors:- add_locale: format: abbreviation add_observer_metadata add_process_metadata add_tags processors:- add_tags: tags: [web, production] target: \"environment\" community_id convert processors: - convert: fields: - {from: \"src_ip\", to: \"source.ip\", type: \"ip\"} - {from: \"src_port\", to: \"source.port\", type: \"integer\"} ignore_missing: true fail_on_error: false decode_base64_field decode_cef decode_csv_fields decode_json_fields decompress_gzip_field dissect processors:- dissect: tokenizer: \"%{key1} %{key2}\" field: \"message\" target_prefix: \"dissect\" dns drop_event processors:- drop_event: when: condition drop_fields processors:- drop_fields: when: condition fields: [\"field1\", \"field2\", ...] ignore_missing: false extract_array processors: - extract_array: field: my_array mappings: source.ip: 0 destination.ip: 1 network.transport: 2 include_fields processors: - include_fields: when: condition fields: [\"field1\", \"field2\", ...] registered_domain rename processors: - rename: fields: - from: \"a.g\" to: \"e.d\" ignore_missing: false fail_on_error: true script timestamp 八. 采集注册文件解析 采集注册文件路径：/var/lib/filebeat/registry/filebeat/data.json [{\"source\":\"/root/mysql-slow-sql-log/mysql-slowsql.log\",\"offset\":1365442,\"timestamp\":\"2019-10-11T09:29:35.185399057+08:00\",\"ttl\":-1,\"type\":\"log\",\"meta\":null,\"FileStateOS\":{\"inode\":2360926,\"device\":2051}}] source # 记录采集日志的完整路径 offset # 已经采集的日志的字节数;已经采集到日志的哪个字节位置 timestamp # 日志最后一次发生变化的时间戳 ttl # 采集失效时间，-1表示只要日志存在，就一直采集该日志 type: meta filestateos # 操作系统相关 　　inode # 日志文件的inode号 　　device # 日志所在磁盘的磁盘编号 硬盘格式化的时候，操作系统自动将硬盘分成了两个区域。 一个是数据区，用来存放文件的数据信息 一个是inode区，用来存放文件的元信息，比如文件的创建者、创建时间、文件大小等等 每一个文件都有对应的inode，里边包含了与该文件有关的一些信息，可以用stat命令查看文件的inode信息 > stat /var/log/messages File: ‘/var/log/messages’ Size: 56216339 Blocks: 109808 IO Block: 4096 regular file Device: 803h/2051d Inode: 1053379 Links: 1 Access: (0600/-rw-------) Uid: ( 0/ root) Gid: ( 0/ root) Access: 2019-10-06 03:20:01.528781081 +0800 Modify: 2019-10-12 13:59:13.059112545 +0800 Change: 2019-10-12 13:59:13.059112545 +0800 Birth: - 2051为十进制数，对应十六进制数803 参考链接 https://www.cnblogs.com/micmouse521/p/8085229.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/filebeat-summary.html":{"url":"origin/filebeat-summary.html","title":"filebeat常用配置以及问题总结","keywords":"","body":"Filebeat常用配置及问题总结 一、常用配置 1、采集时添加字段和标签 filebeat.inputs: - type: log paths: - /var/log/nginx/nginx-access.log exclude_files: [\"_filebeat\", \".gz$\"] # 添加自定义的字段 fields: env: test # 如果该选项设置为true，则新增fields成为顶级字段，而不是将其放在fields目录下。 # 而且自定义的field会覆盖filebeat默认的field fields_under_root: true # 在tags字段中添加自定义的属性 tags: nginx-access-log 2、删除自动添加的无用字段 processors: - drop_fields: fields: [\"agent\", \"input\", \"ecs\"] 二、问题总结 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/filebeat-多实例部署.html":{"url":"origin/filebeat-多实例部署.html","title":"filebeat多实例安装部署","keywords":"","body":"Filebeat的多实例部署 一、上下文 一台服务器为了充分利用资源使用，通常安装了多个系统的组件。例如既是MySQL集群的节点，又是Redis集群的节点。如果该服务器之前已经部署了一个FIlebeat实例用来采集MySQL的慢查询日志，输出日志到指定的Logstash进行处理。而此时，有需求要收集该服务器上另外一个系统的组件日志数据。FIlebeat的配置中无法使用条件判断设置多个不同的输出目的地。此时可以直接部署多实例的filebeat。 二、部署配置 以采集API网关Kong的日志为例. 创建新的filebeat配置文件/etc/filebeat/filebeat-kong.yml 创建Filebeat系统服务启动配置文件 /usr/lib/systemd/system/filebeat-kong.service [Unit] Description=Filebeat sends kong log files to Logstash or directly to Elasticsearch. Documentation=https://www.elastic.co/products/beats/filebeat Wants=network-online.target After=network-online.target [Service] Environment=\"BEAT_LOG_OPTS=-e\" # 指定Filebeat配置文件 Environment=\"BEAT_CONFIG_OPTS=-c /etc/filebeat/filebeat-kong.yml\" # 不同实例filebeat的'path.data'一定要不一样 Environment=\"BEAT_PATH_OPTS=-path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat-kong -path.logs /var/log/filebeat-kong\" ExecStart=/usr/share/filebeat/bin/filebeat $BEAT_LOG_OPTS $BEAT_CONFIG_OPTS $BEAT_PATH_OPTS Restart=always [Install] WantedBy=multi-user.target 启动filebeat服务 sudo systemctl daemon-reload sudo systemctl start filebeat-kong.service sudo systemctl status filebeat-kong.service -l Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/filebeat-modules模块.html":{"url":"origin/filebeat-modules模块.html","title":"Filebeat Modules","keywords":"","body":"Filebeat的Modules模块 一、简介 Filebeat采集日志文件除了可以自定义配置Input采集器、Processor处理器、输出目的地等，还提供大量的模板配置Modules来快速地配置采集通用格式的日志文件。例如Nginx标准格式的日志文件。 Filebeat的Module简化了常见日志格式的收集、解析和可视化。 一个典型的Module(例如，对于Nginx日志的Module)由一个或多个Fileset组成(对于Nginx，则是access和error日志文件)。Fileset包含以下内容: Filebeat输入配置：其中包含查找日志文件的默认路径，而这些默认路径取决于操作系统。Filebeat配置还负责在需要时将多行事件拼接在一起。 Elasticsearch Ingest节点Pipeline定义：用于解析日志行。 字段定义：用于为每个字段配置Elasticsearch的正确类型。它们还包含每个字段的简短描述。 Kibana表盘样本：可以用来可视化日志文件。 Filebeat支持的Modules Filebeat模块需要Elasticsearch 5.2或更高版本。 类型 Modules overview Apache module Auditd module AWS module CEF module Cisco module Coredns Module Elasticsearch module Envoyproxy Module Google Cloud module haproxy module IBM MQ module Icinga module IIS module Iptables module Kafka module Kibana module Logstash module MongoDB module MSSQL module MySQL module nats module NetFlow module Nginx module Osquery module Palo Alto Networks module PostgreSQL module RabbitMQ module Redis module Santa module Suricata module System module Traefik module Zeek (Bro) Module 二、Module配置 1. 加载Modules 在/etc/filebeat/filebeat.yml中配置加载Modules filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: true reload.period: 10s 指定特殊的filebeat全局配置文件来配置加载Modules filebeat -c /etc/filebeat/filebeat-kong.yaml modules enable nginx 2. 查看所有Module filebeat modules list 3. 启用Module Filebeat的Modules配置文件通常在/etc/filebeat/modules.d下 Filebeat提供了几种启用模块的不同方法： 命令行启用module filebeat modules enable module名 在配置文件/etc/filebeat/filebeat.yml中启用Modules filebeat.modules: - module: nginx - module: mysql - module: system 在运行时使用Modules filebeat --modules nginx,mysql,system 4. 配置Module变量参数 Filebeat的Modules配置文件通常在/etc/filebeat/modules.d下。当module不启用时，自带的Module配置文件是以.disabled后缀的。启用后，会自动去掉.disabled后缀，此时可以修改module的默认变量参数 在运行时配置Module变量参数 filebeat -e --modules 模块名 -M \"nginx.access.var.paths=[/var/log/nginx/access.log*]\" 在Modules的配置文件中配置变量参数 以Nginx模块的配置文件为例/etc/filebeat/modules.d/nginx.yml - module: nginx # 设置Nginx访问日志fileset access: input: close_eof: true enabled: true # 日志文件路径。如果为空，默认根据操作系统版本自行选择日志文件路径. var.paths: [ \"/var/log/nginx/access.log\", \"/var/log/nginx/admin_access.log\" ] # 设置Nginx错误日志fileset error: enabled: true # 日志文件路径。如果为空，默认根据操作系统版本自行选择日志文件路径. var.paths: [\"/var/log/nginx/error.log\"] 注意：例如想要给Nginx模块的fileset添加一个参数close_eof: true，可使用以下参数 配置文件 - module: nginx access: input: close_eof: true 命令行 filebeat -e --modules nginx -M \"*.*.input.close_eof=true\" # 或者 filebeat -e --modules nginx -M \"nginx.*.input.close_eof=true\" # 或者 filebeat -e --modules nginx -M \"nginx.access.input.close_eof=true\" Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/filbeat-nginx-module.html":{"url":"origin/filbeat-nginx-module.html","title":"Nginx Module","keywords":"","body":"Filebeat的Nginx模块 一、简介 Filebeat的Nginx Module模块可直接用来处理Nginx标准格式的访问日志和错误日志。 二、启用Nginx模块 1. 加载Modules 在/etc/filebeat/filebeat.yml中配置加载Modules filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: true reload.period: 10s 指定特殊的filebeat全局配置文件来配置加载Modules filebeat -c /etc/filebeat/filebeat-kong.yaml modules enable nginx 2. 启用Nginx 模块 filebeat modules enable nginx 三、配置Nginx模块变量参数 1. 配置Nginx 模块变量参数的方式 Nginx module的配置文件/etc/filebeat/modules.d/nginx.yml - module: nginx # 设置Nginx访问日志 access: input: close_eof: true enabled: true # 日志文件路径。如果为空，默认根据操作系统版本自行选择日志文件路径. var.paths: [ \"/var/log/nginx/access.log\", \"/var/log/nginx/admin_access.log\" ] # 设置Nginx错误日志 error: enabled: true # 日志文件路径。如果为空，默认根据操作系统版本自行选择日志文件路径. var.paths: [\"/var/log/nginx/error.log\"] 在运行时 filebeat -e \\ --modules nginx \\ -M \"nginx.access.var.paths=[ \"/var/log/nginx/access.log\", \"/var/log/nginx/admin_access.log\" ]\" \\ -M \"nginx.error.var.paths=[\"/var/log/nginx/error.log\"]\" \\ -M \"nginx.access.input.close_eof=true\" 四、Nginx模块配置文件详解 1. Nginx模块配置目录结构 Nginx模块中所有的配置文件在/usr/share/filebeat/module/nginx路径下 /usr/share/filebeat/module/nginx ├── access │ ├── config │ │ └── nginx-access.yml │ ├── ingest │ │ └── default.json │ └── manifest.yml ├── error │ ├── config │ │ └── nginx-error.yml │ ├── ingest │ │ └── pipeline.json │ └── manifest.yml └── module.yml 2. module.yml dashboards: - id: 55a9e6e0-a29e-11e7-928f-5dbe6f6f5519 file: Filebeat-nginx-overview.json - id: 046212a0-a2a1-11e7-928f-5dbe6f6f5519 file: Filebeat-nginx-logs.json - id: ML-Nginx-Access-Remote-IP-Count-Explorer file: ml-nginx-access-remote-ip-count-explorer.json - id: ML-Nginx-Remote-IP-URL-Explorer file: ml-nginx-remote-ip-url-explorer.json 3. access/manifest.yml module_version: \"1.0\" var: - name: paths default: - /var/log/nginx/access.log* os.darwin: - /usr/local/var/log/nginx/access.log* os.windows: - c:/programdata/nginx/logs/*access.log* ingest_pipeline: ingest/default.json input: config/nginx-access.yml machine_learning: - name: response_code job: machine_learning/response_code.json datafeed: machine_learning/datafeed_response_code.json min_version: 5.5.0 - name: low_request_rate job: machine_learning/low_request_rate.json datafeed: machine_learning/datafeed_low_request_rate.json min_version: 5.5.0 - name: remote_ip_url_count job: machine_learning/remote_ip_url_count.json datafeed: machine_learning/datafeed_remote_ip_url_count.json min_version: 5.5.0 - name: remote_ip_request_rate job: machine_learning/remote_ip_request_rate.json datafeed: machine_learning/datafeed_remote_ip_request_rate.json min_version: 5.5.0 - name: visitor_rate job: machine_learning/visitor_rate.json datafeed: machine_learning/datafeed_visitor_rate.json min_version: 5.5.0 requires.processors: - name: user_agent plugin: ingest-user-agent - name: geoip plugin: ingest-geoip 4. access/config/nginx-access.yml type: log paths: {{ range $i, $path := .paths }} - {{$path}} {{ end }} exclude_files: [\".gz$\"] processors: - add_locale: ~ 5. access/ingest/default.json { \"description\": \"Pipeline for parsing Nginx access logs. Requires the geoip and user_agent plugins.\", \"processors\": [ { \"grok\": { \"field\": \"message\", \"patterns\": [ \"\\\"?(?:%{IP_LIST:nginx.access.remote_ip_list}|%{DATA:source.address}) - %{DATA:user.name} \\\\[%{HTTPDATE:nginx.access.time}\\\\] \\\"%{DATA:nginx.access.info}\\\" %{NUMBER:http.response.status_code:long} %{NUMBER:http.response.body.bytes:long} \\\"%{DATA:http.request.referrer}\\\" \\\"%{DATA:user_agent.original}\\\"\" ], \"pattern_definitions\": { \"IP_LIST\": \"%{IP}(\\\"?,?\\\\s*%{IP})*\" }, \"ignore_missing\": true } }, { \"grok\": { \"field\": \"nginx.access.info\", \"patterns\": [ \"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\", \"\" ], \"ignore_missing\": true } }, { \"remove\": { \"field\": \"nginx.access.info\" } }, { \"split\": { \"field\": \"nginx.access.remote_ip_list\", \"separator\": \"\\\"?,?\\\\s+\", \"ignore_missing\": true } }, { \"split\": { \"field\": \"nginx.access.origin\", \"separator\": \"\\\"?,?\\\\s+\", \"ignore_missing\": true } }, { \"set\": { \"field\": \"source.ip\", \"value\": \"\" } }, { \"script\": { \"lang\": \"painless\", \"source\": \"boolean isPrivate(def dot, def ip) { try { StringTokenizer tok = new StringTokenizer(ip, dot); int firstByte = Integer.parseInt(tok.nextToken()); int secondByte = Integer.parseInt(tok.nextToken()); if (firstByte == 10) { return true; } if (firstByte == 192 && secondByte == 168) { return true; } if (firstByte == 172 && secondByte >= 16 && secondByte 6. error/manifest.yml module_version: \"1.0\" var: - name: paths default: - /var/log/nginx/error.log* os.darwin: - /usr/local/var/log/nginx/error.log* os.windows: - c:/programdata/nginx/logs/error.log* ingest_pipeline: ingest/pipeline.json input: config/nginx-error.yml 7. error/config/nginx-error.yml type: log paths: {{ range $i, $path := .paths }} - {{$path}} {{ end }} exclude_files: [\".gz$\"] processors: - add_locale: ~ 8. error/ingest/pipeline.json { \"description\": \"Pipeline for parsing the Nginx error logs\", \"processors\": [{ \"grok\": { \"field\": \"message\", \"patterns\": [ \"%{DATA:nginx.error.time} \\\\[%{DATA:log.level}\\\\] %{NUMBER:process.pid:long}#%{NUMBER:process.thread.id:long}: (\\\\*%{NUMBER:nginx.error.connection_id:long} )?%{GREEDYDATA:message}\" ], \"ignore_missing\": true } }, { \"rename\": { \"field\": \"@timestamp\", \"target_field\": \"event.created\" } }, { \"date\": { \"field\": \"nginx.error.time\", \"target_field\": \"@timestamp\", \"formats\": [\"yyyy/MM/dd H:m:s\"], \"ignore_failure\": true } }, { \"date\": { \"if\": \"ctx.event.timezone != null\", \"field\": \"nginx.error.time\", \"target_field\": \"@timestamp\", \"formats\": [\"yyyy/MM/dd H:m:s\"], \"timezone\": \"{{ event.timezone }}\", \"on_failure\": [{\"append\": {\"field\": \"error.message\", \"value\": \"{{ _ingest.on_failure_message }}\"}}] } }, { \"remove\": { \"field\": \"nginx.error.time\" } }], \"on_failure\" : [{ \"set\" : { \"field\" : \"error.message\", \"value\" : \"{{ _ingest.on_failure_message }}\" } }] } 五、示例： 1. Filebeat Nginx模块配置采集API网关Kong日志 Kong日志数据采集处理流程：kong节点 + filbeat ----> Kubernetes上的Logstash ----> Kubernetes上的Elasticsearch Kong使用了Nginx作为基础组件，它的日志也主要是Nginx格式的日志，分为两种：访问日志和错误日志。它的Nginx是安装了Lua模块的，而Lua模块的错误日志和Nginx的错误日志混合在一起。Lua的错误日志格式有的是多行。这就造成整个Nginx错误日志中既有单行错误日志，又有多行错误日志。 直接使用Filebeat的Nginx模块采集日志文件。对于标准格式的Kong访问日志是没有问题的，关键点是错误日志，要修改filebeat的Nginx模块对错误日志文件进行多行采集，设置过滤关键词，将关键词之间的多行合并为一个采集事件。 Kong的日志输出目录：/usr/local/kong/logs。目录下有两种格式的日志文件 Nginx标准日志格式的访问日志文件：/usr/local/kong/logs/admin_access.log /usr/local/kong/logs/access.log 172.17.18.169 - - [21/Oct/2019:11:47:42 +0800] \"GET /oalogin.php HTTP/1.1\" 494 46 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\" 带有Lua模块Nginx的错误日志文件 ：/usr/local/kong/logs/error.log ```bash 2019/10/21 10:58:56 [warn] 14716#0: 17345670 [lua] reports.lua:70: log(): [reports] unknown request scheme: http while logging request, client: 172.17.18.169, server: kong, request: \"GET / HTTP/1.1\", host: \"172.17.18.169\" 2019/10/21 10:59:05 [warn] 14717#0: 17346563 [lua] reports.lua:70: log(): [reports] unknown request scheme: http while logging request, client: 172.17.18.169, server: kong, request: \"GET /routes HTTP/1.1\", host: \"172.17.18.169\" 2019/10/21 11:00:09 [error] 14716#0: *17348732 lua coroutine: runtime error: don't know how to respond to POST stack traceback: coroutine 0: [C]: ? coroutine 1: [C]: in function 'resume' /usr/local/share/lua/5.1/lapis/application.lua:397: in function 'handler' /usr/local/share/lua/5.1/lapis/application.lua:130: in function 'resolve' /usr/local/share/lua/5.1/lapis/application.lua:167: in function [C]: in function 'xpcall' /usr/local/share/lua/5.1/lapis/application.lua:173: in function 'dispatch' /usr/local/share/lua/5.1/lapis/nginx.lua:230: in function 'serve' /usr/local/share/lua/5.1/kong/init.lua:1113: in function 'admin_content' content_by_lua(nginx-kong.conf:190):2: in main chunk, client: 172.17.18.169, server: kong_admin, request: \"POST /routes/smsp-route HTTP/1.0\", host: \"local.api.kong.curouser.com:80\" 2019/10/21 11:06:38 [warn] 14713#0: *17362982 [lua] reports.lua:70: log(): [reports] unknown request scheme: http while logging request, client: 172.17.18.169, server: kong, request: \"GET /upstream HTTP/1.1\", host: \"172.17.18.169\" ``` 修改Nginx模块采集错误日志文件的方式 Filebeat的安装，Nignx模块启用，模块参数配置等操作步骤省略。这里只写针对Nginx错误日志配置进行的修改。 编辑/usr/share/filebeat/module/nginx/error/config/nginx-error.yml # ===========================修改前的====================================== type: log paths: {{ range $i, $path := .paths }} - {{$path}} {{ end }} exclude_files: [\".gz$\"] processors: - add_locale: ~ # ===========================修改后的====================================== type: log paths: {{ range $i, $path := .paths }} - {{$path}} {{ end }} exclude_files: [\".gz$\"] multiline.pattern: '^[0-9]{4}/[0-9]{2}/[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}' multiline.negate: true multiline.match: after Logstash针对FIlebeat发送过来的日志事件进行分割处理的Pipelines #=======================接收Filebeat发送过来的日志事件==================== input { beats { id => \"logstash_kong_beats\" port => 5044 } } #=======================过滤、拆分、转换日志事件============================== filter { if [fileset][name] == \"access\" { grok { match => { \"message\" => [\"%{IPORHOST:[nginx][access][remote_ip]} - %{DATA:[nginx][access][user_name]} \\[%{HTTPDATE:[nginx][access][time]}\\] \\\"%{WORD:[nginx][access][method]} %{DATA:[nginx][access][url]} HTTP/%{NUMBER:[nginx][access][http_version]}\\\" %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \\\"%{DATA:[nginx][access][referrer]}\\\" \\\"%{DATA:[nginx][access][agent]}\\\"\"] } remove_field => \"message\" } mutate { add_field => { \"read_timestamp\" => \"%{@timestamp}\" } } date { match => [ \"[nginx][access][time]\", \"dd/MMM/YYYY:H:m:s Z\" ] remove_field => \"[nginx][access][time]\" } useragent { source => \"[nginx][access][agent]\" target => \"[nginx][access][user_agent]\" remove_field => \"[nginx][access][agent]\" } geoip { source => \"[nginx][access][remote_ip]\" target => \"[nginx][access][geoip]\" } } else if [fileset][name] == \"error\" { grok { match => { \"message\" => [\"%{DATA:[nginx][error][time]} \\[%{DATA:[nginx][error][level]}\\] %{NUMBER:[nginx][error][pid]}#%{NUMBER:[nginx][error][tid]}: (\\*%{NUMBER:[nginx][error][connection_id]} )?%{GREEDYDATA:[nginx][error][message]}\"] } remove_field => \"message\" } mutate { rename => { \"@timestamp\" => \"read_timestamp\" } } date { match => [ \"[nginx][error][time]\", \"YYYY/MM/dd H:m:s\" ] remove_field => \"[nginx][error][time]\" } } } #=======================根据日志事件类型的不同输出到不同elasticsearch索引中==================== output { if [fileset][name] == \"access\" { elasticsearch { id => \"logstash_kong_access_log\" hosts => [\"elasticsearch.elk.svc\"] index => \"kong-accesslog-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true template_name => \"logstash-logger\" user => \"logstash-user\" password => \"logstash-password\" } }else if [fileset][name] == \"error\"{ elasticsearch { id => \"logstash_kong_error_log\" hosts => [\"elasticsearch.elk.svc\"] index => \"kong-errorlog-%{+YYYY.MM.dd}\" document_type => \"_doc\" http_compression => true template_name => \"logstash-curiouser\" user => \"logstash-user\" password => \"logstash-password\" } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/logstash-简介安装配置Pipeline.html":{"url":"origin/logstash-简介安装配置Pipeline.html","title":"Logastash简介安装配置Pipeline","keywords":"","body":"Logstash的简介、安装、配置、Pipeline、插件 一. 简介 官方文档：https://www.elastic.co/guide/en/logstash/current/getting-started-with-logstash.html Logstash是一个开源数据收集引擎，具有实时管道功能。 Logstash可以动态地将来自不同数据源的数据统一起来，并将数据标准化到你所选择的目的地 Logstash 是一款强大的数据处理工具，它可以实现数据传输，格式处理，格式化输出，还有强大的插件功能，常用于日志处理。 Logstash耗资源较大，运行占用CPU和内存高。另外没有消息队列缓存，存在数据丢失隐患 Logstash使用Ruby语言编写的运行在Java虚拟机上的具有收集、分析和转发数据流功能的工具 Logstash使用Pipeline方式进行日志的搜集，处理和输出 Event：logstash将数据流中的每一条数据在input处被转换为event，在output处event再被转换为目标格式的数据 Inputs：用于从数据源获取Event。每个Input启动一个线程，从对应数据源获取数据，将数据写入一个队列 Filters：用于过滤、修改Event Outputs：负责输出Event到其他系统中 Logstash使用Pipeline流水线的形式来处理数据Event事件，大致流程如下 其中inputs和outputs支持codecs（coder&decoder）在1.3.0 版之前，logstash 只支持纯文本形式输入，然后用filter处理它。但现在，我们可以在输入期间处理不同类型的数据。所以现在的数据处理流程 箭头代表数据流向。可以有多个input。中间的queue负责将数据分发到不通的pipline中，每个pipline由batcher，filter和output构成。batcher的作用是批量从queue中取数据（可配置）。 logstash数据流历程 首先有一个输入数据，例如是一个web.log文件，其中每一行都是一条数据。file imput会从文件中取出数据，然后通过json codec将数据转换成logstash event。 这条event会通过queue流入某一条pipline处理线程中，首先会存放在batcher中。当batcher达到处理数据的条件（如一定时间或event一定规模）后，batcher会把数据发送到filter中，filter对event数据进行处理后转到output，output就把数据输出到指定的输出位置。 输出后还会返回ACK给queue，包含已经处理的event，queue会将已处理的event进行标记。 queue分类 In Memory： 在内存中，固定大小，无法处理进程crash. 机器宕机等情况，会导致数据丢失。 Persistent Queue：可处理进程crash情况，保证数据不丢失。保证数据至少消费一次；充当缓冲区，可代替kafka等消息队列作用。 Dead Letter Queues：存放logstash因数据类型错误等原因无法处理的Event Persistent Queue（PQ）处理流程 一条数据经由input进入PQ，PQ将数据备份在disk，然后PQ响应input表示已收到数据； 数据从PQ到达filter/output，其处理到事件后返回ACK到PQ； PQ收到ACK后删除磁盘的备份数据； 二. 安装 1. 安装Java环境 在一些Linux环境下，必须设置JAVA_HOME环境变量，否则Logstash在安装期间没有检测到JAVA_HOME环境变量，会报错并且启动不起来服务。如果JDK目录在/opt下，则 在/usr/bin/下建立软连接指向JAVA_HOME/bin路径下的java 2. 安装Logstash YUM/RPM [elasticsearch-7.x] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md yum install -y logstash-7.2.0 手动下载RPM安装，官方下载链接：https://www.elastic.co/downloads/logstash yum localinstall -y logstash-7*.rpm RPM包安装后各个配置文件的位置 Type Description Default Location Setting home Home directory of the Logstash installation. /usr/share/logstash bin Binary scripts including logstash to start Logstash and logstash-plugin to install plugins /usr/share/logstash/bin settings Configuration files, including logstash.yml, jvm.options, and startup.options /etc/logstash path.settings conf Logstash pipeline configuration files /etc/logstash/conf.d/*.conf See /etc/logstash/pipelines.yml logs Log files /var/log/logstash path.logs plugins Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. /usr/share/logstash/plugins path.plugins data Data files used by logstash and its plugins for any persistence needs. /var/lib/logstash path.data 二进制包 二进制包中各个配置文件的位置 Type Description Default Location Setting home Home directory of the Logstash installation. {extract.path}- Directory created by unpacking the archive bin Binary scripts, including logstash to start Logstash and logstash-plugin to install plugins {extract.path}/bin settings Configuration files, including logstash.yml and jvm.options {extract.path}/config path.settings logs Log files {extract.path}/logs path.logs plugins Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. {extract.path}/plugins path.plugins data Data files used by logstash and its plugins for any persistence needs. {extract.path}/data path.data 3. 启动 以服务形式或命令启动Logstash systemctl start logstash #后台会起一个名叫org.jruby.Main的Java后台进程，用jps -l查看 jps -l 使用二进制执行文件启动 /user/share/logstash/bin/logstash -f logstash.conf --config.reload.automatic #-f 指定配置文件路径 #--config.reload.automatic 自动检测加载配置文件，该参数在有-e参数是不生效 #--config.reload.interval 设置多少秒检测一次配置文件 如果Logstash启动时没有配置自动加载配置文件，重启进程时加上。 4. 验证 /usr/share/logstash/bin/logstash -e 'input { stdin { } } output { stdout {} }' #参数-e：直接从命令行定义配置信息 #配置从标准输入读取输入，然后输出到标准输出 stdin > hello world stdout> 2013-11-21T01:22:14.405+0000 0.0.0.0 hello world #Logstash会在消息上添加时间戳和IP地址 #Ctrl+D 退出Logstash 5. 命令行参数 参数 描述 默认值 -r, --config.reload.automatic Monitor configuration changes and reload whenever it is changed. NOTE: use SIGHUP to manually reload the config false -n, --node.name NAME Specify the name of this logstash instance, if no value is given it will default to the current hostname. 当前主机名 -f, --path.config CONFIG_PATH Load the logstash config from a specific file or directory. If a directory is given, all files in that directory will be concatenated in lexicographical order and then parsed as a single config file. You can also specify wildcards (globs) and any matched files will be loaded in the order described above. -e, --config.string CONFIG_STRING Use the given string as the configuration data. Same syntax as the config file. If no input is pecified, then the following is used as the default input: \"input { stdin { type => stdin } }\" and if no output is specified, then the following is used as the default output: \"output { stdout { codec => rubydebug } }\" If you wish to use both defaults, please use the empty string for the '-e' flag. nil --log.level LEVEL Set the log level for logstash. Possible values are: fatal error warn info debug trace (default: \"info\") -l, --path.logs PATH Write logstash internal logs to the given file. Without this flag, logstash will emit logs to standard output. /usr/share/logstash/logs -t, --config.test_and_exit Check configuration for valid syntax and then exit. false --config.reload.interval RELOAD_INTERVAL How frequently to poll the configuration location for changes, in seconds 3000000000 --http.host HTTP_HOST Web API binding host 127.0.0.1 --http.port HTTP_PORT Web API http port 9600..9700 --log.format FORMAT Specify if Logstash should write its own logs in JSON form (one event per line) or in plain text (using Ruby's Object#inspect) plain --path.settings SETTINGS_DIR Directory containing logstash.yml file. This can also be set through the LS_SETTINGS_DIR environment variable /usr/share/logstash/config -p, --path.plugins PATH A path of where to find plugins. This flag can be given multiple times to include multiple paths. Plugins are expected to be in a specific directory hierarchy: 'PATH/logstash/TYPE/NAME.rb' where TYPE is 'inputs' 'filters', 'outputs' or 'codecs' and NAME is the name of the plugin. [] --path.data PATH This should point to a writable directory. Logstash will use this directory whenever it needs to store data. Plugins will also have access to this path. /usr/share/logstash/data -u, --pipeline.batch.delay DELAY_IN_MS When creating pipeline batches, how long to wait while polling for the next event. 50 --pipeline.id ID Sets the ID of the pipeline. main -b, --pipeline.batch.size SIZE Size of batches the pipeline is to work in. 125 -V, --version Emit the version of logstash and its friends, then exit. -M, --modules.variable MODULES_VARIABLE Load variables for module template. Multiple instances of '-M' or '--modules.variable' are supported. Ignored if '--modules' flag is not used. Should be in the format of '-M \"MODULE_NAME.var.PLUGIN_TYPE.PLUGIN_NAME.VARIABLE_NAME=VALUE\"' as in '-M \"example.var.filter.mutate.fieldname=fieldvalue\"' --modules MODULES Load Logstash modules. Modules can be defined using multiple instances '--modules module1 --modules module2', or comma-separated syntax '--modules=module1,module2' Cannot be used in conjunction with '-e' or '-f' Use of '--modules' will override modules declared in the 'logstash.yml' file. --setup Load index template into Elasticsearch, and saved searches, index-pattern, visualizations, and dashboards into Kibana when running modules. false -w, --pipeline.workers COUNT Sets the number of pipeline workers to run. 20 --config.debug Print the compiled config ruby code out as a debug log (you must also have --log.level=debug enabled). WARNING: This will include any 'password' options passed to plugin configs as plaintext, and may result in plaintext passwords appearing in your logs! false --pipeline.unsafe_shutdown Force logstash to exit during shutdown even if there are still inflight events in memory. By default, logstash will refuse to quit until all received events have been pushed to the outputs. false --java-execution Use Java execution engine. true -i, --interactive SHELL Drop to shell instead of running as normal. Valid shells are \"irb\" and \"pry\" --verbose Set the log level to info. 三. Docker镜像 docker pull docker.elastic.co/logstash/logstash:7.4.0 docker pull logstash:7.4.0 镜像中各个配置文件的位置 Type Description Default Location Setting home Home directory of the Logstash installation. /usr/share/logstash bin Binary scripts, including logstash to start Logstash and logstash-plugin to install plugins /usr/share/logstash/bin settings Configuration files, including logstash.yml and jvm.options /usr/share/logstash/config path.settings conf Logstash pipeline configuration files /usr/share/logstash/pipeline path.config plugins Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only. /usr/share/logstash/plugins path.plugins data Data files used by logstash and its plugins for any persistence needs. /usr/share/logstash/data path.data Note：基于该镜像启动的容器，日志是直接输出到控制台的，无法直接输出到日志文件中 docker镜像是基于.tar.gz格式的二进制包创建的 将pipeline文件挂载到/usr/share/logstash/pipeline/下启动 docker run --rm -it \\ -v ./test.conf:/usr/share/logstash/pipeline/test.conf \\ docker.elastic.co/logstash/logstash:7.4.0 默认pipeline文件：/usr/share/logstash/pipeline/logstash.conf input { beats { port => 5044 } } output { stdout { codec => rubydebug } } 也就是说如果不配置挂载pipeline文件就直接启动容器，logstash将启动一个最小化的pipeline：Beat Input ---> Stdout Output 可通过设置环境变量的形式配置logstash。 docker run --rm -it -e PIPELINE_WORKERS:2 docker.elastic.co/logstash/logstash:7.4.0。例如以下环境变量对应的logstash配置 Environment Variable Logstash Setting PIPELINE_WORKERS pipeline.workers LOG_LEVEL log.level XPACK_MONITORING_ENABLED xpack.monitoring.enabled logstash docker 镜像中的默认配置 http.host 0.0.0.0 xpack.monitoring.elasticsearch.hosts http://elasticsearch:9200 四. 配置 Logstash配置文件中配置项的格式是基于YAML语法，例如： pipeline: batch: size: 125 delay: 50 也可以使用平级格式 pipeline.batch.size: 125 pipeline.batch.delay: 50 配置项的值可以引用系统级别的环境变量 pipeline.batch.size: ${BATCH_SIZE} pipeline.batch.delay: ${BATCH_DELAY:50} node.name: \"node_${LS_NODE_NAME}\" path.queue: \"/tmp/${QUEUE_DIR:queue}\" 如果设置多个自定义的配置项时，推荐使用以下格式 modules: - name: MODULE_NAME1 var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY1: VALUE var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY2: VALUE var.PLUGIN_TYPE2.PLUGIN_NAME2.KEY1: VALUE var.PLUGIN_TYPE3.PLUGIN_NAME3.KEY1: VALUE - name: MODULE_NAME2 var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY1: VALUE var.PLUGIN_TYPE1.PLUGIN_NAME1.KEY2: VALUE 常见的logstash配置 Setting Description Default value node.name A descriptive name for the node. Machine’s hostname path.data The directory that Logstash and its plugins use for any persistent needs. LOGSTASH_HOME/data pipeline.id The ID of the pipeline. main pipeline.java_execution Use the Java execution engine. true pipeline.workers The number of workers that will, in parallel, execute the filter and output stages of the pipeline. If you find that events are backing up, or that the CPU is not saturated, consider increasing this number to better utilize machine processing power. Number of the host’s CPU cores pipeline.batch.size The maximum number of events an individual worker thread will collect from inputs before attempting to execute its filters and outputs. Larger batch sizes are generally more efficient, but come at the cost of increased memory overhead. You may need to increase JVM heap space in the jvm.options config file. See Logstash Configuration Files for more info. 125 pipeline.batch.delay When creating pipeline event batches, how long in milliseconds to wait for each event before dispatching an undersized batch to pipeline workers. 50 pipeline.unsafe_shutdown When set to true, forces Logstash to exit during shutdown even if there are still inflight events in memory. By default, Logstash will refuse to quit until all received events have been pushed to the outputs. Enabling this option can lead to data loss during shutdown. false pipeline.plugin_classloaders (Beta) Load Java plugins in independent classloaders to isolate their dependencies. false path.config The path to the Logstash config for the main pipeline. If you specify a directory or wildcard, config files are read from the directory in alphabetical order. Platform-specific. See Logstash Directory Layout. config.string A string that contains the pipeline configuration to use for the main pipeline. Use the same syntax as the config file. None config.test_and_exit When set to true, checks that the configuration is valid and then exits. Note that grok patterns are not checked for correctness with this setting. Logstash can read multiple config files from a directory. If you combine this setting with log.level: debug, Logstash will log the combined config file, annotating each config block with the source file it came from. false config.reload.automatic When set to true, periodically checks if the configuration has changed and reloads the configuration whenever it is changed. This can also be triggered manually through the SIGHUP signal. false config.reload.interval How often in seconds Logstash checks the config files for changes. 3s config.debug When set to true, shows the fully compiled configuration as a debug log message. You must also set log.level: debug. WARNING: The log message will include any password options passed to plugin configs as plaintext, and may result in plaintext passwords appearing in your logs! false config.support_escapes When set to true, quoted strings will process the following escape sequences: \\n becomes a literal newline (ASCII 10). \\r becomes a literal carriage return (ASCII 13). \\t becomes a literal tab (ASCII 9). \\\\ becomes a literal backslash \\. \\\" becomes a literal double quotation mark. \\' becomes a literal quotation mark. false modules When configured, modules must be in the nested YAML structure described above this table. None queue.type The internal queuing model to use for event buffering. Specify memory for legacy in-memory based queuing, or persisted for disk-based ACKed queueing (persistent queues). memory path.queue The directory path where the data files will be stored when persistent queues are enabled (queue.type: persisted). path.data/queue queue.page_capacity The size of the page data files used when persistent queues are enabled (queue.type: persisted). The queue data consists of append-only data files separated into pages. 64mb queue.max_events The maximum number of unread events in the queue when persistent queues are enabled (queue.type: persisted). 0 (unlimited) queue.max_bytes The total capacity of the queue in number of bytes. Make sure the capacity of your disk drive is greater than the value you specify here. If both queue.max_events and queue.max_bytes are specified, Logstash uses whichever criteria is reached first. 1024mb (1g) queue.checkpoint.acks The maximum number of ACKed events before forcing a checkpoint when persistent queues are enabled (queue.type: persisted). Specify queue.checkpoint.acks: 0 to set this value to unlimited. 1024 queue.checkpoint.writes The maximum number of written events before forcing a checkpoint when persistent queues are enabled (queue.type: persisted). Specify queue.checkpoint.writes: 0 to set this value to unlimited. 1024 queue.checkpoint.retry When enabled, Logstash will retry once per attempted checkpoint write for any checkpoint writes that fail. Any subsequent errors are not retried. This is a workaround for failed checkpoint writes that have been seen only on filesystems with non-standard behavior such as SANs and is not recommended except in those specific circumstances. false queue.drain When enabled, Logstash waits until the persistent queue is drained before shutting down. false dead_letter_queue.enable Flag to instruct Logstash to enable the DLQ feature supported by plugins. false dead_letter_queue.max_bytes The maximum size of each dead letter queue. Entries will be dropped if they would increase the size of the dead letter queue beyond this setting. 1024mb path.dead_letter_queue The directory path where the data files will be stored for the dead-letter queue. path.data/dead_letter_queue http.host The bind address for the metrics REST endpoint. \"127.0.0.1\" http.port The bind port for the metrics REST endpoint. 9600 log.level 设置Logstash日志输出级别 可用值：fatal error warn info debug trace info log.format The log format. Set to json to log in JSON format, or plain to use Object#.inspect. plain path.logs The directory where Logstash will write its log to. LOGSTASH_HOME/logs path.plugins Where to find custom plugins. You can specify this setting multiple times to include multiple paths. Plugins are expected to be in a specific directory hierarchy: PATH/logstash/TYPE/NAME.rb where TYPE is inputs, filters, outputs, or codecs, and NAME is the name of the plugin. Platform-specific. See Logstash Directory Layout. 五. Pipeline 1. 配置项结构 Logstash Pipeline文件的配置项分为三个部分： input{ input插件{ 插件配置项 } } filter{ filter插件{ 插件配置项 } } output{ output插件{ 插件配置项 } } Note: 如果在filter中添加了多种处理规则，则按照它的顺序一一处理，但是有一些插件并不是线程安全的。 如果在filter中指定了两个一样的的插件，这两个任务并不能保证准确的按顺序执行，因此官方也推荐避免在filter中重复使用插件。 2. 插件的条件控制 官方文档：https://www.elastic.co/guide/en/logstash/6.7/event-dependent-configuration.html#conditionals 有时需要在特定条件下过滤或输出事件。为此，您可以使用条件（conditional）来决定filter和output处理特定的事件。比如在elk系统中想要添加一个type类型的关键字来根据不同的条件赋值，最后好做统计。条件语支持if，else if和else语句并且可以嵌套。 条件语法 if EXPRESSION { ... } else if EXPRESSION { ... } else { ... } 操作符 比较操作： 相等: ==, !=, , >, , >= 正则: `=~(匹配正则), !~(不匹配正则) 包含:in(包含), not in(不包含) 布尔操作： and(与), or(或), nand(非与), xor(非或) 一元运算符： !(取反) ()(复合表达式), !()(对复合表达式结果取反) 示例 filter { if [foo] in [foobar] { mutate { add_tag => \"field in field\" } } if [foo] in \"foo\" { mutate { add_tag => \"field in string\" } } if \"hello\" in [greeting] { mutate { add_tag => \"string in field\" } } if [foo] in [\"hello\", \"world\", \"foo\"] { mutate { add_tag => \"field in list\" } } if [missing] in [alsomissing] { mutate { add_tag => \"shouldnotexist\" } } if !(\"foo\" in [\"hello\", \"world\"]) { mutate { add_tag => \"shouldexist\" } } if [message] =~ /\\w+\\s+\\/\\w+(\\/learner\\/course\\/)/ { mutate { add_field => { \"learner_type\" => \"course\" } } } mutate { add_field => { \"show\" => \"This data will be in the output\" } } mutate { add_field => { \"[@metadata][test]\" => \"Hello\" } } mutate { add_field => { \"[@metadata][no_show]\" => \"This data will not be in the output\" } } } output { if \"_grokparsefailure\" not in [tags] { elasticsearch { ... } } if [@metadata][test] == \"Hello\" { stdout { codec => rubydebug } } if [loglevel] == \"ERROR\" and [deployment] == \"production\" { pagerduty { ... } } } 注意： 如果if[foo] in \"String\"在执行这样的语句时无法把该字段值转化成String类型。所以最好要加field if exist判断 if [\"foo\"] { mutate { add_field => \"bar\" => \"%{foo}\" } } 3. 引用event中的字段 直接引用字段，使用[]，嵌套字段使用多层[][]即可 { \"a\": \"1\", \"b\": \"2\", \"c\": { \"c1\": \"3\" } } ----------Pipeline中引用Event中的字段-------------- if [b] =~ \"2\" { .......... } if [c][c1] == \"3\" { ........... } 在字符串中以sprintf方式引用,使用%{} { \"a\": \"1\", \"b\": \"2\", \"c\": { \"c1\": \"3\" } } ----------Pipeline中引用Event中的字段-------------- add_field => { \"test\" => \"test: %{b}\" } add_field => { \"test\" => \"test: %{[c][c1]}\" } 六. Input插件 插件一览表 Plugin Description Github repository azure_event_hubs Receives events from Azure Event Hubs azure_event_hubs beats Receives events from the Elastic Beats framework logstash-input-beats cloudwatch Pulls events from the Amazon Web Services CloudWatch API logstash-input-cloudwatch couchdb_changes Streams events from CouchDB’s _changes URI logstash-input-couchdb_changes dead_letter_queue read events from Logstash’s dead letter queue logstash-input-dead_letter_queue elasticsearch Reads query results from an Elasticsearch cluster logstash-input-elasticsearch exec Captures the output of a shell command as an event logstash-input-exec file Streams events from files logstash-input-file ganglia Reads Ganglia packets over UDP logstash-input-ganglia gelf Reads GELF-format messages from Graylog2 as events logstash-input-gelf generator Generates random log events for test purposes logstash-input-generator github Reads events from a GitHub webhook logstash-input-github google_cloud_storage Extract events from files in a Google Cloud Storage bucket logstash-input-google_cloud_storage google_pubsub Consume events from a Google Cloud PubSub service logstash-input-google_pubsub graphite Reads metrics from the graphite tool logstash-input-graphite heartbeat Generates heartbeat events for testing logstash-input-heartbeat http Receives events over HTTP or HTTPS logstash-input-http http_poller Decodes the output of an HTTP API into events logstash-input-http_poller imap Reads mail from an IMAP server logstash-input-imap irc Reads events from an IRC server logstash-input-irc java_generator Generates synthetic log events core plugin java_stdin Reads events from standard input core plugin jdbc Creates events from JDBC data logstash-input-jdbc jms Reads events from a Jms Broker logstash-input-jms jmx Retrieves metrics from remote Java applications over JMX logstash-input-jmx kafka Reads events from a Kafka topic logstash-input-kafka kinesis Receives events through an AWS Kinesis stream logstash-input-kinesis log4j Reads events over a TCP socket from a Log4j SocketAppender object logstash-input-log4j lumberjack Receives events using the Lumberjack protocl logstash-input-lumberjack meetup Captures the output of command line tools as an event logstash-input-meetup pipe Streams events from a long-running command pipe logstash-input-pipe puppet_facter Receives facts from a Puppet server logstash-input-puppet_facter rabbitmq Pulls events from a RabbitMQ exchange logstash-input-rabbitmq redis Reads events from a Redis instance logstash-input-redis relp Receives RELP events over a TCP socket logstash-input-relp rss Captures the output of command line tools as an event logstash-input-rss s3 Streams events from files in a S3 bucket logstash-input-s3 salesforce Creates events based on a Salesforce SOQL query logstash-input-salesforce snmp Polls network devices using Simple Network Management Protocol (SNMP) logstash-input-snmp snmptrap Creates events based on SNMP trap messages logstash-input-snmptrap sqlite Creates events based on rows in an SQLite database logstash-input-sqlite sqs Pulls events from an Amazon Web Services Simple Queue Service queue logstash-input-sqs stdin Reads events from standard input logstash-input-stdin stomp Creates events received with the STOMP protocol logstash-input-stomp syslog Reads syslog messages as events logstash-input-syslog tcp Reads events from a TCP socket logstash-input-tcp twitter Reads events from the Twitter Streaming API logstash-input-twitter udp Reads events over UDP logstash-input-udp unix Reads events over a UNIX socket logstash-input-unix varnishlog Reads from the varnish cache shared memory log logstash-input-varnishlog websocket Reads events from a websocket logstash-input-websocket wmi Creates events based on the results of a WMI query logstash-input-wmi xmpp Receives events over the XMPP/Jabber protocol logstash-input-xmpp 插件通用配置项 参数 参数值类型 必须 默认值 详解 add_field hash No {} 向事件添加字段。 codec codec No plain 用于输入数据的编解码器，在输入数据之前，输入编解码器是一种方便的解码方法，不需要在你的Logstash管道中使用单独的过滤器 enable_metric boolean No true 禁用或启用这个特定插件实例的指标日志，默认情况下，我们记录所有我们可以记录的指标，但是你可以禁用特定插件的指标集合。 id string No 向插件配置添加唯一的ID，如果没有指定ID，则Logstash将生成一个，强烈建议在配置中设置此ID，当你有两个或多个相同类型的插件时，这一点特别有用。例如，如果你有两个log4j输入，在本例中添加一个命名ID将有助于在使用监视API时监视Logstash。input { kafka { id => \"my_plugin_id\" }} tags array No 向事件添加任意数量的标记，这有助于以后的处理。 type string No 向该输入处理的所有事件添加type字段，类型主要用于过滤器激活，该type作为事件本身的一部分存储，因此你也可以使用该类型在Kibana中搜索它。如果你试图在已经拥有一个type的事件上设置一个type（例如，当你将事件从发送者发送到索引器时），那么新的输入将不会覆盖现有的type，发送方的type集在其生命周期中始终与该事件保持一致，甚至在发送到另一个Logstash服务器时也是如此。 七. Filter插件 插件一览表 Plugin Description Github repository aggregate Aggregates information from several events originating with a single task logstash-filter-aggregate alter Performs general alterations to fields that the mutate filter does not handle logstash-filter-alter bytes Parses string representations of computer storage sizes, such as \"123 MB\" or \"5.6gb\", into their numeric value in bytes logstash-filter-bytes cidr Checks IP addresses against a list of network blocks logstash-filter-cidr cipher Applies or removes a cipher to an event logstash-filter-cipher clone Duplicates events logstash-filter-clone csv Parses comma-separated value data into individual fields logstash-filter-csv date Parses dates from fields to use as the Logstash timestamp for an event logstash-filter-date de_dot Computationally expensive filter that removes dots from a field name logstash-filter-de_dot dissect Extracts unstructured event data into fields using delimiters logstash-filter-dissect dns Performs a standard or reverse DNS lookup logstash-filter-dns drop Drops all events logstash-filter-drop elapsed Calculates the elapsed time between a pair of events logstash-filter-elapsed elasticsearch Copies fields from previous log events in Elasticsearch to current events logstash-filter-elasticsearch environment Stores environment variables as metadata sub-fields logstash-filter-environment extractnumbers Extracts numbers from a string logstash-filter-extractnumbers fingerprint Fingerprints fields by replacing values with a consistent hash logstash-filter-fingerprint geoip Adds geographical information about an IP address logstash-filter-geoip grok Parses unstructured event data into fields logstash-filter-grok http Provides integration with external web services/REST APIs logstash-filter-http i18n Removes special characters from a field logstash-filter-i18n java_uuid Generates a UUID and adds it to each processed event core plugin jdbc_static Enriches events with data pre-loaded from a remote database logstash-filter-jdbc_static jdbc_streaming Enrich events with your database data logstash-filter-jdbc_streaming json Parses JSON events logstash-filter-json json_encode Serializes a field to JSON logstash-filter-json_encode kv Parses key-value pairs logstash-filter-kv memcached Provides integration with external data in Memcached logstash-filter-memcached metricize Takes complex events containing a number of metrics and splits these up into multiple events, each holding a single metric logstash-filter-metricize metrics Aggregates metrics logstash-filter-metrics mutate Performs mutations on fields logstash-filter-mutate prune Prunes event data based on a list of fields to blacklist or whitelist logstash-filter-prune range Checks that specified fields stay within given size or length limits logstash-filter-range ruby Executes arbitrary Ruby code logstash-filter-ruby sleep Sleeps for a specified time span logstash-filter-sleep split Splits multi-line messages into distinct events logstash-filter-split syslog_pri Parses the PRI (priority) field of a syslog message logstash-filter-syslog_pri threats_classifier Enriches security logs with information about the attacker’s intent logstash-filter-threats_classifier throttle Throttles the number of events logstash-filter-throttle tld Replaces the contents of the default message field with whatever you specify in the configuration logstash-filter-tld translate Replaces field contents based on a hash or YAML file logstash-filter-translate truncate Truncates fields longer than a given length logstash-filter-truncate urldecode Decodes URL-encoded fields logstash-filter-urldecode useragent Parses user agent strings into fields logstash-filter-useragent uuid Adds a UUID to events logstash-filter-uuid xml Parses XML into fields logstash-filter-xml 插件通用配置项 Setting Input type Required add_field hash No add_tag array No enable_metric boolean No id string No periodic_flush boolean No remove_field array No remove_tag array No 八. Output插件 插件一览表 Plugin Description Github repository boundary Sends annotations to Boundary based on Logstash events logstash-output-boundary circonus Sends annotations to Circonus based on Logstash events logstash-output-circonus cloudwatch Aggregates and sends metric data to AWS CloudWatch logstash-output-cloudwatch csv Writes events to disk in a delimited format logstash-output-csv datadog Sends events to DataDogHQ based on Logstash events logstash-output-datadog datadog_metrics Sends metrics to DataDogHQ based on Logstash events logstash-output-datadog_metrics elastic_app_search Sends events to the Elastic App Search solution logstash-output-elastic_app_search elasticsearch Stores logs in Elasticsearch logstash-output-elasticsearch email Sends email to a specified address when output is received logstash-output-email exec Runs a command for a matching event logstash-output-exec file Writes events to files on disk logstash-output-file ganglia Writes metrics to Ganglia’s gmond logstash-output-ganglia gelf Generates GELF formatted output for Graylog2 logstash-output-gelf google_bigquery Writes events to Google BigQuery logstash-output-google_bigquery google_cloud_storage Uploads log events to Google Cloud Storage logstash-output-google_cloud_storage google_pubsub Uploads log events to Google Cloud Pubsub logstash-output-google_pubsub graphite Writes metrics to Graphite logstash-output-graphite graphtastic Sends metric data on Windows logstash-output-graphtastic http Sends events to a generic HTTP or HTTPS endpoint logstash-output-http influxdb Writes metrics to InfluxDB logstash-output-influxdb irc Writes events to IRC logstash-output-irc java_sink Discards any events received core plugin java_stdout Prints events to the STDOUT of the shell core plugin juggernaut Pushes messages to the Juggernaut websockets server logstash-output-juggernaut kafka Writes events to a Kafka topic logstash-output-kafka librato Sends metrics, annotations, and alerts to Librato based on Logstash events logstash-output-librato loggly Ships logs to Loggly logstash-output-loggly lumberjack Sends events using the lumberjack protocol logstash-output-lumberjack metriccatcher Writes metrics to MetricCatcher logstash-output-metriccatcher mongodb Writes events to MongoDB logstash-output-mongodb nagios Sends passive check results to Nagios logstash-output-nagios nagios_nsca Sends passive check results to Nagios using the NSCA protocol logstash-output-nagios_nsca opentsdb Writes metrics to OpenTSDB logstash-output-opentsdb pagerduty Sends notifications based on preconfigured services and escalation policies logstash-output-pagerduty pipe Pipes events to another program’s standard input logstash-output-pipe rabbitmq Pushes events to a RabbitMQ exchange logstash-output-rabbitmq redis Sends events to a Redis queue using the RPUSH command logstash-output-redis redmine Creates tickets using the Redmine API logstash-output-redmine riak Writes events to the Riak distributed key/value store logstash-output-riak riemann Sends metrics to Riemann logstash-output-riemann s3 Sends Logstash events to the Amazon Simple Storage Service logstash-output-s3 sns Sends events to Amazon’s Simple Notification Service logstash-output-sns solr_http Stores and indexes logs in Solr logstash-output-solr_http sqs Pushes events to an Amazon Web Services Simple Queue Service queue logstash-output-sqs statsd Sends metrics using the statsd network daemon logstash-output-statsd stdout Prints events to the standard output logstash-output-stdout stomp Writes events using the STOMP protocol logstash-output-stomp syslog Sends events to a syslog server logstash-output-syslog tcp Writes events over a TCP socket logstash-output-tcp timber Sends events to the Timber.io logging service logstash-output-timber udp Sends events over UDP logstash-output-udp webhdfs Sends Logstash events to HDFS using the webhdfs REST API logstash-output-webhdfs websocket Publishes messages to a websocket logstash-output-websocket xmpp Posts events over XMPP logstash-output-xmpp zabbix Sends events to a Zabbix server logstash-output-zabbix 插件通用配置项 Setting Input type Required codec codec No enable_metric boolean No id string No 九. Codec插件 插件一览表 Plugin Description Github repository avro Reads serialized Avro records as Logstash events logstash-codec-avro cef Reads the ArcSight Common Event Format (CEF). logstash-codec-cef cloudfront Reads AWS CloudFront reports logstash-codec-cloudfront cloudtrail Reads AWS CloudTrail log files logstash-codec-cloudtrail collectd Reads events from the collectd binary protocol using UDP. logstash-codec-collectd dots Sends 1 dot per event to stdout for performance tracking logstash-codec-dots edn Reads EDN format data logstash-codec-edn edn_lines Reads newline-delimited EDN format data logstash-codec-edn_lines es_bulk Reads the Elasticsearch bulk format into separate events, along with metadata logstash-codec-es_bulk fluent Reads the fluentd msgpack schema logstash-codec-fluent graphite Reads graphite formatted lines logstash-codec-graphite gzip_lines Reads gzip encoded content logstash-codec-gzip_lines jdots Renders each processed event as a dot core plugin java_line Encodes and decodes line-oriented text data core plugin java_plain Processes text data with no delimiters between events core plugin json Reads JSON formatted content, creating one event per element in a JSON array logstash-codec-json json_lines Reads newline-delimited JSON logstash-codec-json_lines line Reads line-oriented text data logstash-codec-line msgpack Reads MessagePack encoded content logstash-codec-msgpack multiline Merges multiline messages into a single event logstash-codec-multiline netflow Reads Netflow v5 and Netflow v9 data logstash-codec-netflow nmap Reads Nmap data in XML format logstash-codec-nmap plain Reads plaintext with no delimiting between events logstash-codec-plain protobuf Reads protobuf messages and converts to Logstash Events logstash-codec-protobuf rubydebug Applies the Ruby Awesome Print library to Logstash events logstash-codec-rubydebug 十. 插件管理 Logstash 插件是使用 Ruby开发的，Logstash 从很早的1.5.0+版开始，其插件模块和核心模块便分开维护，其插件使用的是 RubyGems包管理器来管理维护。所以 Logstash插件本质上就是自包含的RubyGems。 RubyGems（简称 gems）是一个用于对 Ruby组件进行打包的 Ruby 打包系统。 它提供一个分发 Ruby 程序和库的标准格式，还提供一个管理程序包安装的工具。 插件的名字格式：logstash-{input/output/filter}-插件名 示例：filter中的date插件：logstash-filter-date 1. 安装插件 #以安装dissect插件为例 /usr/share/logstash/bin/logstash-plugin install 插件名 #参数详解： --path.plugins 指定安装路径 2. 查看已安装的插件 /usr/share/logstash/bin/logstash-plugin list #参数详解： --verbose 查看插件的版本 --verbose 查看组（input, filter, codec, output）下面的所有插件。例如查看filter下的所有插件 3. 更新插件 #更新某个插件 /usr/share/logstash/bin/logstash-plugin update 插件名 #更新全部插件 /usr/share/logstash/bin/logstash-plugin update 4. 卸载插件 /usr/share/logstash/bin/logstash-plugin remove 插件名 5. 给插件管理器设置代理 export HTTP_PROXY=http://127.0.0.1:3128 6. 修改插件仓库地址 Logstash插件默认仓库地址是：http://rubygems.org 有一些开源的插件仓库： Geminabox：https://github.com/geminabox/geminabox Gemirro：https://github.com/PierreRambaud/gemirro Gemfury：https://gemfury.com/ Artifactory：http://www.jfrog.com/open-source/ 编辑/usr/share/logstash/Gemfile，将source \"https://rubygems.org\"改为source \"https://my.private.repository\" 十一. 其他操作 1、output-elasticsearch的template模板 PUT _template/logstash { \"order\" : 2, \"version\" : 60001, \"index_patterns\" : [ \"*\" ], \"settings\" : { \"index\" : { \"number_of_replicas\" : \"1\", \"number_of_shards\" : \"2\", \"refresh_interval\" : \"60s\" } }, \"mappings\" : { \"dynamic_templates\" : [ { \"message_field\" : { \"path_match\" : \"message\", \"mapping\" : { \"norms\" : false, \"type\" : \"text\" }, \"match_mapping_type\" : \"string\" } }, { \"string_fields\" : { \"mapping\" : { \"norms\" : false, \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"ignore_above\" : 256, \"type\" : \"keyword\" } } }, \"match_mapping_type\" : \"string\", \"match\" : \"*\" } } ], \"properties\" : { \"@timestamp\" : { \"type\" : \"date\" }, \"geoip\" : { \"dynamic\" : true, \"properties\" : { \"ip\" : { \"type\" : \"ip\" }, \"latitude\" : { \"type\" : \"half_float\" }, \"location\" : { \"type\" : \"geo_point\" }, \"longitude\" : { \"type\" : \"half_float\" } } }, \"@version\" : { \"type\" : \"keyword\" } } }, \"aliases\" : { } } 十二. 监控信息 1、查看pipeline运行监控信息 curl -XGET 'http://logstash实例地址:9600/_node/stats/pipelines/pipeline实例名?pretty' Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-11-16 10:26:20 "},"origin/logstash-filter-grok.html":{"url":"origin/logstash-filter-grok.html","title":"grok插件","keywords":"","body":"Logstash Grok插件 一、简介 Logstash提供了一系列filter过滤插件来处理收集到的log event，根据log event的特征去切分所需要的字段，方便kibana做visualize和dashboard的data analysis。 官方文档：https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html 插件Github：https://github.com/logstash-plugins/logstash-filter-grok 二、默认内置的匹配模式 Grok模块提供了默认内嵌了一些基本匹配模式。可使用以下方式查看支持的匹配模式 https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns http://grokdebug.herokuapp.com/patterns Logstash安装目录：/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns 内置的匹配模式 USERNAME [a-zA-Z0-9._-]+ USER %{USERNAME} EMAILLOCALPART [a-zA-Z][a-zA-Z0-9_.+-=:]+ EMAILADDRESS %{EMAILLOCALPART}@%{HOSTNAME} INT (?:[+-]?(?:[0-9]+)) BASE10NUM (?[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))) NUMBER (?:%{BASE10NUM}) BASE16NUM (?(?\"(?>\\\\.|[^\\\\\"]+)+\"|\"\"|(?>'(?>\\\\.|[^\\\\']+)+')|''|(?>`(?>\\\\.|[^\\\\`]+)+`)|``)) UUID [A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12} # URN, allowing use of RFC 2141 section 2.3 reserved characters URN urn:[0-9A-Za-z][0-9A-Za-z-]{0,31}:(?:%[0-9a-fA-F]{2}|[0-9A-Za-z()+,.:=@;$_!*'/?#-])+ # Networking MAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC}) CISCOMAC (?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4}) WINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2}) COMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2}) IPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)? IPV4 (?[A-Za-z]+:|\\\\)(?:\\\\[^\\\\?*]*)+ URIPROTO [A-Za-z]([A-Za-z0-9+\\-.]+)+ URIHOST %{IPORHOST}(?::%{POSINT:port})? # uripath comes loosely from RFC1738, but mostly from what Firefox # doesn't turn into %XX URIPATH (?:/[A-Za-z0-9$.+!*'(){},~:;=@#%&_\\-]*)+ #URIPARAM \\?(?:[A-Za-z0-9]+(?:=(?:[^&]*))?(?:&(?:[A-Za-z0-9]+(?:=(?:[^&]*))?)?)*)? URIPARAM \\?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\\-\\[\\]<>]* URIPATHPARAM %{URIPATH}(?:%{URIPARAM})? URI %{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})? # Months: January, Feb, 3, 03, 12, December MONTH \\b(?:[Jj]an(?:uary|uar)?|[Ff]eb(?:ruary|ruar)?|[Mm](?:a|ä)?r(?:ch|z)?|[Aa]pr(?:il)?|[Mm]a(?:y|i)?|[Jj]un(?:e|i)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ep(?:tember)?|[Oo](?:c|k)?t(?:ober)?|[Nn]ov(?:ember)?|[Dd]e(?:c|z)(?:ember)?)\\b MONTHNUM (?:0?[1-9]|1[0-2]) MONTHNUM2 (?:0[1-9]|1[0-2]) MONTHDAY (?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]) # Days: Monday, Tue, Thu, etc... DAY (?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?) # Years? YEAR (?>\\d\\d){1,2} HOUR (?:2[0123]|[01]?[0-9]) MINUTE (?:[0-5][0-9]) # '60' is a leap second in most time standards and thus is valid. SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?) TIME (?! HTTPDATE %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT} # Shortcuts QS %{QUOTEDSTRING} # Log formats SYSLOGBASE %{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}: # Log Levels LOGLEVEL ([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?) 使用语法 %{SYNTAX:SEMANTIC} # %{内置的匹配模式:存储该值的变量字段名:数值类型} # 例如 %{NUMBER:row_id:int} # 如：3会被NUBER模式所匹配 三、自定义的匹配模式 方式一：直接在pipeline中定义使用 直接在pipeline中自定义匹配模式的语法规则 (?the pattern here) 示例 filter { grok { patterns_dir => [\"./patterns\"] match => { \"message\" => \"%{IP:client_id_address} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:http_response_time} (?[0-9A-F]{10,11})}\" } } } 方式二：创建自定义pattern文件 创建文件./patterns/postfix POSTFIX_QUEUEID [0-9A-F]{10,11} 在pipeline中使用 filter { grok { patterns_dir => [\"./patterns\"] match => { \"message\" => \"%{IP:client_id_address} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:http_response_time} %{POSTFIX_QUEUEID:queue_id}\" } } } 四、Debug http://grokdebug.herokuapp.com/ Kibana中的开发工具 五、示例 1、Filebeat采集多行MySQL慢查询日志到ES MySQL(5.6)慢查询日志 # Time: 200317 17:29:17 # User@Host: test[test] @ [192.168.1.1] Id: 5 # Query_time: 9.717266 Lock_time: 0.000167 Rows_sent: 3 Rows_examined: 101693 use test; SET timestamp=1584437357; SELECT * FROM test WHERE name like 'aaa' ORDER BY id DESC LIMIT 1000; # Time: 200317 17:32:08 # User@Host: test[test] @ [192.168.10.2] Id: 97 # Query_time: 4.375731 Lock_time: 0.000151 Rows_sent: 25 Rows_examined: 6049071 SET timestamp=1584437528; select * from test where uid='35001' limit 100; Filebeat配置 filebeat.inputs: - type: log paths: - /data/mysql/log/slow.log exclude_files: [\"_filebeat\", \".gz$\"] multiline.pattern: '^# Time:' multiline.negate: true multiline.match: after multiline.max_lines: 20000 Filebeat采集发到logstash的日志格式 { \"@timestamp\": \"2020-03-18T02:52:57.139Z\", \"@metadata\":{ \"beat\": \"filebeat\", \"type\": \"_doc\", \"version\": \"7.5.1\" }, \"host\":{ \"name\": \"test.mysql\" }, \"log\": { \"offset\": 525, \"file\": { \"path\": \"/data/mysql/logs/slow.log\" }, \"flags\": [\"multiline\"] }, \"message\": \"# Time: 200317 17:29:17\\n# User@Host: test[test] @ [192.168.1.1] Id: 5\\n# Query_time: 9.717266 Lock_time: 0.000167 Rows_sent: 3 Rows_examined: 101693\\nuse test;\\nSET timestamp=1584437357;\\nSELECT * FROM test WHERE name like 'aaa' ORDER BY id DESC LIMIT 1000;\" } Logstash filter grok配置 filter { grok { match => { \"message\" => \"(?m)^#\\s*Time:.*\\s+#\\s+User@Host:\\s+%{USER:user}\\[[^\\]]+\\]\\s+@\\s+(?:(?\\S*) )?\\[(?:%{IPV4:clientip})?\\]\\s+Id:\\s+%{NUMBER:row_id:int}\\n#\\s+Query_time:\\s+%{NUMBER:Query_time:float}\\s+Lock_time:\\s+%{NUMBER:lock_time:float}\\s+Rows_sent:\\s+%{NUMBER:Row_sent:int}\\s+Rows_examined:\\s+%{NUMBER:Rows_examined:int}\\n\\s*(?:use %{DATA:database};\\s*\\n)?SET\\s+timestamp=%{NUMBER:timestamp};\\n\\s*(?(?\\w+)\\b.*)$\" } remove_field => [ \"message\" ] } date { match => [ \"timestamp\", \"UNIX\" ] remove_field => [ \"timestamp\" ] } } ES存储的数据格式 { \"_index\": \"mysql-slowlog-2020-03-17\", \"_type\": \"_doc\", \"_id\": \"6nNo6HABfk0PUyuv\", \"_version\": 1, \"_score\": null, \"_source\": { \"lock_time\": 0.000167, \"action\": \"SELECT\", \"user\": \"test\", \"Row_sent\": 3, \"database\": \"test\", \"fields\": { \"env\": \"test\" }, \"Rows_examined\": 101693, \"sql\": \"select * from test where uid='35001' limit 100\", \"@timestamp\": \"2020-03-17T09:29:17.000Z\", \"row_id\": 5, \"host\": { \"name\": \"test.mysql\" }, \"clientip\": \"192.168.1.1\", \"tags\": [ \"mysql-slow-log\", \"beats_input_codec_plain_applied\" ], \"Query_time\": 9.717266, \"@version\": \"1\", \"log\": { \"file\": { \"path\": \"/data/mysql/logs/slow.log\" }, \"flags\": [ \"multiline\" ], \"offset\": 525 } }, \"fields\": { \"@timestamp\": [ \"2020-03-17T09:29:17.000Z\" ] }, \"sort\": [ 1584437357000 ] } 参考 https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html https://www.cnblogs.com/toSeek/p/6117845.html https://www.jianshu.com/p/49ae54a411b8 https://blog.csdn.net/qq_21989939/article/details/79524640 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/logstash-filter-summary.html":{"url":"origin/logstash-filter-summary.html","title":"Logastash常用filter实现的功能","keywords":"","body":"Logstash常用filter实现的功能 1、截取带有文件路径字段中的文件名 filter{ grok { match => { \"[log][file][path]\" => \"%{GREEDYDATA}/%{GREEDYDATA:app}-access.log\" } } } 2、删除json字段 filter{ mutate { remove_field => [ \"@timestamp\" , \"headers\" , \"response.data\"] gsub => [\"message\", \"\\\\\\\", \"\"] } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/elasticsearch-基础知识.html":{"url":"origin/elasticsearch-基础知识.html","title":"基础知识","keywords":"","body":"Elasticsearch基础知识 Document 文档 Elasticsearch 是面向Document文档的，文档是所有可搜索数据的最小单位 文档会被序列化JSON格式，保存在Elasticsearch中 JSON对象由字段组成 每个字段都有对应的字段类型 (字符串/数值/布尔/日期/二进制/范围类型) 每个文档都有一个Unique ID 手动指定ID 通过Elasticsearch自动生成 一个文档由Meta Data元数据与Source Data原始数据组成 { \"_index\": \"***\", # 文档所存在的索引名 \"_type\": \"_doc\", # 文档所属的类型名 \"_id\": \"***\", # 文档的唯一ID \"_version\": 1, # 文档的版本信息 \"_score\": null, # 文档相关性打分 \"_source\": { .... }, # 文档的原始JSON数据 \"fields\": { \"***\": [ \"***\" ] }, # 额外添加的字段 \"sort\": [ 1575256044058 ] # 排序 } Index 索引 索引，即一系列documents的集合。 Shard 分片 分片是独立的，对于一个Search Request的行为，每个分片都会执行这个Request。 分片分为两种类型：主分片（Primary Shard）和副本分片（Replica Shard） 主分片：用以解决数据水平扩展的问题，通过主分片，可以将数据分布到集群内的所有节点上(主从复制) 主分片在索引创建时指定，后续不允许修改，除非reindex 一个分片是一个运行的Lucene实例，Integer.MAX_VALUE - 128 = 2,147,483,519 个docs。 副本分片：用于解决数据高可用的问题，是主分片的拷贝（可以提高读吞吐量） 副本分片数，可动态调整 主分片和备分片不会出现在同一个节点上（防止单点故障 集群节点类型 一个节点就是一个ElasticSearch的实例，本质上就是一个Java进程。 每个节点都有名字，通过配置文件，或者启动时候 -E node.name = node1指定 每一个节点在启动之后，会分配一个UID，保存在data目录下 生产环境中一个节点应该设置单一的角色（意味着节点可以多角色） 节点类型 配置参数 默认值 作用 备注 master eligible node.master true 每个节点启动后，默认就是一个Master eligible节点（可以设置node.master:false 禁止）,Master-eligible节点可以参与选主流程，成为Master节点每个节点上都保存了集群的状态信息(所有节点信息，所有的索引和其相关的Mapping和Setting信息，分片路由信息)，只有Master节点可以修改集群状态信息 可以参加选主 data node.data true 当第一个节点启动，它会将自己选举成Master节点保存包含索引文档的分片数据，执行CRUD、搜索、聚合相关的操作。属于内存、CPU、IO密集型，对硬件资源要求高。 存储数据 ingest node.ingest True ingest节点可以运行一些pipeline的脚本 Coordinating 无 负责接收Client请求，将请求分发到合适的节点，最终把结果汇聚在一起返回给客户端。每个节点默认都起到了Coordinating Node的职责 每个节点默认都是coordinating节点，设置其他类型全部为false machine learning node.ml true(需要enable x-pack) 机器学习 集群状态 ES集群状态有三种： Green：所有主分片和备份分片都准备就绪（分配成功），即使有一台机器挂了（假设一台机器一个实例），数据都不会丢失，但会变成Yellow状态 Yellow：所有主分片准备就绪，但存在至少一个主分片（假设是A）对应的备份分片没有就绪，此时集群属于警告状态，意味着集群高可用和容灾能力下降，如果刚好A所在的机器挂了，并且你只设置了一个备份（已处于未就绪状态），那么A的数据就会丢失（查询结果不完整），此时集群进入Red状态 Red：：至少有一个主分片没有就绪（直接原因是找不到对应的备份分片成为新的主分片）,此时查询的结果会出现数据丢失（不完整） Elasticsearch的写入请求 Elasticsearch的写入请求主要包括：index、create、update、delete、bulk。bulk是实现对前四种的批量操作。 在6.x版本以后实际上走的都是bulk接口了。 create/index是直接新增doc，delete是直接根据_id删除doc。 ES的任意节点都可以作为协调节点(coordinating node)接受请求，当协调节点接受到请求后进行一系列处理，然后通过_routing字段找到对应的primary shard，并将请求转发给primary shard, primary shard完成写入后，将写入并发发送给各replica， raplica执行写入操作后返回给primary shard， primary shard再将请求返回给协调节点 Elasticsearch写入过程 Elasticsearch中每个index由多个shard组成，默认是5个，每个shard分布在不同的机器上。shard分为主分片和副本分片。 ​ 红色：Client Node（客户端节点）绿色：Primary Node（主分片节点）蓝色：Replica Node（副本分片节点） Elasticsearch索引过程 Elasticsearch搜索过程 Elasticsearch的准实时 Elasticsearch的核心优势就是近乎实时，为什么说是近乎实时而非真实意义上的实时呢，因为Elasticsearch能够做到准实时，而并不是完全的实时。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-07-27 18:11:45 "},"origin/elasticsearch--_cat-API.html":{"url":"origin/elasticsearch--_cat-API.html","title":"_cat","keywords":"","body":"Elasticsearch _cat APIs 官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/cat.html 查看_cat API支持的所有Endpoint GET /_cat curl -XGET http://127.0.0.1:9200/_cat /_cat/allocation /_cat/shards /_cat/shards/{index} /_cat/master /_cat/nodes /_cat/tasks /_cat/indices /_cat/indices/{index} /_cat/segments /_cat/segments/{index} /_cat/count /_cat/count/{index} /_cat/recovery /_cat/recovery/{index} /_cat/health /_cat/pending_tasks /_cat/aliases /_cat/aliases/{alias} /_cat/thread_pool /_cat/thread_pool/{thread_pools} /_cat/plugins /_cat/fielddata /_cat/fielddata/{fields} /_cat/nodeattrs /_cat/repositories /_cat/snapshots/{repository} /_cat/templates 查询Endpoint参数 GET /_cat/health?help curl -XGET \"http://127.0.0.1:9200/_cat/health?help\" # 参数全称 | 参数缩写 | 参数详解 ---------------------------------------------------------------------------------------------------- epoch | t,time | seconds since 1970-01-01 00:00:00 timestamp | ts,hms,hhmmss | time in HH:MM:SS cluster | cl | cluster name status | st | health status node.total | nt,nodeTotal | total number of nodes node.data | nd,nodeData | number of nodes that can store data shards | t,sh,shards.total,shardsTotal | total number of shards pri | p,shards.primary,shardsPrimary | number of primary shards relo | r,shards.relocating,shardsRelocating | number of relocating nodes init | i,shards.initializing,shardsInitializing | number of initializing nodes unassign | u,shards.unassigned,shardsUnassigned | number of unassigned shards pending_tasks | pt,pendingTasks | number of pending tasks max_task_wait_time | mtwt,maxTaskWaitTime | wait time of longest task pending active_shards_percent | asp,activeShardsPercent | active number of shards in percent 使用参数控制查询条件 GET /_cat/health?h=st,t #带表头 GET /_cat/health?v&h=st,t 控制查询的输出排序 GET _cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date # 查询出来的Index将会以store.size的大小降序输出。只输出Index名，store.size大小，创建时间戳 curl -XGET \"http://elasticsearch-service.logger.svc:9200/_cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date\" 控制查询的输出格式 GET _cat/indices?v&h=index,store.size,creation.date&s=store.size:desc,creation.date&format=yaml yaml - index: \"test-test-2019.05.21\" store.size: \"4.1gb\" creation.date: \"1558432572904\" - index: \".monitoring-es-7-2019.06.17\" store.size: \"1.2gb\" creation.date: \"1560729605158\" json [ { \"index\" : \"test-test-2019.05.21\", \"store.size\" : \"4.1gb\", \"creation.date\" : \"1558432572904\" }, { \"index\" : \".monitoring-es-7-2019.06.17\", \"store.size\" : \"1.2gb\", \"creation.date\" : \"1560729605158\" } ] text (default) index store.size creation.date test-test-2019.05.21 4.1gb 1558432572904 .monitoring-es-7-2019.06.17 1.2gb 1560729605158 cbor smile Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-11-16 14:38:11 "},"origin/elasticsearch-index-api.html":{"url":"origin/elasticsearch-index-api.html","title":"index","keywords":"","body":"Elasticsearch的Index API 一、DDL 数据定义(索引的创建与删除) 数据定义语言：Data Definition Language 1. 创建索引 PUT /index_name?pretty ======================================================== curl -sk -u username:userpassword -XPUT \"http://localhost:9200/index_name?pretty\" 创建索引时设置参数 curl -sk -u username:userpassword -XPUT \"http://localhost:9200/index_name?pretty\" \\ -H 'Content-Type: application/json' -d' { \"settings\" : { \"number_of_replicas\" : 0 } }' 2. 删除Index DELET /index_name ======================================================== curl -sk -u username:userpassword -XDELETE \"http://127.0.0.1:9200/index_name\" 二、DCL 数据控制(索引的配置) 数据控制语言：Data Control Language 1. 查看索引的设置 GET /index_name/_settings ======================================================== curl -sk -u username:userpassword \"http://127.0.0.1:9200/index_name/_settings\" 2. 查看索引的Mapping GET /index_name/_mapping ======================================================== curl -sk -u username:userpassword \"http://127.0.0.1:9200/index_name/_mapping\" 3. 设置索引Mapping PUT /index_name { \"mappings\": { \"index_name\": { \"dynamic\":\"false\", \"properties\": { \"id\": { \"type\": \"long\" }, \"prd_id\": { \"type\": \"long\" }, \"mer_id\": { \"type\": \"long\" }, \"data_status\": { \"type\": \"text\" }, \"datachange_createtime\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" }, \"datachange_lasttime\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" } } } } } ======================================================== curl -sk -u username:userpassword -XPUT \"http://127.0.0.1:9200/index_name\" -H 'Content-Type: application/json' -d' { \"mappings\": { \"index_name\": { \"dynamic\":\"false\", \"properties\": { \"id\": { \"type\": \"long\" }, \"prd_id\": { \"type\": \"long\" }, \"mer_id\": { \"type\": \"long\" }, \"data_status\": { \"type\": \"text\" }, \"datachange_createtime\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" }, \"datachange_lasttime\": { \"type\": \"date\", \"format\": \"strict_date_optional_time||epoch_millis\" } } } } }' 三、DML 数据操作(文档的增、删、改) 数据操作语言：Data Manipulation Language 1. 向索引中插入一个文档 向索引中插入一个ID为1的文档 PUT /index_name/_doc/1 { \"name\": \"test\" } ======================================================== curl -sk -u username:userpassword \\ -XPUT \"http://localhost:9200/index_name/_doc/1\" \\ -H 'Content-Type: application/json' \\ -d'{ \"name\": \"test\" }' 2. 向索引中批量插入文档 详见Elasticsearch索引文档批量操作 3. 更新指定文档 PUT /index_name/_doc/1?pretty { \"doc\": {\"name\": \"test1\"} } ======================================================== curl -sk -u username:userpassword \\ -XPUT \"http://localhost:9200/index_name/_doc/1\" \\ -H 'Content-Type: application/json' \\ -d ' { \"doc\": {\"name\": \"test1\"} } ' 4. 指定文档新增字段 PUT /index_name/_doc/1?pretty { \"doc\": {\"name\": \"test1\"，\"new_field\": \"testN\"} } ======================================================== curl -sk -u username:userpassword \\ -XPUT \"http://localhost:9200/index_name/_doc/1\" \\ -H 'Content-Type: application/json' \\ -d ' { \"doc\": {\"name\": \"test1\"，\"new_field\": \"testN\"} } ' 5. 删除文档 curl -sk -u username:userpassword \\ -XPOST \"http://localhost:9200/index_name/_delete_by_query\" \\ -H 'Content-Type: application/json' \\ -d ' { \"query\":{ \"term\":{ \"name\" : \"test1\" } } }' 三、DQL 数据查询 (文档的查询) 数据查询语言：Data Query Language 1. 查询索引中的所有文档 只显示前十条 GET /index_name/_search?pretty ======================================================== curl -sk -u username:userpassword \"http://localhost:9200/index_name/_search?pretty\" 2. 查询_id为1的文档 GET /index_name/_doc/1?pretty ======================================================== curl -sk -u username:userpassword \"http://localhost:9200/index_name/_doc/1?pretty\" 3. 查询_id为1的文档的元数据 GET index_name/_doc/1/_source ======================================================== curl -sk -u username:userpassword \"http://localhost:9200/index_name/_doc/1/_source?pretty\" 4. 查询符合指定条件的文档 GET /index_name/_search?q=name:test1 ======================================================== curl -sk -u username:userpassword \"http://localhost:9200/index_name/_search?q=name:test1\" 5. 复杂查询 GET /employee/_doc/_search { \"query\" : { \"bool\": { \"must\": { \"match\" : { \"last_name\" : \"smith\" } }, \"filter\": { \"range\" : { \"age\" : { \"gt\" : 30 } } } } } } #这条语句翻译成sql：select * from employee where last_name='smith' and age > 40 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-01-04 11:39:22 "},"origin/elasticsearch-bulk-api.html":{"url":"origin/elasticsearch-bulk-api.html","title":"bulk","keywords":"","body":"Elasticsearch索引文档的批量操作API：_bulk 一、简介 官方文档 1. API请求URL格式 POST /_bulk { \"index动作\": { \"_index\" : \"索引名\", \"_id\" : \"文档ID\" } }{ \"字段名\" : \"字段值\" } { \"delete动作\": { \"_index\" : \"索引名\", \"_id\" : \"文档ID\" } } { \"create动作\": { \"_index\" : \"索引名\", \"_id\" : \"文档ID\" } }{ \"字段名\" : \"字段值\" } { \"update动作\": { \"_index\" : \"索引名\", \"_id\" : \"文档ID\" } }{ \"doc\" : { \"字段名\" : \"字段值\" } } POST /索引名/_bulk {\"index\":{\"_id\":\"文档ID\"}}{ \"字段名\": \"字段值\" } {\"index\":{\"_id\":\"文档ID\"}}{ \"字段名\": \"字段值\" } 2. 支持的文档操作动作 index 如果索引中已经存在具有相同名称的文档，则创建失败，索引将根据需要添加或替换文档 create 如果索引中已经存在具有相同名称的文档，则创建失败，索引将根据需要添加或替换文档 delete 不期望下一行有文档数据。具有与标准delete API相同的语义 update 期望在下一行中指定部分文档、upsert和脚本及其选项 3. 将文档操作数据存储在文本 文本格式 动作及元数据\\n 数据\\n 动作及元数据\\n 数据\\n .... 动作及元数据\\n 数据\\n 例如操作数据文本test.json数据如下： {\"index\": {\"_index\": \"test\", \"_type\": \"_doc\", \"_id\": 1}} {\"doc\": {\"name\": \"test1\"}} {\"index\": {\"_index\": \"test\", \"_type\": \"_doc\", \"_id\": 2}} {\"doc\": {\"name\": \"test2\"}} ======================================================================== {\"index\":{\"_id\":\"1\"}} { \"name\": \"test1\" } {\"index\":{\"_id\":\"2\"}} { \"name\": \"test2\" } {\"index\":{\"_id\":\"3\"}} { \"name\": \"test3\" } 操作API的Curl命令 curl -X POST \"localhost:9200/_bulk\" -H 'Content-Type: application/json' --data-binary @test.json ======================================================================== curl -X POST \"localhost:9200/test/_bulk\" -H 'Content-Type: application/json' --data-binary @test.json 4. 注意事项 批量操作的响应可能是很大的JSON数据，其中包含执行的每个操作的结果，显示的顺序与请求中出现的操作顺序相同。单个操作的失败不会影响其余操作。 批量操作的响应中没有标识操作成功的计数字段 二、API请求的参数 三、Update动作的参数 doc (partial document) upsert doc_as_upsert script params (for script) lang (for script) _source POST _bulk { \"update\" : {\"_id\" : \"1\", \"_index\" : \"index1\", \"retry_on_conflict\" : 3} } { \"doc\" : {\"field\" : \"value\"} } { \"update\" : { \"_id\" : \"0\", \"_index\" : \"index1\", \"retry_on_conflict\" : 3} } { \"script\" : { \"source\": \"ctx._source.counter += params.param1\", \"lang\" : \"painless\", \"params\" : {\"param1\" : 1}}, \"upsert\" : {\"counter\" : 1}} { \"update\" : {\"_id\" : \"2\", \"_index\" : \"index1\", \"retry_on_conflict\" : 3} } { \"doc\" : {\"field\" : \"value\"}, \"doc_as_upsert\" : true } { \"update\" : {\"_id\" : \"3\", \"_index\" : \"index1\", \"_source\" : true} } { \"doc\" : {\"field\" : \"value\"} } { \"update\" : {\"_id\" : \"4\", \"_index\" : \"index1\"} } { \"doc\" : {\"field\" : \"value\"}, \"_source\": true} 四、操作示例 1. 向指定索引批量插入文档 Kibana Dev Tools Console POST _bulk { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } }{ \"name\" : \"test1\" } { \"index\" : { \"_index\" : \"test\", \"_id\" : \"2\" } }{ \"name\" : \"test2\" } { \"index\" : { \"_index\" : \"test\", \"_id\" : \"3\" } }{ \"name\" : \"test3\" } ======================================================================== POST /test/_bulk {\"index\":{\"_id\":\"1\"}}{ \"name\": \"test1\" } {\"index\":{\"_id\":\"2\"}}{ \"name\": \"test2\" } {\"index\":{\"_id\":\"3\"}}{ \"name\": \"test3\" } Curl命令 curl -XPOST \"http://localhost:9200/_bulk\" \\ -H 'Content-Type: application/json' \\ -d ' { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } }{ \"name\" : \"test1\" } { \"index\" : { \"_index\" : \"test\", \"_id\" : \"2\" } }{ \"name\" : \"test2\" } { \"index\" : { \"_index\" : \"test\", \"_id\" : \"3\" } }{ \"name\" : \"test3\" } ' ======================================================================== curl -XPOST \"http://localhost:9200/test/_bulk\" \\ -H 'Content-Type: application/json' \\ -d ' {\"index\":{\"_id\":\"1\"}}{ \"name\": \"test1\" } {\"index\":{\"_id\":\"2\"}}{ \"name\": \"test2\" } {\"index\":{\"_id\":\"3\"}}{ \"name\": \"test3\" } ' 2. 针对索引文档进行批量操作 Kibana Dev Tools Console POST _bulk { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } }{ \"field1\" : \"value1\" } { \"delete\" : { \"_index\" : \"test\", \"_id\" : \"2\" } } { \"create\" : { \"_index\" : \"test\", \"_id\" : \"3\" } }{ \"field1\" : \"value3\" } { \"update\" : { \"_index\" : \"test\", \"_id\" : \"1\" } }{ \"doc\" : { \"field2\" : \"value2\"} } Curl命令 curl -X POST \"localhost:9200/_bulk?pretty\" \\ -H 'Content-Type: application/json' \\ -d ' { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } } { \"field1\" : \"value1\" } { \"delete\" : { \"_index\" : \"test\", \"_id\" : \"2\" } } { \"create\" : { \"_index\" : \"test\", \"_id\" : \"3\" } } { \"field1\" : \"value3\" } { \"update\" : { \"_index\" : \"test\", \"_id\" : \"1\" } } { \"doc\" : {\"field2\" : \"value2\"} } ' Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/elasticsearch-ingest节点.html":{"url":"origin/elasticsearch-ingest节点.html","title":"Ingest节点的Pipeline","keywords":"","body":"ElasticSearch的Ingest角色节点 一、简介 Elasticsearch集群中的每一个节点有着各自的角色，不同的功能，共同保证集群存储分片、分词索引、聚合搜索等功能。 Master节点：负责集群相关的操作，例如创建或删除索引，跟踪哪些节点是集群的一部分，以及决定将哪些分片分配给哪些节点。 拥有稳定的主节点是衡量集群健康的重要标志。 Data节点：保存包含索引文档的分片数据，执行CRUD、搜索、聚合相关的操作。属于内存、CPU、IO密集型，对硬件资源要求高。 Coordinating节点： 每一个节点都默认设置为了协调节点。 搜索请求或大容量索引请求可能涉及不同数据节点上的数据。例如，搜索请求是分两个阶段执行的，由接收客户端请求的节点(即协调节点)进行协调。在分散阶段，协调节点将请求转发给持有数据的数据节点。每个数据节点在本地执行请求并将结果返回给协调节点。在收集阶段，协调节点将每个数据节点的结果简化为单个全局结果集。 Ingest节点：可以看作是数据前置处理转换的节点。在实际的文档索引发生之前，Ingest节点会拦截批量和索引请求，然后使用ingest Pipeline对文档进行过滤、转换等数据转换预处理操作，然后将文档传递回索引或批量API。类似于 logstash 中 filter 的作用。 Ingest是5.X版本就有的特性 Ingest节点是通过包含多个processor的pipeline对文档进行预处理操作，processor是实际处理数据的插件。 默认情况下，所有节点都启用Ingest角色，因此任何节点都可以处理Ingest任务 可以创建专用的Ingest节点 要禁用节点的Ingest功能，需要在elasticsearch.yml 设置\"node.ingest：false\" 二、Ingest Pipeline与Logstash Filter Logstash处理数据的流程：logstash在pipeline filter中设置不同的插件对从Input传过来的数据进行加工处理，再输出带output中。 Easticsearch Ingest Pipeline节点处理数据的流程：Ingest Pipeline是Ingest节点上用于 Logstash Filter Ingest Pipeline 支持的数据源 大量的输入和输出插件（比如：kafka，redis等）可供使用 不能从外部来源（例如消息队列或数据库）提取数据，必须批量bulk或索引index请求将数据推送到 Elasticsearch 应对数据激增的能力不同 Logstash 可在本地对数据进行缓冲以应对采集骤升情况。Logstash 支持与大量不同的消息队列类型进行集成。 极限情况下会出现：在长时间无法联系上 Elasticsearch 或者 Elasticsearch 无法接受数据的情况下，均有可能会丢失数据。 处理能力不同 支持的插件和功能点较Ingest节点多很多。 支持为数不多处理器操作。Ingest节点管道只能在单一事件的上下文中运行。Ingest通常不能调用其他系统或者从磁盘中读取数据。 排他式功能支持不同 支持采集附件处理器插件，此插件可用来处理和索引常见格式（例如 PPT、XLS 和 PDF）的附件。 不支持如上文件附件类型。 三、Ingest Pipeline 1. Ingest Pipeline的定义及使用 Ingest Pipeline中每个processor实现了对文档的某种转换，如移除某个字段，重命名某个字段等操作。pipeline定义语法格式如下： PUT _ingest/pipeline/my-pipeline-id { \"description\" : \"...\", # Pipeline功能描述(必须，string类型) \"version\" : 123, # 用于管理ingest pipeline的版本号(可选，Integer类型) \"processors\" : [ ... ] # 指定1个或多个processor(必须，数组类型) } 要使用某个pipeline，只需要在请求中简单的指定pipeline的id就可以了： PUT my-index/_doc/doc_id?pipeline=my_pipeline_id { \"a\": \"b\", \"foo\": \"bar\" } 2. Ingest Pipeline的管理API ① Put 添加或更新Pipeline PUT /_ingest/pipeline/my-pipeline-id { \"description\" : \"describe pipeline\", \"version\" : 123, \"processors\" : [ { \"set\" : { \"field\": \"foo\", \"value\": \"bar\" } } ] } ② Get 查看指定的Pipeline GET _ingest/pipeline/my-pipeline-id 查看Pipeline的指定参数，例如查看Pipeline的版本号字段 GET /_ingest/pipeline/my-pipeline-id?filter_path=*.version ③ Delete 删除指定Pipeline DELETE /_ingest/pipeline/my-pipeline-id 删除模糊匹配的Pipeline DELETE /_ingest/pipeline/pipeline-* 删除所有Pipeline DELETE /_ingest/pipeline/* ④ 模拟测试 调用Ingest pipeline对指定的文档进行模拟测试。可以指定一个现有的Ingest pipeline来对提供的文档进行模拟测试，也可以在请求体中提供Ingest pipeline定义。 POST _ingest/pipeline/_simulate { \"pipeline\": { \"description\": \"template\", \"processors\": [ { \"set\": { \"field\": \"\", \"value\": \" \" } }, { \"set\": { \"field\": \"time\", \"value\": \"\" } } ] }, \"docs\": [ { \"_index\": \"simulate_test\", \"_source\": { \"name\": \"kyle\", \"age\": 18, \"birth\": \"1993-09-01\" } }, { \"_index\": \"simulate_test\", \"_source\": { \"name\": \"reason\", \"age\": 20, \"birth\": \"1990-02-03\" } } ] } 模拟测试调用已经创建的Ingest Pipeline POST /_ingest/pipeline/my-pipeline-id/_simulate { \"docs\": [ { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"foo\": \"bar\" } }, { \"_index\": \"index\", \"_id\": \"id\", \"_source\": { \"foo\": \"rab\" } } ] } docs(必须, 数组)字段支持的参数: _index：(可选, string类型) 包含文档的索引名 _id：(可选, string类型) 文档的唯一标识 _source：(必须, json对象) 文档的JSON数据 3. Index Setting设置索引默认Ingest Pipeline 可在Index Setting中设置“index.default_pipeline”参数指定默认Ingest Pipeline。如果Index Setting设置了默认Ingest Pipeline,但Ingest Pipeline不存在，索引请求将会失败。参数值设为_none则表示不使用Ingest Pipeline进行文档预处理 PUT test/_settings { \"number_of_replicas\": 0, \"index\":{ \"analysis.analyzer.default.type\":\"ik_max_word\", \"analysis.search_analyzer.default.type\":\"ik_smart\", \"default_pipeline\": \"my-pipeline-id\" } } 四、Ingest Pipeline中的Processors Processor的配置格式如下 { \"PROCESSOR_NAME\" : { ... processor configuration options ... } } 所有Processors支持以下通用参数 tag：只是Pipeline中特定Processors实例化的字符串标识符。tag字段不影响处理Processors的行为，但是对于特定Processors的记录和跟踪错误非常有用。 on_failure：用于设置Pipeline或Processor中的异常处理。详情见Ingest Pipeline的异常处理 if：设置判断条件来决定Processors是否处理符合条件的文档。详情见Processor中的条件判断 1. Processor获取、处理文档中的字段数据 获取文档_source 原始数据中的字段 { \"set\": { \"field\": \"my_field\", \"value\": 582.1 } } # 或者 { \"set\": { \"field\": \"_source.my_field\", \"value\": 582.1 } } 获取文档Metadata元数据中的字段 Processor可直接处理文档Metadata元数据中_index,_type,_id,_routing Elasticsearch不允许原始数据_source中的字段与Metadata元数据中的字段相同 { \"set\": { \"field\": \"_id\", \"value\": \"1\" } } 获取Ingest的元数据字段 除了文档Metadata元数据和_source原始数据中的字段外，Processor可以在文档处理过程中添加与Ingest相关的元数据。Ingest元数据是暂时的，在文档被管道处理之后就会丢失，因此不会被索引。 例如ingest会在在_ingest下添加了ingest时间戳，用于标识对文档进行预处理的时间，获取方式如下： # 该示例添加了一个名称为received的字段。该值是es收到index 或 bulk 请求预处理文档的时间。 { \"set\": { \"field\": \"received\", \"value\": \"\" } } 2. Processor中的条件判断 Ingest pipeline的processor支持if判断来决定是否处理指定条件的文档。if字段必须包含返回布尔值的脚本。如果脚本的计算结果为true，那么将为给定的文档执行Processor，否则将跳过它。 Ingest pipeline processor中的if判断语句会被解释为elasticsearch官方支持的“Painless script”格式脚本 if字段使用脚本选项中定义的脚本字段作为对象，并通过脚本处理程序中脚本使用的相同的ctx变量访问文档的只读版本。 ① 在判断条件中获取文档中的嵌套字段 在文档中原始数据_source下有大量的嵌套JSON数据，那如何在Processor中的条件获取中嵌套较深的字段数据呢？可使用“a.b.c”这种形式获取。 如果原始数据中没有a.b存在，条件语句会抛出“NullPointerExceptions”的异常，可在Processor的条件判断引用字段时使用“?.” PUT _ingest/pipeline/drop_guests_network { \"processors\": [ { \"drop\": { \"if\": \"ctx.network?.name == 'Guest'\" } } ] } ② 复杂的条件判断 例如可以在drop processor中，判断原始数据某个数组类型的字段中是否包含\"prod\"特殊字符 PUT _ingest/pipeline/not_prod_dropper { \"processors\": [ { \"drop\": { \"if\": \"\"\" Collection tags = ctx.tags; if(tags != null){ for (String tag : tags) { if (tag.toLowerCase().contains('prod')) { return false; } } } return true; \"\"\" } } ] } # 以下文档会被丢弃 POST test/_doc/1?pipeline=not_prod_dropper { \"tags\": [\"application:myapp\", \"env:Stage\"] } # 以下文档不会被丢弃 POST test/_doc/2?pipeline=not_prod_dropper { \"tags\": [\"application:myapp\", \"env:Production\"] } ③ 判断条件的正则表达式 如果要在if条件中使用正则表达式，需要在elasticsearch.yml中设置script.painless.regex.enabled: true PUT _ingest/pipeline/check_url { \"processors\": [ { \"set\": { \"if\": \"ctx.href?.url =~ /^http[^s]/\", \"field\": \"href.insecure\", \"value\": true } } ] } PUT _ingest/pipeline/check_url { \"processors\": [ { \"set\": { \"if\": \"ctx.href?.url != null && ctx.href.url.startsWith('http://')\", \"field\": \"href.insecure\", \"value\": true } } ] } ④ Pipeline Processor中的条件判断 可在Pipeline Processor中设置判断条件来决定是否调用其他Pipeline PUT _ingest/pipeline/logs_pipeline { \"description\": \"A pipeline of pipelines for log files\", \"version\": 1, \"processors\": [ { \"pipeline\": { \"if\": \"ctx.service?.name == 'apache_httpd'\", \"name\": \"httpd_pipeline\" } }, { \"pipeline\": { \"if\": \"ctx.service?.name == 'syslog'\", \"name\": \"syslog_pipeline\" } }, { \"fail\": { \"message\": \"This pipeline requires service.name to be either `syslog` or `apache_httpd`\" } } ] } 3. 内置的Processors Append Processor Bytes Processor Circle Processor Convert Processor Date Processor Date Index Name Processor Dissect Processor Dot Expander Processor Drop Processor Fail Processor Foreach Processor GeoIP Processor Grok Processor Gsub Processor HTML Strip Processor Join Processor JSON Processor KV Processor Lowercase Processor Pipeline Processor Remove Processor Rename Processor Script Processor Set Processor Set Security User Processor Split Processor Sort Processor Trim Processor Uppercase Processor URL Decode Processor User Agent processor 4. 自定义processors 自定义的processors必须让所有elasticsearch节点都要安装，在elasticsearch.yml中添加“plugin.mandatory：ingest-attachment” 五、Ingest Pipeline的异常处理 针对一些比较复杂的Pipeline，其中可能定义了多个Processor进行文档处理，而这些Processor是按照顺序执行，如果在执行过程中一个遇到了异常，后续processor将不会执行，这是不可取的。 可以在pipeline或processor语法块中使用on_failure参数进行异常捕获。 如果在processor语法块指定了on_failure配置，不管它是否为空，processor抛出的任何异常都会被捕获，而Pipeline将继续执行其他的processor。 因为可以在on_failure语句的范围内定义更多的处理器，所以可以嵌套失败处理。 同时也可以设置\"on_failure\": true进行忽略异常，而不做任何处理 { \"description\" : \"my first pipeline with handled exceptions\", \"processors\" : [ { \"rename\" : { \"field\" : \"foo\", \"target_field\" : \"bar\", \"ignore_failure\" : true } } ] } 以下Ingest Pipeline在rename processor中设置了当文档中没有指定字段\"foo\"时，会在异常处理参数中使用set processor添加\"error\"字段 { \"description\" : \"my first pipeline with handled exceptions\", \"processors\" : [ { \"rename\" : { \"field\" : \"foo\", \"target_field\" : \"bar\", \"on_failure\" : [ { \"set\" : { \"field\" : \"error\", \"value\" : \"field \\\"foo\\\" does not exist, cannot rename to \\\"bar\\\"\" } } ] } } ] } 以下Ingest Pipeline在全局定义块中设置了当匹pipeline其中processor处理抛出异常，整个pipeline出错时，会在异常处理参数中使用set processor添加\"_index\"字段 { \"description\" : \"my first pipeline with handled exceptions\", \"processors\" : [ ... ], \"on_failure\" : [ { \"set\" : { \"field\" : \"_index\", \"value\" : \"failed-\" } } ] } 六、Filebeat Modules模块的Ingest Pipeline 以Filebeat Nginx模块处理访问日志的Ingest PIpeline为例，文件路径：/usr/share/filebeat/module/nginx/access/ingest/default.json { \"description\": \"Pipeline for parsing Nginx access logs. Requires the geoip and user_agent plugins.\", \"processors\": [ { \"grok\": { \"field\": \"message\", \"patterns\": [ \"\\\"?(?:%{IP_LIST:nginx.access.remote_ip_list}|%{DATA:source.address}) - %{DATA:user.name} \\\\[%{HTTPDATE:nginx.access.time}\\\\] \\\"%{DATA:nginx.access.info}\\\" %{NUMBER:http.response.status_code:long} %{NUMBER:http.response.body.bytes:long} \\\"%{DATA:http.request.referrer}\\\" \\\"%{DATA:user_agent.original}\\\"\" ], \"pattern_definitions\": { \"IP_LIST\": \"%{IP}(\\\"?,?\\\\s*%{IP})*\" }, \"ignore_missing\": true } }, { \"grok\": { \"field\": \"nginx.access.info\", \"patterns\": [ \"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\", \"\" ], \"ignore_missing\": true } }, { \"remove\": { \"field\": \"nginx.access.info\" } }, { \"split\": { \"field\": \"nginx.access.remote_ip_list\", \"separator\": \"\\\"?,?\\\\s+\", \"ignore_missing\": true } }, { \"split\": { \"field\": \"nginx.access.origin\", \"separator\": \"\\\"?,?\\\\s+\", \"ignore_missing\": true } }, { \"set\": { \"field\": \"source.ip\", \"value\": \"\" } }, { \"script\": { \"lang\": \"painless\", \"source\": \"boolean isPrivate(def dot, def ip) { try { StringTokenizer tok = new StringTokenizer(ip, dot); int firstByte = Integer.parseInt(tok.nextToken()); int secondByte = Integer.parseInt(tok.nextToken()); if (firstByte == 10) { return true; } if (firstByte == 192 && secondByte == 168) { return true; } if (firstByte == 172 && secondByte >= 16 && secondByte 七、logstash对接Ingest Pipeline input { beats { port => 5044 id => \"log-logstash-input-beat\" } } filter { ...省略... } output { if \"app-log\" in [tags] { elasticsearch { id => \"logstash-app-log-json\" hosts => [\"http://localhost:9200\"] index => \"%{[app]}-%{+YYYY-MM-dd}\" # 此处指定使用的ingest Pipeline pipeline => \"ingest pipeline的名字\" http_compression => true user => \"elastic\" password => \"***elastic用户名的密码***\" } } 参考文档 https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html https://blog.csdn.net/laoyang360/article/details/93376355 https://www.elastic.co/guide/en/logstash/current/use-ingest-pipelines.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-08 18:03:33 "},"origin/elasticsearch-数据的分配路由.html":{"url":"origin/elasticsearch-数据的分配路由.html","title":"数据的路由分配","keywords":"","body":"Elasticsearch数据的分配路由 一、简介 二、索引分片的路由分配 2、给节点打上标签 elasticsearch.yml node.attr.size: medium 或启动命令 ./bin/elasticsearch -Enode.attr.size=medium 3. PUT test/_settings { \"index.routing.allocation.include.size\": \"big\", \"index.routing.allocation.include.rack\": \"rack1\" } 4. ](https://github.com/elastic/elasticsearch/edit/7.5/docs/reference/index-modules/allocation/filtering.asciidoc) index.routing.allocation.include.{attribute}`** Assign the index to a node whose {attribute} has at least one of the comma-separated values. index.routing.allocation.require.{attribute} Assign the index to a node whose {attribute} has all of the comma-separated values. index.routing.allocation.exclude.{attribute} Assign the index to a node whose {attribute} has none of the comma-separated values. 内置的attribute： _name Match nodes by node name _host_ip Match nodes by host IP address (IP associated with hostname) _publish_ip Match nodes by publish IP address _ip Match either _host_ip or _publish_ip _host Match nodes by hostname _id Match nodes by node id PUT test/_settings { \"index.routing.allocation.include._ip\": \"192.168.2.*\" } 5. PUT loginmac-201905/_settings { \"index\": { \"routing\": { \"allocation\": { \"require\": { \"box_type\": \"warm\" } } } } } 6. POST /_cluster/reroute { \"commands\": [ { \"move\": { \"index\": \"loginmac-201905\", \"shard\": 2, \"from_node\": \"node-248\", \"to_node\": \"node-12\" } } ] } 参考 https://www.elastic.co/guide/en/elasticsearch/reference/7.5/shard-allocation-filtering.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/es-thread-pool.html":{"url":"origin/es-thread-pool.html","title":"es进程池","keywords":"","body":"ElasticSearch的进程池 一、简介 每个Elasticsearch节点内部都维护着多个线程池，每一类型的操作都被分配于不同的线程池 二、线程池类型 fixed 有着固定个数的线程池，个数由size参数指定。允许你指定一个队列（大小使用queue_size属性指定，默认是-1，即无限制）用来保存请求，直到有一个空闲的线程来执行请求。如果Elasticsearch无法把请求放到队列中（队列满了），该请求将被拒绝。 threadpool.write.size: 8 threadpool.write.queue_size: 100 scaling 动态个数的线程池，个数与工作负载成比例，在core参数与max参数之间浮动，同样keep_alive参数指定了闲置线程被回收的时间。 threadpool.warmer.core: 1 threadpool.warmer.max: 8 threadpool.warmer.keep_alive: 2m fixed_auto_queue_size 此功能是试验性的，在将来的版本中可能会完全更改或删除。 Elastic会尽力解决所有问题，但是实验性功能不受官方GA功能的支持SLA约束。不推荐使用[7.7.0，不推荐使用实验性fixed_auto_queue_size线程池类型，该类型将在8.0中删除。 固定个数但大小浮动的线程池，个数由size参数指定，大小在min_queue_size与max_queue_size之间浮动 thread_pool.search.queue_size: 500 #queue_size允许控制没有线程执行它们的挂起请求队列的初始大小。 thread_pool.search.size: 200 #size参数控制线程数，默认为核心数乘以5。 thread_pool.search.min_queue_size:10 #min_queue_size设置控制queue_size可以调整到的最小量。 thread_pool.search.max_queue_size: 1000 #max_queue_size设置控制queue_size可以调整到的最大量。 thread_pool.search.auto_queue_frame_size: 2000 #auto_queue_frame_size设置控制在调整队列之前进行测量的操作数。它应该足够大，以便单个操作不会过度偏向计算。 thread_pool.search.target_response_time: 6s #target_response_time是时间值设置，指示线程池队列中任务的目标平均响应时间。如果任务通常超过此时间，则将调低线程池队列以拒绝任务。 三、核心线程池 线程 类型 作用 默认配置 generic scaling 用于一些通用操作，如node discovery search fixed_auto_queue_size 索引的count/search/sugges操作 size = int((可用cpu核心数*3)/2)+ 1queue = 1000 search_throttled fixed_auto_queue_size search_throttled类型索引的count/search/suggest/get操作 size =1queue = 100 write fixed 对单个文档的index/delete/update操作和bulk批量插入操作 size = 可用cpu核心数(最多多1个)queue = 1000 get fixed get操作 size = 可用cpu核心数queue = 1000 analyze fixed analyze操作 size = 1queue = 16 snapshot scaling 对索引的snapshot/restore操作 keep-alive of 5m and a max of min(5, (可用cpu核心数)/2). system_write fixed 系统索引的写操作 maximum size of min(5, (可用cpu核心数/ 2) system_read fixed 系统索引的读操作 maximum size of min(5, (可用cpu核心数/ 2) refresh scaling refresh操作 keep-alive of 5m and a max of min(10, (可用cpu核心数)/2). flush scaling flush，synced flush，translog fsync等操作 keep-alive of 5m and a max of min(5, (可用cpu核心数)/2). force_merge fixed force merge操作 size = 1 ，队列没有大小限制 fetch_shard_store scaling 监控分片的存储 keep-alive of 5m and 最大大小为(2*可用cpu核心数) fetch_shard_started scaling 监控分片的状态 keep-alive of 5m and 最大大小为(2*可用cpu核心数) listener scaling 设置为true时，主要用于为Java客户端执行动作 max of min(10, (可用cpu核心数) / 2) 从ElasticSearch5.0 开始，无法通过api更改线程池的配置，需要更改elasticsearch.yml并重启才能生效配置 四、线程信息 1、查询节点上的热点线程 API接口 GET /_nodes/hot_threads # 查询指定节点上的热点进程 GET /_nodes/节点ID/hot_threads 接口参数 ignore_idle_threads（可选，布尔值，默认为true） 如果为true，则会过滤掉已知的空闲线程（例如，在套接字选择中等待，或从空队列中获取任务） interval：（可选，时间单位，默认为500ms） 执行热点线程的采样间隔 snapshots：（可选，整数，默认为10） 它是要获取的堆栈跟踪（在特定时间点嵌套的方法调用序列）数量 threads（可选，整数，默认为3。也就是返回TOP 3 热点线程。） 查看由type参数确定的信息，ElasticSearch将采用指定数量的最“热门”线程。 master_timeout：（可选，时间单位，默认为30s） 指定等待连接到主节点的时间段。如果在超时到期之前未收到任何响应，则请求将失败并返回错误。 timeout：（可选，时间单位，默认为30s） 指定等待响应的时间段。如果在超时到期之前未收到任何响应，则请求将失败并返回错误。 type：（可选，字符串，默认cpu） 要采样的类型。可用的选项是：block线程阻塞状态的时间；cpu线程占据CPU时间；wait 线程等待状态的时间。 返回信息样本 ::: {log-node1}{w_AbxWEDSqa11saW1WXjEQ}{M2p9u125R_2wiVznDV2-ug}{192.168.1.6}{192.168.1.6:9300}{dilm}{ml.machine_memory=16656637952, rack=r1, xpack.installed=true, ml.max_open_jobs=20} Hot threads at 2020-12-15T09:48:50.840Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true: 2.9% (14.3ms out of 500ms) cpu usage by thread 'elasticsearch[log-node1][refresh][T#1]' 8/10 snapshots sharing following 2 elements java.base@13.0.1/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) java.base@13.0.1/java.lang.Thread.run(Thread.java:830) 2.4% (12.1ms out of 500ms) cpu usage by thread 'elasticsearch[log-node1][refresh][T#2]' 8/10 snapshots sharing following 2 elements java.base@13.0.1/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) java.base@13.0.1/java.lang.Thread.run(Thread.java:830) ::: {log-node2}{fllcX9yABXqLYabqgUi7Bw}{YJmD534TlykN_xDx11duA}{192.168.1.7}{192.168.1.7:9300}{dilm}{ml.machine_memory=33566380032, rack=r1, ml.max_open_jobs=20, xpack.installed=true} Hot threads at 2020-12-15T09:48:50.840Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true: 3.4% (17.1ms out of 500ms) cpu usage by thread 'elasticsearch[log-node2][refresh][T#3]' 8/10 snapshots sharing following 2 elements java.base@13.0.1/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) java.base@13.0.1/java.lang.Thread.run(Thread.java:830) 2、查询节点上的线程详细信息 GET /_nodes/thread_pool { \"_nodes\" : { \"total\" : 2, \"successful\" : 2, \"failed\" : 0 }, \"cluster_name\" : \"log\", \"nodes\" : { \"fllcX9yf455LYabqgUi7Bw\" : { \"name\" : \"log-node2\", \"transport_address\" : \"192.168.1.7:9300\", \"host\" : \"192.168.1.7\", \"ip\" : \"192.168.1.7\", \"version\" : \"7.5.1\", \"build_flavor\" : \"default\", \"build_type\" : \"rpm\", \"build_hash\" : \"3ae9ac9a93c9512551cf95d88e1e18d96\", \"roles\" : [ \"ingest\", \"master\", \"data\", \"ml\" ], \"attributes\" : { \"ml.machine_memory\" : \"33566380032\", \"rack\" : \"r1\", \"ml.max_open_jobs\" : \"20\", \"xpack.installed\" : \"true\" }, \"thread_pool\" : { \"watcher\" : { \"type\" : \"fixed\", \"size\" : 40, \"queue_size\" : 1000 }, \"force_merge\" : { \"type\" : \"fixed\", \"size\" : 1, \"queue_size\" : -1 }, # .....省略..... } } } } 参考 https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html#fixed-auto-queue-size Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-08 13:59:06 "},"origin/elasticsearch-benchmarks.html":{"url":"origin/elasticsearch-benchmarks.html","title":"elasticsearch性能测试","keywords":"","body":"ElasticSearch性能测试esrally 一、简介 官方文档：https://esrally.readthedocs.io/en/stable/ GitHub：https://github.com/elastic/rally 二、安装配置esrally 1、安装esrally pip3 install esrally brew install pbzip2 2、配置 三、命令详解 1、命令格式 esrally [-h] [--version] {race,list,info,create-track,generate,compare,download,install,start,stop} 可选参数: -h, --help show this help message and exit --version show program's version number and exit 子命令: {race,list,info,create-track,generate,compare,download,install,start,stop} race Run a benchmark list List configuration options info Show info about a track create-track Create a Rally track from existing data generate Generate artifacts compare Compare two races download Downloads an artifact install Installs an Elasticsearch node locally start Starts an Elasticsearch node locally stop Stops an Elasticsearch node locally Find out more about Rally at https://esrally.readthedocs.io/en/2.2.0/ 2、子命令 ①列出内置的测试数据 $ esrally list tracks 测试数据 测试数据描述 文档个数 压缩后大小 未压缩大小 Default Challenge All Challenges geonames POIs from Geonames 11,396,503 252.9 MB 3.3 GB append-no-conflicts append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts,append-fast-with-conflicts,significant-text percolator Percolator benchmark based on AOL queries 2,000,000 121.1 kB 104.9 MB append-no-conflicts append-no-conflicts http_logs HTTP server log data 247,249,096 1.2 GB 31.1 GB append-no-conflicts append-no-conflicts,runtime-fields,append-no-conflicts-index-only,append-sorted-no-conflicts,append-index-only-with-ingest-pipeline,update,append-no-conflicts-index-reindex-only geoshape Shapes from PlanetOSM 60,523,283 13.4 GB 45.4 GB append-no-conflicts append-no-conflicts metricbeat Metricbeat data 1,079,600 87.7 MB 1.2 GB append-no-conflicts append-no-conflicts geopoint Point coordinates from PlanetOSM 60,844,404 482.1 MB 2.3 GB append-no-conflicts append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts nyc_taxis Taxi rides in New York in 2015 165,346,692 4.5 GB 74.3 GB append-no-conflicts append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts-index-only,update,append-ml,date-histogram geopointshape Point coordinates from PlanetOSM indexed as geoshapes 60,844,404 470.8 MB 2.6 GB append-no-conflicts append-no-conflicts,append-no-conflicts-index-only,append-fast-with-conflicts so Indexing benchmark using up to questions and answers from StackOverflow 36,062,278 8.9 GB 33.1 GB append-no-conflicts append-no-conflicts eventdata This benchmark indexes HTTP access logs generated based sample logs from the elastic.co website using the generator available in https://github.com/elastic/rally-eventdata-track 20,000,000 756.0 MB 15.3 GB append-no-conflicts append-no-conflicts,transform eql EQL benchmarks based on endgame index of SIEM demo cluster 60,782,211 4.5 GB 109.2 GB default default nested StackOverflow Q&A stored as nested docs 11,203,029 663.3 MB 3.4 GB nested-search-challenge nested-search-challenge,index-only noaa Global daily weather measurements from NOAA 33,659,481 949.4 MB 9.0 GB append-no-conflicts append-no-conflicts,append-no-conflicts-index-only,top_metrics,aggs pmc Full text benchmark with academic papers from PMC 574,199 5.5 GB 21.7 GB append-no-conflicts append-no-conflicts,append-no-conflicts-index-only,append-sorted-no-conflicts,append-fast-with-conflicts ②创建测试实例 esrally create-track \\ --track=http_logs \\ --target-hosts=127.0.0.1:9200 \\ --client-options=\"timeout:60,basic_auth_user:'elastic',basic_auth_password:'*****'\" \\ --indices=\"products,companies\" \\ --output-path=~/tracks track.json contains the actual Rally track. For details see the track reference. companies.json and products.json contain the mapping and settings for the extracted indices. *-documents.json(.bz2) contains the sources of all the documents from the extracted indices. The files suffixed with -1k contain a smaller version of the document corpus to support test mode. ③ ④ ⑤ ⑥ ⑦ ⑧ ⑨ 三、性能测试 1、安装esrally pip3 install esrally 2、创建测试任务和数据 esrally create-track \\ --track=http_logs \\ --target-hosts=127.0.0.1:9200 \\ --client-options=\"timeout:60,basic_auth_user:'elastic',basic_auth_password:'*****\" \\ --indices=\"products,companies\" \\ --output-path=~/tracks 3、 esrally race --distribution-version=6.0.0 --track=geopoint --challenge=append-fast-with-conflicts 三、 esrally list tracks esrally list races esrally create-track \\ --track=http_logs \\ --target-hosts=127.0.0.1:9200 \\ --client-options=\"timeout:60,basic_auth_user:'elastic',basic_auth_password:'*****'\" \\ --indices=\"products,companies\" \\ --output-path=~/tracks esrally race \\ --target-hosts=127.0.0.1:9200 \\ --client-options=\"timeout:60,basic_auth_user:'elastic',basic_auth_password:'*****'\" \\ --track=geonames Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/es-template.html":{"url":"origin/es-template.html","title":"es template模板","keywords":"","body":"ElasticSearch模板 一、简介 索引模板是预先定义好的在创建新索引时自动应用的模板，主要包括索引设置、映射和模板优先级等配置。 二、模板管理 1、查看模板 GET /_template/template_ # 通过模糊匹配得到多个模板信息 GET /_template/template* # 批量查看模板 GET /_template/template_1,template_2 2、创建模板 PUT /_template/template_1 { \"template\" : \"*\", \"order\" : 0, \"settings\" : { \"number_of_shards\" : 1 }, \"version\": 123 } 3、删除模板 DELETE /_template/template_1 三、模板详解 template大致分成setting和mappings两部分： settings：作用于index的一些相关配置信息，如分片数、副本数，tranlog同步条件、refresh等。 mappings: 主要是一些说明信息，大致又分为_all、_source、prpperties这三部分： all：主要指的是AllField字段，我们可以将一个或多个都包含进来，在进行检索时无需指定字段的情况下检索多个字段。设置“all\" : {\"enabled\" : true} source：主要指的是SourceField字段，Source可以理解为ES除了将数据保存在索引文件中，另外还有一份源数据。_source字段在我们进行检索时相当重要，如果在{\"enabled\" : false}情况下默认检索只会返回ID， 你需要通过Fields字段去到索引中去取数据，效率不是很高。但是enabled设置为true时，索引会比较大，这时可以通过Compress进行压缩和inclueds、excludes来在字段级别上进行一些限制，自定义哪些字段允许存储。 properties：这是最重要的步骤，主要针对索引结构和字段级别上的一些设置。 咱们通常在elasticsearch中 post mapping信息，每重新创建索引便到设置mapping，分片，副本信息。非常繁琐。强烈建议大家通过设置template方式设置索引信息。设置索引名，通过正则匹配的方式匹配到相应的模板。 直接修改mapping的优先级>索引template。索引匹配了多个template，当属性等配置出现不一致的，以order的最大值为准，order默认值为0 多个模板同时匹配，以order顺序倒排，order越大，优先级越高** 四、常用操作 1、调整主分片个数 对于数据规模较小、索引个数较多的场景，建议调小主分片个数，以减轻索引元数据对堆内存的压力 { \"index_patterns\" : [\"*\"], \"order\" : 2, // 请确保模板中 order 字段的值大于1 \"settings\" : { \"index\": { \"number_of_shards\" : 1 } } } 2、调整refresh时间 希望让索引的文档在10s之后就能被搜索到，并应用于所有的 search-* 索引 { \"index_patterns\" : [\"search-*\"], \"order\" : 2, // 请确保模板中 order 字段的值大于1 \"settings\" : { \"index\": { \"refresh_interval\": \"10s\" } } } 3、调整字段类型 在默认模板中，我们将 string 类型字段动态映射为 keyword 类型，以防止对所有文本类型数据都进行全文索引。您可以根据业务需求，修改指定 string 类型字段为 text，使其可以全文索引。 { \"index_patterns\" : [\"*\"], \"order\" : 2, // 请确保模板中 order 字段的值大于1 \"mappings\": { \"properties\": { \"字段名\": { \"type\": \"text\" } } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-06-10 18:36:43 "},"origin/es-plugin-chinese-ik.html":{"url":"origin/es-plugin-chinese-ik.html","title":"插件：中文分词器ik","keywords":"","body":"ElasticSearch的中文分词器 一、简介 elasticsearch官方默认的分词插件，对中文分词效果不理想。 ik 带有两个分词器 ik_max_word ：会将文本做最细粒度的拆分；尽可能多的拆分出词语。比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合； ik_smart：会做最粗粒度的拆分；已被分出的词语将不会再次被其它词语占有。比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”。 Ik插件Github: https://github.com/medcl/elasticsearch-analysis-ik 二、安装 在线安装（所有ES节点都要安装） ES_JAVA_OPTS=\"-Dhttp.proxyHost=100.129.58.13 -Dhttp.proxyPort=3128 -Dhttps.proxyHost=100.129.58.13 -Dhttps.proxyPort=3128\" /usr/share/elasticsearch/bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip 重启ES集群（一个节点一个节点地重启，直到重启的节点上分片已平衡，保证服务不中断） 三、分词验证 1、创建Index curl -XPUT http://localhost:9200/index 2、创建Mapping时指定分词器 curl -XPOST http://localhost:9200/index/fulltext/_mapping -H 'Content-Type:application/json' -d' { \"properties\": { \"content\": { \"type\": \"text\", \"analyzer\": \"ik_max_word\", \"search_analyzer\": \"ik_max_word\" } } }' 3、插入数据Doc curl -XPOST http://localhost:9200/index/fulltext/1 -H 'Content-Type:application/json' -d' {\"content\":\"美国留给伊拉克的是个烂摊子吗\"} ' curl -XPOST http://localhost:9200/index/fulltext/2 -H 'Content-Type:application/json' -d' {\"content\":\"公安部：各地校车将享最高路权\"} ' curl -XPOST http://localhost:9200/index/fulltext/3 -H 'Content-Type:application/json' -d' {\"content\":\"中韩渔警冲突调查：韩警平均每天扣1艘中国渔船\"} ' curl -XPOST http://localhost:9200/index/fulltext/4 -H 'Content-Type:application/json' -d' {\"content\":\"中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首\"} ' 4、中文关键词查询 curl -XPOST http://localhost:9200/index/fulltext/_search -H 'Content-Type:application/json' -d' { \"query\" : { \"match\" : { \"content\" : \"中国\" }}, \"highlight\" : { \"pre_tags\" : [\"\", \"\"], \"post_tags\" : [\"\", \"\"], \"fields\" : { \"content\" : {} } } } ' 5、对比结果 { \"took\": 14, \"timed_out\": false, \"_shards\": { \"total\": 5, \"successful\": 5, \"failed\": 0 }, \"hits\": { \"total\": 2, \"max_score\": 2, \"hits\": [ { \"_index\": \"index\", \"_type\": \"fulltext\", \"_id\": \"4\", \"_score\": 2, \"_source\": { \"content\": \"中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首\" }, \"highlight\": { \"content\": [ \"中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首 \" ] } }, { \"_index\": \"index\", \"_type\": \"fulltext\", \"_id\": \"3\", \"_score\": 2, \"_source\": { \"content\": \"中韩渔警冲突调查：韩警平均每天扣1艘中国渔船\" }, \"highlight\": { \"content\": [ \"均每天扣1艘中国渔船 \" ] } } ] } } 四、配置IK分词插件的热词更新 修改 IK 的配置文件IKAnalyzer.cfg.xml ：{conf}/analysis-ik/config/IKAnalyzer.cfg.xml 或者 {plugins}/elasticsearch-analysis-ik-*/config/IKAnalyzer.cfg.xml IK Analyzer 扩展配置 custom/mydict.dic;custom/single_word_low_freq.dic custom/ext_stopword.dic location http://xxx.com/xxx.dic 其中 location 是指一个 url，比如 http://yoursite.com/getCustomDict，该请求只需满足以下两点即可完成分词热更新。 该 http 请求需要返回两个头部(header)，一个是 Last-Modified，一个是 ETag，这两者都是字符串类型，只要有一个发生变化，该插件就会去抓取新的分词进而更新词库。 该 http 请求返回的内容格式是一行一个分词，换行符用 \\n 即可。 满足上面两点要求就可以实现热更新分词了，不需要重启 ES 实例。 可以将需自动更新的热词放在一个 UTF-8 编码的 .txt 文件里，放在 nginx 或其他简易 http server 下，当 .txt 文件修改时，http server 会在客户端请求该文件时自动返回相应的 Last-Modified 和 ETag。可以另外做一个工具来从业务系统提取相关词汇，并更新这个 .txt 文件。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-26 17:17:19 "},"origin/elasticsearch-7.1的xpack权限控制.html":{"url":"origin/elasticsearch-7.1的xpack权限控制.html","title":"Xpack","keywords":"","body":"一、Context 之前ELK套装安装X-Pack的安全功能时，只有安装30天的试用许可证时间，以允许访问所有功能。 当许可证到期时，X-Pack将以降级模式运行。可以购买订阅以继续使用X-Pack组件的全部功能（https://www.elastic.co/subscriptions）。但是,最近官方从6.8.0和7.1.0开始免费提供安全功能. 本次实验,所有ELK组件版本均为7.1.0,以容器单节点运行 二. Elasticsearch开启Xpack elasticsearch的容器化部署参考笔记: ElasticSearch的容器化部署.md 配置参数可以通过环境变量的方式注入,主要的几个环境变量参数 xpack.monitoring.collection.enabled(开启自我监控): true path.repo(设置snapshot存储仓库的路径): /usr/share/elasticsearch/snapshots-repository discovery.type(设置当前节点为单节点模式): single-node cluster.name(设置elasticsearch的集群名): curiouser bootstrap.memory_lock: 'true' TZ(设置时区): Asia/Shanghai ES_JAVA_OPTS(设置elasticsearch的JVM堆栈大小): '-Xms1g -Xmx2g' ELASTIC_USERNAME: \"kibana\" ELASTIC_PASSWORD: \"kibana\" xpack.security.enabled: 'true' xpack.security.transport.ssl.enabled: \"true\" xpack.security.transport.ssl.verification_mode: \"certificate\" xpack.security.transport.ssl.keystore.path: \"/usr/share/elasticsearch/config/certs/elastic-certificates.p12\" xpack.security.transport.ssl.truststore.path: \"/usr/share/elasticsearch/config/certs/elastic-certificates.p12\" xpack.security.http.ssl.enabled: \"false\" 查看elasticsearch是否开启xpack的安全验证 curl -XGET 'localhost:9200/_cat/health?v&pretty' # curl -XGET \"http://127.0.0.1:9200/_cat/health?v&pretty\" # 使用上述命令会返回401,提示未授权验证,使用以下命令进行安全验证地访问 curl --user kibana:****kibana用户的密码**** -XGET 'localhost:9200/_cat/health?v&pretty' 三、Kibana开启Xpack kibana的容器化部署详见笔记: Kibana的容器化部署.md 配置参数可以通过环境变量的方式注入,主要的几个环境变量参数: ELASTICSEARCH_USERNAME: kibana用户 ELASTICSEARCH_PASSWORD: kibana用户的随机密码 TZ(设置时区): Asia/Shanghai 镜像中默认指定的elasticsearch地址为:http://elasticsearch:9200,刚好在open shift中部署的elasticsearch的svc名为\"elasticsearch\",它的访问方式为:http://elasticsearch:9200或者http://elasticsearch.命名空间.svc:9200 登录Kibana进行验证 使用elastic 超级用户进行登录，密码来自 setup-passwords 命令输出的结果 四、Logstash开启Xpack 配置logstash发送监控数据到elasticsearch xpack.monitoring.elasticsearch.hosts: \"http://elasticsearch:9200\" xpack.monitoring.enabled: \"true\" xpack.monitoring.elasticsearch.username: \"logstash_system\" xpack.monitoring.elasticsearch.password: \"***logstash_system用户的密码****\" 在kibana中查看logstash的监控数据 在kibana中创建logstash-pipeline角色,授予\"manage_index_template\",\"monitor\"的集群权限和\"write\",\"delete\",\"create_index\",\"manage_ilm\",\"manage\"的Index权限,然后绑定到logstash-pipeline用户上,用以创建Index并向其中写入数据 在pipeline的elasticsearch output插件中设置用户和密码 output{ elasticsearch{ hosts => \"elasticsearch:9200\" index => \"%{AppID}-%{+YYYY.MM.dd}\" user => \"logstash-pipeline\" password => \"****logstash-pipeline用户密码****\" } } 查看logstash的pipeline是否将数据写入的elasticsearch 附录：Kibana上的角色权限 Cluster相关的角色权限 角色权限 权限描述 all Privileges to create snapshots for existing repositories. Can also list and view details on existing repositories and snapshots. create_snapshot Privileges to create snapshots for existing repositories. Can also list and view details on existing repositories and snapshots. manage Builds on monitor and adds cluster operations that change values in the cluster. This includes snapshotting, updating settings, and rerouting. It also includes obtaining snapshot and restore status. This privilege does not include the ability to manage security. manage_ccr All cross-cluster replication operations related to managing follower indices and auto-follow patterns. It also includes the authority to grant the privileges necessary to manage follower indices and auto-follow patterns. This privilege is necessary only on clusters that contain follower indices. manage_data_frame_transforms All operations on index templates. manage_ilm All operations on index templates. manage_index_templates All operations on index templates. manage_ingest_pipelines All operations on ingest node pipelines. manage_ml All machine learning operations, such as creating and deleting datafeeds, jobs, and model snapshots.Note：Datafeeds that were created prior to version 6.2 or created when security features were disabled run as a system user with elevated privileges, including permission to read all indices. Newer datafeeds run with the security roles of the user who created or updated them. manage_pipeline All operations on ingest pipelines. manage_rollup All rollup operations, including creating, starting, stopping and deleting rollup jobs. manage_saml Enables the use of internal Elasticsearch APIs to initiate and manage SAML authentication on behalf of other users. manage_security All security-related operations such as CRUD operations on users and roles and cache clearing. manage_token All security-related operations on tokens that are generated by the Elasticsearch Token Service. manage_watcher All watcher operations, such as putting watches, executing, activate or acknowledging.Note：Watches that were created prior to version 6.1 or created when the security features were disabled run as a system user with elevated privileges, including permission to read and write all indices. Newer watches run with the security roles of the user who created or updated them. monitor All cluster read-only operations, like cluster health and state, hot threads, node info, node and cluster stats, and pending cluster tasks. monitor_data_frame_transforms All read-only operations related to data frames. monitor_ml All read-only machine learning operations, such as getting information about datafeeds, jobs, model snapshots, or results. monitor_rollup All read-only rollup operations, such as viewing the list of historical and currently running rollup jobs and their capabilities. monitor_watcher All read-only watcher operations, such as getting a watch and watcher stats. read_ccr All read-only cross-cluster replication operations, such as getting information about indices and metadata for leader indices in the cluster. It also includes the authority to check whether users have the appropriate privileges to follow leader indices. This privilege is necessary only on clusters that contain leader indices. read_ilm All read-only index lifecycle management operations, such as getting policies and checking the status of index lifecycle management transport_client All privileges necessary for a transport client to connect. Required by the remote cluster to enable Cross Cluster Search. Index相关的角色权限 角色权限 权限描述 all Any action on an index create Privilege to index documents. Also grants access to the update mapping action.NoteThis privilege does not restrict the index operation to the creation of documents but instead restricts API use to the index API. The index API allows a user to overwrite a previously indexed document. create_index Privilege to create an index. A create index request may contain aliases to be added to the index once created. In that case the request requires the manage privilege as well, on both the index and the aliases names. delete Privilege to delete documents. delete_index Privilege to delete an index. index Privilege to index and update documents. Also grants access to the update mapping action. manage All monitor privileges plus index administration (aliases, analyze, cache clear, close, delete, exists, flush, mapping, open, force merge, refresh, settings, search shards, templates, validate). manage_follow_index All actions that are required to manage the lifecycle of a follower index, which includes creating a follower index, closing it, and converting it to a regular index. This privilege is necessary only on clusters that contain follower indices. manage_ilm All index lifecycle management operations relating to managing the execution of policies of an index This includes operations like retrying policies, and removing a policy from an index. manage_leader_index All actions that are required to manage the lifecycle of a leader index, which includes forgetting a follower. This privilege is necessary only on clusters that contain leader indices. monitor All actions that are required for monitoring (recovery, segments info, index stats and status). read Read-only access to actions (count, explain, get, mget, get indexed scripts, more like this, multi percolate/search/termvector, percolate, scroll, clear_scroll, search, suggest, tv). read_cross_cluster Read-only access to the search action from a remote cluster. view_index_metadata Read-only access to index metadata (aliases, aliases exists, get index, exists, field mappings, mappings, search shards, type exists, validate, warmers, settings, ilm). This privilege is primarily available for use by Kibana users. write Privilege to perform all write operations to documents, which includes the permission to index, update, and delete documents as well as performing bulk operations. Also grants access to the update mapping action. 参考链接 https://www.elastic.co/cn/blog/getting-started-with-elasticsearch-security https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html https://www.elastic.co/guide/en/elastic-stack-overview/7.1/get-started-logstash-user.html https://www.elastic.co/guide/en/logstash/current/ls-security.html https://www.elastic.co/guide/en/logstash/current/docker-config.html#docker-env-config Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/elasticSearch-索引的快照备份与恢复.html":{"url":"origin/elasticSearch-索引的快照备份与恢复.html","title":"Snapshots","keywords":"","body":"一、Context shared file system：NFS S3 HDFS 二、使用NFS作为快照仓库后端存储 1. 在es集群中的某一个节点创建NFS文件系统，ES集群节点进行挂载 yum install -y nfs-utils rpcbind ;\\ systemctl enable nfs ;\\ systemctl enable rpcbind ;\\ systemctl start nfs ;\\ systemctl start rpcbind ;\\ mkdir -p /data/es/Elastic-SnapShots ;\\ echo \"/data/es/Elastic-SnapShots 172.16.3.0/24(rw,sync,no_root_squash,no_subtree_check) \" >> /etc/exports ;\\ export -r ;\\ showmount -e 127.0.0.1 2. 集群其他节点挂载NFS共享目录 yum install nfs-utils -y ;\\ mkdir -p /data/es/Elastic-SnapShots ;\\ echo \"172.16.3.5:/data/es/Elastic-SnapShots /data/es/Elastic-SnapShots nfs defaults 0 0\" >> /etc/fstab ;\\ mount -a ;\\ df -mh 3. 给elasticsearch授予共享目录/data/es/Elastic-SnapShots权限 chown -R elasticsearch:elasticsearch /data/es/Elastic-SnapShots 4. ES集群所有节点配置文件设置 echo 'path.repo: [\"/data/es/Elastic-SnapShots\"]' >> /etc/elasticsearch/elasticsearch.yml ;\\ systemctl restart elasticsearch;\\ systemctl status elasticsearch 三、使用HDFS作为快照仓库后端存储 ES版本：5.6.8 HDFS版本：2.6.0 1、所有ES节点安装repository-hdfs插件 在线安装插件 /usr/share/elasticsearch/bin/elasticsearch-plugin install repository-hdfs 离线安装插件，插件下载地址：https://artifacts.elastic.co/downloads/elasticsearch-plugins/repository-hdfs/repository-hdfs-5.6.8.zip wget https://artifacts.elastic.co/downloads/elasticsearch-plugins/repository-hdfs/repository-hdfs-5.6.8.zip ;\\ /usr/share/elasticsearch/bin/elasticsearch-plugin install file:///root/repository-hdfs-5.6.8.zip 2、重启ES集群所有节点 systemctl restart elasticsearch ;\\ systemctl status elasticsearch 3、后续创建HDFS类型仓库时遇到的问题 ES会以elasticsearch用户(即启动elasticsearch后台进程的用户)在HDFS的/user下创建文件时提示权限不足。所以修改HDFS上/user的权限 hdfs dfs -chmod -R 777 /user 如果HDFS集群在ES集群外面，ES中的Hadoop客户端向通过Hadoop NameNode节点返回的DataNode节点写数据时会找不到DataNode节点。因为创建仓库时只是指定NameNode节点的外网地址，而返回的DataNode节点IP地址是DataNode向NameNode节点注册的内网IP地址，ES集群根本无法访问到。所以打通两者之间的网络。 四、在 kibana 的 Dev Tools 上管理快照仓库 1、注册NFS类型的快照仓库 PUT /_snapshot/快照仓库名 { \"type\": \"fs\", \"settings\": { \"compress\": true, \"location\": \"/data/es/Elastic-SnapShots\" } } ## settings的其他参数： # chunk_size Big files can be broken down into chunks during snapshotting if needed. The chunk size can be specified in bytes or by using size value notation, i.e. 1g, 10m, 5k. Defaults to null (unlimited chunk size). #max_restore_bytes_per_sec Throttles per node restore rate. Defaults to 40mb per second. #max_snapshot_bytes_per_sec Throttles per node snapshot rate. Defaults to 40mb per second. #readonly Makes repository read-only. Defaults to false. 2、注册HDFS类型的快照仓库 PUT _snapshot/快照仓库名 { \"type\": \"hdfs\", \"settings\": { \"uri\": \"hdfs://172.16.3.10:9000\", \"compress\": true, \"path\": \"elasticsearch/respositories\" } } ##settings的其他参数： ​ #uri The uri address for hdfs. ex: \"hdfs://:/\". (Required) #path The file path within the filesystem where data is stored/loaded. ex: \"path/to/file\". (Required) #load_defaults Whether to load the default Hadoop configuration or not. (Enabled by default) #conf. Inlined configuration parameter to be added to Hadoop configuration. (Optional) Only client oriented properties from the hadoop core and hdfs configuration files will be recognized by the plugin. #compress Whether to compress the metadata or not. (Disabled by default) #chunk_size Override the chunk size. (Disabled by default) #security.principal Kerberos principal to use when connecting to a secured HDFS cluster. If you are using a service principal for your elasticsearch node, you may use the _HOST pattern in the principal name and the plugin will replace the pattern with the hostname of the node at runtime (see Creating the Secure Repository). 3、删除快照仓库 DELETE /_snapshot/快照仓库名 4、查看所有的快照仓库 GET _snapshot/_all 五、快照管理 1、创建包含所有Index的全量快照 PUT /_snapshot/my_backup/snapshot_1?wait_for_completion=true 2、创建中包含指定索引的快照 PUT /_snapshot/快照仓库名/快照名?wait_for_completion=true { \"indices\": \"index-A,index-B\", \"ignore_unavailable\": true, \"include_global_state\": false } 3、查看仓库中所有的快照 GET _snapshot/快照仓库名/_all GET _cat/snapshots/快照仓库名 curl -XGET \"http://127.0.0.1:9200/_snapshot/快照仓库名/_all\" | jq -r '.snapshots[].snapshot' 4、删除快照 DELETE _snapshot/快照仓库名/快照名 5、查看多个快照的状态 GET /_snapshot/快照仓库名/快照1,快照2/_status 6、查看某个快照状态 GET _snapshot/快照仓库名/快照/_status GET _snapshot/快照仓库名/快照_1,快照名_2/_status 7、恢复一个快照 POST _snapshot/快照仓库名/快照名/_restore # 当恢复快照中的索引名已存在时，可重命名要恢复的索引名 POST _snapshot/快照仓库名/快照名/_restore { \"indices\": \"索引名\", \"rename_pattern\": \"索引名\", \"rename_replacement\": \"索引名-2\" } 8、快速搜索包含指定关键词的快照并拼成删除语句 export NEED_DELETE=2022-03-1 && \\ curl -s -u 用户:密码 -XGET \"http://localhost:9200/_snapshot/NAS-NFS-Snapshots-Repository/_all\" | \\ jq -r '.snapshots[] | select(.snapshot | contains(\"'$NEED_DELETE'\")) | \"DELETE _snapshot/快照仓库名/\\(.snapshot)\" ' 六、使用 _cat API格式化查询快照仓库中的的快照 使用Snapshot API查出来的信息是JSON格式的，后续处理比较麻烦。可使用\"_cat\" API Endpoint格式化查询输出Snapshot仓库中的快照信息。关于\"_cat\" API的详细使用信息详见Elasticsearch的\"_cat\"API 1、查看_cat的snapshots API的所有参数 GET _cat/snapshots?help 或 curl -XGET \"http://localhost:9200/_cat/snapshots?help\" 名字 简称 描述 id id,snapshot unique snapshot status s,status snapshot name start_epoch ste,startEpoch start time in seconds since 1970-01-01 00:00:00 start_time sti,startTime start time in HH:MM:SS end_epoch ete,endEpoch end time in seconds since 1970-01-01 00:00:00 end_time eti,endTime end time in HH:MM:SS duration dur,duration duration indices i,indices number of indices successful_shards ss,successful_shards number of successful shards failed_shards fs,failed_shards number of failed shards total_shards ts,total_shards number of total shards reason r,reason reason for failures 2、示例 例如只查看快照仓库中的快照名并排序 GET _cat/snapshots/pvc-snap-repo?h=id&s=id 或 curl -XGET \"http://elasticsearch:9200/_cat/snapshots/pvc-snap-repo?h=id&s=id\" # 返回的结果格式是纯文本的 # apm-7.1.1-metric-2019.07.16-snapshot-2019.07.22 # apm-7.1.1-onboarding-2019.07.16-snapshot-2019.07.22 # apm-7.1.1-span-2019.07.16-snapshot-2019.07.22 # apm-7.1.1-transaction-2019.07.16-snapshot-2019.07.22 # ansi-kpo-tek1269219-h136-2019.07.16-snapshot-2019.07.22 # curiouser-ocp-allinone-audit-2019.08.15-snapshot-2019.08.22 # kibana_sample_data_logs-snapshot-2019.07.22 # springboot2-demo-dev-2019.07.12-snapshot-2019.07.15 # springboot2-demo-dev-2019.07.13-snapshot-2019.07.17 七、常用脚本 1、按月份快照索引 #!/bin/bash index_name=test-app for i in {1..12} ;do month=2020-0$i index=`curl -s -u elastic:*** -XGET \"http://127.0.0.1:9200/_cat/indices/$index_name-$month*?h=i\" | tr '\\n' ','` curl -u elastic:*** \\ -XPUT \"http://127.0.0.1:9200/_snapshot/***/collection-$index_name-$month?wait_for_completion=true\" \\ -H \"Content-Type: application/json\" \\ -d '{\"indices\": \"'$index'\",\"ignore_unavailable\": true,\"include_global_state\": false}' curl -u elastic:*** -XDELETE \"http://127.0.0.1:9200/$index_name-$month*\" done 八、更新 Elasticsearch 7.2.0新版本有了管理Snapshot Repository的新功能 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-07-29 13:30:26 "},"origin/elasticsearch-插件管理.html":{"url":"origin/elasticsearch-插件管理.html","title":"插件管理","keywords":"","body":"ES自带的有插件管理脚本命令 以RPM方式安装的ES，插件管理脚本在/usr/share/elasticsearch/bin/elasticsearch-plugin。该脚本能安装，列出，移除插件 $> cd /usr/share/elasticsearch/bin/ $> ./elasticsearch-plugin list #列出所有插件 $> ./elasticsearch-plugin install plugin_name #安装插件 $> ./elasticsearch-plugin remove plugin_name #卸载插件 #该脚本的参数 #-v 输出详细信息 #-s 输出最简信息 脚本返回状态码 0 : everything was OK 64 : unknown command or incorrect option parameter 74 : IO error 70 : any other error 设置代理来安装插件 $ sudo ES_JAVA_OPTS=\"-Dhttp.proxyHost=代理服务器IP地址 -Dhttp.proxyPort=代理服务器端口 -Dhttps.proxyHost=代理服务器IP地址 -Dhttps.proxyPort=代理服务器端口\" bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-07-28 09:40:23 "},"origin/elasticsearch-index-clean-snapshots.html":{"url":"origin/elasticsearch-index-clean-snapshots.html","title":"索引快照清理策略","keywords":"","body":"ElasticSearch索引的快照、清理策略 一、简介 当使用ES存储接入的应用日志时，日志索引会日益增多。而真实的日志查询需求一般是要求半月可查，存储半年到一年。应用每天产生的日志普遍会存到对应ES中以当天日期命名的索引中。查询时根据需求，最多查询半月对应索引里的数据。至于超过半月以上的日志索引数据，可快照文件，存储到文件系统中。在有特殊场景需求时进行快照恢复进行查询。这样减小ES的索引压力，提高查询效率。 需求 将半月以上的日志索引快照成文件，存储到快照仓库中 删除已快照的日志索引 定时检测创建超过15天的索引并快照、清理 钉钉通知快照后删除的索引名称 脚本执行错误时告警 二、基于API的Shell脚本 1、Shell脚本 通过环境变量设置参数 脚本执行完成后发送钉钉通知，显示脚本涉及到的ES索引 集成Sentry告警，每当脚本执行出错时将时间发送至Sentry，再由Sentry进行邮件告警 可使用Linux cron工具或K8S上的cornjob定时每天早上1点执行该脚本 #!/bin/bash export SENTRY_DSN=http://*****@sentry.okd.curiouser.com/28 eval \"$(sentry-cli bash-hook)\" elasticsearch_host=${ELASTICSEARCH_HOST:192.168.1.2} elasticsearch_username=${ELASTICSEARCH_USERNAME:cronjob} elasticsearch_password=${ELASTICSEARCH_PASSWORD:******} elasticsearch_index_expiry_day=${ELASTICSEARCH_INDEX_EXPIRY_DAY:15} elasticsearch_exclude_index=${ELASTICSEARCH_EXCLUDE_INDEX:.*} elasticsearch_snapshots_repository=${ELASTICSEARCH_SNAPSHOTS_REPOSITORY:***} elasticsearch_index_expiry_sec=$((elasticsearch_index_expiry_day*86400)) elasticsearch_url=\"http://${elasticsearch_host}:9200\" allIndex=`curl -s -u ${elasticsearch_username}:${elasticsearch_password} -XGET \"${elasticsearch_url}/_cat/indices/_all?h=index\"` excludeIndex=`curl -s -u ${elasticsearch_username}:${elasticsearch_password} -XGET \"${elasticsearch_url}/_cat/indices/${elasticsearch_exclude_index}/?h=i\"` indices=`echo -e \"$allIndex\\n\"\"$excludeIndex\" |sort -n |uniq -u` for i in $indices ; do createdateincludemesc=`curl -s -u ${elasticsearch_username}:${elasticsearch_password} -XGET \"${elasticsearch_url}/_cat/indices/$i?h=cd\"` ; createdate=$((createdateincludemesc/1000)) currentdate=`date +%s` durationtime=$((currentdate-createdate)) ; if [ $durationtime -gt $elasticsearch_index_expiry_sec ] ;then snapshotsIndices=$i\"\\n\"${snapshotsIndices} fi done for i in `echo -e $snapshotsIndices` ; do if [ `curl -o /dev/null -w \"%{http_code}\\n\" -s -u ${elasticsearch_username}:${elasticsearch_password} -XPUT \"${elasticsearch_url}/_snapshot/${elasticsearch_snapshots_repository}/%3C$i-%7Bnow%2Fd%7D%3E?wait_for_completion=true\" -H 'Content-Type: application/json' -d'{\"indices\": \"'$i'\",\"ignore_unavailable\": true,\"include_global_state\": false}'` = 200 ] ;then if [ `curl -o /dev/null -w \"%{http_code}\\n\" -s -u ${elasticsearch_username}:${elasticsearch_password} -XDELETE \"${elasticsearch_url}/$i\"` = 200 ] ;then echo -e \"The Index $i \\t have been snapshoted to repository and deleted !\" ; else echo \"$i failed to delete \" ; fi else echo \"$i failed to snapshot \" ; fi done curl -s -o /dev/null 'https://oapi.dingtalk.com/robot/send?access_token=*****' \\ -H 'Content-Type: application/json' \\ -d '{\"msgtype\": \"text\", \"text\": {\"content\": \"已成功将以下'\"$elasticsearch_index_expiry_day\"'天之前的索引进行了快照：\\n'\"$snapshotsIndices\"'\"} }' 2、脚本部署执行 ①Linux的Cronjob echo \"0 1 * * * /opt/es-index-snapshots.sh\" > /etc/crontab ②Kubernetes的cronjob 1、构建Cronjob镜像 Dockerfile FROM centos:7 RUN curl -sL https://sentry.io/get-cli/ | bash ADD ./es-index-snapshots.sh /usr/sbin/es-index-snapshots.sh Entrypoint [\"/bin/sh\",\"-c\"] CMD [\"/usr/sbin/es-index-snapshots.sh\"] docker build -t es-index-snapshots:v1 . 2、k8s资源声明文件 es-index-snapshots-cronjob.yml apiVersion: batch/v1beta1 kind: CronJob metadata: name: es-index-snapshots-cronjob namespace: logging spec: concurrencyPolicy: Allow failedJobsHistoryLimit: 1 schedule: 0 1 * * * startingDeadlineSeconds: 200 successfulJobsHistoryLimit: 3 suspend: false jobTemplate: spec: template: spec: containers: - env: - name: TZ value: Asia/Shanghai - name: ELASTICSEARCH_HOST value: *.logging.svc - name: ELASTICSEARCH_USERNAME value: cronjob - name: ELASTICSEARCH_PASSWORD value: \"***\"\" - name: ELASTICSEARCH_INDEX_EXPIRY_DAY value: \"15\" - name: ELASTICSEARCH_EXCLUDE_INDEX value: .* - name: ELASTICSEARCH_SNAPSHOTS_REPOSITORY value: \"***\" image: es-index-snapshots:v1 imagePullPolicy: Always name: es-index-snapshots-cronjob resources: limits: cpu: 600m memory: 800Mi requests: cpu: 300m memory: 500Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst imagePullSecrets: - name: harbor-secrets restartPolicy: OnFailure schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 kubectl apply -f es-index-snapshots-cronjob.yml 三、基于 Python版本SDK的脚本 1、Python脚本 脚本依据索引的创建时间进行处理的。例如设置快照删除15天以前的索引，判断计算的期限是以索引的创建时间算起的15天 定时检测将指定期限以上的索引快照成文件，存储到快照仓库中，然后删除 钉钉通知进行处理的索引名称 脚本执行错误时进行Sentry告警 #!/usr/bin/python3 # -*- coding: UTF-8 -*- import json,time,requests,sentry_sdk from elasticsearch import Elasticsearch es = Elasticsearch( [\"192.168.1.6\",\"192.168.1.7\"], # es用户角色权限要求：集群权限：monitor、create_snapshot 索引权限：*(所有索引) monitor、delete_index http_auth=('es用户名', 'es用户密码'), scheme=\"http\", port=9200, http_compress=True ) app_index_retain_day=15 nginx_index_retain_day=15 # Sentry DSN sentry_sdk.init(dsn='http://*****:*****@sentry.Curiouser.com/12') # 钉钉机器人Token dingding_webhook_token=\"*****\" # 获取所有索引 def getAllIndex(): return es.cat.indices('*', h='index,cd', format='json', s='index') # 将获取到的所有索引去除\".\"开头的、名字异常的或想排除的 def getExcludeSystemAndAberrantIndex(): return list(filter(lambda x: (not ( x['index'].startswith('.') or '%{[app]}' in x['index'] or x['index'].startswith('gitlab-production') or x['index'].startswith('jaeger') )), getAllIndex())) # 获取应用日志索引 def getAppIndex(): return list(filter(lambda x: ( not ('nginx' in x['index'] or 'mysql-slowlog' in x['index'] )), getExcludeSystemAndAberrantIndex())) # 获取Nginx日志索引 def getNginxIndex(): return list(filter(lambda x: ( 'nginx' in x['index'] ), getExcludeSystemAndAberrantIndex())) # 获取MySQL慢日志索引 def getMysqlSlowQueryLogIndex(): return list(filter(lambda x: ('mysql-slowlog' in x['index']), getAllIndex())) # Snapshots索引 def snapshotIndex(index): index_body = {\"indices\": index } print(index) return es.snapshot.create(body=index_body,repository='NAS-NFS-Snapshots-Repository', wait_for_completion='true', request_timeout=300, snapshot= index+'-snapshoted-'+ time.strftime('%m-%d') ) # 删除索引 def deleteIndex(index): es.indices.delete(index=index) # 钉钉通知 def dingdingNotification(token,msg,day): url = \"https://oapi.dingtalk.com/robot/send?access_token=\"+token headers = { \"Content-Type\": \"application/json\", \"Charset\": \"UTF-8\" } # 构建请求数据，post请求 data = { \"msgtype\": \"text\", \"text\": { \"content\": msg+\"\\n\" }, \"at\": { \"isAtAll\": 'true' } } if not requests.post(url, data=json.dumps(data), headers=headers) : print(\"发送钉钉通知失败！\") sentry_sdk.capture_exception(Exception(\"发送钉钉通知失败！\")) # 将创建日志超过指定天数的日志索引快照到存储仓库中，然后删除 def snapshotAndDeleteAppIndex(type,day): if type == 'app' : snapshoted_deleted_app_indices=[] for i in getAppIndex(): cts=time.time() if ( (cts - int(i[\"cd\"])/1000) ) > day*86400 : if 'SUCCESS' in snapshotIndex(i[\"index\"])['snapshot'][\"state\"]: deleteIndex(i['index']) print(i['index']+ \"已在ES中快照并删除！\") snapshoted_deleted_app_indices.append(i['index']) else: print(\"应用日志索引：\"+i['index']+\"快照失败\") sentry_sdk.capture_exception(Exception(\"应用日志索引：\"+i['index']+\"快照失败\")) continue if snapshoted_deleted_app_indices : Notification_Context=\"[索引快照清理任务]\\n成功将以下\"+str(day)+\"天之前的应用日志索引进行了快照\\n\"+\"\\n\".join(str(i) for i in snapshoted_deleted_app_indices) dingdingNotification(dingding_webhook_token,Notification_Context,day) else: Notification_Context = \"[索引快照清理任务]\\n没有超过\"+ str(day)+\"天的应用日志索引需要被快照删除！\" dingdingNotification(dingding_webhook_token, Notification_Context, day) elif type == 'nginx' : snapshoted_deleted_nginx_indices = [] for i in getNginxIndex(): cts=time.time() if ( (cts - int(i[\"cd\"])/1000) ) > day*86400 : if 'SUCCESS' in snapshotIndex(i[\"index\"])['snapshot'][\"state\"]: deleteIndex(i['index']) print(i['index'] + \"已在ES中快照并删除！\") snapshoted_deleted_nginx_indices.append(i['index']) else: print(\"Nginx日志索引：\" + i['index'] + \"快照失败\") sentry_sdk.capture_exception(Exception(\"Nginx日志索引：\" + i['index'] + \"快照失败\")) continue if snapshoted_deleted_nginx_indices: Notification_Context = \"[索引快照清理任务]\\n成功将以下\" + str(day) + \"天之前的应用Nginx索引进行了快照\\n\" + \"\\n\".join(str(i) for i in snapshoted_deleted_nginx_indices) dingdingNotification(dingding_webhook_token, Notification_Context, day) else: Notification_Context = \"[索引快照清理任务]\\n没有超过\" + str(day) + \"天的应用Nginx日志索引需要被快照删除！\" dingdingNotification(dingding_webhook_token, Notification_Context, day) elif type == 'mysqlslowlog' : snapshoted_deleted_mysqlslowlog_indices = [] for i in getMysqlSlowQueryLogIndex(): cts = time.time() if ((cts - int(i[\"cd\"]) / 1000)) > day * 86400: if 'SUCCESS' in snapshotIndex(i[\"index\"])['snapshot'][\"state\"]: deleteIndex(i['index']) print(i['index'] + \"已在ES中快照并删除！\") snapshoted_deleted_mysqlslowlog_indices.append(i['index']) else: print(\"MySQL慢查询日志索引：\" + i['index'] + \"快照失败\") sentry_sdk.capture_exception(Exception(\"MySQL慢查询日志索引：\" + i['index'] + \"快照失败\")) continue if snapshoted_deleted_mysqlslowlog_indices : Notification_Context = \"[索引快照清理任务]\\n成功将以下\" + str(day) + \"天之前的MySQL慢查询日志索引进行了快照\\n\" + \"\\n\".join(str(i) for i in snapshoted_deleted_mysqlslowlog_indices) dingdingNotification(dingding_webhook_token, Notification_Context, day) else: Notification_Context = \"[索引快照清理任务]\\n没有超过\" + str(day) + \"天的MySQL慢查询日志索引需要被快照删除！\" dingdingNotification(dingding_webhook_token, Notification_Context, day) def main(): print(\"=====================\"+time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())+\"开始清理es中的索引=====================\") print(\"................开始清理\"+app_index_retain_day+\"天前应用相关索引................\") # 快照删除指定期限之前的应用索引 snapshotAndDeleteAppIndex('app',app_index_retain_day) print(\"................开始清理\"+nginx_index_retain_day+\"天前nginx相关索引................\") # 快照删除指定期限之前Nginx索引 snapshotAndDeleteAppIndex('nginx',nginx_index_retain_day) exit(0) if __name__ == \"__main__\" : main() 2、requirements.txt elasticsearch==7.0.0 pyyaml requests sentry_sdk 3、操作步骤 Python版本：3 默认清理策略 快照删除指定日期前的应用日记索引 快照删除指定日期前的应用Nginx日记索引 （索引名包含Nginx关键字的） 安装依赖 pip3 install -r requierements.txt 执行脚本 PYTHONIOENCODING=utf-8 python3 es-index-snapshots-clean.py Crontab定时执行脚本：每天凌晨1点执行 0 0 1 * * ? python3 es-index-snapshots-clean.py 四、官方的curator索引管理工具 https://www.elastic.co/guide/en/elasticsearch/client/curator/5.8/index.html 五、ES自带的ILM(index lifecycle management)功能 https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-lifecycle-management.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-06-02 13:43:04 "},"origin/elasticsearch-sample-data.html":{"url":"origin/elasticsearch-sample-data.html","title":"官方示例数据集","keywords":"","body":"一、示例数据集说明 示例数据集下载地址： curl -O https://download.elastic.co/demos/kibana/gettingstarted/8.x/shakespeare.json && \\ curl -O https://download.elastic.co/demos/kibana/gettingstarted/8.x/accounts.zip && \\ curl -O https://download.elastic.co/demos/kibana/gettingstarted/8.x/logs.jsonl.gz && \\ unzip accounts.zip && \\ gunzip logs.jsonl.gz ①shakespeare.json 莎士比亚所有的作品集。数据集的数据组织格式： { \"line_id\": INT, \"play_name\": \"String\", \"speech_number\": INT, \"line_number\": \"String\", \"speaker\": \"String\", \"text_entry\": \"String\", } ②accounts.json 随机生成的虚拟账号信息，数据集的数据组织格式 { \"account_number\": INT, \"balance\": INT, \"firstname\": \"String\", \"lastname\": \"String\", \"age\": INT, \"gender\": \"M or F\", \"address\": \"String\", \"employer\": \"String\", \"email\": \"String\", \"city\": \"String\", \"state\": \"String\" } ③logs.json 随机生成的日志数据，日志数据有几十个不同的字段，但是在教程中关注的字段如下： { \"memory\": INT, \"geo.coordinates\": \"geo_point\" \"@timestamp\": \"date\" } 二、映射数据集 ​ 在导入数据集之前，我们需要为各个字段建立一个映射。映射把索引里的文档划分成逻辑组，定义字段的特性，如字段是否可被搜索、是否被标记、是否能被拆分成多个文字等。 ①映射莎士比亚作品数据集 PUT /shakespeare { \"mappings\": { \"properties\": { \"speaker\": {\"type\": \"keyword\"}, \"play_name\": {\"type\": \"keyword\"}, \"line_id\": {\"type\": \"integer\"}, \"speech_number\": {\"type\": \"integer\"} } } } #因为speaker和play_name字段是“keyword”字段，所以他们不参与处理分析。字符类型的字段被当做做单一单元，即使字段值有多个字符 ②日志数据需要一个映射表明地理位置的经纬度，通过在那些字段使用一个geo_point类型。 PUT /logstash-2015.05.18 { \"mappings\": { \"properties\": { \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } } PUT /logstash-2015.05.19 { \"mappings\": { \"properties\": { \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } } PUT /logstash-2015.05.20 { \"mappings\": { \"properties\": { \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } } ③账号数据不需要任何映射，直接用ElasticSearch的bulk API导入数据 ④Curl命令导入 curl -X PUT \"localhost:9200/shakespeare?pretty\" -H 'Content-Type: application/json' -d' { \"mappings\": { \"properties\": { \"speaker\": {\"type\": \"keyword\"}, \"play_name\": {\"type\": \"keyword\"}, \"line_id\": {\"type\": \"integer\"}, \"speech_number\": {\"type\": \"integer\"} } } } ' curl -X PUT \"localhost:9200/logstash-2015.05.18?pretty\" -H 'Content-Type: application/json' -d' { \"mappings\": { \"properties\": { \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } } ' curl -X PUT \"localhost:9200/logstash-2015.05.19?pretty\" -H 'Content-Type: application/json' -d' { \"mappings\": { \"properties\": { \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } } ' curl -X PUT \"localhost:9200/logstash-2015.05.20?pretty\" -H 'Content-Type: application/json' -d' { \"mappings\": { \"properties\": { \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } } ' 三、导入数据集 1、在数据集所在主机的shell利用curl命令导入数据集 ①导入莎士比亚作品数据集到shakespeare索引里 curl -u elastic:密码 -H 'Content-Type: application/x-ndjson' \\ -XPOST '127.0.0.1:9200/shakespeare/_bulk?pretty' \\ --data-binary @shakespeare.json ②导入日志数据集 curl -u elastic:密码 -H 'Content-Type: application/x-ndjson' \\ -XPOST '127.0.0.1:9200/_bulk?pretty' \\ --data-binary @logs.jsonl ③导入账号数据集到bank索引 curl -u elastic:密码 -H 'Content-Type: application/x-ndjson' \\ -XPOST '127.0.0.1:9200/bank/account/_bulk?pretty' \\ --data-binary @accounts.json 2、验证数据是否导入成功 在kibana的Dev Tools工具利用命令查看所有索引信息 GET _cat/indices/bank,shakespeare,logstash-2015*?v health status index pri rep docs.count docs.deleted store.size pri.store.size green open bank 5 1 1000 0 418.2kb 418.2kb green open shakespeare 5 1 111396 0 17.6mb 17.6mb green open logstash-2015.05.18 5 1 4631 0 15.6mb 15.6mb green open logstash-2015.05.19 5 1 4624 0 15.7mb 15.7mb green open logstash-2015.05.20 5 1 4750 0 16.4mb 16.4mb 四、数据查询 1、只显示某些字段 GET /bank/_search { \"query\": { \"match_all\": {} }, \"_source\": [\"account_number\", \"balance\"] } # 或者 curl -X GET \"localhost:9200/bank/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"match_all\": {} }, \"_source\": [\"account_number\", \"balance\"] }' 2、查询某字段值为20的Doc GET /bank/_search { \"query\": { \"match\": { \"account_number\": 20 } } } # 或者 curl -X GET \"localhost:9200/bank/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"match\": { \"account_number\": 20 } } }' 3、查询某字段包含\"mill\"的Doc GET /bank/_search { \"query\": { \"match\": { \"address\": \"mill\" } } } # 或者 curl -X GET \"localhost:9200/bank/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"match\": { \"address\": \"mill\" } } }' 4、查询某字段包含\"mill\"或\"lane\"的Doc GET /bank/_search { \"query\": { \"match\": { \"address\": \"mill lane\" } } } # 或者 curl -X GET \"localhost:9200/bank/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"match\": { \"address\": \"mill lane\" } } }' GET /bank/_search { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } } # 或者 curl -X GET \"localhost:9200/bank/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } } ' GET /bank/_search { \"query\": { \"bool\": { \"should\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } } # 或者 curl -X GET \"localhost:9200/bank/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"bool\": { \"should\": [ { \"match\": { \"address\": \"mill\" } }, { \"match\": { \"address\": \"lane\" } } ] } } }' 五、数据搜索及可视化 1、创建索引模式 2、Discover中搜索数据 3、可视化数据 饼图显示bank数据各个收入范围的年龄分布 参考 https://www.elastic.co/guide/en/kibana/7.9/tutorial-build-dashboard.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/elasticsearch-cluster-upgrade.html":{"url":"origin/elasticsearch-cluster-upgrade.html","title":"集群升级","keywords":"","body":"Elasticsearch集群升级 一、简介 对于升级重启情况，在执行升级操作时，集群可能： 滚动重启（节点依次重启，期间服务可以正常访问，但性能可能受到部分影响，建议在集群负载不高时进行） 全量重启（所有节点完全关闭后重启，期间服务不可访问，需谨慎选择） 版本升级路径 Upgrade from 7.17.5 的推荐升级路径 以前的 7.17 版本（例如7.17.0） 滚动升级 至7.17.5 7.0–7.16 滚动升级 至7.17.5 6.8 滚动升级 至7.17.5 6.0–6.7 先滚动升级 至6.8再滚动升级 至7.17.5 5.6 先滚动升级 至6.8再滚动升级 至7.17.5 5.0–5.5 先滚动升级 至5.6再滚动升级 至6.86再滚动升级 至7.17.5 支持滚动升级的版本路径 同一主要版本的次要版本之间 从 5.6 到 6.8 从 6.8 到 7.17.5 从 7.17.0 到 7.17.5 的任何版本 从 6.7 或更早版本直接升级到 7.17.5 需要 完全重启集群。 不支持从 6.8 到 7.0 的升级路径（全集群重启和滚动升级）。 二、升级步骤 0、升级检查 检查新版本使用的Java版本，如果Java版本不兼容，在第四步之后升级Java 1、禁用分片分配 PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": \"primaries\" } } 2、(可选的)停止不必要的索引并执行同步刷新 POST _flush/synced 3、(可选的)暂时停止与活动机器学习作业和数据馈送相关的任务 POST _flush/synced 4、关闭单个节点 systemctl stop elasticsearch.service 5、升级关闭的节点 二进制包 rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch [elasticsearch] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=0 autorefresh=1 type=rpm-md yum install --enablerepo=elasticsearch elasticsearch 6、升级插件 /usr/share/elasticsearch/bin/elasticsearch-plugin list /usr/share/elasticsearch/bin/elasticsearch-plugin remove analysis-smartcn repository-s3 /usr/share/elasticsearch/bin/elasticsearch-plugin install analysis-smartcn repository-s3 7、启动升级的节点 systemctl start elasticsearch 在安装过程中，旧的配置文件不会被覆盖，新的配置文件会以rpmnew结尾 启动过程中，可能旧的配置文件的值被废弃了或无效的，需要根据具体报错及时修改 8、等待节点恢复 没有未分配的分片 节点状态由Yellow转为Green # 查看集群健康状态 GET /_cluster/health GET _cat/health?v=true # 查看集群恢复状态 GET _cat/recovery 9、重复4~8步骤升级余下节点 10、重新启用分片分配 PUT _cluster/settings { \"persistent\": { \"cluster.routing.allocation.enable\": null } } 11、升级Kibana /etc/yum.repos.d/kibana.repo [kibana] name=Kibana repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=0 autorefresh=1 type=rpm-md yum install --enablerepo=kibana kibana 参考 https://www.elastic.co/guide/en/elasticsearch/reference/7.17/setup-upgrade.html https://www.elastic.co/guide/en/elasticsearch/reference/7.17/rolling-upgrades.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-04 14:49:55 "},"origin/elasticsearch-optimizing.html":{"url":"origin/elasticsearch-optimizing.html","title":"优化","keywords":"","body":"Elasticsearch性能优化 一、写入性能优化 在文档写入时，会根据_routing来计算（OperationRouting类）得出文档要写入哪个分片。这里的写入请求只会写主分片，当主分片写入成功后，会同时把写入请求发送给所有的副本分片，当副本分片写入成功后，会传回返回信息给主分片，主分片得到所有副本分片的返回信息后，再返回给客户端。 在写入时，我们可以在Request自己指定_routing，也可以在Mapping指定文档中的Field值作为_routing。如果没有指定_routing，则会把_id作为_routing进行计算。由于写入时，具有相同_routing的文档一定会分配在同一个分片上，所以如果是自定义的_routing，在查询时，一定要指定_routing进行查询，否则是查询不到文档的。这并不是局限性，恰恰相反，指定_routing的查询，性能上会好很多，因为指定_routing意味着直接去存储数据的shard上搜索，而不会搜索所有shard。 二、索引性能优化 段合并 关闭索引 三、搜索性能优化 优化索引速度 1. 使用bulk批量操作 批量请求将比单文档索引请求产生更好的性能。 为了知道批量请求的最佳大小，您应该在具有单个shard的单个节点上运行基准测试。首先尝试一次索引100个文档，然后索引200个，然后索引400个，等等，在每次基准测试运行时将批量请求中的文档数量增加一倍。当索引速度开始趋于稳定时，您就知道已经达到了数据批量请求的最佳大小。 2. 查询返回大小 尽量使用 Scroll滚动查询API。 3. 按照日期规划索引 4. 索引分片个数设置 5. 索引分片副本数设置 6. 禁止大文档 禁止单个Document的大小超过默认设置http.max_content_length(默认值100MB)（如果单个doc大小超过了设置值，elasticsearch会直接拒绝索引）。 虽然可修改http.max_content_length参数提高默认doc大小，但 Lucene引擎依旧会有2GB大小的限制 单个大doc会加重网络、内存和磁盘的消耗 7. 禁止节点开启Swapping 8. 节点给系统缓存预留内存 文件系统缓存将用于缓冲I / O操作 9. 文档ID尽量自动生成 10. 节点硬件尽量选性能好的 11. 提高索引缓存区大小 12. 使用多线程分散写入操作 使用单个线程发送批处理写入请求 13. 调整索引刷新间：refresh_interval 默认情况下索引的refresh_interval为1秒,这意味着数据写1秒后就可以被搜索到,每次索引的 refresh 会产生一个新的 lucene 段,这会导致频繁的 segment merge 行为,如果你不需要这么高的搜索实时性,应该降低索引refresh 周期,如:index.refresh_interval: 120s 二、优化查询速度 三、优化硬盘使用 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/elasticsearch-问题总结.html":{"url":"origin/elasticsearch-问题总结.html","title":"问题总结","keywords":"","body":"一、xpack的monitoring功能“导致”failed to flush export bulks和 there are no ingest nodes in this cluster”报错 原因： xpack的monitoring功能需要定义exporter用于导出监控数据， 默认的exporter是local exporter，也就是直接写入本地的集群，并且要求节点开启了ingest选项。 解决方案: 将集群的结点配置里的ingest角色打开 或者在集群设置elasticsearch.yml里，将local exporter的use ingest关掉 xpack.monitoring.exporters.my_local: type: local use_ingest: false 但一般的，使用local cluster监控自己存在很大的问题，故障发生时，监控也没法看到了。 生产上最好是设置一个单独的监控集群，然后可以配置一个HTTP exporter，将监控数据送往这个监控集群 参考： https://www.elastic.co/guide/en/x-pack/5.5/monitoring-cluster.html#http-exporter-reference https://elasticsearch.cn/question/1915 二、监控日志索引Index的保存期限为7天 Elasticsearch的监控日志索引Index为\".monitoring-*\"开头的，保存期限为7天，7天之后会自动删除。 参考 https://discuss.elastic.co/t/how-system-index-like-monitoring-es-6-2018-02-06-are-being-deleted-automatically/119578 三、字段过大导致kibana搜索是分片失败 报错： The length of [response.keyword] field of [SwiBc3YBv0gFs9LK4P1_] doc of [docc-2020-12-18] index has exceeded [1000000] - maximum allowed to be analyzed for highlighting. This maximum can be set by changing the [index.highlight.max_analyzed_offset] index level setting. For large texts, indexing with offsets or term vectors is recommended! 原因：某个字段超出了字符偏移量上限 解决方案 PUT /分片失败的索引/_settings { \"index\" : { \"highlight.max_analyzed_offset\" : 60000000 } } 四、单节点ES默认索引副本与分片的问题 1. 修改索引模板中的默认值 curl -X PUT http://localhost:9200/_template/default -H 'Content-Type: application/json' -d '{\"index_patterns\": [\"*\"],\"order\": -1,\"settings\": {\"number_of_shards\": \"1\",\"number_of_replicas\": \"0\"}}' # 或者 PUT /_template/default {\"index_patterns\": [\"*\"],\"order\": -1,\"settings\": {\"number_of_shards\": \"1\",\"number_of_replicas\": \"0\"}} 2. 对于已创建的索引 curl -X PUT http://localhost:9200/_settings -H 'Content-Type: application/json' -d '{\"index\": {\"number_of_shards\": \"1\",\"number_of_replicas\": \"0\"}}' 会报以下的错误 {\"error\":{\"root_cause\":[{\"type\":\"cluster_block_exception\",\"reason\":\"blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];\"}],\"type\":\"cluster_block_exception\",\"reason\":\"blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];\"},\"status\":403} 执行以下命令进行修复 curl -X PUT http://localhost:9200/_settings -H 'Content-Type: application/json' -d '{\"index\": {\"blocks\": {\"read_only_allow_delete\": \"false\"}}}' 参考：https://gist.github.com/angristan/9d251d853d11f265899b8a4725bff756 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-07-27 15:01:03 "},"origin/kubernete-prometheus.html":{"url":"origin/kubernete-prometheus.html","title":"Kubernetes的监控体系","keywords":"","body":"Kubernetes下的监控体系 一、监控对象 监控对象 示例 监控信息暴露方式 k8s主机 k8s依赖的主机 node exporter以daemonset形式每个节点部署一个 K8s上的中间件 redis、mysql、kafka、mongo 每种服务自带对应的exporter K8s上的系统服务 traefik traefik自带暴露metrics k8s上的应用服务 application 部署的应用开起的 Metrics apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: application-php-fpm namespace: monitoring spec: endpoints: - interval: 15s port: phpfpm-exporter namespaceSelector: matchNames: - test - stg - monitoring selector: matchLabels: prometheus-target: application-php-fpm --- apiVersion: v1 kind: Service metadata: labels: prometheus-target: application-php-fpm name: nginx namespace: stg spec: ports: - name: nginx port: 80 protocol: TCP targetPort: 80 - name: phpfpm-exporter port: 9253 protocol: TCP targetPort: 9253 selector: app: saas-base-service Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:45:03 "},"origin/prometheus-basic.html":{"url":"origin/prometheus-basic.html","title":"Prometheus基础概念及PromSQL","keywords":"","body":"Prometheus基础概念及PromSQL 一、简介 prometheus 是一个开源的系统监控和告警的工具包，其采用pull方式采集时间序列，通过http协议传输。 prometheus的优势在于，它是一个基于服务的告警系统，针对不同的服务，有不同的exporter，可以实现不一样的效果。 最初由SoundCloud发布。它通过HTTP协议从远程的机器收集数据并存储在本地的时序数据库上。它提供了一个简单的网页界面、一个功能强大的查询语言以及HTTP接口等等。 Prometheus可以通过安装在远程机器上的exporter来收集监控数据。 官网：https://prometheus.io/ Github：https://github.com/prometheus/prometheus 二、架构 prometheus的核心是一个时间序列数据库，我们可以通过它抓取并存储数据，并通过prometheus定义的一些查询语句来获取我们需要的数据 exporter的核心是一个静态web，通过不断更新的静态web暴露metric值 alertmanager是一个报警接口，接收prometheus推送的告警，并通过自己定义的一些规则去进行告警 Pushgateway 程序，主要是实现接收由Client push过来的指标数据，在指定的时间间隔，由主程序来抓取 三、存储方式 Prometheus提供了两种存储方式，分别是本地存储和远端存储 Prometheus本地存储简介 Prometheus的本地存储被称为Prometheus TSDB，目前是V3版本，根据官方介绍其有着非常高效的时间序列数据存储方法，每个采样数据仅占3.5byte左右空间，上百万条时间序列，30s间隔，保存60天，仅占200多G空间 默认情况下，Prometheus将采集到的数据保存在本地的TSDB数据库中，默认目录为Prometheus安装目录下的data目录。数据写入过程为先把数据写入wal日志并放在内存中，然后2个小时后将内存数据放入一个新的block块，同时再把新的数据写入内存并在2小时后再保存至一个新的block块，依次类推 涉及到两个组成部分：block和wal block Prometheus TSD将存储的监控数据按时间分割为block，block的大小并不固定，默认最小的block保存2h的数据，随着数据量的不断增加，TSDB会将小的block合并为大的block，例如将3个2h的block合并为一个6h的block，这样不仅可以减少数据存储，还可以减少block个数，便于对数据进行检索。 在实际存储中，block就是Prometheus TSDB数据目录下那些以01开头的存储目录。block主要包含4个部分：chunks、index、meta.josn、tombstones chunks，主要用于保存压缩后的时序数据。每个chunk的大小为512M，如果超过，则会被分割为多个chunk保存，且以数字编号命名 index，是为了对时序数据进行快速检索和查询而设计，主要用来记录chunk中时序的偏移位置 meta.json，记录block的元数据信息，主要包括一个数据块记录样本的起始时间、截至时间、样本数、时序数和数据源等信息，这些元数据信息在后期对block进行维护（删除过期block、合并block等）时会用到。 tombstones，用于对数据进行软删除。TSDB在删除block数据块时会将整个目录删除，但如果只删除一部分数据块的内容，则可以通过 tombstones进行软删除 WAL WAL（write-ahead logging，预写日志）是关系型数据库中利用日志来实现事务性和持久性的一种技术，即在进行某个操作之前先将这件事情记录下来，以便之后对数据进行回滚、重试等操作并保证数据可靠性。 Prometheus为了防止丢失暂存在内存中的还未被写入磁盘的监控数据’、引入了WAL机制。WAL被分割为默认大小为128M的文件段，文件段以数字命名，例如00000001、00000002等，以此类推。 按照每种对象设定的采集周期，Prometheus会将周期性采集的监控数据先写入head-block中，但这些数据没有被持久化，TSDB通过WAL将提交的数据先保存到磁盘中，在TSDB宕机重启后，会首先启动多协程读取WAL，从而恢复之前的状态。 另外从Prometheus v2.19.0开始，Prometheus引入了内存映射，将head-block中已填充的完整的chunk，刷新到磁盘（即保存在chunks_head目录下的数据）并从磁盘进行内存映射，同时仅将引用存储在内存中。通过内存映射，可以在需要时使用该引用将chunk动态加载到内存中。这是操作系统提供的功能。通过引入内存映射，减少了Prometheus的内存消耗，虽然填充完毕的chunk会被刷到磁盘上，但是对于该部分的操作预写入日志不会被删除，直到该chunk所属的block完整落盘 四、Metrics数据模型 Prometheus 中存储的数据为时间序列，是由 metric 的名字和一系列的标签（键值对）唯一标识的，不同的标签则代表不同的时间序列 metric ：该名字应该具有语义，一般用于表示 metric 的功能，例如：http_requests_total, 表示 http 请求的总数。 metric 名字由 ASCII字符，数字，下划线，以及冒号组成，且必须满足正则表达式 [a-zA-Z_:][a-zA-Z0-9_:]* 标 签：使同一个时间序列有了不同维度的识别。例如 http_requests_total{method=\"Get\"} 表示所有 http 请求中的 Get 请求。当 method=\"post\" 时，则为新的一个 metric。 标签中的键由 ASCII字符，数字，以及下划线组成，且必须满足正则表达式 [a-zA-Z_:][a-zA-Z0-9_:]*。 样 本：实际的时间序列，每个序列包括一个 float64 的值和一个毫秒级的时间戳。 格式：{=, …}， 例如：http_requests_total{method=\"POST\",endpoint=\"/api/tracks\"}。 五、Metrics类型 Counter：只增不减的计数器 计数器可以用于记录只会增加不会减少的指标类型。比如记录应用请求的总量，cpu使用时间等 对于Counter类型的指标，只包含一个inc()方法，用于计数器+1 一般而言，Counter类型的metrics指标在命名中我们使用_total结束，如http_requests_total Gauge：可增可减的仪表盘 对于这类可增可减的指标，可以用于反应应用的当前状态。例如在监控主机时，主机当前空闲的内存大小，可用内存大小。或者容器当前的cpu使用率,内存使用率。 对于Gauge指标的对象则包含两个主要的方法inc()以及dec(),用户添加或者减少计数。 Histogram：自带buckets区间用于统计分布统计图 主要用于在指定分布范围内(Buckets)记录大小或者事件发生的次数。 Summary:：客户端定义的数据分布统计图 Summary和Histogram非常类型相似，都可以统计事件发生的次数或者大小，以及其分布情况。 Summary和Histogram都提供了对于事件的计数_count以及值的汇总_sum。 因此使用_count,和_sum时间序列可以计算出相同的内容，例如http每秒的平均响应时间：rate(basename_sum[5m]) /rate(basename_count[5m])。 同时Summary和Histogram都可以计算和统计样本的分布情况，比如中位数，9分位数等等。其中 0.0 不同在于Histogram可以通过histogram_quantile函数在服务器端计算分位数。 而Sumamry的分位数则是直接在客户端进行定义。 因此对于分位数的计算。 Summary在通过PromQL进行查询时有更好的性能表现，而Histogram则会消耗更多的资源。相对的对于客户端而言Histogram消耗的资源更少。 六、PromSQL查询语法 1、查询语法规则 ``` ## 2、时间范围查询 - **查询瞬时向量** ```bash # 瞬时向量表达式，选择当前最新的数据 node_uname_info # 瞬时向量表达式，选择当前最新的数据 node_uname_info{} 查询范围向量 # 查询以当前时间为基准，5分钟内的数据 node_uname_info [5m] 查询位移时间的向量 # 查询以当前时间为基准，1小时前的瞬时样本数据 node_uname_info offset 1h 综合查询 # 查询以当前时间为基准，1小时前1小时内的数据 node_uname_info [1h] offset 1h 时间范围查询支持的时间单位： ms ： 毫秒 s ：秒 m ：分支 h ：小时 d： 天（24小时） w ：周（7天） y ：年（365天） 七、配置 1、配置Prometheus ①检查配置文件语法 promtool check config /etc/prometheus/prometheus.yml ②在运行时重载配置文件 在启动prometheus时添加参数： prometheus \\ --config.file=/opt/prometheus/prometheus.yml \\ --storage.tsdb.path=/data/prometheus/data \\ --web.enable-lifecycle 然后通过Reastful接口触发重载配置文件 curl -XPOST http://127.0.0.1:9090/-/reload 或者给prometheus进程发送SIGHUP信号 kill -HUP prometheus进程号 如果变更后的配置文件语法有错误，则不会重载生效。触发重载前，可使用promtool check检查配置文件语法。 参考：https://prometheus.io/docs/prometheus/latest/configuration/configuration/#%3Cscrape_config%3E 2、配置node_exporter 参考文档：https://prometheus.io/docs/guides/node-exporter/ scrape_configs: - job_name: 'node' static_configs: - targets: - 部署node_exporter主机IP地址:9100 - 部署node_exporter主机IP地址:9100 3、配置blackbox_exporter 参考文档：https://github.com/prometheus/blackbox_exporter#prometheus-configuration scrape_configs: - job_name: \"blackbox\" scrape_interval: 10s scrape_timeout: 5s metrics_path: /probe params: module: [http_2xx] static_configs: - targets: - http://代探测的URL relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 部署blackbox_exporter主机IP地址:9115 八、API 目前，Prometheus API 的稳定版本为V1，针对该API的访问路径为 /api/v1。API支持的请求模式有GET和POST两种，当正常响应时，会返回2xx的状态码。当API正常响应后，将返回如下的Json数据格式。反之，当API调用失败时，则可能返回以下几种常见的错误提示码： 400 Bad Request 参数丢失或不正确时出现。 422 Unprocessable Entity 当表达无法被执行时。 503 Service Unavailiable 查询超时或中止时。 请求参数格式 query=: Prometheus expression query string. time=: Evaluation timestamp. Optional. timeout=: Evaluation timeout. Optional. Defaults to and is capped by the value of the -query.timeout flag. 1、admin api 默认情况下，管理时间序列 API 是被禁用的，在 Prometheus 的启动参数中添加--web.enable-admin-api参数启用 ①删除时间序列指标 PUT /api/v1/admin/tsdb/delete_series 参数： match[]= : Metrics的名称 start= : 开始的时间戳 end= : 结束的时间戳 # 删除某个标签匹配的数据 $ curl -X POST -w \"%{http_code}\" 'http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]={instance=\".*\"}' # 删除某个指标数据 $ curl -X POST -w \"%{http_code}\" 'http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]={node_load1=\".*\"}' # 根据时间删除 $ curl -X POST -w \"%{http_code}\" 'http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]={node_load1=\".*\"}&start ②删除软删除的数据 从磁盘中删除已删除的数据，并清理现有的逻辑删除。还可以在删除操作后释放空间。 POST /api/v1/admin/tsdb/clean_tombstones PUT /api/v1/admin/tsdb/clean_tombstones # 成功则返回 204 的请求状态码 $ curl -s -XPOST -w \"%{http_code}\" http://localhost:9090/api/v1/admin/tsdb/clean_tombstones 2、resource api ①查询某一时刻的metrics数据 GET /api/v1/query POST /api/v1/query 参数： query=: Prometheus 表达式查询字符串。 time=: 时间戳，可选参数。 timeout=: 查询超时设置，可选参数，默认将使用-query.timeout的全局参数。 $ curl -s 'http://localhost:9090/api/v1/query?' \\ -d 'query=up{instance=\"localhost:9090\"}' \\ -d 'time=2023-08-23T08:01:00.001Z' | jq -r '.' ②查询metrics时间范围内的数据 GET /api/v1/query_range POST /api/v1/query_range 参数： query=: Prometheus 表达式查询字符串。 start=: 开始时间戳，可选参数 end=: 结束时间戳，可选参数 step=: 查询解析步长，采用持续时间格式或浮点秒数 timeout=: 查询超时设置，可选参数，默认将使用-query.timeout的全局参数。 $ curl -s 'http://localhost:9090/api/v1/query_range?' \\ -d 'query=up' \\ -d 'start=2023-08-23T08:01:00.001Z' \\ -d 'end=2023-08-23T08:30:00.001Z' \\ -d 'step=15s' | jq -r '.' ③格式化查询metrics数据 GET /api/v1/format_query POST /api/v1/format_query 参数： query=: Prometheus 表达式查询字符串。 $ curl -s 'http://localhost:9090/api/v1/format_query?query=foo/bar' | jq -r '.' ④根据标签查询metrics数据 GET /api/v1/series POST /api/v1/series 参数： match[]=: 至少提供一个 match[] start=: 开始时间戳，可选参数 end=: 结束时间戳，可选参数 $ curl -s 'http://localhost:9090/api/v1/series?' \\ -d 'match[]=up' \\ -d 'match[]=node_disk_io_now{job=\"node_exporter\"}' | jq -r '.' ⑤查询metrics的标签 GET /api/v1/labels POST /api/v1/labels 参数： match[]=: 可选 start=: 开始时间戳，可选参数 end=: 结束时间戳，可选参数 $ curl -s 'http://localhost:9090/api/v1/labels?' | jq -r '.' $ curl -s 'http://localhost:9090/api/v1/labels?' \\ -d 'match[]=up' | jq -r '.' ⑥查询标签的值 GET /api/v1/label//values 参数： match[]=: 可选 start=: 开始时间戳，可选参数 end=: 结束时间戳，可选参数 $ curl -s 'http://localhost:9090/api/v1/label/job/values' | jq -r '.' ⑦查询 Target GET /api/v1/targets 参数: state=: target的状态 scrapePool==: $ curl -s http://localhost:9090/api/v1/targets?state=active | jq -r '.' ⑧查询配置文件 GET /api/v1/status/config $ curl -s http://localhost:9090/api/v1/status/config | jq -r '.' ⑨查询启动的参数 GET /api/v1/status/flags $ curl -s http://localhost:9090/api/v1/status/flags | jq -r '.' ⑩查询TSDB的状态 $ curl -s http://localhost:9090/api/v1/status/tsdb | jq -r '.' ⑪查询 WAL 重放状态 GET /api/v1/status/walreplay $ curl -s http://localhost:9090/api/v1/status/walreplay | jq -r '.' Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-12-08 09:34:25 "},"origin/kube-prometheus.html":{"url":"origin/kube-prometheus.html","title":"kube-prometheus: Prometheus Operator安装部署","keywords":"","body":"kube Prometheus：Prometheus Operator 一、简介 为了方便大家使用prometheus，Coreos出了提供了一个Operator，而为了方便大家一站式的监控方案就有了项目kube-prometheus是一个脚本项目，它主要使用jsonnet写成，其作用呢就是模板+参数然后渲染出yaml文件集，主要是作用是提供一个开箱即用的监控栈，用于kubernetes集群的监控和应用程序的监控。 这个项目主要包括以下软件栈 Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter Prometheus Adapter for Kubernetes Metrics APIs Kube-state-metrics Grafana 说是开箱即用，确实也是我们只需要clone下来，然后kubectl apply ./manifests，manifests目录中生成的是预先生成的yaml描述文件，有诸多不方便的地方，比如说 镜像仓库的地址都在gcr和query.io，这两个地址国内拉起来都费劲 没有持久化存储promethus的数据 二、安装 1、默认安装 git clone https://github.com/coreos/kube-prometheus.git -b release-0.5 cd kube-prometheus kubectl apply -f manifests/setup kubectl apply -f manifests/ 之后手动修改Prometheus的k8s资源声明对象 2、定制化安装 ①安装jb、jsonnet、gojsontoyaml wget -c https://github.com/jsonnet-bundler/jsonnet-bundler/releases/download/v0.4.0/jb-darwin-amd64 -o /usr/local/bin/jb chmod +x /usr/local/bin/jb git clone https://github.com/brancz/gojsontoyaml.git cd gojsontoyaml go build chmod +x gojsontoyaml mv gojsontoyaml /usr/local/bin/ echo '{\"test\":\"test string with\\\\nmultiple lines\"}' | gojsontoyaml ②下载依赖 mkdir kube-prometheus-k8s118; cd kube-prometheus-k8s118 jb init # 初始化jb，创建依赖描述文件`jsonnetfile.json` jb install github.com/coreos/kube-prometheus/jsonnet/kube-prometheus@release-0.5 jb update ③编写编译命令的脚本build.sh #!/usr/bin/env bash # This script uses arg $1 (name of *.jsonnet file to use) to generate the manifests/*.yaml files. set -e set -x # only exit with zero if all commands of the pipeline exit successfully set -o pipefail # Make sure to use project tooling PATH=\"$(pwd)/tmp/bin:${PATH}\" # Make sure to start with a clean 'manifests' dir rm -rf manifests mkdir -p manifests/setup # Calling gojsontoyaml is optional, but we would like to generate yaml, not json jsonnet -J vendor -m manifests \"${1-example.jsonnet}\" | xargs -I{} sh -c 'cat {} | gojsontoyaml > {}.yaml' -- {} # Make sure to remove json files find manifests -type f ! -name '*.yaml' -delete rm -f kustomization ④编写定制化Prometheus的配置文件 在第三部jb init产生的k8s118.json中进行定制化Prometheus local k = import 'ksonnet/ksonnet.beta.3/k.libsonnet'; local ingress = k.extensions.v1beta1.ingress ; local ingressRule = ingress.mixin.spec.rulesType; local pvc = k.core.v1.persistentVolumeClaim; local registry = import 'registry.libsonnet'; local imagepullsecret = k.apps.v1beta2.deployment.mixin.spec.template.spec; local httpIngressPath = ingressRule.mixin.http.pathsType; local secret = k.core.v1.secret; local kp = (import 'kube-prometheus/kube-prometheus.libsonnet') + { _config+:: { namespace: 'monitoring', imageRepos+:: { prometheus: \"192.168.1.60/monitoring/prometheus\", alertmanager: \"192.168.1.60/monitoring/alertmanager\", kubeStateMetrics: \"192.168.1.60/monitoring/kube-state-metrics\", kubeRbacProxy: \"192.168.1.60/monitoring/kube-rbac-proxy\", nodeExporter: \"192.168.1.60/monitoring/node-exporter\", prometheusOperator: \"192.168.1.60/monitoring/prometheus-operator\", grafana: \"192.168.1.60/monitoring/grafana\", prometheusAdapter: \"192.168.1.60/monitoring/k8s-prometheus-adapter-amd64\", metricsServer: \"192.168.1.60/monitoring/metrics-server-amd64:v0.2.0\", }, grafana+:: { config+: { sections+: { server+: { root_url: 'http://grafana.apps.k8s118.curiouser.com/', }, smtp: { enabled: 'true', host: 'smtp.163.com:25', user: '****', password: '****', from_address: '****', from_name: 'Grafana', skip_verify: 'true', }, }, }, }, registry+:: { name: \"192.168.1.60\", secret_name: \"harbor-secret\", username: \"****\", password: \"****\", secret: local data = { 'auths': { [$._config.registry.name]: { auth: std.base64($._config.registry.username + \":\" + $._config.registry.password), }, }, }; local base = {'.dockerconfigjson': std.base64(std.toString(data))}; local name = $._config.registry.secret_name; secret.mixin.metadata.withNamespace($._config.namespace) + secret.new(name=name, data=base, type='kubernetes.io/dockerconfigjson') }, }, alertmanager+: { alertmanager+: { spec+: { imagePullSecrets: [{name: $._config.registry.secret_name}], } }, }, grafana+: { deployment+: imagepullsecret.withImagePullSecrets( {name: $._config.registry.secret_name} ), }, kubeStateMetrics+: { deployment+: imagepullsecret.withImagePullSecrets( {name: $._config.registry.secret_name} ), }, nodeExporter+: { daemonset+: imagepullsecret.withImagePullSecrets( {name: $._config.registry.secret_name} ), }, prometheusAdapter+: { deployment+: imagepullsecret.withImagePullSecrets( {name: $._config.registry.secret_name} ), }, prometheusOperator+: { deployment+: imagepullsecret.withImagePullSecrets( {name: $._config.registry.secret_name} ), }, prometheus+:: { prometheus+: { spec+: { retention: '7d', storage: { volumeClaimTemplate: pvc.new() + pvc.mixin.spec.withAccessModes('ReadWriteOnce') + pvc.mixin.spec.resources.withRequests({ storage: '50Gi' }), }, }, }, }, ingress+:: { 'grafana': ingress.new() + ingress.mixin.metadata.withName('grafana') + ingress.mixin.metadata.withNamespace($._config.namespace) + ingress.mixin.spec.withRules( ingressRule.new() + ingressRule.withHost('grafana.apps.k8s118.curiouser.com') + ingressRule.mixin.http.withPaths( httpIngressPath.new() + httpIngressPath.mixin.backend.withServiceName('grafana') + httpIngressPath.mixin.backend.withServicePort('http') ) ), 'alertmanager-main': ingress.new() + ingress.mixin.metadata.withName('alertmanager-main') + ingress.mixin.metadata.withNamespace($._config.namespace) + ingress.mixin.spec.withRules( ingressRule.new() + ingressRule.withHost('alertmanager.apps.k8s118.curiouser.com') + ingressRule.mixin.http.withPaths( httpIngressPath.new() + httpIngressPath.mixin.backend.withServiceName('alertmanager-main') + httpIngressPath.mixin.backend.withServicePort('web') ) ), 'prometheus-k8s': ingress.new() + ingress.mixin.metadata.withName('prometheus-k8s') + ingress.mixin.metadata.withNamespace($._config.namespace) + ingress.mixin.spec.withRules( ingressRule.new() + ingressRule.withHost('prometheus.apps.k8s118.curiouser.com') + ingressRule.mixin.http.withPaths( httpIngressPath.new() + httpIngressPath.mixin.backend.withServiceName('prometheus-k8s') + httpIngressPath.mixin.backend.withServicePort('web') ) ), } }; { ['setup/0namespace-' + name]: kp.kubePrometheus[name] for name in std.objectFields(kp.kubePrometheus) } + // { ['setup/0secret-' + name]: kp.secret[name] for name in std.objectFields(kp.secret)}+ { ['setup/prometheus-operator-' + name]: kp.prometheusOperator[name] for name in std.filter((function(name) name != 'serviceMonitor'), std.objectFields(kp.prometheusOperator)) } + // serviceMonitor is separated so that it can be created after the CRDs are ready { 'prometheus-operator-serviceMonitor': kp.prometheusOperator.serviceMonitor } + { ['node-exporter-' + name]: kp.nodeExporter[name] for name in std.objectFields(kp.nodeExporter) } + { ['kube-state-metrics-' + name]: kp.kubeStateMetrics[name] for name in std.objectFields(kp.kubeStateMetrics) } + { ['alertmanager-' + name]: kp.alertmanager[name] for name in std.objectFields(kp.alertmanager) } + { ['prometheus-' + name]: kp.prometheus[name] for name in std.objectFields(kp.prometheus) } + { ['prometheus-adapter-' + name]: kp.prometheusAdapter[name] for name in std.objectFields(kp.prometheusAdapter) } + { ['grafana-' + name]: kp.grafana[name] for name in std.objectFields(kp.grafana) } + { [name + '-ingress']: kp.ingress[name] for name in std.objectFields(kp.ingress) } ⑤开始编译 ./build.sh jsonnetfile.json 编译完成后，会在manifests目录下生产K8s资源声明文件。 ⑥产生的K8s资源对象 manifests/setup/*.yaml namespace: monitoring customresourcedefinition.apiextensions.k8s.io: alertmanagers.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io: podmonitors.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io: prometheuses.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io: prometheusrules.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io: servicemonitors.monitoring.coreos.com customresourcedefinition.apiextensions.k8s.io: thanosrulers.monitoring.coreos.com clusterrole.rbac.authorization.k8s.io: prometheus-operator clusterrolebinding.rbac.authorization.k8s.io: prometheus-operator deployment.apps: prometheus-operator service: prometheus-operator serviceaccount: prometheus-operator manifests/*.yaml alertmanager.monitoring.coreos.com: main secret: alertmanager-main service: alertmanager-main serviceaccount: alertmanager-main servicemonitor.monitoring.coreos.com: alertmanager secret: grafana-datasources configmap: grafana-dashboard-apiserver configmap: grafana-dashboard-cluster-total configmap: grafana-dashboard-controller-manager configmap: grafana-dashboard-k8s-resources-cluster configmap: grafana-dashboard-k8s-resources-namespace configmap: grafana-dashboard-k8s-resources-node configmap: grafana-dashboard-k8s-resources-pod configmap: grafana-dashboard-k8s-resources-workload configmap: grafana-dashboard-k8s-resources-workloads-namespace configmap: grafana-dashboard-kubelet configmap: grafana-dashboard-namespace-by-pod configmap: grafana-dashboard-namespace-by-workload configmap: grafana-dashboard-node-cluster-rsrc-use configmap: grafana-dashboard-node-rsrc-use configmap: grafana-dashboard-nodes configmap: grafana-dashboard-persistentvolumesusage configmap: grafana-dashboard-pod-total configmap: grafana-dashboard-prometheus-remote-write configmap: grafana-dashboard-prometheus configmap: grafana-dashboard-proxy configmap: grafana-dashboard-scheduler configmap: grafana-dashboard-statefulset configmap: grafana-dashboard-workload-total configmap: grafana-dashboards deployment.apps: grafana service: grafana serviceaccount: grafana servicemonitor.monitoring.coreos.com: grafana clusterrole.rbac.authorization.k8s.io: kube-state-metrics clusterrolebinding.rbac.authorization.k8s.io: kube-state-metrics deployment.apps: kube-state-metrics service: kube-state-metrics serviceaccount: kube-state-metrics servicemonitor.monitoring.coreos.com: kube-state-metrics clusterrole.rbac.authorization.k8s.io: node-exporter clusterrolebinding.rbac.authorization.k8s.io: node-exporter daemonset.apps: node-exporter service: node-exporter serviceaccount: node-exporter servicemonitor.monitoring.coreos.com: node-exporter apiservice.apiregistration.k8s.io: v1beta1.metrics.k8s.io configured clusterrole.rbac.authorization.k8s.io: prometheus-adapter clusterrole.rbac.authorization.k8s.io: system:aggregated-metrics-reader clusterrolebinding.rbac.authorization.k8s.io: prometheus-adapter clusterrolebinding.rbac.authorization.k8s.io: resource-metrics:system:auth-delegator clusterrole.rbac.authorization.k8s.io: resource-metrics-server-resources configmap: adapter-config deployment.apps: prometheus-adapter rolebinding.rbac.authorization.k8s.io: resource-metrics-auth-reader service: prometheus-adapter serviceaccount: prometheus-adapter clusterrole.rbac.authorization.k8s.io: prometheus-k8s clusterrolebinding.rbac.authorization.k8s.io: prometheus-k8s servicemonitor.monitoring.coreos.com: prometheus-operator prometheus.monitoring.coreos.com: k8s rolebinding.rbac.authorization.k8s.io: prometheus-k8s-config rolebinding.rbac.authorization.k8s.io: prometheus-k8s rolebinding.rbac.authorization.k8s.io: prometheus-k8s rolebinding.rbac.authorization.k8s.io: prometheus-k8s role.rbac.authorization.k8s.io: prometheus-k8s-config role.rbac.authorization.k8s.io: prometheus-k8s role.rbac.authorization.k8s.io: prometheus-k8s role.rbac.authorization.k8s.io: prometheus-k8s prometheusrule.monitoring.coreos.com: prometheus-k8s-rules service: prometheus-k8s serviceaccount: prometheus-k8s servicemonitor.monitoring.coreos.com: prometheus servicemonitor.monitoring.coreos.com: kube-apiserver servicemonitor.monitoring.coreos.com: coredns servicemonitor.monitoring.coreos.com: kube-controller-manager servicemonitor.monitoring.coreos.com: kube-scheduler servicemonitor.monitoring.coreos.com: kubelet ⑦预拉取镜像并推送到私有仓库中 sync-to-internal-registry.jsonnet local kp = import 'kube-prometheus/kube-prometheus.libsonnet'; local l = import 'kube-prometheus/lib/lib.libsonnet'; local config = kp._config; local makeImages(config) = [ { name: config.imageRepos[image], tag: config.versions[image], } for image in std.objectFields(config.imageRepos) ]; local upstreamImage(image) = '%s:%s' % [image.name, image.tag]; local downstreamImage(registry, image) = '%s/%s:%s' % [registry, l.imageName(image.name), image.tag]; local pullPush(image, newRegistry) = [ 'docker pull %s' % upstreamImage(image), 'docker tag %s %s' % [upstreamImage(image), downstreamImage(newRegistry, image)], 'docker push %s' % downstreamImage(newRegistry, image), ]; local images = makeImages(config); local output(repository) = std.flattenArrays([ pullPush(image, repository) for image in images ]); function(repository='my-registry.com/repository') std.join('\\n', output(repository)) 生成拉取镜像，并修改推送镜像的命令 $ jsonnet -J vendor -S --tla-str repository=192.168.1.60/monitoring sync-to-internal-registry.jsonnet # 会生成以下命令，复制执行 docker pull quay.io/prometheus/alertmanager:v0.20.0 docker tag quay.io/prometheus/alertmanager:v0.20.0 192.168.1.60/monitoring/alertmanager:v0.20.0 docker push 192.168.1.60/monitoring/alertmanager:v0.20.0 docker pull jimmidyson/configmap-reload:v0.3.0 docker tag jimmidyson/configmap-reload:v0.3.0 192.168.1.60/monitoring/configmap-reload:v0.3.0 docker push 192.168.1.60/monitoring/configmap-reload:v0.3.0 docker pull grafana/grafana:6.6.0 docker tag grafana/grafana:6.6.0 192.168.1.60/monitoring/grafana:6.6.0 docker push 192.168.1.60/monitoring/grafana:6.6.0 docker pull quay.io/coreos/kube-rbac-proxy:v0.4.1 docker tag quay.io/coreos/kube-rbac-proxy:v0.4.1 192.168.1.60/monitoring/kube-rbac-proxy:v0.4.1 docker push 192.168.1.60/monitoring/kube-rbac-proxy:v0.4.1 docker pull quay.io/coreos/kube-state-metrics:1.9.5 docker tag quay.io/coreos/kube-state-metrics:1.9.5 192.168.1.60/monitoring/kube-state-metrics:1.9.5 docker push 192.168.1.60/monitoring/kube-state-metrics:1.9.5 docker pull quay.io/prometheus/node-exporter:v0.18.1 docker tag quay.io/prometheus/node-exporter:v0.18.1 192.168.1.60/monitoring/node-exporter:v0.18.1 ⑧部署定制化的Prometheus到k8s集群 kubectl apply -f manifests/setup kubectl apply -f manifests/ # 创建harbor registry用户登录信息的secret kubectl create secret docker-registry harbor-secret --docker-server=192.168.1.60 --docker-username=k8s --docker-password=*** --docker-email=*** -n monitoring # 批量在monitoring命名空间下serviceaccount中添加imagePullSercret for i in `k get sa |awk '{print $1}'|grep -v NAME` ; do kubectl -n monitoring patch serviceaccount $i -p '{\"imagePullSecrets\": [{\"name\": \"harbor-secret\"}]}' ;done ⑨(可选)清理Prometheus资源 kubectl delete --ignore-not-found=true -f manifests/ -f manifests/setup 3、参考 https://qingmu.io/2019/08/30/Customize-your-kube-prometheus/ https://github.com/coreos/kube-prometheus#cluster-creation-tools https://github.com/coreos/kube-prometheus#customization-examples https://github.com/coreos/kube-prometheus/blob/master/docs/developing-prometheus-rules-and-grafana-dashboards.md https://github.com/coreos/kube-prometheus/issues/308 三、ServiceMonitor服务发现 1、监控k8s集群外的Ceph ①在cephMonitor节点部署Ceph exporter docker run -d --restart=always -v /etc/ceph:/etc/ceph -p=9128:9128 --name=ceph-export digitalocean/ceph_exporter --rgw.mode=1 ②创建对应的Endpoint、Service、ServiceMonitor apiVersion: v1 kind: Endpoints metadata: name: outcluster-exporter-ceph namespace: monitoring subsets: - addresses: - ip: 192.168.1.60 ports: - name: http port: 9128 protocol: TCP --- apiVersion: v1 kind: Service metadata: # 关键点：为了能被servicemonitor识别，添加对应的标签 labels: prometheus-target: outcluster-exporter-ceph name: outcluster-exporter-ceph namespace: monitoring spec: type: ClusterIP ports: - name: http port: 9128 protocol: TCP targetPort: http # 关键点：由于ceph exporter不在k8s集群中，不写常规service中的selector。 --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: k8s-apps: outcluster-exporter-ceph name: outcluster-exporter-ceph namespace: monitoring spec: endpoints: - interval: 15s port: http namespaceSelector: matchNames: - monitoring selector: matchLabels: # 指定prometheus去匹配有对应标签的service中监控数据 prometheus-target: outcluster-exporter-ceph 2、监控k8s集群外的主机 ①对应主机部署Node exporter 参考：二进制部署Prometheus生态组件 ②创建对应的Endpoint、Service、ServiceMonitor apiVersion: v1 kind: Endpoints metadata: name: outcluster-exporter-node-tools namespace: monitoring labels: prometheus-target: outcluster-exporter-node subsets: - addresses: - ip: 192.168.1.60 ports: - name: http port: 9100 protocol: TCP --- apiVersion: v1 kind: Service metadata: # 关键点：为了能被servicemonitor识别，添加对应的标签 labels: prometheus-target: outcluster-exporter-node name: outcluster-exporter-node-tools namespace: monitoring spec: type: ClusterIP ports: - name: http port: 9100 protocol: TCP targetPort: http # 关键点：由于node exporter不在k8s集群中，不写常规service中的selector。 --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: k8s-apps: outcluster-exporter-node name: outcluster-exporter-node namespace: monitoring spec: endpoints: - interval: 15s port: http namespaceSelector: matchNames: - monitoring selector: matchLabels: # 指定prometheus去匹配有对应标签的service中监控数据 prometheus-target: outcluster-exporter-node Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-09-22 12:56:03 "},"origin/prometheus-binary-docker-deploy.html":{"url":"origin/prometheus-binary-docker-deploy.html","title":"二进制Docker部署Prometheus生态","keywords":"","body":"部署Prometheus生态组件 一、二进制部署 1.1、下载安装 Prometheus、Node_exporter、Black_exporter prometheus_version=2.28.0 && \\ blackbox_exporter_version=0.19.0 && \\ node_exporter_version=1.1.2 && \\ wget https://github.com/prometheus/prometheus/releases/download/v$prometheus_version/prometheus-$prometheus_version.linux-amd64.tar.gz -P /opt && \\ tar -zxvf /opt/prometheus-$prometheus_version.linux-amd64.tar.gz -C /opt && \\ ln -s /opt/prometheus-$prometheus_version.linux-amd64 /opt/prometheus && \\ wget https://github.com/prometheus/blackbox_exporter/releases/download/v$0.19.0/blackbox_exporter-$0.19.0.linux-amd64.tar.gz -P /opt && \\ tar -zxvf /opt/blackbox_exporter-$blackbox_exporter_version.linux-amd64.tar.gz -C /opt && \\ ln -s /opt/blackbox_exporter-$blackbox_exporter_version.linux-amd64 /opt/blackbox_exporter && \\ wget https://github.com/prometheus/node_exporter/releases/download/v$node_exporter_version/node_exporter-$node_exporter_version.linux-amd64.tar.gz -C /opt && \\ tar -zxvf /opt/node_exporter-$node_exporter_version.linux-amd64.tar.gz -C /opt && \\ ln -s /opt/node_exporter-1.1.2.linux-amd64 /opt/node_exporter && \\ rm -rf /opt/*.tar.gz && \\ echo -e \"export PROMETHEUS_HOME=/opt/prometheus\\nexport NODE_EXPORTER_HOME=/opt/node_exporter\\nexport BLACK_EXPORTER_HOME=/opt/blackbox_exporter\\nexport PATH=\\$PATH:\\$PROMETHEUS_HOME:\\$NODE_EXPORTER_HOME:\\$BLACK_EXPORTER_HOME\" >> /etc/profile && \\ source /etc/profile && \\ prometheus --version && \\ blackbox_exporter --version && \\ node_exporter --version Grafana apt-get install -y apt-transport-https apt-get install -y software-properties-common wget wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add - echo \"deb https://packages.grafana.com/oss/deb stable main\" | sudo tee -a /etc/apt/sources.list.d/grafana.list apt-get update apt-get install grafana -y grafana-server -vv 2、配置启动 Prometheus mkdir -p /data/prometheus/data && \\ bash -c 'cat >/etc/systemd/system/prometheus.service Node exporter bash -c 'cat >/etc/systemd/system/node_exporter.service Blackbox exporter bash -c 'cat >/etc/systemd/system/blackbox_exporter.service Grafana mkdir -p /data/grafana/{data,logs} && \\ systemctl daemon-reload && \\ systemctl start grafana-server && \\ systemctl status grafana-server && \\ netstat -lanp|grep 3000 && \\ systemctl enable grafana-server 二、Docker部署 docker run \\ -p 9090:9090 \\ -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml \\ prom/prometheus docker run -d \\ -p 3000:3000 \\ --name=grafana \\ -e \"GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource\" \\ grafana/grafana Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/prometheus-Ceph-Exporter对接Prometheus以监控ceph集群.html":{"url":"origin/prometheus-Ceph-Exporter对接Prometheus以监控ceph集群.html","title":"Ceph Exporter","keywords":"","body":"一、Overview 由于在Openshift集群外使用了Ceph RBD和Ceph Filesystem作为PV的后端动态存储文件系统，所以ceph的集群监控也可使用Prometheus体系中的Ceph Exporter，接入到Openshift集群中的Prometheus。 二、以DaemonSet的形式部署Ceph Exporter --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: labels: app: ceph-exporter name: ceph-exporter spec: selector: matchLabels: app: ceph-exporter template: metadata: labels: app: ceph-exporter spec: containers: - image: digitalocean/ceph_exporter imagePullPolicy: IfNotPresent name: ceph-exporter ports: - containerPort: 9128 hostPort: 9128 name: http protocol: TCP resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/ceph name: ceph-confdir resources: limits: cpu: 200m memory: 400Mi requests: cpu: 100m memory: 200Mi dnsPolicy: ClusterFirst hostNetwork: true hostPID: true nodeSelector: beta.kubernetes.io/os: linux restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: node-exporter　#使用Node-Exporter创建的ServiceAccount serviceAccountName: node-exporter terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master volumes: - hostPath: path: /etc/ceph #将ceph节点的配置文件路径暴露给exporter type: \"\" name: ceph-confdir templateGeneration: 1 updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate --- apiVersion: v1 kind: Endpoints metadata: labels: k8s-app: ceph-exporter name: ceph-exporter subsets: - addresses: - ip: 192.168.1.96 nodeName: allinone.okd311.curiouser.com targetRef: kind: Pod ports: - name: http port: 9128 protocol: TCP --- apiVersion: v1 kind: Service metadata: annotations: prometheus.io/port: '9128' prometheus.io/scrape: 'true' labels: k8s-app: ceph-exporter name: ceph-exporter spec: clusterIP: None ports: - name: http port: 9128 protocol: TCP targetPort: http selector: app: ceph-exporter sessionAffinity: None type: ClusterIP --- apiVersion: route.openshift.io/v1 kind: Route metadata: annotations: openshift.io/host.generated: 'true' labels: k8s-app: ceph-exporter name: ceph-exporter spec: port: targetPort: http to: kind: Service name: ceph-exporter weight: 100 wildcardPolicy: None 三、Ceph Exporter对接Prometheus 备份Prometheus原配置文件secret Prometheus原始配置secret文件 创建新的Prometheus配置secret 在原Prometheus配置文件中添加consul 服务发现和ceph-exporter相关的配置 ...省略... - job_name: consul-prometheus metrics_path: /monitor/prometheus scrape_interval: 20s scheme: http scrape_timeout: 5s consul_sd_configs: - server: consul-server.consul.svc:8500 services: [] scheme: http allow_stale: true refresh_interval: 20s - job_name: openshift-monitoring/ceph-exporter/0 honor_labels: false kubernetes_sd_configs: - role: endpoints namespaces: names: - openshift-monitoring scrape_interval: 30s scheme: http relabel_configs: - action: keep source_labels: - __meta_kubernetes_service_label_k8s_app regex: ceph-exporter - action: keep source_labels: - __meta_kubernetes_endpoint_port_name regex: http - source_labels: - __meta_kubernetes_namespace target_label: namespace - source_labels: - __meta_kubernetes_pod_name target_label: pod - source_labels: - __meta_kubernetes_service_name target_label: service - source_labels: - __meta_kubernetes_service_name target_label: job replacement: ${1} - source_labels: - __meta_kubernetes_service_label_k8s_app target_label: job regex: (.+) replacement: ${1} - target_label: endpoint replacement: http ...省略... 替换Prometheus的POD secret 四、验证 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/prometheus-blackbox-exporter.html":{"url":"origin/prometheus-blackbox-exporter.html","title":"Blackbox Exporter","keywords":"","body":"Blackbox exporter 一、简介 黑盒监控即以用户的身份测试服务的外部可见性，常见的黑盒监控包括HTTP 探针、TCP 探针 等用于检测站点或者服务的可访问性，以及访问效率等。 黑盒监控相较于白盒监控最大的不同在于黑盒监控是以故障为导向当故障发生时，黑盒监控能快速发现故障，而白盒监控则侧重于主动发现或者预测潜在的问题。一个完善的监控目标是要能够从白盒的角度发现潜在问题，能够在黑盒的角度快速发现已经发生的问题。 Blackbox Exporter 是 Prometheus 社区提供的官方黑盒监控解决方案，其允许用户通过：HTTP、HTTPS、DNS、TCP 以及 ICMP 的方式对网络进行探测 GitHub：https://github.com/prometheus/blackbox_exporter 二、安装 1、二进制 blackbox_exporter_version=0.18.0 && \\ wget https://github.com/prometheus/blackbox_exporter/releases/download/v$blackbox_exporter_version/blackbox_exporter-$blackbox_exporter_version.linux-amd64.tar.gz && \\ tar -zxvf blackbox_exporter-$blackbox_exporter_version.linux-amd64.tar.gz -C /opt && \\ rm -f blackbox_exporter-$blackbox_exporter_version.linux-amd64.tar.gz && \\ ln -s /opt/blackbox_exporter-0.18.0.linux-amd64 /opt/blackbox_exporter && \\ echo -e \"export BLACKBOX_EXPORTER_HOEM=/opt/blackbox_exporter\\nexport PATH=\\$PATH:\\$BLACKBOX_EXPORTER_HOEM\" >> /etc/profile && \\ source /etc/profile && \\ blackbox_exporter --help 命令参数 usage: blackbox_exporter [] Flags: -h, --help Show context-sensitive help (also try --help-long and --help-man). --config.file=\"blackbox.yml\" Blackbox exporter configuration file. --web.listen-address=\":9115\" The address to listen on for HTTP requests. --timeout-offset=0.5 Offset to subtract from timeout in seconds. --config.check If true validate the config file and then exit. --history.limit=100 The maximum amount of items to keep in the history. --web.external-url= The URL under which Blackbox exporter is externally reachable (for example, if Blackbox exporter is served via a reverse proxy). Used for generating relative and absolute links back to Blackbox exporter itself. If the URL has a path portion, it will be used to prefix all HTTP endpoints served by Blackbox exporter. If omitted, relevant URL components will be derived automatically. --web.route-prefix= Prefix for the internal routes of web endpoints. Defaults to path of --web.external-url. --log.level=info Only log messages with the given severity or above. One of: [debug, info, warn, error] --log.format=logfmt Output format of log messages. One of: [logfmt, json] --version Show application version. 启动 nohup blackbox_exporter --config.file=配置文件路径 --其他参数 > /var/log/blackbox_exporter.log 2>&1 & 2、Docker docker run --rm -d \\ -p 9115:9115 \\ --name blackbox_exporter \\ -v `配置文件路径`:/config \\ prom/blackbox-exporter:master --config.file=/config/blackbox.yml 3、Kubernetes apiVersion: apps/v1 kind: Deployment metadata: name: blackbox namespace: monitoring labels: app: blackbox-exporter spec: replicas: 2 revisionHistoryLimit: 3 selector: matchLabels: app: blackbox-exporter strategy: rollingUpdate: maxSurge: 30% maxUnavailable: 30% type: RollingUpdate template: metadata: labels: app: blackbox-exporter spec: containers: - image: prom/blackbox-exporter:master name: blackbox-exporter args: - --config.file=/etc/blackbox_exporter/blackbox.yml # ConfigMap 中的配置文件 - --log.level=info # 日志级别，可以把级别调到 error ports: - containerPort: 9115 name: http volumeMounts: - name: config mountPath: /etc/blackbox_exporter volumes: - name: config configMap: name: blackbox-config nodeSelector: role: monitoring --- apiVersion: v1 kind: ConfigMap metadata: name: blackbox-config namespace: monitoring data: blackbox.yml: |- modules: http_2xx: prober: http timeout: 10s http: valid_status_codes: [0,200] baidu-header: prober: http timeout: 10s http: valid_status_codes: [0,200] method: GET headers: Access-Token: *** --- apiVersion: v1 kind: Service metadata: labels: app: blackbox-exporter name: blackbox-exporter namespace: monitoring spec: ports: - name: http port: 9115 targetPort: http selector: app: blackbox-exporter 三、配置文件 配置文件详解：https://github.com/prometheus/blackbox_exporter/blob/master/CONFIGURATION.md 示例配置文件：https://github.com/prometheus/blackbox_exporter/blob/master/example.yml 1、配置文件结构 2、配置值类型 : 布尔值，可选 true | false : 整型值 : 与正则表达式[0-9] +（ms | [smhdwy]）匹配的持续时间 : 当前工作目录中的有效路径 : 字符串 : 包含密码的常规字符串，例如密码 : 正则表达式 3、HTTP探针配置 # 此探针接受的响应状态代码。默认为2xx。 [ valid_status_codes: , ... | default = 2xx ] # 此探针接受的HTTP版本。 [ valid_http_versions: , ... ] # The HTTP method the probe will use. [ method: | default = \"GET\" ] # 为探针设置的HTTP标头。 headers: [ : ... ] # 用于解压缩响应的压缩算法（gzip，br，deflate，identity）。如果指定了“ Accept-Encoding”标头，则必须使压缩算法，表示使用此选项是可接受的。例如可以使用`compression：gzip`和`Accept-Encoding：br，gzip`或`Accept-Encoding：br; q = 1.0，gzip; q = 0.9`。 gzip是 # 可接受的质量低于br的质量不会使配置无效，因为您可能会测试服务器即使请求也不会返回br编码的内容。在另一方面，“ compression：gzip”和“ Accept-Encoding：br，identity”不是有效的配置，因为您要求不返回gzip，并尝试解压缩服务器返回的任何内容都可能会失败。[压缩： |默认=“”] [ compression: | default = \"\" ] # 探针是否将遵循任何重定向 [ no_follow_redirects: | default = false ] # 如果存在SSL，则探测失败 [ fail_if_ssl: | default = false ] # 如果不存在SSL，则探测失败。 [ fail_if_not_ssl: | default = false ] # 如果响应内容与正则表达式匹配，则探测失败 fail_if_body_matches_regexp: [ - , ... ] # 如果响应内容与正则表达式不匹配，则探测失败 fail_if_body_not_matches_regexp: [ - , ... ] # 如果响应头与正则表达式匹配，则探测失败。对于具有多个值的标头，如果至少一个匹配，则失败。 fail_if_header_matches: [ - , ... ] # 如果响应头与正则表达式不匹配，则探测失败。对于具有多个值的标头，如果一个也不匹配，则失败。 fail_if_header_not_matches: [ - , ... ] # 为此探针配置TLS协议 tls_config: [ ] # 为此探针配置HTTP基本身份验证凭据。 basic_auth: [ username: ] [ password: ] [ password_file: ] # 为此探针配置访问目标的Bearer token [ bearer_token: ] # 为此探针配置访问目标的Bearer token文件 [ bearer_token_file: ] # 用于连接到目标的HTTP代理服务器。 [ proxy_url: ] # 为此探针配置IP协议（ip4，ip6） [ preferred_ip_protocol: | default = \"ip6\" ] [ ip_protocol_fallback: | default = true ] # 为此探针配置访问目标的HTTP请求主体。 body: [ ] 匹配Header的正则表达式配置 header: , regexp: , [ allow_missing: | default = false ] 4、TCP探针配置 # The IP protocol of the TCP probe (ip4, ip6). [ preferred_ip_protocol: | default = \"ip6\" ] [ ip_protocol_fallback: ] # The source IP address. [ source_ip_address: ] # The query sent in the TCP probe and the expected associated response. # starttls upgrades TCP connection to TLS. query_response: [ - [ [ expect: ], [ send: ], [ starttls: ] ], ... ] # Whether or not TLS is used when the connection is initiated. [ tls: ] # Configuration for TLS protocol of TCP probe. tls_config: [ ] 5、DNS探针配置 # The IP protocol of the DNS probe (ip4, ip6). [ preferred_ip_protocol: | default = \"ip6\" ] [ ip_protocol_fallback: ] # The source IP address. [ source_ip_address: ] [ transport_protocol: | default = \"udp\" ] # udp, tcp # Whether to use DNS over TLS. This only works with TCP. [ dns_over_tls: ] # Configuration for TLS protocol of DNS over TLS probe. tls_config: [ ] query_name: [ query_type: | default = \"ANY\" ] [ query_class: | default = \"IN\" ] # List of valid response codes. valid_rcodes: [ - ... | default = \"NOERROR\" ] validate_answer_rrs: fail_if_matches_regexp: [ - , ... ] fail_if_all_match_regexp: [ - , ... ] fail_if_not_matches_regexp: [ - , ... ] fail_if_none_matches_regexp: [ - , ... ] validate_authority_rrs: fail_if_matches_regexp: [ - , ... ] fail_if_all_match_regexp: [ - , ... ] fail_if_not_matches_regexp: [ - , ... ] fail_if_none_matches_regexp: [ - , ... ] validate_additional_rrs: fail_if_matches_regexp: [ - , ... ] fail_if_all_match_regexp: [ - , ... ] fail_if_not_matches_regexp: [ - , ... ] fail_if_none_matches_regexp: [ - , ... ] 6、ICMP探针配置 # The IP protocol of the ICMP probe (ip4, ip6). [ preferred_ip_protocol: | default = \"ip6\" ] [ ip_protocol_fallback: ] # The source IP address. [ source_ip_address: ] # Set the DF-bit in the IP-header. Only works with ip4, on *nix systems and # requires raw sockets (i.e. root or CAP_NET_RAW on Linux). [ dont_fragment: | default = false ] # The size of the payload. [ payload_size: ] 7、TLS相关配置 # Disable target certificate validation. [ insecure_skip_verify: | default = false ] # The CA cert to use for the targets. [ ca_file: ] # The client cert file for the targets. [ cert_file: ] # The client key file for the targets. [ key_file: ] # Used to verify the hostname for the targets. [ server_name: ] 四、其他 1、 参考： https://www.qikqiak.com/post/blackbox-exporter-on-prometheus/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-07-04 18:17:51 "},"origin/prometheus-nginx-exporter.html":{"url":"origin/prometheus-nginx-exporter.html","title":"收集Nginx内置Metrics","keywords":"","body":"收集Nginx内置Metrics到Prometheus 一、简介 Nginx的ngx_http_stub_status_module模块可以暴露出来nginx的一些性能信息。但是这些信息不是prometheus的metrics信息格式。所以需要一个第三方服务用于转换这些数据，然后才能让prometheus拉取信息。 二、配置 1、Nginx 文档：http://nginx.org/en/docs/http/ngx_http_stub_status_module.html#variables ngx_http_stub_status_module模块没有内置到nginx。如果需要开起，在编译Nginx时添加--with-http_stub_status_module参数 server { listen 8080; location /basic_status { stub_status; } } 访问http://localhost:8080/basic_status Active connections: 2 server accepts handled requests 37 37 519 Reading: 0 Writing: 1 Waiting: 1 2、Nginx Exporter 文档：https://github.com/nginxinc/nginx-prometheus-exporter docker run -d \\ --name nginx-prometheus-exporter \\ -p 9113:9113 \\ nginx/nginx-prometheus-exporter:0.9.0 -nginx.scrape-uri=http://localhost:8080/basic_status 访问http://localhost:9113/metrics # HELP nginx_connections_accepted Accepted client connections # TYPE nginx_connections_accepted counter nginx_connections_accepted 37 # HELP nginx_connections_active Active client connections # TYPE nginx_connections_active gauge nginx_connections_active 2 # HELP nginx_connections_handled Handled client connections # TYPE nginx_connections_handled counter nginx_connections_handled 37 # HELP nginx_connections_reading Connections where NGINX is reading the request header # TYPE nginx_connections_reading gauge nginx_connections_reading 0 # HELP nginx_connections_waiting Idle client connections # TYPE nginx_connections_waiting gauge nginx_connections_waiting 1 # HELP nginx_connections_writing Connections where NGINX is writing the response back to the client # TYPE nginx_connections_writing gauge nginx_connections_writing 1 # HELP nginx_http_requests_total Total http requests # TYPE nginx_http_requests_total counter nginx_http_requests_total 519 # HELP nginx_up Status of the last metric scrape # TYPE nginx_up gauge nginx_up 1 # HELP nginxexporter_build_info Exporter build information # TYPE nginxexporter_build_info gauge nginxexporter_build_info{commit=\"5f88afbd906baae02edfbab4f5715e06d88538a0\",date=\"2021-03-22T20:16:09Z\",version=\"0.9.0\"} 1 3、Prometheus # ...省略... scrape_configs: - job_name: 'nginx_exporter' static_configs: - targets: ['localhost:9113'] 4、Grafana Grafana仪表盘：https://grafana.com/grafana/dashboards/11280/reviews 优化修改后的仪表盘JSON文件：nginx-exporter-dashboard.json 三、总结 暴露出来的Nginx信息太少，没有监控价值 参考： https://cloud.tencent.com/document/product/1416/56039 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/grafana-backup-restore.html":{"url":"origin/grafana-backup-restore.html","title":"Grafana的备份恢复","keywords":"","body":"Grafana的备份恢复与升级 一、备份恢复 官方文档：https://grafana.com/docs/grafana/latest/installation/upgrading/#backup 数据存储文件 MySQL 备份： mysqldump -u root -p[root_password] [grafana] > grafana_backup.sql 恢复：mysql -u root -p grafana Postgres 备份： pg_dump grafana > grafana_backup 恢复：psql grafana Sqlite(默认) 备份：直接备份DB文件grafana.db，默认路径：/var/lib/grafana/ 恢复： 直接将备份的grafana.db文件复制到/var/lib/grafana/下 修改权限：sudo chown nobody.nogroup grafana.db && sudo chmod 640 grafana.db 配置文件 备份：直接备份/etc/grafana/grafana.ini。（对于部署在k8s中的，配置文件是使用configmap挂载的可以不用备份） 恢复：直接恢复使用备份文件/etc/grafana/grafana.ini 已安装的插件 备份：直接备份插件目录 恢复 直接恢复备份的插件目录 升级插件：grafana-cli plugins update-all 二、问题总结 1、删除默认组织后使用SQLite备份文件grafana.db恢复时报错 报错信息： Datasource provisioning error: failed to provision \"prometheus\" data source: Organization not found 解决方案： ​ 使用Navicat连接SQLite备份文件grafana.db，在main.org表中添加一条记录 INSERT INTO \"main\".\"org\" (\"id\", \"version\", \"name\", \"address1\", \"address2\", \"city\", \"state\", \"zip_code\", \"country\", \"billing_email\", \"created\", \"updated\") VALUES (2, 3, 'Main Org.', '', '', '', '', '', '', NULL, '2020-11-26 03:39:06', '2020-11-26 03:39:06'); Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/jaeger.html":{"url":"origin/jaeger.html","title":"Jaeger","keywords":"","body":"Jaeger 一、简介 Jaeger是一个基于opentracing规范的链路追踪工具平台 1、架构及组件 带有Kafka缓存同时可以进行ETL的架构 jaeger-client Jaeger 的客户端，实现了 OpenTracing 的 API，支持主流编程语言。客户端直接集成在目标 Application 中，其作用是记录和发送 Span 到 Jaeger Agent。在 Application 中调用 Jaeger Client Library 记录 Span 的过程通常被称为埋点。 jaeger-agent 暂存 Jaeger Client 发来的 Span，并批量向 Jaeger Collector 发送 Span，一般每台机器上都会部署一个 Jaeger Agent。官方的介绍中还强调了 Jaeger Agent 可以将服务发现的功能从 Client 中抽离出来，不过从架构角度讲，如果是部署在 Kubernetes 或者是 Nomad 中，Jaeger Agent 存在的意义并不大。 jaeger-collector 接受 Jaeger Agent 发来的数据，并将其写入存储后端，目前支持采用 Cassandra 和 Elasticsearch 作为存储后端。个人还是比较推荐用 Elasticsearch，既可以和日志服务共用同一个 ES，又可以使用 Kibana 对 Trace 数据进行额外的分析。架构图中的存储后端是 Cassandra，旁边还有一个 Spark，讲的就是可以用 Spark 等其他工具对存储后端中的 Span 进行直接分析。 jaeger-query & jaeger-ui 读取存储后端中的数据，以直观的形式呈现 jaeger-ingester ingester从名字就可以看出是从kafka里面取数据然后做持久化的，关于这个持久化的对象，jaeger原生是选择了elasticsearch（简称es），之索引选择es是因为查询非常方便（反向索引），性能也不错。 2、概念 Trace：一条调用链。 Span：可以理解成一条调用链的一个环节。一条 Trace可以被认为是一个由多个 Span 组成的DAG图。 Tag：标签集 Log：日志集 References：Span与Span之间的关系。OpenTracing目前定义了两种关系：ChildOf（父子） 和 FollowsFrom（跟随） SpanContext：Span上下文对象，主要保存了traceId spanId等，用来跨进程边界传输。不同进程间可以利用SpanContext建立References关系。 （下个服务拿到traceId就可以知道自己改归属于哪个调用链， 拿到spanId就知道自己的父span是谁 ） 3、采样速率 支持设置采样率是 Jaeger 的一个亮点，生产环境系统性能很重要，所以对于所有的请求都开启 Trace 显然会带来比较大的压力，另外，大量的数据也会带来很大存储压力。为了尽量消除分布式追踪采样对系统带来的影响，设置采样率是一个很好的办法。Jaeger官方 支持四种采样类别，分别是 Constant：全量采集，采样率设置0,1 分别对应打开和关闭 Probabilistic：概率采集，默认万份之一，取值可在 0至1之间，单位为百分比。0.5代表50% Rate Limiting：限速采集，每秒只能采集一定量的数据，如设置2的话，就是每秒采集2个链路数据 Remote ：**是遵循远程设置，取值的含义和 probabilistic 相同，都意为采样的概率，只不过设置为 remote 后，Client 会从 Jaeger Agent 中动态获取采样率设置。 guaranteedThroughput: 复合采样，至少每秒采样lowerBound次（rateLimiting），超过lowerBound次的话，按照samplingRate概率来采样 4、组件端口作用 组件 端口 协议 描述 Agent 6831 UDP 应用程序向代理发送跟踪的端口，接受 Jaeger.thrift而不是 Compact thrift协议通过兼容性Thrift协议，接收Jaeger thrift类型数据 Agent 6832 UDP 通过二进制Thrift协议，接收Jaeger thrift类型数据 ，需要某些不支持压缩的客户端库 Agent 5775 UDP 通过兼容性Thrift协议，接收Zipkin thrift类型数据 Agent 5778 HTTP 配置控制服务接口 Agent 14271 HTTP 监控信息端口 Collector 14250 TCP Agent 发送 Proto 格式数据 Collector 14267 TCP 接收客户端Jaeger thrift类型的数据 Collector 14268 HTTP 接收客户端Zipkin thrift类型的数据 Collector 14269 HTTP 监控信息端口 Query 16686 HTTP Jaeger UI页面端口 Query 16687 HTTP 监控信息端口 ingester 14270 HTTP 监控信息端口 二、安装 docker镜像 描述 all-in-one 专为快速本地测试而设计。它使用内存存储组件启动 Jaeger UI、收集器、查询和代理。$ docker pull jaegertracing/all-in-one:1.34 example-hotrod 演示分布式跟踪功能的示例应用程序“HotROD”.$ docker pull jaegertracing/example-hotrod:1.34 jaeger-agent 从 Jaeger 客户端接收 span 并转发给收集器。设计为作为 sidecar 或主机agents运行$ docker pull jaegertracing/jaeger-agent:1.34 jaeger-collector 从agents或直接从客户端接收 spans并将它们保存在持久存储中$ docker pull jaegertracing/jaeger-collector:1.34 jaeger-query 提供 Jaeger UI 和从存储中检索跟踪的 API.$ docker pull jaegertracing/jaeger-query:1.34 jaeger-ingester 收集器的替代品；从 Kafka topic读取spans并将它们保存到存储中$ docker pull jaegertracing/jaeger-ingester:1.34 spark-dependencies 一个 Apache Spark 作业，它从存储中收集 Jaeger spans ，分析服务之间的关联，并将它们存储起来以供以后在 Jaeger UI 中展示$ docker pull jaegertracing/spark-dependencies:latest jaeger-operator 用于打包、部署和管理 Jaeger 的 Kubernetes Operator.$ docker pull jaegertracing/jaeger-operator:1.34 jaeger-cassandra-schema 用于初始化 Cassandra 键空间和模式的实用程序脚本。$ docker pull jaegertracing/jaeger-cassandra-schema:1.34 jaeger-es-index-cleaner 用于从 Elasticsearch 中清除旧索引的实用程序脚本，因为 ES 不支持数据 TTL。.$ docker pull jaegertracing/jaeger-es-index-cleaner:1.34 1. Docker 1.1 Allinone docker run -d --name jaeger \\ -p 16686:16686 \\ jaegertracing/all-in-one:1.34 或者在Windows或MacOS或Linux中使用二进制文件进行部署 https://github.com/jaegertracing/jaeger/releases/ 2. Kubernetes 2.1 helm helm repo ad jaegertracing https://jaegertracing.github.io/helm-charts helm repo update helm install --namespace jaeger jaeger jaegertracing/jaeger \\ --set tag=\"1.34\" \\ --set provisionDataStore.cassandra=false \\ --set storage.type=elasticsearch \\ --set storage.elasticsearch.host=elasticsearch的IP地址(不用带协议，直接IP地址即可) \\ --set storage.elasticsearch.user=elasticsearch用户名(如果使用自定义创建的es用户,权限要求: 集群权限：monitor、manage_index_templates；运行身份权限：apm_system;索引权限:对索引模式jaeger*拥有all权限) \\ --set storage.elasticsearch.password=elasticsearch用户密码 \\ --set agent.serviceMonitor.enabled=true \\ --set collector.serviceMonitor.enabled=true \\ --set collector.service.zipkin.port=9411 --set query.ingress.enabled=true \\ --set \"query.ingress.hosts={jaeger的ingress域名1(例如：jaeger1.curiouser.com),域名2(例如：jaeger2.curiouser.com)}\" \\ --set query.serviceMonitor.enabled=true # 如果要部署HotROD容器，需要添加设置以下参数 --set hotrod.enabled=true \\ --set hotrod.ingress.enabled=true \\ --set hotrod.ingress.hosts={HotROD的ingress域名1(例如：jaeger-hotrod1.curiouser.com)} 2.2 k8s operator 暂无 三、HotROD演示数据 HotROD（Rides on Demand）是一个演示应用程序，由几个微服务组成，说明了OpenTracing API的使用。 相关文档： https://medium.com/opentracing/take-opentracing-for-a-hotrod-ride-f6e3141f7941 https://www.jaegertracing.io/docs/1.34/getting-started/#all-in-one docker run -d --name jaeger-hotrod \\ --link jaeger \\ -p 8080-8083:8080-8083 \\ -e JAEGER_AGENT_HOST=\"jaeger\" \\ jaegertracing/example-hotrod:1.34 all 访问：http://127.0.0.1:8080 点击进行导入数据，之后相关的请求HotROD容器日志中 四、tracegen压力测试 jaeger-tracegen可用于生成简单跨度的连续流。主要用于压力测试。可以控制worker的数量和在每个worker中生成的span。同时，它还可以在 firehose 模式下生成 span（在 span 上设置一个标志以跳过索引） docker run --rm --link jaeger \\ -e JAEGER_AGENT_HOST=jaeger \\ -e JAEGER_AGENT_PORT=6831 \\ jaegertracing/jaeger-tracegen:1.34 --workers=10 --traces=10 jaeger-tracegen参数 -debug Whether to set DEBUG flag on the spans to force sampling -duration duration For how long to run the test -firehose Whether to set FIREHOSE flag on the spans to skip indexing -marshal Whether to marshal trace context via HTTP headers -pause duration How long to pause before finishing trace (default 1µs) -service string Service name to use (default \"tracegen\") -traces int Number of traces to generate in each worker (ignored if duration is provided (default 1) -workers int Number of workers (goroutines) to run (default 1) 参考：https://www.jaegertracing.io/docs/1.34/tools/ 五、应用接入 1、Golang应用 2、Java应用 3、Python应用 4、PHP应用 参考 https://www.cnblogs.com/whuanle/p/14598049.html https://www.csdn.net/tags/MtjaEgzsMTM5OTgtYmxvZwO0O0OO0O0O.html https://mp.weixin.qq.com/s?__biz=MzU3NTY3MTQzMg==&mid=2247485019&idx=1&sn=d86a00a4ba9ce5e80f1f4f34db3faf7c http://t.zoukankan.com/ChangAn223-p-11458226.html https://cloud.tencent.com/developer/article/1903277 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-06-02 13:43:04 "},"origin/sentry.html":{"url":"origin/sentry.html","title":"Sentry日志聚合告警平台","keywords":"","body":"一、简介 虽然我们有很多工具可以让开发工作变得更容易，但是发现和排查线上问题的过程仍然在很多时候让我们觉得很痛苦。当生产系统中产生了一个bug时，我们如何快速地得到报警？如何评估它的影响和紧迫性？如何快速地找到问题的根源？当hotfix完修复程序后，又如何知道它是否解决了问题？ Sentry在帮助我们与现有流程集成时回答了这些问题。例如，线上有一个bug，代码的某处逻辑的NullPointerException造成了这个问题，Sentry会立即发现错误，并通过邮件或其他基于通知规则的集成通知到相关责任人员，这个通知可以把我们引入到一个指示板，这个指示板为我们提供了快速分类问题所需的上下文，如：频率、用户影响、代码那一部分受到影响以及那个团队可能是问题的所有者。 Sentry是一个实时事件的日志聚合平台。 Sentry 的目的是为了让我们专注于系统与程序的异常信息，目的是提高排查问题的效率，日志事件的量到达一个限制时甚至丢弃一些内容。官方也提倡正确设置 Sentry 接收的日志 level 的同时，用户也能继续旧的日志备份 Sentry 是带有一定策略的问题分析工具，以样本的形式展示部分原始日志的信息。信息不全面的同时，使用过程中也可能出现 Sentry 聚合所带来的负面影响，特别是日志记录质量不够的情况下。 与传统的监控系统相比，Sentry 更依赖于发出的日志报告，而另外一些隐藏的逻辑问题或者业务问题很可能是不会得到反馈的。不能作为日志的替代品。 sentry主要是为让我们专注于系统和程序的异常信息，提高排查效率，日志事件的量达到一个限制值的时候可能还会丢弃一些内容。官方也提倡正确设置sentry接收的日志level的等级时，也能继续旧的日志备份。 不是排查的万能工具 sentry是带有问题聚合功能的分析工具，所以如果样本提供的内容不全面。日志记录的质量不高的情况，对于错误的快速排查，可能没有实质性的帮助。 不能作为传统监控的替代品 与传统监控系统相比，sentry更依赖发出的日志报告，而另外一些隐藏的逻辑问题或者业务问题可能不会得到反馈的。 那么Sentry是如何实现实时日志监控报警的呢？ 首先，Sentry是一个C/S架构，分为服务端和客户端 。SDK我们需要在自己应用中集成Sentry的SDK才能在应用发生错误是将错误信息发送给Sentry服务端。根据语言和框架的不同，我们可以选择自动或自定义设置特殊的错误类型报告给Sentry服务端。 而Sentry的服务端分为web、cron、worker这几个部分，应用（客户端）发生错误后将错误信息上报给web，web处理后放入消息队列或Redis内存队列，worker从队列中消费数据进行处理。 官方文档：https://docs.sentry.io/ GitHub：https://github.com/getsentry/sentry Sentry支持的客户端SDK： 二、部署 Sentry 本身是基于 Django 开发的，需要PostgreSQL、 Redis存储数据。 9.1.2之前可以使用一个镜像，通过不同的启动参数直接部署各个组件。之后10版本以上，官方推荐on-premise方式部署（添加了好多组件来优化性能，各个组件的镜像也分开了）参考：https://docs.sentry.io/server/installation 9.1.2版本的镜像：https://hub.docker.com/_/sentry/ Kubernetes Helm Charts Redis Charts: https://github.com/helm/charts/tree/master/stable/redis Postgresql Charts: https://github.com/helm/charts/tree/master/stable/postgresql Sentry Charts: https://github.com/helm/charts/tree/master/stable/sentry OpenShift 资源声明文件部署 资源声明文件 Docker部署9.1.2以下版本 1、部署Redis和PostgreSQL步骤省略 需要注意的是在PostgreSQL中创建EXTENSION表 postgres=# \\c sentry You are now connected to database \"sentry\" as user \"postgres\". sentry=# CREATE EXTENSION IF NOT EXISTS citext; 2、定制镜像、修改配置文件 定制镜像细节参考第四章第一节 创建配置文件sentry.conf.py from sentry.conf.server import * # NOQA from sentry.utils.types import Bool, Int import os import os.path import six CONF_ROOT = os.path.dirname(__file__) postgres = env('SENTRY_POSTGRES_HOST') or (env('POSTGRES_PORT_5432_TCP_ADDR') and 'postgres') if postgres: DATABASES = { 'default': { 'ENGINE': 'sentry.db.postgres', 'NAME': ( env('SENTRY_DB_NAME') or env('POSTGRES_ENV_POSTGRES_USER') or 'postgres' ), 'USER': ( env('SENTRY_DB_USER') or env('POSTGRES_ENV_POSTGRES_USER') or 'postgres' ), 'PASSWORD': ( env('SENTRY_DB_PASSWORD') or env('POSTGRES_ENV_POSTGRES_PASSWORD') or '' ), 'HOST': postgres, 'PORT': ( env('SENTRY_POSTGRES_PORT') or '' ), }, } # You should not change this setting after your database has been created unless you have altered all schemas first SENTRY_USE_BIG_INTS = True # If you're expecting any kind of real traffic on Sentry, we highly recommend # configuring the CACHES and Redis settings ########### # General # ########### # Instruct Sentry that this install intends to be run by a single organization # and thus various UI optimizations should be enabled. SENTRY_SINGLE_ORGANIZATION = env('SENTRY_SINGLE_ORGANIZATION', True) ######### # Redis # ######### # Generic Redis configuration used as defaults for various things including: # Buffers, Quotas, TSDB redis = env('SENTRY_REDIS_HOST') or (env('REDIS_PORT_6379_TCP_ADDR') and 'redis') if not redis: raise Exception('Error: REDIS_PORT_6379_TCP_ADDR (or SENTRY_REDIS_HOST) is undefined, did you forget to `--link` a redis container?') redis_password = env('SENTRY_REDIS_PASSWORD') or '' redis_port = env('SENTRY_REDIS_PORT') or '6379' redis_db = env('SENTRY_REDIS_DB') or '0' SENTRY_OPTIONS.update({ 'redis.clusters': { 'default': { 'hosts': { 0: { 'host': redis, 'password': redis_password, 'port': redis_port, 'db': redis_db, }, }, }, }, }) ######### # Cache # ######### # Sentry currently utilizes two separate mechanisms. While CACHES is not a # requirement, it will optimize several high throughput patterns. memcached = env('SENTRY_MEMCACHED_HOST') or (env('MEMCACHED_PORT_11211_TCP_ADDR') and 'memcached') if memcached: memcached_port = ( env('SENTRY_MEMCACHED_PORT') or '11211' ) CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': [memcached + ':' + memcached_port], 'TIMEOUT': 3600, } } # A primary cache is required for things such as processing events SENTRY_CACHE = 'sentry.cache.redis.RedisCache' ######### # Queue # ######### # See https://docs.getsentry.com/on-premise/server/queue/ for more # information on configuring your queue broker and workers. Sentry relies # on a Python framework called Celery to manage queues. rabbitmq = env('SENTRY_RABBITMQ_HOST') or (env('RABBITMQ_PORT_5672_TCP_ADDR') and 'rabbitmq') if rabbitmq: BROKER_URL = ( 'amqp://' + ( env('SENTRY_RABBITMQ_USERNAME') or env('RABBITMQ_ENV_RABBITMQ_DEFAULT_USER') or 'guest' ) + ':' + ( env('SENTRY_RABBITMQ_PASSWORD') or env('RABBITMQ_ENV_RABBITMQ_DEFAULT_PASS') or 'guest' ) + '@' + rabbitmq + '/' + ( env('SENTRY_RABBITMQ_VHOST') or env('RABBITMQ_ENV_RABBITMQ_DEFAULT_VHOST') or '/' ) ) else: BROKER_URL = 'redis://:' + redis_password + '@' + redis + ':' + redis_port + '/' + redis_db ############### # Rate Limits # ############### # Rate limits apply to notification handlers and are enforced per-project # automatically. SENTRY_RATELIMITER = 'sentry.ratelimits.redis.RedisRateLimiter' ################## # Update Buffers # ################## # Buffers (combined with queueing) act as an intermediate layer between the # database and the storage API. They will greatly improve efficiency on large # numbers of the same events being sent to the API in a short amount of time. # (read: if you send any kind of real data to Sentry, you should enable buffers) SENTRY_BUFFER = 'sentry.buffer.redis.RedisBuffer' ########## # Quotas # ########## # Quotas allow you to rate limit individual projects or the Sentry install as # a whole. SENTRY_QUOTAS = 'sentry.quotas.redis.RedisQuota' ######## # TSDB # ######## # The TSDB is used for building charts as well as making things like per-rate # alerts possible. SENTRY_TSDB = 'sentry.tsdb.redis.RedisTSDB' ########### # Digests # ########### # The digest backend powers notification summaries. SENTRY_DIGESTS = 'sentry.digests.backends.redis.RedisBackend' ############## # Web Server # ############## # If you're using a reverse SSL proxy, you should enable the X-Forwarded-Proto # header and set `SENTRY_USE_SSL=1` if env('SENTRY_USE_SSL', False): SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https') SESSION_COOKIE_SECURE = True CSRF_COOKIE_SECURE = True SOCIAL_AUTH_REDIRECT_IS_HTTPS = True SENTRY_WEB_HOST = '0.0.0.0' SENTRY_WEB_PORT = 9000 SENTRY_WEB_OPTIONS = { 'http': '%s:%s' % (SENTRY_WEB_HOST, SENTRY_WEB_PORT), 'protocol': 'uwsgi', # This is need to prevent https://git.io/fj7Lw 'uwsgi-socket': None, 'http-keepalive': True, 'memory-report': False, # 'workers': 3, # the number of web workers } ############### # Mail Server # ############### email = env('SENTRY_EMAIL_HOST') or (env('SMTP_PORT_25_TCP_ADDR') and 'smtp') if email: SENTRY_OPTIONS['mail.backend'] = 'smtp' SENTRY_OPTIONS['mail.host'] = email SENTRY_OPTIONS['mail.from'] = env('SENTRY_SERVER_EMAIL') SENTRY_OPTIONS['mail.username'] = env('SENTRY_EMAIL_USER') or '' SENTRY_OPTIONS['mail.password'] = env('SENTRY_EMAIL_PASSWORD') or '' SENTRY_OPTIONS['mail.port'] = int(env('SENTRY_EMAIL_PORT') or 25) SENTRY_OPTIONS['mail.use-tls'] = env('SENTRY_EMAIL_USE_TLS', False) SENTRY_OPTIONS['mail.list-namespace'] = env('SENTRY_EMAIL_LIST_NAMESPACE') or 'localhost' else: SENTRY_OPTIONS['mail.backend'] = 'dummy' # The email address to send on behalf of SENTRY_OPTIONS['mail.from'] = env('SENTRY_SERVER_EMAIL') or 'root@localhost' # If you're using mailgun for inbound mail, set your API key and configure a # route to forward to /api/hooks/mailgun/inbound/ SENTRY_OPTIONS['mail.mailgun-api-key'] = env('SENTRY_MAILGUN_API_KEY') or '' # If you specify a MAILGUN_API_KEY, you definitely want EMAIL_REPLIES if SENTRY_OPTIONS['mail.mailgun-api-key']: SENTRY_OPTIONS['mail.enable-replies'] = True else: SENTRY_OPTIONS['mail.enable-replies'] = env('SENTRY_ENABLE_EMAIL_REPLIES', False) if SENTRY_OPTIONS['mail.enable-replies']: SENTRY_OPTIONS['mail.reply-hostname'] = env('SENTRY_SMTP_HOSTNAME') or '' ########## # Docker # ########## # Docker's environment configuration needs to happen # prior to anything that might rely on these values to # enable more \"smart\" configuration. ENV_CONFIG_MAPPING = { 'SENTRY_SECRET_KEY': 'system.secret-key', 'SENTRY_SLACK_CLIENT_ID': 'slack.client-id', 'SENTRY_SLACK_CLIENT_SECRET': 'slack.client-secret', 'SENTRY_SLACK_VERIFICATION_TOKEN': 'slack.verification-token', 'SENTRY_GITHUB_APP_ID': ('github-app.id', Int), 'SENTRY_GITHUB_APP_CLIENT_ID': 'github-app.client-id', 'SENTRY_GITHUB_APP_CLIENT_SECRET': 'github-app.client-secret', 'SENTRY_GITHUB_APP_WEBHOOK_SECRET': 'github-app.webhook-secret', 'SENTRY_GITHUB_APP_PRIVATE_KEY': 'github-app.private-key', 'SENTRY_VSTS_CLIENT_ID': 'vsts.client-id', 'SENTRY_VSTS_CLIENT_SECRET': 'vsts.client-secret', 'GOOGLE_CLIENT_ID': 'auth-google.client-id', 'GOOGLE_CLIENT_SECRET': 'auth-google.client-secret', } def bind_env_config(config=SENTRY_OPTIONS, mapping=ENV_CONFIG_MAPPING): \"\"\" Automatically bind SENTRY_OPTIONS from a set of environment variables. \"\"\" for env_var, item in six.iteritems(mapping): # HACK: we need to check both in `os.environ` and `env._cache`. # This is very much an implementation detail leaking out # due to assumptions about how `env` would be used previously. # `env` will pop values out of `os.environ` when they are seen, # so checking against `os.environ` only means it's likely # they won't exist if `env()` has been called on the variable # before at any point. So we're choosing to check both, but this # behavior is different since we're trying to only conditionally # apply variables, instead of setting them always. if env_var not in os.environ and env_var not in env._cache: continue if isinstance(item, tuple): opt_key, type_ = item else: opt_key, type_ = item, None config[opt_key] = env(env_var, type=type_) # If this value ever becomes compromised, it's important to regenerate your # SENTRY_SECRET_KEY. Changing this value will result in all current sessions # being invalidated. secret_key = env('SENTRY_SECRET_KEY') if not secret_key: raise Exception('Error: SENTRY_SECRET_KEY is undefined, run `generate-secret-key` and set to -e SENTRY_SECRET_KEY') if 'SENTRY_RUNNING_UWSGI' not in os.environ and len(secret_key) 3、初始化PostgreSQL数据库 docker run -it \\ -e TZ='Asia/Shanghai' \\ -e SENTRY_SECRET_KEY='TGQ3RzNUM0lkSVRlcWZBTzczZUtPeW40ZkpPbjlhbnpRVTJjYXdESQ==' \\ -e SENTRY_SERVER_EMAIL='*****@163.com' \\ -e SENTRY_EMAIL_USE_TLS=False \\ -e SENTRY_EMAIL_PASSWORD=***** \\ -e SENTRY_EMAIL_USER='*****@163.com' \\ -e SENTRY_EMAIL_PORT=25 \\ -e SENTRY_EMAIL_HOST='smtp.163.com' \\ -e SENTRY_REDIS_HOST='192.168.1.67' \\ -e SENTRY_POSTGRES_PORT=5432 \\ -e SENTRY_POSTGRES_HOST=192.168.1.67 \\ -e SENTRY_DB_PASSWORD=sentry \\ -e SENTRY_DB_USER=sentry \\ -e SENTRY_DB_NAME=sentry \\ --name sentry-db-init \\ sentry-9.1.2-ldap-dingtalk:v1 sentry upgrade --noinput 4、初始化用户信息 docker run -it \\ -e TZ='Asia/Shanghai' \\ -e SENTRY_SECRET_KEY='TGQ3RzNUM0lkSVRlcWZBTzczZUtPeW40ZkpPbjlhbnpRVTJjYXdESQ==' \\ -e SENTRY_SERVER_EMAIL='*****@163.com' \\ -e SENTRY_EMAIL_USE_TLS=False \\ -e SENTRY_EMAIL_PASSWORD=***** \\ -e SENTRY_EMAIL_USER='*****@163.com' \\ -e SENTRY_EMAIL_PORT=25 \\ -e SENTRY_EMAIL_HOST='smtp.163.com' \\ -e SENTRY_REDIS_HOST='192.168.1.67' \\ -e SENTRY_POSTGRES_PORT=5432 \\ -e SENTRY_POSTGRES_HOST=192.168.1.676\\ -e SENTRY_DB_PASSWORD=sentry \\ -e SENTRY_DB_USER=sentry \\ -e SENTRY_DB_NAME=sentry \\ --name sentry-user-init \\ sentry-9.1.2-ldap-dingtalk:v1 sentry createuser --no-input --email '*****@163.com' --superuser --password 18526 5、部署Worker docker run -d \\ -p 30276:9000 \\ --privileged \\ --restart=always \\ -v ./worker-data:/var/lib/sentry/files/ \\ -e SENTRY_USE_LDAP=True \\ -e TZ='Asia/Shanghai' \\ -e SENTRY_SECRET_KEY='TGQ3RzNUM0lkSVRlcWZBTzczZUtPeW40ZkpPbjlhbnpRVTJjYXdESQ==' \\ -e SENTRY_SERVER_EMAIL='****@163.com' \\ -e SENTRY_EMAIL_USE_TLS=False \\ -e SENTRY_EMAIL_PASSWORD=**** \\ -e SENTRY_EMAIL_USER='****@163.com' \\ -e SENTRY_EMAIL_PORT=25 \\ -e SENTRY_EMAIL_HOST='smtp.163.com' \\ -e SENTRY_REDIS_HOST='192.168.1.67' \\ -e SENTRY_POSTGRES_PORT=5432 \\ -e SENTRY_POSTGRES_HOST=192.168.1.67 \\ -e SENTRY_DB_PASSWORD=sentry \\ -e SENTRY_DB_USER=sentry \\ -e SENTRY_DB_NAME=sentry \\ --name sentry-worker \\ sentry-9.1.2-ldap-dingtalk:v1 run worker 6、部署Web docker run -d \\ -p 30275:9000 \\ --privileged \\ --restart=always \\ -v ./web-data:/var/lib/sentry/files/ \\ -v ./sentry.conf.py:/etc/sentry/sentry.conf.py \\ -e LDAP_LOGLEVEL=INFO \\ -e LDAP_SENTRY_USERNAME_FIELD=cn \\ -e LDAP_FIND_GROUP_PERMS=False \\ -e LDAP_CACHE_GROUPS=True \\ -e LDAP_GROUP_CACHE_TIMEOUT=3600 \\ -e LDAP_GROUP_TYPE=groupOfUniqueNames \\ -e LDAP_MAP_FULL_NAME=gecos \\ -e LDAP_USER_FILTER='(&(memberOf=cn=sentry,cn=groups,dc=ldap,dc=synology,dc=curiouser,dc=com)(cn=%(user)s))' \\ -e LDAP_USER_DN='cn=users,dc=ldap,dc=synology,dc=curiouser,dc=com' \\ -e LDAP_BIND_PASSWORD=jL6u49t5A9P5 \\ -e LDAP_BIND_DN='uid=root,cn=users,dc=ldap,dc=synology,dc=curiouser,dc=com' \\ -e LDAP_SERVER='ldap://192.168.1.67:389' \\ -e SENTRY_USE_LDAP=True \\ -e TZ='Asia/Shanghai' \\ -e SENTRY_SECRET_KEY='TGQ3RzNUM0lkSVRlcWZBTzczZUtPeW40ZkpPbjlhbnpRVTJjYXdESQ==' \\ -e SENTRY_SERVER_EMAIL='****@163.com' \\ -e SENTRY_EMAIL_USE_TLS=False \\ -e SENTRY_EMAIL_PASSWORD=**** \\ -e SENTRY_EMAIL_USER='****@163.com' \\ -e SENTRY_EMAIL_PORT=25 \\ -e SENTRY_EMAIL_HOST='smtp.163.com' \\ -e SENTRY_REDIS_HOST='192.168.1.67' \\ -e SENTRY_POSTGRES_PORT=5432 \\ -e SENTRY_POSTGRES_HOST=192.168.1.67 \\ -e SENTRY_DB_PASSWORD=sentry \\ -e SENTRY_DB_USER=sentry \\ -e SENTRY_DB_NAME=sentry \\ --name sentry-web \\ sentry-9.1.2-ldap-dingtalk:v1 三、配置 在Sentry完成一个项目的设置后，您将获得一个我们称之为DSN或数据源名称的值.它看起来很像一个标准的URL，但它实际上只是Sentry SDK所需的配置的标识.它由几个部分组成，包括协议，公共密钥和密钥，服务器地址和项目标识符。 '{PROTOCOL}://{PUBLIC_KEY}:{SECRET_KEY}@{HOST}/{PATH}{PROJECT_ID}' 四、Sentry集成LDAP认证登陆和钉钉群机器人 由于Sentry没有集成LDAP认证的官方插件，所以推荐了第三方插件sentry-ldap-auth来实现。 仅只是使用邮件的方式进行通知，这是不够的。Sentry默认的通知方式中是不支持钉钉的，它并没有直接集成钉钉。而是集成一个Webhook，这个Webhook发送的数据格式与钉钉群机器人回调地址接受的数据格式不一致，无法直接回调钉钉群机器人通知。所以只能集成第三方插件来支持。目前有两个 官方说明文档：https://docs.sentry.io/server/plugins/#rd-party-plugins Sentry-ldap-auth插件地址：https://github.com/Banno/getsentry-ldap-auth Sentry-Dingtalk插件地址：https://github.com/anshengme/sentry-dingding 第三方制作集成Sentry-ldap-auth插件Docker镜像的GitHub：https://github.com/locaweb/docker-sentry-ldap 相关文章：https://www.cnblogs.com/cjsblog/p/10585213.html 1、修改默认镜像，添加插件 Dockerfile FROM docker.io/sentry:9.1.2 RUN apt-get -qq update && DEBIAN_FRONTEND=noninteractive apt-get install -y -q libxslt1-dev libxml2-dev libpq-dev libldap2-dev libsasl2-dev libssl-dev RUN echo \"sentry-ldap-auth\\npython-decouple==3.0\\ndjango-auth-ldap /tmp/req.txt && \\ pip install -r /tmp/req.txt && \\ apt-get remove -y -q libxslt1-dev libxml2-dev libpq-dev libldap2-dev libsasl2-dev libssl-dev && \\ rm -rf /var/lib/apt/lists/* /tmp/req.txt Makefile IMAGE_BASE = harbor.curiouser.cn IMAGE_NAME = tools/sentry-9.1.2-ldap-dingtalk IMAGE_VERSION = v1 all: build build: docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} . push: docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} 2、修改部署charts中的configmap，添加插件认证代码 Sentry Charts目录下templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: # .....上文省略...... data: # .....上文省略...... sentry.conf.py: |- # .....上文省略...... #################### # LDAP settings ## #################### SENTRY_USE_LDAP = env('SENTRY_USE_LDAP', False) if SENTRY_USE_LDAP: import ldap from django_auth_ldap.config import LDAPSearch, GroupOfUniqueNamesType AUTH_LDAP_SERVER_URI = env('LDAP_SERVER', 'ldap://localhost：389') AUTH_LDAP_BIND_DN = env('LDAP_BIND_DN', '') AUTH_LDAP_BIND_PASSWORD = env('LDAP_BIND_PASSWORD', '') # 设置搜索用户配置 AUTH_LDAP_USER_SEARCH = LDAPSearch( env('LDAP_USER_DN'), ldap.SCOPE_SUBTREE, env('LDAP_USER_FILTER', '(&(objectClass=inetOrgPerson)(cn=%(user)s))') ) # 设置搜索用户组配置 #AUTH_LDAP_GROUP_SEARCH = LDAPSearch( # env('LDAP_GROUP_DN', ''), # ldap.SCOPE_SUBTREE, # env('LDAP_GROUP_FILTER', '(objectClass=groupOfUniqueNames)') #) if env('LDAP_GROUP_TYPE', '') == 'groupOfUniqueNames': AUTH_LDAP_GROUP_TYPE = GroupOfUniqueNamesType() elif env('LDAP_GROUP_TYPE', '') == 'groupOfNames': AUTH_LDAP_GROUP_TYPE = GroupOfNamesType() elif env('LDAP_GROUP_TYPE', '') == 'posixGroup': AUTH_LDAP_GROUP_TYPE = PosixGroupType() elif env('LDAP_GROUP_TYPE', '') == 'nestedGroupOfNames': AUTH_LDAP_GROUP_TYPE = NestedGroupOfNamesType() AUTH_LDAP_REQUIRE_GROUP = None AUTH_LDAP_DENY_GROUP = None # 用户属性Mapping AUTH_LDAP_USER_ATTR_MAP = { 'username': env('LDAP_SENTRY_USER_FIELD', 'cn'), 'name': env('LDAP_MAP_FULL_NAME', 'sn'), 'email': env('LDAP_MAP_MAIL', 'mail') } # 用户搜索缓存 AUTH_LDAP_FIND_GROUP_PERMS = env('LDAP_FIND_GROUP_PERMS', False) AUTH_LDAP_CACHE_GROUPS = env('LDAP_CACHE_GROUPS', True) AUTH_LDAP_GROUP_CACHE_TIMEOUT = env('LDAP_GROUP_CACHE_TIMEOUT', 3600) # 用户在Sentry中的对应关系 AUTH_LDAP_DEFAULT_SENTRY_ORGANIZATION = env('LDAP_DEFAULT_SENTRY_ORGANIZATION','sentry') AUTH_LDAP_SENTRY_ORGANIZATION_ROLE_TYPE = 'manager' AUTH_LDAP_SENTRY_SUBSCRIBE_BY_DEFAULT = False AUTH_LDAP_SENTRY_ORGANIZATION_GLOBAL_ACCESS = True AUTH_LDAP_SENTRY_USERNAME_FIELD = env('LDAP_SENTRY_USERNAME_FIELD', 'cn') AUTHENTICATION_BACKENDS = AUTHENTICATION_BACKENDS + ('sentry_ldap_auth.backend.SentryLdapBackend',) ldap_is_active = env('LDAP_GROUP_ACTIVE', '') ldap_is_superuser = env('LDAP_GROUP_SUPERUSER', '') ldap_is_staff = env('LDAP_GROUP_STAFF', '') if ldap_is_active or ldap_is_superuser or ldap_is_staff: AUTH_LDAP_USER_FLAGS_BY_GROUP = { 'is_active': ldap_is_active, 'is_superuser': ldap_is_superuser, 'is_staff': ldap_is_staff, } # django_auth_ldap日志输出 import logging logger = logging.getLogger('django_auth_ldap') logger.addHandler(logging.StreamHandler()) ldap_loglevel = getattr(logging, env('LDAP_LOGLEVEL', 'DEBUG')) logger.setLevel(ldap_loglevel) LOGGING['overridable'] = ['sentry', 'django_auth_ldap'] LOGGING['loggers']['django_auth_ldap'] = {'handlers': ['console'],'level': env('LDAP_LOGLEVEL','DEBUG')} # .....下文省略...... 3、在Values文件中添加Web组件部署时的环境变量 values-dev.yaml ... web: ... env: ... - name: TZ value: Asia/Shanghai - name: SENTRY_USE_LDAP value: 'True' - name: LDAP_SERVER value: 'ldap://openldap.openldap.svc:389' - name: LDAP_BIND_DN value: 'cn=readonly,dc=curiouser,dc=com' - name: LDAP_BIND_PASSWORD value: readonly - name: LDAP_USER_DN value: 'ou=employee,dc=curiouser,dc=com' - name: LDAP_USER_FILTER value: >- (&(memberOf=cn=sentry,ou=apps,dc=curiouser,dc=com)(cn=%(user)s)) - name: LDAP_SENTRY_USER_FIELD value: 'cn' - name: LDAP_MAP_FULL_NAME value: 'sn' - name: LDAP_MAP_MAIL value: 'mail' - name: LDAP_GROUP_TYPE value: groupOfUniqueNames - name: LDAP_GROUP_CACHE_TIMEOUT value: '3600' - name: LDAP_CACHE_GROUPS value: 'True' - name: LDAP_DEFAULT_SENTRY_ORGANIZATION value: sentry - name: LDAP_FIND_GROUP_PERMS value: 'False' - name: LDAP_SENTRY_USERNAME_FIELD value: 'cn' - name: LDAP_LOGLEVEL value: INFO 五、Kubernetes event 客户端 Github 地址：https://github.com/getsentry/sentry-kubernetes helm repo add googleapis-incubator https://kubernetes-charts-incubator.storage.googleapis.com helm upgrade --install sentry-kubernetes-events googleapis-incubator/sentry-kubernetes \\ --namespace tools \\ --set sentry.dsn= \\ --set sentry.environment=test \\ --set sentry.release=test 六、Sentry-cli客户端 Sentry命令行客户端，作为为一个独立于代码SDK之外的客户端，通常可以用于发送一些自定义事件到服务端。例如在一些定时任务Bash脚本中集成，将一些自定义的事件同步到Sentry服务端，做到自定义事件监控。 官方文档：https://docs.sentry.io/cli/ GitHub地址：https://github.com/getsentry/sentry-cli 1. 下载安装 手动下载安装 github下载地址：https://github.com/getsentry/sentry-cli/releases/ 脚本下载安装 curl -sL https://sentry.io/get-cli/ | bash NPM下载安装 # 全局安装 npm install -g @sentry/cli --unsafe-perm Docker Image $ docker pull getsentry/sentry-cli $ docker run --rm -v $(pwd):/work getsentry/sentry-cli --help 2. 配置 全局配置文件：~/.sentryclirc INI语法格式 [auth] token=your-auth-token 环境变量 默认会读取当前.env 文件加载环境变量。可设置SENTRY_LOAD_DOTENV=0禁止 export SENTRY_AUTH_TOKEN=your-auth-token 命令行参数 sentry-cli --auth-token your-auth-token 项目配置文件 支持加载.properties，也可通过环境变量SENTRY_PROPERTIES指定项目配置文件路径 环境变量的形式 配置文件中的形式 描述 SENTRY_AUTH_TOKEN auth.token 与Sentry服务端通信用的认证Token SENTRY_API_KEY auth.api_key The legacy API key for authentication if you have one. SENTRY_URL defaults.url The URL to use to connect to sentry. This defaults to https://sentry.io/. SENTRY_ORG defaults.org The slug of the organization to use for a command. SENTRY_PROJECT defaults.project The slug of the project to use for a command. http.keepalive This ini only setting is used to control the behavior of the SDK with regards to HTTP keepalives. The default is true but it can be set to false to disable keepalive support. http_proxy http.proxy_url The URL that should be used for the HTTP proxy. The standard http_proxy environment variable is also honored. Note that it is lowercase. http.proxy_username This ini only setting sets the proxy username in case proxy authentication is required. http.proxy_password* This ini only setting sets the proxy password in case proxy authentication is required. http.verify_ssl This can be used to disable SSL verification when set to false. You should never do that unless you are working with a known self signed server locally. http.check_ssl_revoke If this is set to false then SSL revocation checks are disabled on Windows. This can be useful when working with a corporate SSL MITM proxy that does not properly implement revocation checks. Do not use this unless absolutely necessary. SENTRY_HTTP_MAX_RETRIES http.max_retries Sets the maximum number of retry attempts for upload operations (e.g., uploads of release files and debug symbols). The default is 5. ui.show_notifications If this is set to false some operating system notifications are disabled. This currently primarily affects xcode builds which will not show notifications for background builds. SENTRY_LOG_LEVEL log.level Configures the log level for the SDK. The default is warning. If you want to see what the library is doing you can set it to info which will spit out more information which might help to debug some issues with permissions. dsym.max_upload_size Sets the maximum upload size in bytes (before compression) of debug symbols into one batch. The default is 35MB or 100MB (depending on the version of sentry-cli) which is suitable for sentry.io but if you are using a different sentry server you might want to change this limit if necessary. SENTRY_NO_PROGRESS_BAR If set to 1, then sentry-cli will not display progress bars for any operations. SENTRY_DISABLE_UPDATE_CHECK update.disable_check If set to true, then the automatic update check in sentry-cli is disabled. This was introduced in 1.17. Versions before that did not include an update check. The update check is also not enabled for npm based installations of sentry-cli at the moment. DEVICE_FAMILY device.family Device family value reported to Sentry. DEVICE_MODEL device.model Device model value reported to Sentry. 3. 获取Auth Token 4. 获取并设置项目DSN 创建项目，获取DSN 设置DSN环境变量 export SENTRY_DSN=https://:@sentry.io/ 5. 验证配置文件 $ sentry-cli info Sentry Server: http://sentry-web-sentry.apps.okd311.curiouser.com Default Organization: Sentry Default Project: sentry-cli Authentication Info: Method: Auth Token User: *** Scopes: - event:admin - event:read - member:read - org:read - project:read - project:releases - team:read 6. Sentry-cli命令行参数 $ sentry-cli OPTIONS: --api-key 指定Sentry API key. --auth-token 指定Sentry auth token. -h, --help 打印帮助信息 --log-level 设置日志输出级别(日志级别:TRACE、DEBUG、INFO、WARN、ERROR) --url 指定Sentry服务端地址.[默认：https://sentry.io/] -V, --version 打印版本信息 子命令： bash-hook Prints out a bash script that does error handling. difutil Locate or analyze debug information files. help 显示帮助信息 info 打印Sentry服务端信息 issues Manage issues in Sentry. login Authenticate with the Sentry server. projects 管理sentry项目Manage projects on Sentry. react-native Upload build artifacts for react-native projects. releases Manage releases on Sentry. repos Manage repositories on Sentry. send-event Send a manual event to Sentry. uninstall Uninstall the sentry-cli executable. update Update the sentry-cli executable. upload-dif Upload debugging information files. upload-proguard Upload ProGuard mapping files to a project. 7. 手动发送事件 命令行参数 $ sentry-cli send-event [选项] 选项: -d, --dist Set the distribution. -E, --env 发送事件时一起发送指定的环境变量 -e, --extra ... 给事件添加额外信息 -f, --fingerprint ... 修改事件指纹. -h, --help 打印帮助信息 -l, --level 设置事件的严重程度或日志级别(默认error) --log-level 设置日志数据级别(TRACE, DEBUG, INFO, WARN, ERROR] --logfile 从日志文件中读取日志作为事件消息的补充内容(只读取文件中最近的100条数据) -m, --message ... 设置事件消息 -a, --message-arg ... 设置事件参数 --no-environ 设置发送事件时不一起发送系统环境变量 -p, --platform 设置事件所处平台。默认是'other' -r, --release 指定事件版本 -t, --tag ... 给事件添加标签 -u, --user ... 给事件添加用户信息 --with-categories Parses off a leading category for breadcrumbs from the logfile 示例 sentry-cli send-event \\ -m \"this is sentry-cli test event\" \\ -t sentry-cli:test \\ --no-environ \\ -l info \\ -p centos \\ -e a:b \\ -e c:d \\ -u root:curiouser \\ --log-level DEBUG\\ -a hahha \\ -E HOSTNAME:$HOSTNAME \\ --logfile output.log \\ -r v1 事件发送成功后，命令行会返回事件编号 8. 事件信息显示 9. Bash脚本执行异常监控 对于bash脚本，可以通过使用sentry-cli bash钩子自动发送错误异常到sentry服务端，并定位出异常所在行。 sentry-cli实际上只在启用set -e时才有效(在默认情况下，它将为您启用set -e)。 sentry-cli会注册EXIT和ERR Trap。 只需要在你的bash脚本开头添加 #!/bin/bash export SENTRY_DSN= eval \"$(sentry-cli bash-hook)\" Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-22 21:01:18 "},"origin/sentry-logstash对接Sentry.html":{"url":"origin/sentry-logstash对接Sentry.html","title":"Logstash与Sentry对接","keywords":"","body":"Logstah与Sentry的对接 一、简介 Sentry作为一个日志异常告警平台，对于异常日志聚合的告警，功能很强大。而基于ELK的日志系统，只能采集、聚合、存储应用日志，无法针对日志中的异常进行检测，聚合告警。所以可以在Logstash采集过程中输出一份日志数据到Sentry中进行聚合告警。 logstash-output-sentry插件：https://github.com/javiermatos/logstash-output-sentry 相关文章： https://medium.com/@yscaliskan/how-to-use-logstash-along-with-sentry-6c3d27790a38 https://clarkdave.net/2014/01/tracking-errors-with-logstash-and-sentry/ 整体对接思路 Filebeat file Input(多行采集+打标签) -------> Filebeat processor(添加字段) -------> Filebeat logatash output(输出到filebeat进行加工处理) Logstash beat input (监听) -------> Logstash dissect filter(判断符合标签的事件+从事件原始日志中映射提取字段) -------> Logstash sentry Output(输出到Sentry中) Filebeat 采集、输出要求: 可以多行采集(设置上下日志事件的标识),多行采集的日志信息到统一放到日志事件的“message”字段中 添加采集的日志类型字段，添加与Sentry相关信息(sentry上项目的ID、key、Secret)的字段 删除一些默认添加的字段信息 以日志中该日志产生的时间为事件的时间，而不是采集时的时间为事件时间 Logtash 接收、处理要求： 根据filebeat传送过来的事件中的类型字段判断是否进行过滤加工 二、上下文 以API网关Kong的Nginx的错误日志为例（该Nginx安装了LUA模块，错误日志里面有lua模块的错误日志）。日志文件中的一行代表着一个nginx出错的事件，示例如下： 2018/11/28 18:16:25 [warn] 2201#0: 9081632 [lua] cluster.lua:182: set_peer_down(): [lua-cassandra] setting host at 172.17.1.8 DOWN, context: ngx.timer 2018/11/28 18:16:25 [error] 2201#0: 9081632 [lua] init.lua:365: [cluster_events] failed to poll: failed to retrieve events from DB: [Unavailable exception] Cannot achieve consistency level LOCAL_ONE, context: ngx.timer 2019/11/28 18:16:26 [warn] 27201#0: 90815632 this is a warn log event 2019/11/28 18:16:26 [fatal] 27201#0: 90815632 this is a fatal log event 每一行日志可大致格式分为: 时间戳 日志级别 进程号 抛弃该处数据 具体错误日志 三、配置 1. Filebeat配置 filebeat.inputs: - type: log enabled: true paths: - /root/Curiouser/test.log exclude_files: [\"_filebeat\", \".gz$\"] recursive_glob.enabled: true multiline.pattern: '^[0-9]{4}/[0-9]{2}/[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}' multiline.negate: true multiline.match: after tags: sentry-alert fields: service_name => \"kong\" sentry_project_id => \"7\" sentry_project_key => \"***\" sentry_project_secret => \"***\" processors: - drop_fields: fields: [\"agent\", \"tags\", \"input\", \"ecs\"] output.logstash: hosts: [\"127.0.0.1:5044\"] 2. Logstash安装sentry output插件 /usr/share/logstash/bin/logstash-plugin install logstash-output-sentry /usr/share/logstash/bin/logstash-plugin list 2. Logstash配置 input { beats { port => 5044 } } filter { # 判断\"tag\"包含\"log-alert\"标签的日志事件进行加工处理 if [tags] == \"log-alert\" { # 映射原始日志，从中提取数据赋予指定的字段（按行为单位） dissect { mapping => { \"message\" => \"%{timestamp} %{+timestamp} [%{level}] %{thread} %{} %{message}\" } } # 提取日志的产生时间作为事件的时间戳。 date { match => [ \"timestamp\", \"yyyy/MM/dd HH:mm:ss\" ] remove_field => \"timestamp\" } # 替换原始日志中的日志级别字段,sentry支持的日志级别为warning,而原始日志中的日志级别字段是warn，索引需要转换。 mutate { gsub => [ \"level\", \"warn\", \"warning\" ] } } } output { # 判断日志级别为\"warning\",\"error\",\"fatal\"的日志事件,发送到sentry if [level] == \"warning\" or [level] == \"error\" or [level] == \"fatal\" { sentry { message => \"message\" threads => 'thread' level => \"level\" tags => 'service:\"service_name\"' url => \"http://sentry.curiouser.com/api\" key => '%{sentry_project_key}' secret => '%{sentry_project_secret}' project_id => '%{sentry_project_id}' } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/docker原理.html":{"url":"origin/docker原理.html","title":"Docker原理","keywords":"","body":"Docker原理 一、简介 二、Namespaces 命名空间（namespaces）是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法。在日常使用 Linux 或者 macOS 时，我们并没有运行多个完全分离的服务器的需要，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，这是很多时候我们都不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到完全隔离，就像运行在多台不同的机器上一样。 CLONE_NEWCGROUP CLONE_NEWIPC CLONE_NEWNET CLONE_NEWNS CLONE_NEWPID CLONE_NEWUSER CLONE_NEWUTS 如果 Docker 的容器通过 Linux 的命名空间完成了与宿主机进程的网络隔离，但是却有没有办法通过宿主机的网络与整个互联网相连，就会产生很多限制，所以 Docker 虽然可以通过命名空间创建一个隔离的网络环境，但是 Docker 中的服务仍然需要与外界相连才能发挥作用。 每一个使用 docker run 启动的容器其实都具有单独的网络命名空间，Docker 为我们提供了四种不同的网络模式，Host、Container、None 和 Bridge 模式。 三、Control Groups 四、Union Filesystem Linux 的命名空间和控制组分别解决了不同资源隔离的问题，前者解决了进程、网络以及文件系统的隔离，后者实现了 CPU、内存等资源的隔离，但是在 Docker 中还有另一个非常重要的问题需要解决 - 也就是镜像。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/docker-process-manager.html":{"url":"origin/docker-process-manager.html","title":"容器中的进程管理","keywords":"","body":"Docker容器中进程管理工具 一、简介 为了防止容器中直接使用ENTRYPOINT或CMD指令启动命令或应用程序产生PID为1的进程无法处理传递信号给子进程或者无法接管孤儿进程，进而导致产生大量的僵尸进程。对于没有能力处理以上两个进程问题的PID进程，建议使用dumb-int或tini这种第三方工具来充当1号进程。 Linux系统中，PID为1的进程需要担任两个重要的使命： 传递信号给子进程 如果pid为1的进程，无法向其子进程传递信号，可能导致容器发送SIGTERM信号之后，父进程等待子进程退出。此时，如果父进程不能将信号传递到子进程，则整个容器就将无法正常退出，除非向父进程发送SIGKILL信号，使其强行退出，这就会导致一些退出前的操作无法正常执行，例如关闭数据库连接、关闭输入输出流等。 接管孤儿进程，防止出现僵尸进程 如果一个进程中A运行了一个子进程B，而这个子进程B又创建了一个子进程C，若子进程B非正常退出（通过SIGKILL信号，并不会传递SIGKILL信号给进程C），那么子进程C就会由进程A接管，一般情况下，我们在进程A中并不会处理对进程C的托管操作（进程A不会传递SIGTERM和SIGKILL信号给进程C），结果就导致了进程B结束了，倒是并没有回收其子进程C，子进程C就变成了僵尸进程。 在docker中，docker stop命令会发送SIGTERM信号给容器的主进程来处理。如果主进程没有处理这个信号，docker会在等待一段优雅grace的时间后，发送SIGKILL信号来强制终止 二、容器中僵尸进程的危害 详情参考： 1、https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/ 2、https://medium.com/@gchudnov/trapping-signals-in-docker-containers-7a57fdda7d86 三、dumb-int Github：https://github.com/Yelp/dumb-init dumb-int是一个用C写的轻量级进程管理工具。类似于一个初始化系统， 它充当PID 1，并立即以子进程的形式允许您的命令，注意在接收到信号时正确处理和转发它们 dumb-init 解决上述两个问题：向子进程代理发送信号和接管子进程。 默认情况下，dumb-init 会向子进程的进程组发送其收到的信号。原因也很简单，前面已经提到过，像 bash 这样的应用，自己接收到信号之后，不会向子进程发送信号。当然，dumb-init 也可以通过设置环境变量DUMB_INIT_SETSID=0来控制只向它的直接子进程发送信号。 另外 dumb-init 也会接管失去父进程的进程，确保其能正常退出。 安装 Alpine镜像的APK可以直接安装 FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache dumb-init ENTRYPOINT [\"dumb-init\", \"--\"] CMD [\"/usr/local/bin/docker-entrypoint.sh\"] 二进制安装 RUN version=v1.2.2 && \\ wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/$version/dumb-init_$version_amd64 && \\ chmod +x /usr/local/bin/dumb-init DEB/RPM安装 RUN version=v1.2.2 && \\ wget https://github.com/Yelp/dumb-init/releases/download/$version/dumb-init_$version_amd64.deb | dpkg -i apt-get instal dumb-init pip安装 pip install dumb-init 三、tini Github：https://github.com/krallin/tini 安装 Alpine镜像的APK可以直接安装 FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache tini ENTRYPOINT [\"tini\", \"--\"] CMD [\"/your/program\", \"-and\", \"-its\", \"arguments\"] 四、应用场景 1、php-fpm进程的接管 针对php应用，通常采用nginx+php-fpm的架构来处理请求。为了保证php-fpm进程出现意外故障能够自动恢复，通常使用supervisor进程管理工具进行守护。php-fpm的进程管理类也类似于nginx，由master，worker进程组成。master进程不处理请求，而是由worker进程处理！master进程只负责管理worker进程。 master进程负责监听子进程的状态，子进程挂掉之后，会发信号给master进程，然后master进程重新启一个新的worker进程。 进程号 父进程号 进程 21 10 master 22 21 |----worker1 23 21 |----worker2 使用Supervisor启动、守护php-fpm进程时的进程树 进程号 父进程号 进程 10 9 supervisor 21 10 |---master 22 21 |----worker1 23 21 |----worker2 # 使用supervisor启动、守护的是php-fpm的master进程，然后master进程再根据配置启动对应数量的worker进程。 当php-fpm的master进程意外退出后的进程树 进程号 父进程号 进程 10 9 supervisor 22 1 worker1 23 1 worker2 # 此时worker进程成为僵尸进程，被1号进程接管 此时supervisor检测到php-fpm master进程不存在就会在重新创建一个新的php-fpm master进程。但是会因为原先的php-fpm worker没有被杀掉，成为僵尸进程、依旧占用着端口而失败。本以为php-fpm会 参考 https://www.infoq.cn/article/2016/01/dumb-init-Docker https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/ https://medium.com/@gchudnov/trapping-signals-in-docker-containers-7a57fdda7d86 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/docker-user-process-manage.html":{"url":"origin/docker-user-process-manage.html","title":"容器中的用户管理","keywords":"","body":"Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/docker-summary.html":{"url":"origin/docker-summary.html","title":"Docker常见操作","keywords":"","body":"Docker常见操作 1、容器访问宿主机 通过域名host.docker.internal 或者docker.for.mac.host.internal（MacOS版下多出来的DNS域名） Docker Daemon不要设置非默认DNS，不然无法使用上述域名 通过docker0网络的默认网关地址：例如分配容器网络子网是172.17.0.0/16，那网关地址为172.17.0.2 在默认的bridge模式下，docker0网络的默认网关即是宿主机 因为MacOS的Docker Desktop底层使用的虚拟机，所有Docker0网卡无法直接看到 参考：https://cxybb.com/article/qq_38403662/102555888 2、格式化输出容器相关信息 ① 格式化输出镜像大小 echo -e \"大小\\t镜像\\n\" && docker images --format '{{.Size}}\\t{{.Repository}}:{{.Tag}}' | sed 's/ //' | sort -h ②列出docker各网络模式下容器IP地址等信息 docker network inspect -f '{{println}}{{.Driver}}网络 {{range .IPAM.Config}}{{printf \"(网段: %s 网关: %s)\" .Subnet .Gateway}}{{end}}{{println}} {{range .Containers}}{{printf \"%s\" .Name}}{{printf \"\\t\"}}{{printf \"IP地址: %s\" .IPv4Address}} {{printf \"MAC地址: %s\" .MacAddress}} {{println}} {{end}}' $(docker network ls -q) 3、docker run覆盖掉默认ENTREYPOINT docker run --rm --name test -it --entrypoint bash nginx 4、Dockerfile中指定shell环境 SHELL [\"/bin/bash\", \"-c\"] RUN pwd Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-11 16:09:36 "},"origin/docker-Dockerfile中CMD与ENTRYPOINT命令的区别.html":{"url":"origin/docker-Dockerfile中CMD与ENTRYPOINT命令的区别.html","title":"Dockerfile中CMD与ENTRYPOINT命令的区别","keywords":"","body":"CMD与ENTRYPOINT区别 CMD命令设置容器启动后默认执行的命令及其参数，但CMD设置的命令能够被docker run命令后面的命令行参数替换 ENTRYPOINT配置容器启动时的执行命令（不会被忽略，一定会被执行，即使运行 docker run时指定了其他命令） ENTRYPOINT 的 Exec 格式用于设置容器启动时要执行的命令及其参数，同时可通过CMD命令或者命令行参数提供额外的参数 ENTRYPOINT 中的参数始终会被使用，这是与CMD命令不同的一点 1. Shell格式和Exec格式命令 Shell格式：指令 CMD java -jar test.jar Exec格式：指令 [\"executable\", \"param1\", \"param2\", ...] ENTRYPOINT [\"java\", \"-jar\", \"test.jar\"] 2. Shell格式和Exec格式命令的区别 Shell格式中的命令会直接被Shell解析 Exec格式不会直接解析，需要加参数 3. CMD和ENTRYPOINT指令支持的命令格式 CMD 指令的命令支持以下三种格式: Exec格式: CMD [\"executable\",\"param1\",\"param2\"] Exec参数: CMD [\"param1\",\"param2\"] 用来为ENTRYPOINT 提供参数 Shell格式: CMD command param1 param2 ENTRYPOINT 指令的命令支持以下了两种格式: Exec格式：可用使用CMD的参数和可使用docker run [image] 参数后面追加的参数 Shell格式 ：不会使用 CMD参数，可使用docker run [image] 参数后面追加的参数 4. 示例 ENTRYPOINT的Exec格式 # Dockerfile FROM centos ENTRYPOINT [\"/bin/echo\", \"Hello\"] # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello Test ENTRYPOINT的Exec格式 + CMD的Exec格式 # Dockerfile FROM centos ENTRYPOINT [\"/bin/echo\", \"Hello\"] CMD [\"Word\"] # 启动容器的命令: docker run -it [image] # 输出: Hello Word # 启动容器的命令: docker run -it [image] Test # 输出: Hello Test ENTRYPOINT的Exec格式 + CMD的shell格式 # Dockerfile FROM centos ENTRYPOINT [\"/bin/echo\", \"Hello\"] CMD Word # 启动容器的命令: docker run -it [image] # 输出: Hello /bin/sh -c Word # 启动容器的命令: docker run -it [image] Test # 输出: Hello Test ENTRYPOINT的shell格式 # Dockerfile FROM centos ENTRYPOINT /bin/echo \"Hello\" # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello ENTRYPOINT的shell格式 + CMD的Shell格式 # Dockerfile FROM centos ENTRYPOINT /bin/echo \"Hello\" CMD Word # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello ENTRYPOINT的shell格式 +CMD的Exec格式 # Dockerfile FROM centos ENTRYPOINT /bin/echo \"Hello\" CMD [\"Word\"] # 启动容器的命令: docker run -it [image] # 输出: Hello # 启动容器的命令: docker run -it [image] Test # 输出: Hello 参考链接 https://blog.csdn.net/weixin_42971363/article/details/91506844 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/docker-使用Makefile操作Dockerfile.html":{"url":"origin/docker-使用Makefile操作Dockerfile.html","title":"使用Makefile操作Dockerfile.md","keywords":"","body":" IMAGE_BASE = docker-registry-default.apps.okd311.curiouser.com/openshift IMAGE_NAME = demo-springboot2 IMAGE_VERSION = latest IMAGE_TAGVERSION = $(GIT_COMMIT) all: build tag push build: docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} . tag: docker tag ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_TAGVERSION} push: docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_TAGVERSION} makefile中的命令必须以tab作为开头(分隔符),不能用扩展的tab即用空格代替的tab。(如果是vim编辑的话,执行 set noexpandtab)。否则会报如下错误：`Makefile:10: * multiple target patterns. Stop.`** Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-03-24 18:26:53 "},"origin/docker-multi-stage-build.html":{"url":"origin/docker-multi-stage-build.html","title":"多阶段构建","keywords":"","body":"Docker多阶段构建 一、简介 在编写Dockerfile构建docker镜像时，常遇到以下问题： RUN命令会让镜像新增layer，导致镜像变大，虽然通过&&连接多个命令能缓解此问题，但如果命令之间用到docker指令例如COPY、WORKDIR等，依然会导致多个layer； 有些工具在构建过程中会用到，但是最终的镜像是不需要的（例如用maven编译构建java工程），这要求Dockerfile的编写者花更多精力来清理这些工具，清理的过程又可能导致新的layer； 为了解决上述问题，从17.05版本开始Docker在构建镜像时增加了新特性：多阶段构建(multi-stage builds)，将构建过程分为多个阶段，每个阶段都可以指定一个基础镜像，这样在一个Dockerfile就能将多个镜像的特性同时用到 二、实践操作 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/docker-alpine.html":{"url":"origin/docker-alpine.html","title":"Alpine镜像","keywords":"","body":"Alpine镜像 一、简介 Alpine 操作系统是一个面向安全的轻型 Linux 发行版。它不同于通常 Linux 发行版，Alpine 采用了 musl libc 和 busybox 以减小系统的体积和运行时资源消耗，但功能上比 busybox 又完善的多，因此得到开源社区越来越多的青睐。在保持瘦身的同时，Alpine 还提供了自己的包管理工具 apk，可以通过 https://pkgs.alpinelinux.org/packages 网站上查询包信息，也可以直接通过 apk 命令直接查询和安装各种软件。 Alpine 由非商业组织维护的，支持广泛场景的 Linux发行版，它特别为资深/重度Linux用户而优化，关注安全，性能和资源效能。Alpine 镜像可以适用于更多常用场景，并且是一个优秀的可以适用于生产的基础系统/环境。 Alpine Docker 镜像也继承了 Alpine Linux 发行版的这些优势。相比于其他 Docker 镜像，它的容量非常小，仅仅只有 5 MB 左右（对比 Ubuntu 系列镜像接近 200 MB），且拥有非常友好的包管理机制。官方镜像来自 docker-alpine 项目。 目前 Docker 官方已开始推荐使用 Alpine 替代之前的 Ubuntu 做为基础镜像环境。这样会带来多个好处。包括镜像下载速度加快，镜像安全性提高，主机之间的切换更方便，占用更少磁盘空间等。 官方网站：https://alpinelinux.org/ Github：https://github.com/alpinelinux/docker-alpine Dockerhub：https://hub.docker.com/_/alpine 二、APK包管理器 可在包管理中心查看支持的包：https://pkgs.alpinelinux.org/packages 1、apk命令详解 命令格式 apk 子命令 参数项 全局参数项 -h, --help Show generic help or applet specific help -p, --root DIR Install packages to DIR -X, --repository REPO Use packages from REPO -q, --quiet Print less information -v, --verbose Print more information (can be doubled) -i, --interactive Ask confirmation for certain operations -V, --version Print program version and exit -f, --force Enable selected --force-* (deprecated) --force-binary-stdout Continue even if binary data is to be output --force-broken-world Continue even if 'world' cannot be satisfied --force-non-repository Continue even if packages may be lost on reboot --force-old-apk Continue even if packages use unsupported features --force-overwrite Overwrite files in other packages --force-refresh Do not use cached files (local or from proxy) -U, --update-cache Alias for --cache-max-age 1 --progress Show a progress bar --progress-fd FD Write progress to fd --no-progress Disable progress bar even for TTYs --purge Delete also modified configuration files (pkg removal) and uninstalled packages from cache (cache clean) --allow-untrusted Install packages with untrusted signature or no signature --wait TIME Wait for TIME seconds to get an exclusive repository lock before failing --keys-dir KEYSDIR Override directory of trusted keys --repositories-file REPOFILE Override repositories file --no-network Do not use network (cache is still used) --no-cache Do not use any local cache path --cache-dir CACHEDIR Override cache directory --cache-max-age AGE Maximum AGE (in minutes) for index in cache before refresh --arch ARCH Use architecture with --root --print-arch Print default arch and exit commit参数项 -s, --simulate Show what would be done without actually doing it --clean-protected Do not create .apk-new files in configuration dirs --overlay-from-stdin Read list of overlay files from stdin --no-scripts Do not execute any scripts --no-commit-hooks Skip pre/post hook scripts (but not other scripts) --initramfs-diskless-boot Enables options for diskless initramfs boot (e.g. skip hooks) 子命令 \u0016①安装与删除 add：安装包 --initdb Initialize database -u, --upgrade Prefer to upgrade package -l, --latest Select latest version of package (if it is not pinned), and print error if it cannot be installed due to other dependencies -t, --virtual NAME Instead of adding all the packages to 'world', create a new virtual package with the listed dependencies and add that to 'world'; the actions of the command are easily reverted by deleting the virtual package del：卸载并删除包 -r, --rdepends Recursively delete all top-level reverse dependencies too ②包的元信息管理 fix：在不改动主要的依赖的情况下进行包的修复或者升级 -d, --depends Fix all dependencies too -r, --reinstall Reinstall the package (default) -u, --upgrade Prefer to upgrade package -x, --xattr Fix packages with broken xattrs --directory-permissions Reset all directory permissions update：从远程仓库获取信息更新本地仓库索引 upgrade：令升级系统已安装的所以软件包（一般包括内核），当然也可指定仅升级部分软件包（通过-u或–upgrade选择指定 -a, --available Resets versioned world dependencies, and changes to prefer replacing or downgrading packages (instead of holding them) if the currently installed package is no longer available from any repository -l, --latest Select latest version of package (if it is not pinned), and print error if it cannot be installed due to other dependencies --no-self-upgrade Do not do early upgrade of 'apk-tools' package --self-upgrade-only Only do self-upgrade cache：对缓存进行操作，比如对缺失的包进行缓存或者对于不需要的包进行缓存删除 -u, --upgrade Prefer to upgrade package -l, --latest Select latest version of package (if it is not pinned), and print error if it cannot be installed due to other dependencies ③查询搜索包 info：列出所有已安装的软件包 -L, --contents List contents of the PACKAGE -e, --installed Check if PACKAGE is installed -W, --who-owns Print the package owning the specified file -R, --depends List packages that the PACKAGE depends on -P, --provides List virtual packages provided by PACKAGE -r, --rdepends List all packages depending on PACKAGE --replaces List packages whom files PACKAGE might replace -i, --install-if List the PACKAGE's install_if rule -I, --rinstall-if List all packages having install_if referencing PACKAGE -w, --webpage Show URL for more information about PACKAGE -s, --size Show installed size of PACKAGE -d, --description Print description for PACKAGE --license Print license for PACKAGE -t, --triggers Print active triggers of PACKAGE -a, --all Print all information about PACKAGE list：按照指定条件进行包的列表信息显示 -I, --installed List installed packages only -O, --orphaned List orphaned packages only -a, --available List available packages only -u, --upgradable List upgradable packages only -o, --origin List packages by origin -d, --depends List packages by dependency -P, --providers List packages by provider search：查询相关的包的详细信息，支持正则 -a, --all Show all package versions (instead of latest only) -d, --description Search package descriptions (implies -a) -x, --exact Require exact match (instead of substring match) -e Synonym for -x (deprecated) -o, --origin Print origin package name instead of the subpackage -r, --rdepends Print reverse dependencies of package --has-origin List packages that have the given origin dot：生成依赖之间的关联关系图（使用箭头描述） --errors Output only parts of the graph which are considered erroneous: e.g. cycles and missing packages --installed Consider only installed packages policy：显示包的仓库策略信息 ④源管理 stats：显示仓库和包的安装相关的统计信息 index：使用文件生成仓库索引文件 -o, --output FILE Write the generated index to FILE -x, --index INDEX Read INDEX to speed up new index creation by reusing the information from an old index -d, --description TEXT Embed TEXT as description and version information of the repository index --rewrite-arch ARCH Use ARCH as architecture for all packages fetch：从全局仓库下载包到本地目录 -L, --link Create hard links if possible -R, --recursive Fetch the PACKAGE and all its dependencies --simulate Show what would be done without actually doing it -s, --stdout Dump the .apk to stdout (incompatible with -o, -R, --progress) -o, --output DIR Directory to place the PACKAGEs to verify：验证包的完整性和签名信息 manifest：显示package各组成部分的checksum 2、操作 ①安装软件 FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache git ②替换Alpine的软件源 常见国内Alpine软件源： 阿里云 FROM alpilne:3.11.5 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories 中科大 FROM alpilne:3.11.5 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories ③安装bash FROM alpine:3.11.5 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \\ && apk add --no-cache bash bash-doc bash-completion ④安装telnet FROM alpine:3.11.5 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \\ && apk add --no-cache busybox-extras ⑤安装Docker Client和Make FROM alpine:3.11.5 RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \\ && apk add --no-cache docker-cli make ⑥修改用户的所属用户组 FROM alpine:3.11.5 RUN sed -i 's/1001/0/g' /etc/passwd ⑦设置系统语言为“en_US.UTF-8”，以防中文乱码 FROM alpine:3.11.5 ENV LANG=en_US.UTF-8 \\ LANGUAGE=en_US.UTF-8 RUN apk --no-cache add ca-certificates \\ && wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \\ && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.29-r0/glibc-2.29-r0.apk \\ && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.29-r0/glibc-bin-2.29-r0.apk \\ && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.29-r0/glibc-i18n-2.29-r0.apk \\ && apk add glibc-2.29-r0.apk glibc-bin-2.29-r0.apk glibc-i18n-2.29-r0.apk \\ && rm -rf /usr/lib/jvm glibc-2.29-r0.apk glibc-bin-2.29-r0.apk glibc-i18n-2.29-r0.apk \\ && /usr/glibc-compat/bin/localedef --force --inputfile POSIX --charmap UTF-8 \"$LANG\" || true \\ && echo \"export LANG=$LANG\" > /etc/profile.d/locale.sh \\ && apk del glibc-i18n 参考： https://github.com/gliderlabs/docker-alpine/issues/144 https://gist.github.com/alextanhongpin/aa55c082a47b9a1b0060a12d85ae7923 ⑧设置时区 FROM alpine:3.11.5 ENV TZ=Asia/Shanghai RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \\ && apk add --no-cache tzdata \\ && cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ && echo \"Asia/Shanghai\" > /etc/timezone 参考 https://yeasy.gitbooks.io/docker_practice/cases/os/alpine.html https://blog.csdn.net/liumiaocn/article/details/87603628 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/dockerfile-hadolint.html":{"url":"origin/dockerfile-hadolint.html","title":"语法扫描工具Hadolint","keywords":"","body":"Dockerfile扫描工具Hadolint 一、简介 Hadolint是一个用Haskell 编写的开源Dockerfiles语法检查、优化工具。 Github地址：https://github.com/hadolint/hadolint 在线扫描：https://hadolint.github.io/hadolint/ 二、安装与配置 1、安装 OSX brew brew install hadolint Windows scoop scoop install hadolint docker docker pull hadolint/hadolint:latest-debian docker pull hadolint/hadolint:latest-alpine 2、配置 hadolint会按照以下顺序读取配置文件 $PWD/.hadolint.yaml $XDG_CONFIG_HOME/hadolint.yaml ~/.config/hadolint.yaml 扫描时指定配置文件 hadolint --config /path/to/config.yaml Dockerfile 配置忽略规则 echo \"export XDG_CONFIG_HOME=~/.hadolint\" >> /etc/profile && \\ mkdir ~/.hadolint && \\ source /etc/profile && \\ 创建并编辑配置vi ~/.hadolint/hadolint.yaml ignored: - DL3000 - SC1010 trustedRegistries: - docker.io - my-company.com:5000 3、命令参数 hadolint [-v|--version] [-c|--config FILENAME] [-f|--format ARG] [DOCKERFILE...] [--ignore RULECODE] [--trusted-registry REGISTRY (e.g. docker.io)] 可选参数: -h,--help Show this help text -v,--version Show version -c,--config FILENAME Path to the configuration file -f,--format ARG The output format for the results [tty | json | checkstyle | codeclimate | codacy] (default: tty) --ignore RULECODE A rule to ignore. If present, the ignore list in the config file is ignored --trusted-registry REGISTRY (e.g. docker.io) A docker registry to allow to appear in FROM instructions 三、扫描Dockerfile hadolint Dockerfile hadolint --ignore DL3003 --ignore DL3006 Dockerfile # 或者 docker run --rm -i hadolint/hadolint 输出扫描结果 Dockerfile:2 DL3020 Use COPY instead of ADD for files and folders Dockerfile:3 DL3025 Use arguments JSON notation for CMD and ENTRYPOINT arguments 四、与CI流程与编辑器的集成 官方集成示例文档：https://github.com/hadolint/hadolint/blob/master/docs/INTEGRATION.md 编辑器 VSCode Sublime Text 3 Vim and NeoVim Atom CI流程 1、Travis CI # Use container-based infrastructure for quicker build start-up sudo: false # Use generic image to cut start-up time language: generic env: # Path to 'hadolint' binary HADOLINT: \"${HOME}/hadolint\" install: # Download hadolint binary and set it as executable - curl -sL -o ${HADOLINT} \"https://github.com/hadolint/hadolint/releases/download/v1.17.5/hadolint-$(uname -s)-$(uname -m)\" && chmod 700 ${HADOLINT} script: # List files which name starts with 'Dockerfile' # eg. Dockerfile, Dockerfile.build, etc. - git ls-files --exclude='Dockerfile*' --ignored | xargs --max-lines=1 ${HADOLINT} 2、GitHub Actions name: Lint Dockerfile on: push jobs: linter: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Lint Dockerfile uses: brpaz/hadolint-action@master with: dockerfile: \"Dockerfile\" 3、Gitlab CI lint_dockerfile: image: hadolint/hadolint:latest-debian script: - hadolint Dockerfile 4、Jenkins declarative pipeline stage (\"lint dockerfile\") { agent { docker { image 'hadolint/hadolint:latest-debian' } } steps { sh 'hadolint dockerfiles/* | tee -a hadolint_lint.txt' } post { always { archiveArtifacts 'hadolint_lint.txt' } } } 5、Jenkins K8S plugin 声明hadolint pod - name: hadolint image: hadolint/hadolint:latest-debian imagePullPolicy: Always command: - cat tty: true stage('lint dockerfile') { steps { container('hadolint') { sh 'hadolint dockerfiles/* | tee -a hadolint_lint.txt' } } post { always { archiveArtifacts 'hadolint_lint.txt' } } } 6、Bitbucket Pipelines pipelines: default: - step: image: hadolint/hadolint:latest-debian script: - hadolint Dockerfile 五、扫描规则 DL开头的规则是hadolint的 SC开头的规则是ShellChecke的 Rule 描述 DL3000 Use absolute WORKDIR. DL3001 For some bash commands it makes no sense running them in a Docker container like ssh, vim, shutdown, service, ps, free, top, kill, mount, ifconfig. DL3002 Last user should not be root. DL3003 Use WORKDIR to switch to a directory. DL3004 Do not use sudo as it leads to unpredictable behavior. Use a tool like gosu to enforce root. DL3005 Do not use apt-get upgrade or dist-upgrade. DL3006 Always tag the version of an image explicitly. DL3007 Using latest is prone to errors if the image will ever update. Pin the version explicitly to a release tag. DL3008 Pin versions in apt-get install. DL3009 Delete the apt-get lists after installing something. DL3010 Use ADD for extracting archives into an image. DL3011 Valid UNIX ports range from 0 to 65535. DL3012 Provide an email address or URL as maintainer. DL3013 Pin versions in pip. DL3014 Use the -y switch. DL3015 Avoid additional packages by specifying --no-install-recommends. DL3016 Pin versions in npm. DL3017 Do not use apk upgrade. DL3018 Pin versions in apk add. Instead of apk add use apk add =. DL3019 Use the --no-cache switch to avoid the need to use --update and remove /var/cache/apk/* when done installing packages. DL3020 Use COPY instead of ADD for files and folders. DL3021 COPY with more than 2 arguments requires the last argument to end with / DL3022 COPY --from should reference a previously defined FROM alias DL3023 COPY --from cannot reference its own FROM alias DL3024 FROM aliases (stage names) must be unique DL3025 Use arguments JSON notation for CMD and ENTRYPOINT arguments DL3026 Use only an allowed registry in the FROM image DL3027 Do not use apt as it is meant to be a end-user tool, use apt-get or apt-cache instead DL3028 Pin versions in gem install. Instead of gem install use gem install : DL4000 MAINTAINER is deprecated. DL4001 Either use Wget or Curl but not both. DL4003 Multiple CMD instructions found. DL4004 Multiple ENTRYPOINT instructions found. DL4005 Use SHELL to change the default shell. DL4006 Set the SHELL option -o pipefail before RUN with a pipe in it SC1000 $ is not used specially and should therefore be escaped. SC1001 This \\c will be a regular 'c' in this context. SC1007 Remove space after = if trying to assign a value (or for empty string, use var='' ...). SC1010 Use semicolon or linefeed before done (or quote to make it literal). SC1018 This is a unicode non-breaking space. Delete it and retype as space. SC1035 You need a space here SC1045 It's not foo &; bar, just foo & bar. SC1065 Trying to declare parameters? Don't. Use () and refer to params as $1, $2 etc. SC1066 Don't use $ on the left side of assignments. SC1068 Don't put spaces around the = in assignments. SC1077 For command expansion, the tick should slant left (` vs ´). SC1078 Did you forget to close this double-quoted string? SC1079 This is actually an end quote, but due to next char, it looks suspect. SC1081 Scripts are case sensitive. Use if, not If. SC1083 This {/} is literal. Check expression (missing ;/\\n?) or quote it. SC1086 Don't use $ on the iterator name in for loops. SC1087 Braces are required when expanding arrays, as in ${array[idx]}. SC1095 You need a space or linefeed between the function name and body. SC1097 Unexpected ==. For assignment, use =. For comparison, use [ .. ] or [[ .. ]]. SC1098 Quote/escape special characters when using eval, e.g. eval \"a=(b)\". SC1099 You need a space before the #. SC2002 Useless cat. Consider `cmd ..orcmd file ..` instead. SC2015 Note that `A && B C` is not if-then-else. C may run when A is true. SC2026 This word is outside of quotes. Did you intend to 'nest '\"'single quotes'\"' instead'? SC2028 echo won't expand escape sequences. Consider printf. SC2035 Use ./*glob* or -- *glob* so names with dashes won't become options. SC2039 In POSIX sh, something is undefined. SC2046 Quote this to prevent word splitting SC2086 Double quote to prevent globbing and word splitting. SC2140 Word is in the form \"A\"B\"C\" (B indicated). Did you mean \"ABC\" or \"A\\\"B\\\"C\"? SC2154 var is referenced but not assigned. SC2155 Declare and assign separately to avoid masking return values. SC2164 Use `cd ... exitin casecd` fails. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/dockerfile-optimization.html":{"url":"origin/dockerfile-optimization.html","title":"Dockerfile优化","keywords":"","body":"Dockerfile优化 一、指令格式化 LABEL LABEL vendor=ACME\\ Incorporated \\ com.example.is-beta= \\ com.example.is-production=\"\" \\ com.example.version=\"0.0.1-beta\" \\ com.example.release-date=\"2015-02-12\" ENV Dockerfile中ENV指令像RUN指令一样，每一个都会创建一个临时层。 ENV JAVA_HOME=/opt/jdk1.8.0_241 \\ CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib ENV PATH=$PATH:$JAVA_HOME/bin RUN RUN apt-get update && \\ apt-get install -y --no-install-recommends \\ apt-transport-https \\ ca-certificates && \\ rm -rf /var/lib/apt/lists/* 二、指令优化 1、减少RUN指令，合并命令 RUN useradd -s /sbin/nologin -m -u 1001 curiouser && \\ mkdir -p /home/curiouser/{data,logs} && \\ rm -rf /etc/yum.repos.d/C* && \\ yum install -q -y git && \\ yum clean all && \\ curl -s http://192.168.1.7/repository/tools/jdk-8u241-linux-x64.tar.gz | tar -xC /opt/ 2、使用COPY来代替ADD 对于使用ADD指令下载远程服务器上的tar包并解压，建议使用以下方式代替 RUN curl -s http://192.168.1.7/repository/tools/jdk-8u241-linux-x64.tar.gz | tar -xC /opt/ 三、最小化基础镜像，减小镜像体积 1、尽量使用Alpine作为基础镜像 Alpine镜像大小最多才几MB。 使用APK命令装最小化需求的软件包 FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache git 四、尽量不使用root用户 在做基础运行时镜像时，创建运行时普通用户和用户组，并做工作区与权限限制，启动服务时尽量使用普通用户。 gosu FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache gosu 参考： https://blog.csdn.net/boling_cavalry/article/details/93380447 五、使用进程管理工具来处理进程信号 为防止容器中的进程变成僵尸进程， dumb-init Github地址：https://github.com/Yelp/dumb-init FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache dumb-init # Runs \"/usr/bin/dumb-init -- /my/script --with --args\" ENTRYPOINT [\"dumb-init\", \"--\"] # or if you use --rewrite or other cli flags # ENTRYPOINT [\"dumb-init\", \"--rewrite\", \"2:3\", \"--\"] CMD [\"/my/script\", \"--with\", \"--args\"] 参考： https://www.infoq.cn/article/2016/01/dumb-init-Docker https://www.cnblogs.com/sunsky303/p/11046681.html 六、移除所有缓存等不必要信息 删除解压后的源压缩包（参考第二章第二节） 清理包管理器下载安装软件时的缓存 使用Alipine镜像中APK命令安装包时记得加上--no-cache 使用Ubuntu镜像中的APT命令安装软件后记得rm -rf /var/lib/apt/lists/* 七、使用合理的ENTRYPOINT脚本 示例： #!/bin/bash set -e if [ \"$1\" = 'postgres' ]; then chown -R postgres \"$PGDATA\" if [ -z \"$(ls -A \"$PGDATA\")\" ]; then gosu postgres initdb fi exec gosu postgres \"$@\" fi exec \"$@\" 八、其他建议 1、设置时区 FROM alpine:3.11.5 ENV TZ=Asia/Shanghai RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g' /etc/apk/repositories \\ && apk add --no-cache tzdata \\ && cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ && echo \"Asia/Shanghai\" > /etc/timezone 2、设置系统语言 FROM alpine:3.11.5 ENV LANG=en_US.UTF-8 \\ LANGUAGE=en_US.UTF-8 RUN apk --no-cache add ca-certificates \\ && wget -q -O /etc/apk/keys/sgerrand.rsa.pub https://alpine-pkgs.sgerrand.com/sgerrand.rsa.pub \\ && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.29-r0/glibc-2.29-r0.apk \\ && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.29-r0/glibc-bin-2.29-r0.apk \\ && wget -q https://github.com/sgerrand/alpine-pkg-glibc/releases/download/2.29-r0/glibc-i18n-2.29-r0.apk \\ && apk add glibc-2.29-r0.apk glibc-bin-2.29-r0.apk glibc-i18n-2.29-r0.apk \\ && rm -rf /usr/lib/jvm glibc-2.29-r0.apk glibc-bin-2.29-r0.apk glibc-i18n-2.29-r0.apk \\ && /usr/glibc-compat/bin/localedef --force --inputfile POSIX --charmap UTF-8 \"$LANG\" || true \\ && echo \"export LANG=$LANG\" > /etc/profile.d/locale.sh \\ && apk del glibc-i18n 3、使用Label标注作者、软件版本等元信息 FROM alpine:3.11.5 LABEL Author=Curiouser \\ Mail=****@163.com \\ PHP=7.3 \\ Tools=“git、vim、curl” \\ Update=\"添加用户组\" 4、指定工作区 WORKDIR /var/wwww 5、RUN指令显示优化 RUN set -eux ; \\ ls -al 九、镜像构建 1、命名 原则是见名知意。可使用三段式 镜像仓库地址/类型库/镜像名:版本号 registry/runtime/Java:8.1.2 registry/runtime/php-fpm-nginx:7.3-1.14 registry/cicd/kubctl-helm:1.17-3.0 registry/cicd/git-compose-docker:v1 registry/applications/demo:git_commit_id 2、使用Makefile IMAGE_BASE = registry/runtime IMAGE_NAME = php-fpm IMAGE_VERSION = 7.3 all: build push build: docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} . push: docker push ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} # 构建并推送 make # 仅构建 make build # 仅推送 make push 参考 https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ https://www.artindustrial-it.com/2017/09/20/10-best-practices-for-creating-good-docker-images/ https://gist.github.com/StevenACoffman/41fee08e8782b411a4a26b9700ad7af5 https://snyk.io/blog/10-docker-image-security-best-practices/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/shell-scripts-summary.html":{"url":"origin/shell-scripts-summary.html","title":"Shell脚本","keywords":"","body":"Shell脚本总结 一、简介 #!/bin/sh # 脚本中的第一行的“#!”为Shebang字符串。通常出现在类Unix系统的脚本中第一行，作为前两个字符。在直接调用脚本时，系统的程序载入器会分析 Shebang 后的内容，将这些内容作为解释器指令，并调用该指令，将载有 Shebang 的文件路径作为该解释器的参数，执行脚本，从而使得脚本文件的调用方式与普通的可执行文件类似。例如，以指令#!/bin/sh开头的文件，在执行时会实际调用 /bin/sh 程序（通常是 Bourne shell 或兼容的 shell，例如 bash、dash 等）来执行。 # 如果脚本文件中没有#!这一行，那么执行时会默认采用当前Shell去解释这个脚本(即：SHELL环境变量）。 # 如果#!之后的解释程序是一个可执行文件，那么执行这个脚本时，它就会把文件名及其参数一起作为参数传给那个解释程序去执行。 # 如果#!指定的解释程序没有可执行权限，则会报错 bad interpreter: Permission denied。如果#!指定的解释程序不是一个可执行文件，那么指定的解释程序会被忽略，# 转而交给当前的SHELL去执行这个脚本。 # 如果#!指定的解释程序不存在，那么会报错 bad interpreter: No such file or directory。注意：#!之后的解释程序，需要写其绝对路径（如：#!/bin/bash），它是不会自动到环境变量PATH中寻找解释器的。要用绝对路径是因为它会调用系统调用execve，这可以用strace工具来查看。 # 脚本文件必须拥有可执行权限。可通过chmod +x [filename] 来添加可执行权限。 # 当然，如果你使用类似于 bash test.sh ，python train.py这样的命令来执行脚本，那么#!这一行将会被忽略掉，解释器当然是用命令行中显式指定的解释器。 二、变量 1、Shell内置变量 内置变量 描述 $? 上一条命令执行状态 0 代表执行成功，1代表执行失败 $0~$9 位置参数1-9 ${10} 位置参数10 $# 位置参数个数 $$ 脚本进程的PID $- 传递到脚本中的标识 $! 运行在后台的最后一个作业的进程ID(PID) $_ 之前命令的最后一个参数 $@ 传递给脚本或函数的所有参数。被双引号(\" \")包含时，与 $* 稍有不同 $* 传递给脚本或函数的所有参数 $0 脚本的文件名 ${@: -1} 传递给脚本或函数的最后一个参数 ${@:1:$#-1} 传递给脚本或函数除最后一个参数以外的所有参数 $* 和 $@ 的区别 $* 和 $@ 都表示传递给函数或脚本的所有参数，不被双引号(\" \")包含时，都以\"$1\" \"$2\" … \"$n\" 的形式输出所有参数。但是当它们被双引号(\" \")包含时，\"$*\" 会将所有的参数作为一个整体，以\"$1 $2 …\\$n\"的形式输出所有参数；\"$@\" 会将各个参数分开，以”\\$1” \"\\$2\" … \"$n\" 的形式输出所有参数。 2、变量的定义、赋值 ①将命令输出赋值变量 var=`shell命令` # `是反引号 var=$(shell命令) var=' line 1 line 2 line 3 ' ②读取标准输入赋值给变量 read -p \"请输入一个字符： \" key echo $key 3、变量的引用 ①基础引用 $var ${var} ${var:defaultvalue} ②变量的引用默认值 表达式 含义 ${var_DEFAULT} 如果var没有被声明, 那么就以$DEFAULT作为其值 ${var=DEFAULT} 如果var没有被声明, 那么就以$DEFAULT作为其值 ${var:-DEFAULT} 如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 ${var:=DEFAULT} 如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 ${var+OTHER} 如果var声明了, 那么其值就是$OTHER, 否则就为null字符串 ${var:+OTHER} 如 果var被设置了, 那么其值就是$OTHER, 否则就为null字符串 ${var?ERR_MSG} 如果var没 被声明, 那么就打印$ERR_MSG ${var:?ERR_MSG} 如果var没 被设置, 那么就打印$ERR_MSG ${!varprefix*} 匹配之前所有以varprefix开头进行声明的变量 ${!varprefix@} 匹配之前所有以varprefix开头进行声明的变量 ③用变量值作为新变量名 $ name=test $ test_p=123 $ echo `eval echo '$'\"$name\"\"_p\"` 123 或者 $ var=\"world\" $ declare \"hello_$var=value\" $ echo $hello_world value 或者（ bash 4.3+） $ hello_world=\"value\" $ var=\"world\" $ declare -n ref=hello_$var $ echo $ref value 或者 $ hello_world=\"value\" $ var=\"world\" $ ref=\"hello_$var\" $ echo ${!ref} value 参考：https://github.com/dylanaraps/pure-bash-bible#variables name_1=aa name_2=bbb for i in ${!name_@} ;do # ${!name_@}仅限在sh、 bash中使用 echo \"\\$i为当前变量名：\" $i echo \"\\${!i}当前变量名的值：\" ${!i} echo \"\\${i/name/name_var}可替换当前变量名中的name为name_var: \" ${i/name/name_var} done 4、变量的数值运算 ①加减乘除 #样本数据 a=120 b=110 ((c=$a+$b)) #结果：230 ((d=$a-$b)) #结果：10 ((e=$a*$b)) #结果：13200 ((f=$a/$b)) #结果：1 c=$((a+b)) #结果：220 d=$((a-b)) #结果：20 e=$((a*b)) #结果：12000 f=$((a/b)) #结果：1 c=`expr a+b` #结果：220 d=`expr $a - $b` #结果：20 e=`expr $a \\* $b` #结果：12000 f=`expr $a / $b` #结果：1 ②自增 a=1 #第一种整型变量自增方式 a=$(($a+1)) echo $a #第二种整型变量自增方式 a=$[$a+1] echo $a #第三种整型变量自增方式 a=`expr $a + 1` echo $a #第四种整型变量自增方式 let a++ echo $a #第五种整型变量自增方式 let a+=1 echo $a #第六种整型变量自增方式 ((a++)) echo $a 5、数值变量的判断 -gt 大于，如[ $a -gt $b ] -lt 小于，如[ $a -lt $b ] -eq 等于，如[ $a -eq $b ] -ne 不等于，如[ $a -ne $b ] -ge 大于等于，如[ $a -ge $b ] -le 小于等于 ，如 [ $a -le $b ] 大于(需要双括号),如:(($a > $b)) >= 大于等于(需要双括号),如:(($a >= $b)) 6、变量的处理 ①变量输出多行变一行并追加字符 $ echo $a 1 2 3 $ echo $a | tr '\\n' ',’ 1,2,3, ②位数截取 a=1110418197875 # 截去后三位,要求只取\"1110418197875\" # 方式1: 数值运算 b=$((a/1000)) # 方式2：字符截取（将数值变量当成字符串来处理） c=${a:0:-3} 7、数组 ①定义赋值 tests=('a1a' 'b2b' 'c3c') ②切割字符串为数组 test=\"a,b-,d\" # bash中的 read 读取输入为数组的参数为 -a IFS='-' read -a tests ③输出 # 数组索引，bash是从0开始 ，zsh中索引是从1开始。 tests=('a1a' 'b2b' 'c3c') echo ${tests[1]} # bash输出为\"b2b\", zsh则输出\"a1a\" # 输出，bash只显示第一个\"a1a\"，zsh显示所有元素 echo $tests ④遍历循环 # 切割其他变量出为数组变量 a=$(echo \"a,b-,d\") IFS='-' read -A -r tests 8、多行文本变量 ①定义赋值、引用、输出 tests='a1a b2b c3c ' echo $tests # a1a b2b c3c echo \"$tests\" # a1a # b2b # c3c # 总共是有四行的输出，最后一个是空行 ②遍历循环 # zsh中的方法 for test in ${(f)tests}; do echo \"测试: \"$test done # bash中的方法 for test in ${tests}; do echo \"测试: $test\" done 三、文件目录的判断 [ -a FILE ] 如果 FILE 存在则为真。 [ -b FILE ] 如果 FILE 存在且是一个块文件则返回为真。 [ -c FILE ] 如果 FILE 存在且是一个字符文件则返回为真。 [ -d FILE ] 如果 FILE 存在且是一个目录则返回为真。 [ -e FILE ] 如果 指定的文件或目录存在时返回为真。 [ -f FILE ] 如果 FILE 存在且是一个普通文件则返回为真。 [ -g FILE ] 如果 FILE 存在且设置了SGID则返回为真。 [ -h FILE ] 如果 FILE 存在且是一个符号符号链接文件则返回为真。（该选项在一些老系统上无效） [ -k FILE ] 如果 FILE 存在且已经设置了冒险位则返回为真。 [ -p FILE ] 如果 FILE 存并且是命令管道时返回为真。 [ -r FILE ] 如果 FILE 存在且是可读的则返回为真。 [ -s FILE ] 如果 FILE 存在且大小非0时为真则返回为真。 [ -u FILE ] 如果 FILE 存在且设置了SUID位时返回为真。 [ -w FILE ] 如果 FILE 存在且是可写的则返回为真。（一个目录为了它的内容被访问必然是可执行的） [ -x FILE ] 如果 FILE 存在且是可执行的则返回为真。 [ -O FILE ] 如果 FILE 存在且属有效用户ID则返回为真。 [ -G FILE ] 如果 FILE 存在且默认组为当前组则返回为真。（只检查系统默认组） [ -L FILE ] 如果 FILE 存在且是一个符号连接则返回为真。 [ -N FILE ] 如果 FILE 存在 and has been mod如果ied since it was last read则返回为真。 [ -S FILE ] 如果 FILE 存在且是一个套接字则返回为真。 [ FILE1 -nt FILE2 ] 如果 FILE1 比 FILE2 新, 或者 FILE1 存在但是 FILE2 不存在则返回为真。 [ FILE1 -ot FILE2 ] 如果 FILE1 比 FILE2 老, 或者 FILE2 存在但是 FILE1 不存在则返回为真。 [ FILE1 -ef FILE2 ] 如果 FILE1 和 FILE2 指向相同的设备和节点号则返回为真。 四、字符串的处理 1、截取 表达式 含义 ${#string} $string的字符个数 ${string:position} 在$string中, 从位置$position开始提取子串 ${string:position:length} 在$string中, 从位置$position开始提取长度为$length的子串 ${string#substring} 从 变量$string的开头, 删除最短匹配$substring的子串 ${string##substring} 从 变量$string的开头, 删除最长匹配$substring的子串 ${string%substring} 从 变量$string的结尾, 删除最短匹配$substring的子串 ${string%%substring} 从 变量$string的结尾, 删除最长匹配$substring的子串 ${string/substring/replacement} 使用$replacement, 来代替第一个匹配的$substring ${string//substring/replacement} 使用$replacement, 代替所有匹配的$substring ${string/#substring/replacement} 如果$string的前缀匹配$substring, 那么就用$replacement来代替匹配到的$substring ${string/%substring/replacement} 如果$string的后缀匹配$substring, 那么就用$replacement来代替匹配到的$substring expr match \"$string\" '$substring' 匹配$string开头的$substring* 的长度 expr \"$string\" : '$substring' 匹 配$string开头的$substring* 的长度 expr index \"$string\" $substring 在$string中匹配到的$substring的第一个字符出现的位置 expr substr $string $position $length 在$string中 从位置$position开始提取长度为$length的子串 expr match \"$string\" '\\($substring\\)' 从$string的 开头位置提取$substring* expr \"$string\" : '\\($substring\\)' 从$string的 开头位置提取$substring* expr match \"$string\" '.*\\($substring\\)' 从$string的 结尾提取$substring* expr \"$string\" : '.*\\($substring\\)' 从$string的 结尾提取$substring* ①#号从左边开始，删除第一次匹配到条件的左边字符，保留右边字符 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a#*/};echo $b # 结果：openshift/origin-metrics-cassandra:v3.9 ②##号从左边开始，删除最后一次匹配到条件的左边字符，保留右边字符 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a##*/};echo $b # 结果：origin-metrics-cassandra:v3.9 ③%号从右边开始，删除第一次匹配到条件的右边内容，保留左边字符（不保留匹配条件） # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a%/*};echo $b # 结果：docker.io/openshift ④ %%号从右边开始，删除最后一次匹配到条件的右边内容，保留左边字符（不保留匹配条件） # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a%%/*};echo $b # 结果：docker.io ⑤从左边第几个字符开始，及字符的个数 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:0:5};echo $b # 结果：docke ⑥从左边第几个字符开始，一直到结束 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:7};echo $b # 结果：io/openshift/origin-metrics-cassandra:v3.9 ⑦从右边第几个字符开始，向右截取 length 个字符。 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:0-8:5};echo $b # 结果：dra:v ⑧从右边第几个字符开始，一直到结束 # 样本: a=\"docker.io/openshift/origin-metrics-cassandra:v3.9\" b=${a:0-8};echo $b # 结果：dra:v3.9 ⑨截取字符串中的ip # 样本: a=\"当前 IP：123.456.789.172 来自于：中国 上海 上海 联通\" b=${a//[!0-9.]/};echo $b 或者 echo $a | grep -o -E \"[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]*\" # 结果：123.456.789.172 ⑩提取字符串中的数字 echo \"test-v1.1.0\" | tr -cd '[0-9.]' # 输出1.1.0 aa=\"test-v1.1.0\" | echo ${aa//[!0-9.]/} # 输出1.1.0 2、包含判断 样本数据 a=\"test\" b=\"curiouser\" c=\"test hahah devops\" ①通过grep来判断 if `echo $c |grep -q $a` ;then echo \"$c\" \" ----包含--- \" \"$a\" else echo \"$c\" \" ----不包含--- \" \"$a\" fi ②字符串运算符 if [[ $c =~ $a ]] ;then echo \"$c\" \" ----包含--- \" \"$a\" else echo \"$c\" \" ----不包含--- \" \"$a\" fi ③用通配符*号 用通配符*号代替str1中非str2的部分，如果结果相等说明包含，反之不包含 if [[ $c == *$a* ]] ;then echo \"$c\" \" ----包含--- \" \"$a\" else echo \"$c\" \" ----不包含--- \" \"$a\" fi ④利用替换 if [[ ${c/$a//} == $c ]] ;then echo \"$c\" \" ----不包含--- \" \"$a\" else echo \"$c\" \" ----包含--- \" \"$a\" fi 五、语句控制 1、IF 判断 if [ command ]; then 符合该条件执行的语句 fi if [ command ]; then command执行返回状态为0要执行的语句 else command执行返回状态为1要执行的语句 fi if [ command1 ]; then command1执行返回状态为0要执行的语句 elif [ command2 ]; then command2执行返回状态为0要执行的语句 else command1和command2执行返回状态都为1要执行的语句 fi PS: [ command ]，command前后要有空格 2、for循环 ①数字性循环 #!/bin/bash for((i=1;i #!/bin/bash for i in $(seq 1 10) do echo $(expr $i \\* 3 + 1); done #!/bin/bash for i in {1..10} do echo $(expr $i \\* 3 + 1); done #!/bin/bash awk 'BEGIN{for(i=1; i ②字符性循环 #!/bin/bash for i in `ls`; do echo $i is file name\\! ; done #!/bin/bash for i in $* ; do echo $i is input chart\\! ; done #!/bin/bash for i in f1 f2 f3 ; do echo $i is appoint ; done #!/bin/bash list=\"rootfs usr data data2\" for i in $list; do echo $i is appoint ; done ③路径查找 #!/bin/bash for file in /proc/*; do echo $file is file path \\! ; done #!/bin/bash for file in $(ls *.sh) do echo $file is file path \\! ; done 3、While循环 while condition ; do statements ... done Note: 和if一样，condition可以有一系列的statements组成，值是最后的statment的exit status 4、Util循环 until [condition-is-true] ; do statements ... done Note: 执行statements，直至command正确运行。在循环的顶部判断条件,并且如果条件一直为false那就一直循环下去 六、自定义函数 格式 [ function ] 函数名 [()] { action; [return int;] } # 1.函数在被调用前先声明好 # 2.function关键字可有无 调用 $ 函数名() $ 函数名(参数1,参数2) $ 函数名 参数1 参数2 $ (函数名 参数1 参数2) 七、并发处理 注意 在某些情况下，并发执行（使用&符号）可能会因为系统资源的限制而导致并发执行的效果不明显，甚至比串行执行更慢。 适合并发处理的场景 逻辑复杂，耗时比较长的 程序需要对外交互的，耗时比较长的。例如连接数据库，请求访问外部系统的 示例 例如要同时测试多个服务器 IP的端口是否开起，响应时间多少。 #!/bin/bash # 功能函数，测试服务器端口是否打开和响应时间。tcping可以测试端口的响应时间 # 函数，测试服务器端口和响应时间 pingMultServer(){ # 使用tcping测试端口响应时间 tcp_ping=`tcping -c 2 $1 $2 2>/dev/null | grep \"Average\" | awk -F\"[='ms'.]\" '{print $14}'` if [ \"$tcp_ping\" ];then return $tcp_ping else # 如果tcping不可用，使用常规ping o_ping=`ping -W 2 -c 2 $1 2>/dev/null | grep \"round-trip\" | awk -F '[=|/ |.]' '{print $10}'` [ \"$o_ping\" ] && return \"$o_ping\" || return 0 fi } # 原始服务器信息 orign_servers='192.168.1.1:8001,192.168.1.1:8002,192.168.1.1:8443,192.168.1.1:9443' # 存储ping结果的数组 ping=() # 将原始服务器信息解析成服务器数组 IFS=',' read -a servers 类型 user System total 并发执行 1.01s 0.46s 5.17s 串行执行 0.97s 0.39s 38.97s send_email $1 $client $client_random_password /etc/openvpn/client/$client.ovpn send_email $1 $client $clientcnname $client_random_password /etc/openvpn/client/$client.ovpn Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-21 14:19:25 "},"origin/bash-scirpts.html":{"url":"origin/bash-scirpts.html","title":"常用bash脚本功能","keywords":"","body":"0、常用脚本参数 # 在\"set -e\"之后出现的代码，一旦出现了返回值非零，整个脚本就会立即退出，注：set +e表示关闭，-e选项，set -e表示重新打开-e选项。 set -e # 表示shell脚本执行时如果遇到不存在的变量会报错并停止执行。默认不加-u参数的情况下，shell脚本遇到不存在的变量不会报错，会继续执行。 set -u set -o pipefail #设置了这个选项以后，包含管道命令的语句的返回值，会变成最后一个返回非零的管道命令的返回值。 1、判断curl返回状态码 #!/bin/bash response=$(curl -sL -o /dev/null -w %{http_code} https://baidu.com) if [[ $response -ge 200 && $response -le 299 ]] ;then echo 'check point success' else echo 'check point fail' fi 2、读取文件中的配置到变量中 #!/bin/bash # 配置文件中的配置项格式为key1=value1，一行一个配置项 while read line;do eval \"$line\" done 3、根据console输入条件执行 #!/bin/bash echo \"选择以下功能:\" echo \" 0) 功能0\" echo \" 1) 功能1\" echo \" 2) 功能2\" echo \" 3) 功能3\" echo \" 4) 功能4\" read -p \"功能选项[4]: \" option until [[ -z \"$option\" || \"$option\" =~ ^[0-4]$ ]]; do read -p \"$option为无效的选项，请重新输入功能选项: \" option done case \"$option\" in 0) echo \"功能0已执行!\" ;; 1) echo \"功能1已执行!\" exit ;; 2) echo \"功能2已执行!\" exit ;; 3) echo \"功能3已执行!\" exit ;; # 默认选项 4|\"\") echo \"功能4已执行!\" exit ;; esac 4、将指定输出内容写入文件 { echo \"hahh\" echo \"lalal\" } > /tmp/test 5、判断变量是否存在或为空 if [ -z ${var+x} ]; then echo \"var is unset\"; else echo \"var is set to '$var'\"; fi 参考：https://stackoverflow.com/questions/3601515/how-to-check-if-a-variable-is-set-in-bash #!/bin/bash # -z 选项用于检查变量是否为空字符串，如果是，则返回true if [ -z \"${VAR}\" ]; then echo \"VAR is empty\" fi # -n 选项用于检查变量是否为非空字符串，如果是，则返回true。 if [ -n \"${VAR}\" ]; then echo \"VAR is not empty\" fi 在测试变量是否为空时，最好使用双引号将变量括起来，以避免在变量为空时出现语法错误。 6、换算秒为分钟、小时 #!/bin/bash a=60100 swap_seconds () { SEC=$1 (( SEC = 60 && SEC 3600 )) && echo -e \"持续时间: $(( SEC / 3600 ))小时$(( (SEC % 3600) / 60 ))分钟$(( (SEC % 3600) % 60 ))秒\\c\" } b=`swap_seconds $a` echo $b 输出 持续时间: 16小时41分钟40秒 7、脚本命令行参数的传递与判断 #!/bin/bash main() { if [[ $# == 1 ]]; then case $1 in \"-h\") echo \"脚本使用方法: \" echo \" ./gitlab-pipeline.sh git仓库名1 git仓库名2 ... tag名(tag命名规则为: *-v加数字)\" exit ;; \"--help\") echo \"脚本使用方法: \" echo \" ./gitlab-pipeline.sh git仓库名1 git仓库名2 ... tag名(tag命名规则为: *-v加数字)\" exit ;; *) echo \"参数错误！\" exit ;; esac fi } main $* 8、检测docker 容器的启动状态 # 第一步：判断镜像是否存在 if [ `docker images --format {{.Repository}}:{{.Tag}} |grep -Fx 192.168.1.7:32772/applications/$CI_PROJECT_NAME:${CI_COMMIT_SHORT_SHA};echo $?` -eq 0 ];then docker pull 192.168.1.7:32772/applications/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA ; fi; # 第二步：判断是否已经有重名的容器在运行或者处在其他状态。重名的，先删掉，在启动；不重名的直接启动 if [ `docker ps -a --format {{.Names}} |grep -Fx $CI_PROJECT_NAME > /dev/null ;echo $?` -eq 0 ] ;then docker rm -f $CI_PROJECT_NAME ; docker run -d --name $CI_PROJECT_NAME -p 30088:8080 192.168.1.7:32772/applications/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA ; else docker run -d --name $CI_PROJECT_NAME -p 30088:8080 192.168.1.7:32772/applications/$CI_PROJECT_NAME:$CI_COMMIT_SHORT_SHA ; fi; # 第三步：循环五次判断容器的监控检测是否处于什么状态。健康状态就直接退出循环，不健康显示健康检查日志，正在启动的直接显示。处于其他状态的直接显示状态 n=0; while true ;do container_state=`docker inspect --format='{{json .State.Health.Status}}' $CI_PROJECT_NAME`; case $container_state in '\"starting\"' ) echo \"应用容器正在启动！\"; ;; '\"healthy\"' ) echo \"应用容器已启动，状态健康！\"; break; ;; '\"unhealthy\"' ) echo \"应用容器健康检测失败！\"; docker inspect --format='{{json .State.Health.Log}}' $CI_PROJECT_NAME; ;; * ) echo \"未知的状态:$container_state\"; ;; esac; sleep 1s; n=$(($n+1)); if [ $n -eq 5 ]; then break ; fi ; done 9、检查常见系统命令是否安装 check_commands_exists() { need_commands=($*) no_commands=\"\" for command in \"${need_commands[@]}\" ; do if ! command -v $command &> /dev/null; then no_commands+=\" $command\" fi done if [[ $no_commands ]]; then echo -e \"\\033[31m$no_commands 命令不存在，正在下载安装！\\033[0m\" os_type=$(uname -s) case \"$os_type\" in Linux*) os_distribution=$(lsb_release -si) if [ \"$os_distribution\" = \"Ubuntu\" ] || [ \"$os_distribution\" = \"Debian\" ]; then apt install -y $no_commands >/dev/null 2>&1 elif [ \"$os_distribution\" = \"CentOS\" ]; then yum install -y $no_commands >/dev/null 2>&1 elif [ \"$os_distribution\" = \"Fedora\" ]; then dnf install -y $no_commands >/dev/null 2>&1 fi if [[ $? == 0 ]] ;then echo -e \"\\033[31m$no_commands 命令已安装！\\033[0m\" fi ;; Darwin*) brew install $no_commands >/dev/null 2>&1 if [[ $? == 0 ]] ;then echo -e \"\\033[31m$no_commands 命令已安装！\\033[0m\" fi ;; CYGWIN*|MINGW32*|MSYS*|MINGW*) echo -e \"\\033[31mWindows操作系统，请手动安装！\\033[0m\" ;; *) echo -e \"\\033[31m未知的操作系统\\033[0m\" return ;; esac fi } check_commands_exists jq pidof 10、检查系统网络 check_network() { ping -c 4 114.114.114.114 >/dev/null if [ ! $? -eq 0 ]; then echo -e \"\\033[31mIP地址无法ping通，请检查网络连接！！！\\033[0m\" exit fi ping -c 4 www.baidu.com >/dev/null if [ ! $? -eq 0 ]; then echo -e \"\\033[31m域名无法Ping通，请检查DNS配置！！！\\033[0m\" exit fi curl -s --retry 2 --connect-timeout 2 www.baidu.com >/dev/null if [ ! $? -eq 0 ]; then echo -e \"\\033[31m域名无法Ping通，请检查DNS配置！！！\\033[0m\" exit fi } 11、发送钉钉通知 function sendDingDing(){ Ding_Webhook_Token='钉钉机器人的WebHook Token' Ding_Webhook_keyword=\"可用作消息标题\" send_result=$(curl -sS \\ -XPOST https://oapi.dingtalk.com/robot/send?access_token=\"$Ding_Webhook_Token\" \\ -H 'Content-Type: application/json' \\ -d '{\"msgtype\": \"markdown\",\"markdown\": {\"title\": \"'$Ding_Webhook_Keyword'\",\"text\": \"'\"$*\"'\"},\"at\": {\"isAtAll\": true}}') if [ $(echo $send_result | jq -r '.errcode') -eq 0 ]; then echo \"发送成功: \\n\" $*\" \" else echo \"发送失败: \\n\" $*\"\\n\" echo $send_result fi } sendDingDing \"$(date +\"%F %H:%M:%S\") 需要告警的消息\" .zshrc function sendDingDing() { setopt no_nomatch Ding_Webhook_Token='钉钉机器人的WebHook Token' Ding_Webhook_Keyword='可用作消息标题' send_result=$(curl -sS \\ -XPOST https://oapi.dingtalk.com/robot/send?access_token=\"$Ding_Webhook_Token\" \\ -H 'Content-Type: application/json' \\ -d '{\"msgtype\": \"markdown\",\"markdown\": {\"title\": \"'$Ding_Webhook_Keyword'\",\"text\": \"'\"$*\"'\"},\"at\": {\"isAtAll\": true}}') if [ $(echo $send_result | jq -r '.errcode') -eq 0 ]; then echo \"发送成功: \\n\" $*\" \" else echo \"发送失败: \\n\" $*\"\\n\" echo $send_result fi } function kfdd() { sendDingDing \"$(date +\"%F %H:%M:%S\") \"$*\" \" } 12、清理保持备份个数 清理指定目录北备份文件，并保持指定个数。 function clean_backups (){ if [ $# -gt 2 ];then echo \"参数错误\" exit 1 else BAK_HOME=$1 keepNum=$2 fileNum=$(find ${BAK_HOME} -maxdepth 1 -type d -ctime -1 ! -path $BAK_HOME |wc -l) if [ $fileNum -gt $keepNum ] ;then find ${BAK_HOME} -maxdepth 1 -type d -ctime -1 ! -path $BAK_HOME | head -n `expr $fileNum - $keepNum` | xargs rm -rf fi fi } # 调用 clean_backups \"./backups\" 2 13、格式化输出 ①并列格式化输出 tests=' a1a b2b c3c d4d e5e f6f g7g h8h i9i j10j k11k ' count=1 colume=5 # 控制一行输出几列 for test in ${tests}; do printf \"\\e[;36m%-2s. %-10s\\033[0m\" \"$count\" \"$test\" if [ $((count % $colume)) -eq 0 ]; then echo \"\" fi ((count++)) done # 针对文本变量的变量，bash中是\"for test in ${tests}; do ... done\" 。 # zsh中是\"for test in ${(f)tests}; do ... done\"。 # 格式化输出 1 . a1a 2 . b2b 3 . c3c 4 . d4d 5 . e5e 6 . f6f 7 . g7g 8 . h8h 9 . i9i 10. j10j 11. k11k Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:36:27 "},"origin/maven-Settings配置文件详解.html":{"url":"origin/maven-Settings配置文件详解.html","title":"Mave Settings文件详解","keywords":"","body":"一、settings.xml文件作用 从settings.xml的文件名就可以看出，它是用来设置maven参数的配置文件。并且settings.xml是maven的全局配置文件。而pom.xml文件是所在项目的局部配置。Settings.xml中包含类似本地仓储位置、修改远程仓储服务器、认证信息等配置。 二、settings.xml文件位置 settings.xml文件一般存在于两个位置： 全局配置: ${M2_HOME}/conf/settings.xml 用户配置: user.home/.m2/settings.xml Note： 局部配置优先于全局配置。配置优先级从高到低：pom.xml> user settings > global settings 如果这些文件同时存在，在应用配置时，会合并它们的内容，如果有重复的配置，优先级高的配置会覆盖优先级低的。 三、settings.xml元素详解 settings.xml中的顶级元素 LocalRepository 该值表示构建系统本地仓库的路径。其默认值：~/.m2/repository。 ${user.home}/.m2/repository InteractiveMode 表示maven是否需要和用户交互以获得输入。如果maven需要和用户交互以获得输入，则设置成true，反之则应为false。默认为true。 true UsePluginRegistry maven是否需要使用plugin-registry.xml文件来管理插件版本。如果需要让maven使用文件~/.m2/plugin-registry.xml来管理插件版本，则设为true。默认为false。 false Offline 表示maven是否需要在离线模式下运行。如果构建系统需要在离线模式下运行，则为true，默认为false。 当由于网络设置原因或者安全因素，构建服务器不能连接远程仓库的时候，该配置就十分有用。 false PluginGroups 当插件的组织id（groupId）没有显式提供时，供搜寻插件组织Id（groupId）的列表。 该元素包含一个pluginGroup元素列表，每个子元素包含了一个组织Id（groupId）。 当我们使用某个插件，并且没有在命令行为其提供组织Id（groupId）的时候，Maven就会使用该列表。默认情况下该列表包含了org.apache.maven.plugins和org.codehaus.mojo。 org.codehaus.mojo Servers 仓库的下载和部署是在pom.xml文件中的repositories和distributionManagement元素中定义的。然而，一般类似用户名、密码（有些仓库访问是需要安全认证的）等信息不应该在pom.xml文件中配置，这些信息可以配置在settings.xml中。 server001 my_login my_password ${usr.home}/.ssh/id_dsa some_passphrase 664 775 Mirrors 为仓库列表配置的下载镜像列表。 planetmirror.com PlanetMirror Australia http://downloads.planetmirror.com/pub/maven2 central Proxies 用来配置不同的代理 myproxy true http proxy.somewhere.com 8080 proxyuser somepassword *.google.com|ibiblio.org Profiles 根据环境参数来调整构建配置的列表。settings.xml中的profile元素是pom.xml中profile元素的裁剪版本。 它包含了id、activation、repositories、pluginRepositories和 properties元素。这里的profile元素只包含这五个子元素是因为这里只关心构建系统这个整体（这正是settings.xml文件的角色定位），而非单独的项目对象模型设置。如果一个settings.xml中的profile被激活，它的值会覆盖任何其它定义在pom.xml中带有相同id的profile。 test Activation 自动触发profile的条件逻辑。 如pom.xml中的profile一样，profile的作用在于它能够在某些特定的环境中自动使用某些特定的值；这些环境通过activation元素指定。 activation元素并不是激活profile的唯一方式。settings.xml文件中的activeProfile元素可以包含profile的id。profile也可以通过在命令行，使用-P标记和逗号分隔的列表来显式的激活（如，-P test） false 1.5 Windows XP Windows x86 5.1.2600 mavenVersion 2.0.3 ${basedir}/file2.properties ${basedir}/file1.properties 注：在maven工程的pom.xml所在目录下执行mvn help:active-profiles命令可以查看中央仓储的profile是否在工程中生效。 properties 对应profile的扩展属性列表。 maven属性和ant中的属性一样，可以用来存放一些值。这些值可以在pom.xml中的任何地方使用标记${X}来使用，这里X是指属性的名称。属性有五种不同的形式，并且都能在settings.xml文件中访问 1.0通过${project.version}获得version的值。 3. settings.x: 指代了settings.xml中对应元素的值。例如：false通过 ${settings.offline}获得offline的值。 4. Java System Properties: 所有可通过java.lang.System.getProperties()访问的属性都能在POM中使用该形式访问，例如 ${java.home}。 5. x: 在元素中，或者外部文件中设置，以$的形式使用。{someVar}的形式使用。 --> ${user.home}/our-project 注：如果该profile被激活，则可以在pom.xml中使用${user.install}。 Repositories 远程仓库列表，它是maven用来填充构建系统本地仓库所使用的一组远程仓库 codehausSnapshots Codehaus Snapshots false always warn http://snapshots.maven.codehaus.org/maven2 default pluginRepositories 发现插件的远程仓库列表。 和repository类似，只是repository是管理jar包依赖的仓库，pluginRepositories则是管理插件的仓库。 maven插件是一种特殊类型的构件。由于这个原因，插件仓库独立于其它仓库。pluginRepositories元素的结构和repositories元素的结构类似。每个pluginRepository元素指定一个Maven可以用来寻找新插件的远程地址 ActiveProfiles 手动激活profiles的列表，按照profile被应用的顺序定义activeProfile。 该元素包含了一组activeProfile元素，每个activeProfile都含有一个profile id。任何在activeProfile中定义的profile id，不论环境设置如何，其对应的 profile都会被激活。如果没有匹配的profile，则什么都不会发生。 例如，env-test是一个activeProfile，则在pom.xml（或者profile.xml）中对应id的profile会被激活。如果运行过程中找不到这样一个profile，Maven则会像往常一样运行。 env-test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-11-05 17:54:19 "},"origin/maven-生命周期阶段.html":{"url":"origin/maven-生命周期阶段.html","title":"Maven 生命周期阶段","keywords":"","body":"Maven的生命周期以及阶段插件 Maven拥有三个生命周期，每个生命周期包含一些阶段，这些阶段是有顺序的，并且后面的阶段依赖于前面的阶段。 运行任何一个阶段的时候，它前面的所有阶段都会被运行 Maven三个生命周期只是定义了各个阶段要做的事情、但是不做任何实际工作、实际工作都是由插件的目标来完成的。插件以独立的形式存在、Maven会在需要的时候下载并使用插件 一个插件有可能有多个功能、每个功能就是一个目标。比如maven-dependency-plugin有十多个目标、每个目标对应了一个功能。插件的目标为dependency:analyze、dependency:tree和dependency:list。通用写法：插件前缀:插件目标。比如compiler:compile 一、Maven的生命周期阶段及插件绑定 上面三个生命周期中有很多原来的生命周期阶段没有默认绑定插件、也就意味着默认情况下他们没有任何意义。当然如果我们有自己特殊的处理、可以为他们绑定特殊的插件、比如下面会有提到的在打包的时候生成jar包的源码、可以在default生命周期的verify阶段绑定生成源码插件的生成源码的目标。 二、自定义插件绑定 自定义绑定允许我们自己掌控插件目标与生命周期的结合、下面以生成项目主代码的源码jar为例。使用到的插件和他的目标为：maven-source-plugin:jar-no-fork、将其绑定到default生命周期阶段verify上（可以任意指定三套生命周期的任意阶段）、在项目的POM配置中（也可以在父POM中配置、后面聚合与继承会有提到） org.apache.maven.plugins maven-source-plugin 2.1.1 attach-sources verify jar-no-fork # build元素下的plugins子元素中声明插件的使用。使用的maven-source-plugin插件，其groupId为org.apache.maven.plugins（官方插件的groupId），version版本为2.1.1.对于自定义绑定的插件，应应指定一个非快照的版本，避免插件版本变化造成构件不稳定。 execution元素用来配置执行的任务。上面配置了一个id为attach-source的任务，通过phrase将其绑定到了verify生命周期阶段上，再通过goals配置指定要执行的插件目标（及插件功能）。 需要注意的是： 即使不通过phrase来配置生命周期阶段，有的插件也定义了默认的生命周期阶段。可使用maven-help-plugin来查看插件的详细信息。例如： mvn help：describe-Dplugin=org.apache.maven.plugins:maven-source-plugin:2.1.1 上述配置有插件的坐标声明、还有excutions下面每个excution子元素配置的执行的一个个任务、通过phase指定与生命周期的那个阶段绑定、在通过goals指定执行绑定插件的哪些目标。 当插件的目标绑定到不同的生命周期阶段的时候、插件目标的执行顺序是有生命周期阶段的顺序决定的、当多个插件目标绑定到同一生命周期阶段的时候、顺序是按照插件声明的顺序来决定目标的执行顺序。 三、插件配置 有三种方式可配置插件功能目标执行任务时的参数 1、命令行 例如maven-surefire-Plugin提供了maven.test.skip参数，当值为true时就跳过单元测试。在命令行时，加上-D参数配置该插件的参数就能跳过单元测试 maven install -Dmaven.test.skip=true 参数-D是Java自带的，其功能就是通过命令行设置Java系统环境变量。 2、POM中设置插件的全局任务配置 例如在下面POM文件中配置了maven-source-plugin插件要编译什么java版本的源代码，生成什么java版本的字节码文件 org.apache.maven.plugins maven-source-plugin 2.1.1 1.5 1.5 这样，不管绑定到compile阶段的maven-source-plugin：compile任务，还是绑定到test-compile阶段的maven-source-plugin：testCompiler任务，就都能使用到该配置来基于Java 1.5版本来执行任务 3、POM中插件任务配置 用户还可以在POM文件中定义某插件的任务参数。例如以maven-antrun-plugin插件为例。他有一个run目标，可以用来在Maven中调用Ant任务。用户可以将maven-antrun-plugin的目标run绑定到多个生命周期阶段上，再加以不同的配置，就可以让Maven在不同的生命阶周期段执行不同的任务 org.apache.maven.plugins maven-antrun-plugin 1.3 ant-validate validate run HAHAHAHHAHAHAHHAHAHAHHAHAHA ant-verify verify run lalallalal 参考链接 https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#Lifecycle_Reference https://maven.apache.org/plugins/index.html https://blog.csdn.net/zhaojianting/article/details/80321488 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/maven-operations.html":{"url":"origin/maven-operations.html","title":"Maven常见操作","keywords":"","body":"Maven常见操作 1、设置代理 方式一：settings.xml中配置 ...省略... true http proxy.somewhere.com 8080 proxyuser somepassword www.google.com|*.somewhere.com ...省略... 方式二：设置环境变量 export MAVEN_OPTS=\"-DsocksProxyHost=127.0.0.1 -DsocksProxyPort=$PORT\" # 或者 -Dhttp.proxyHost= -Dhttp.proxyPort= -Dhttps.proxyHost= -Dhttps.proxyPort= Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/git-原理.html":{"url":"origin/git-原理.html","title":"git原理","keywords":"","body":"git原理 一、git是什么？ Git是一款免费、开源的分布式版本控制系统 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 当我们把代码从git hub档下来或者说初始化git项目后，便有了这三个分区的概念 二、git是怎么储存信息的 git object有三种类型： Blob Tree Commit 在Git仓库里面，HEAD、分支、普通的Tag可以简单的理解成是一个指针，指向对应commit的SHA1值。 四、git命令 分区转换命令 1. git add 数据从工作区转移至暂存区 2. git commit 数据从暂存区转移至版本库，也就是本地仓库 3. git push 数据从版本库中发送到远程仓库 分区对比命名 4. git diff 工作区与暂存区对比 5. git diff head 工作区与版本库对比 6. git diff -cached 暂存区与版本库对比 参考 https://zhuanlan.zhihu.com/p/96631135 https://www.zhihu.com/search?type=content&q=git Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/git-常用操作.html":{"url":"origin/git-常用操作.html","title":"git常用操作","keywords":"","body":"Git常用操作 1、删除远程仓库分支 git push 远程仓库别名 :远程仓库中的分支 2、克隆远程仓库指定分支到本地指定路径 git clone -b 3、拉取远程仓库指定分支到本地仓库的特定分支 git fetch 远程仓库中分支名:本地分支名 #使用该方式会在本地新建分支，但是不会自动切换到该本地分支，需要手动checkout切换分支。 git remote update ;\\ git checkout -b local_branch / #使用该方式会在本地新建分支，并自动切换到该本地分支。采用此种方法建立的本地分支会和远程分支建立映射关系。 4、Git代理设置 # 设置使用HTTP类型的代理 git config --global https.proxy http://127.0.0.1:1080 git config --global https.proxy https://127.0.0.1:1080 # 取消代理的设置 git config --global --unset http.proxy git config --global --unset https.proxy # 设置使用socks5类型的代理 git config --global http.proxy 'socks5://127.0.0.1:1081' git config --global https.proxy 'socks5://127.0.0.1:1081' # 查看代理设置 git config --global --get http.proxy git config --global --get https.proxy 5、查看单个文件修改历史 git log --pretty=oneline 文件名 5096d69f*** handle merge file conflict a7593501*** fix: 同步 master docker 文件夹所有文件 git show a7593501*** 6、修改已提交的Commint git commit --amend 7、HTTP方式设置记住用户名和密码 设置记住密码（默认15分钟） git config --global credential.helper cache 设置时间，例如设置一个小时之后失效 git config credential.helper 'cache --timeout=3600' 长期存储密码 git config --global credential.helper store 增加远程地址的时候设置密码 http://yourname:password@git.oschina.net/name/project.git 8、Git tag使用 # 查看tag git tag # 打某个分支的tag git tag -a 0.0.0_b1 -m \"test tag\" # 将本地tag推送到远程仓库中 git push origin --tags # 删除本地tag git tag -d 0.0.0_b3 # 删除远程仓库中的tag git push origin :tags/0.0.0_b1 9、获取最近一次提交的commit id # 获取完整commit id git rev-parse HEAD # 获取8位commit id git rev-parse --short HEAD 10、commit回退 # 进回退到最近一个的上一个commit git reset --hard HEAD^ # 回退到指定commit。commit的ID可残缺地写 git reset --hard # 查看已回退commit的历史，并恢复回退的commit git reflog git reset --hard 11、本地分支Merge git checkout master # 当前所处master分支，以下命令是将develop分支合并到master分支 git merge develop 12、解决合并冲突 git diff --name-only --diff-filter=U 13、对比差异 git diff master..develop git diff master... 14、显示commit的author和committer 关于Author和Committer的区别，请参考：https://stackoverflow.com/questions/6755824/what-is-the-difference-between-author-and-committer-in-git # 查看最近一个commit的author和committer git log --format=\"%an %cn\" -n 1 # 查看commit Hash后的文件内容 git cat-file -p 15、清除历史commit创建新分支 # 当某一个分支积累了太多commit历史信息，管理起来比较麻烦。或者某些commit信息包含敏感想要清除掉。 git checkout --orphan git add . git commit -am \"reset commit info\" git push origin 16、设置当个仓库的用户名邮箱地址 在项目根目录下进行单独配置 git config user.name \"用户名\" git config user.email \"邮箱地址\" git config --list 也可以直接在项目根目录下的.git/config文件中直接追加以下内容 [user] name = 用户名 email = 邮箱地址 17、git push参数与Git alias git push \\ -o merge_request.create \\ -o merge_request.target=develop \\ -o merge_request.title=test-push-option \\ -o merge_request.assign=tester \\ -o merge_request.description='test git push option' \\ -o ci.skip 使用git Alias 快速设置参数 git config --global alias.mwps \"push -o merge_request.create -o merge_request.target=develop -o merge_request.merge_when_pipeline_succeeds\" git mwps origin 参考：https://docs.gitlab.com/ee/user/project/push_options.html#push-options-for-merge-requests Git问题总结 fatal: I don't handle protocol 'http' 原因：.git/config中的远程仓库URL可能有乱码 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-04-17 19:44:32 "},"origin/git-gitignore文件.html":{"url":"origin/git-gitignore文件.html","title":"git .gitignore文件","keywords":"","body":"git的.gitignore文件 一、简介 一般来说每个Git项目中都需要一个“.gitignore”文件，这个文件的作用就是告诉Git哪些文件不需要添加到版本管理中。 二、用法 # 此为注释 – 将被 Git 忽略 *.a # 忽略所有 .a 结尾的文件 !lib.a # 但 lib.a 除外 /TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO build/ # 忽略 build/ 目录下的所有文件 doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt 注意 这样没有扩展名的文件在Windows下不太好创建，方法：创建一个文件，文件名为：“.gitignore.”，注意前后都有一个点。保存之后系统会自动重命名为“.gitignore”。 假设我们只有过滤规则没有添加规则，那么我们就需要把/mtk/目录下除了one.txt以外的所有文件都写出来！ 如果你不慎在创建.gitignore文件之前就push了项目，那么即使你在.gitignore文件中写入新的过滤规则，这些规则也不会起作用，Git仍然会对所有文件进行版本管理。简单来说，出现这种问题的原因就是Git已经开始管理这些文件了，所以你无法再通过过滤规则过滤它们。所以一定要养成在项目开始就创建.gitignore文件的习惯，否则一旦push，处理起来会非常麻烦 三、样本 参考https://github.com/github/gitignore/blob/master/Maven.gitignore target/ pom.xml.tag pom.xml.releaseBackup pom.xml.versionsBackup pom.xml.next release.properties dependency-reduced-pom.xml buildNumber.properties .mvn/timing.properties .mvn/wrapper/maven-wrapper.jar ### IDES ### ### STS ### .apt_generated .classpath .factorypath .project .settings .springBeans .sts4-cache ### IntelliJ IDEA ### .idea *.iws *.iml *.ipr ### NetBeans ### /nbproject/private/ /nbbuild/ /dist/ /nbdist/ /.nb-gradle/ /build/ ### VS Code ### .vscode/ ### OS ### .DS_Store Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/origin-git-merge-git-rebase.html":{"url":"origin/origin-git-merge-git-rebase.html","title":"git merge与git rebase","keywords":"","body":"Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/git-submodule.html":{"url":"origin/git-submodule.html","title":"git submodule","keywords":"","body":"Git Submodule子模块 一、简介 经常碰到这种情况：当你在一个Git 项目上工作时，你需要在其中使用另外一个Git 项目。也许它是一个第三方开发的Git 库或者是你独立开发和并在多个父项目中使用的。这个情况下一个常见的问题产生了：你想将两个项目单独处理但是又需要在其中一个中使用另外一个。 在Git 中你可以用子模块submodule来管理这些项目，submodule允许你将一个Git 仓库当作另外一个Git 仓库的子目录。这允许你克隆另外一个仓库到你的项目中并且保持你的提交相对独立。 二、git submodule命令详解 Git submodule命令 git submodule 指令 指令 add add [-b ] [-f|--force] [--name ] [--reference ] [--depth ] [--] [] status status [--cached] [--recursive] [--] [...] init init [--] [...] deinit deinit [-f|--force] (--all|[--] ...) update update [--init] [--remote] [-N|--no-fetch] [--[no-]recommend-shallow] [-f|--force] [--checkout|--rebase|--merge] [--reference ] [--depth ] [--recursive] [--jobs ] [--] [...] summary summary [--cached|--files] [(-n|--summary-limit) ] [commit] [--] [...] foreach foreach [--recursive] sync sync [--recursive] [--] [...] absorbgitdirs 三、操作 添加子模块 git submodule add ssh://git@gitlab.apps.okd311.curiouser.com:30022/Demo/git-submodule-public.git public 查看子模块 git submodule 1b76c7ccb8e9a8460690433ffe5e14c3e9219890 public (heads/master) 初始化子模块 更新子模块 git submodule update 更新子模块 git submodule update --remote 参考 https://www.cnblogs.com/Again/articles/6686105.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/git-workflows.html":{"url":"origin/git-workflows.html","title":"git workflow工作流","keywords":"","body":"Git WorkFlow工作流 一、简介 二、Git Flow 三、Gitlab Flow 四、GitHub Flow 参考 https://www.atlassian.com/git/tutorials/comparing-workflows https://blog.csdn.net/qq_32452623/article/details/78905181 https://segmentfault.com/a/1190000007692929 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/git-hooks.html":{"url":"origin/git-hooks.html","title":"git hook钩子","keywords":"","body":"Git Hook钩子 一、简介 官方文档：https://git-scm.com/docs/githooks/en 官方中文文档：https://git-scm.com/book/zh/v2 Hooks(钩子),是一些存放于$GIT_DIR/hooks文件夹的小脚本,在特定条件下触发动作。当执行git init，几个示例hook将复制到新资源库的hooks文件夹, 但默认情况下他们都是禁用状态，要启用一个hook(钩子),请移除其.sample后缀. 二、Hook类型 https://imweb.io/topic/5b13aa38d4c96b9b1b4c4e9d http://www.360doc.com/content/16/1014/18/10058718_598431211.shtml https://blog.51cto.com/u_13187574/2083938 applypatch-msg 触发时机：由git am触发 接受的参数： 提交的commit msg临时文件路径 可通过--no-verify 略过 如果以非0状态退出，那么git am将在patch(补丁)应用之前取消. pre-commit 触发时机：由git commit触发, 在commit msg被创建之前执行. 不接受参数 可通过--no-verify 略过 如果以非0状态退出，将导致git commit被取消 post-commit 触发时机：由git commit触发, 当commit完成后执行 不接受参数 触发端 类型 触发时机 接受参数 Client端 pre-merge-commit Client端 prepare-commit-msg 由'git commit',在准备好默认log信息后触发,但此时,编辑器尚未启动 它可能接受1到3个参数. 第一个参数是包含commit msg的文件路径. 第二个参数是commit msg的来源, 可能的值有: message (当使用-m 或-F 选项); template (当使用-t 选项,或commit.template配置项已经被设置); merge (当commit是一个merge或者.git/MERGE_MSG存在); squash(当.git/SQUASH_MSG文件存在); commit, 且附带该commit的SHA1 (当使用-c, -C 或 --amend). 是 Client端 commit-msg 是 Client端 post-commit 由git commit触发. 当commit完成后执行 不接受参数 Client端 Server端 pre-receive Server端 update Server端 post-receive 不接受参数 Server端 Server端 Server端 Server端 Server端 Server端 Server端 三、编译工具集成 1、NPM Husky npm install -g husky 2、Maven 文档：https://blog.csdn.net/leandzgc/article/details/107974276 3、Composer composer-git-hooks 文档：https://packagist.org/packages/brainmaestro/composer-git-hooks composer global require brainmaestro/composer-git-hooks composer.json { \".....\", \"extra\": { \"hooks\": { \"config\": { // 配置执行失败时停止（当钩子是一系列命令时，当命令失败时停止执行会很有用） \"stop-on-failure\": [\"pre-push\"], // 配置自定义hook \"custom-hooks\": [\"pre-flow-feature-start\"] }, \"pre-commit\": [ \"echo committing as $(git config user.name)\", \"php-cs-fixer fix .\" // fix style ], \"pre-flow-feature-start\": [ \"echo 'Starting a new feature...'\" ], // verify commit message. ex: ABC-123: Fix everything \"commit-msg\": \"grep -q '[A-Z]+-[0-9]+.*' $1\", \"pre-push\": [ \"php-cs-fixer fix --dry-run .\" // check style \"phpunit\" ], \"post-merge\": \"composer install\" \"...\": \"...\" } } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/git-standard-commit-message.html":{"url":"origin/git-standard-commit-message.html","title":"git提交规范","keywords":"","body":"git提交规范 一、规范的好处 当我们提交代码的时候，需要编写提交信息（commit message）。而提交信息的主要用途是：告诉这个项目的人，这次代码提交里做了些什么。所以每次的提交信息大家应该按照某种规范进行提交，最好是能有规范和工具的约束。 易于阅读，在最短时间清楚每次提交的意义 良好的Git提交日志非常重要，最明显的一点是，它让整个Git提交历史的阅读变得非常轻松。大多数情况下，看提交历史的人跟提交代码的人都不是同一个人，当别人阅读你的提交历史时，他很可能是不知道具体代码细节的，你如何在最短的时间内让他一眼知道每次提交的意义 每次提交影响的具体范围？ 这个bug在哪次提交中被修复了？ 这个新功能是在哪次提交中增加的？ 修改是否向下兼容？ 是否回滚了代码？ 是否只是修改了文档、调整了代码格式？ 是否修改了测试、是否进行了重构？ 是否对代码进行了性能优化？ 提供更多的历史信息，方便快速浏览。 git log HEAD --pretty=format:%s 便于快速查找信息 可以过滤筛选某些commit git log HEAD --grep feature 生成CHANGELOG 规范的Git提交历史，还可以直接生成项目发版的CHANGELOG 二、开源社区的Angular提交规范 与我们日常工作稍有不同的是：工作中的 Release 计划一般都是事先安排好的，不需要一些 CHANGELOG 什么的。而开源应用、开源库需要有对应的 CHANELOG，则添加了什么功能、修改了什么等等。毕竟有很多东西是由社区来维护的。目前，社区有多种 Commit message 的写法规范。Angular 规范是目前使用最广的写法，比较合理和系统化，并且有配套的工具。 1. Angular规范的格式： 每个提交消息都由一个标题、一个正文和一个页脚组成。而标题又具有特殊格式，包括修改类型、影响范围和内容主题 (): [body正文] [footer注脚] =============中文版================== 注意： <>中为必填项 []中为可选项 提交消息的任何一行都不能超过100个字符（为了让消息在GitHub以及各种Git工具中都更容易阅读） type: 修改类型 每个类型值都表示了不同的含义，类型值必须是以下的其中一个： 主要type feat：提交了新功能 fix：修复了bug 次要type docs：只改动了文档相关的内容 style：调整代码格式，未修改代码逻辑（比如修改空格、格式化、缺少分号等） refactor：代码重构，既没修复bug也没有添加新功能 build：构造工具或者外部依赖的改动，例如webpack，npm，pom 特殊type perf：性能优化，提高性能的代码更改 test：添加或修改代码测试 ci：与CI（持续集成服务）有关的改动 chore：对构建流程或辅助工具和依赖库（如文档生成等）的更改 当一次改动包括主要type与次要type时，统一采用主要type。 scope: 影响范围 scope也为必填项，用于描述改动的范围，格式为项目名/模块名，例如：node-pc/common rrd-h5/activity，而we-sdk不需指定模块名。如果一次commit修改多个模块，建议拆分成多次commit，以便更好追踪和维护。 当修改影响多个范围时，也可以使用“*”。 它可以是你提交代码实际影响到的任何内容。例如$location、$browser、$compile、$rootScope、ngHref、ngClick、ngView等，唯一需要注意的是它必须足够简短。 subject: 标题 标题是对变更的简明描述： 使用祈使句，现在时态：是“change”不是“changed”也不是“changes” 不要大写首字母 结尾不要使用句号 body: 正文 正文是对标题的补充，但它不是必须的。和标题一样，它也要求使用祈使句且现在时态，正文应该包含更详细的信息，如代码修改的动机，与修改前的代码对比等。 footer: 注脚 当有以下两种情况需要写footer注脚： 不兼容的改变 如果当前代码有重大更改，应该以BREAKING CHANGE这个词开头，带一个空格或者两个换行符，然后是对变动的描述，变动理由以及如何迁移。 关闭issue 如果当前commit针对某个issue，可以以Closes为开头。 任何Breaking Changes（破坏性变更，不向下兼容）都应该在页脚中进行说明，它经常也用来引用本次提交解决的GitHub Issue。 Breaking Changes应该以“BREAKING CHANGE:”开头，然后紧跟一个空格或两个换行符，其他要求与前面一致。 Revert 还有一种特殊情况，如果当前 commit 用于撤销以前的 commit，则必须以revert:开头，后面跟着被撤销 Commit 的 Header。 revert: feat(pencil): add 'graphiteWidth' option This reverts commit 667ecc1654a317a13331b17617d973392f415f02. Body部分的格式是固定的，必须写成This reverts commit .，其中的hash是被撤销 commit 的 SHA 标识符。 如果当前 commit 与被撤销的 commit，在同一个发布（release）里面，那么它们都不会出现在 Change log 里面。如果两者在不同的发布，那么当前 commit，会出现在 Change log 的Reverts小标题下面。 2、提交规范的辅助工具 IDEA插件Git Commit Template commitizen: 辅助撰写格式化的commit message 安装、配置 npm install -g commitizen commitizen工具是基于Node.js的，而非前端项目工程目录下是没有package.json文件，所以会报错： npm WARN saveError ENOENT: no such file or directory, open './package.json' npm WARN enoent ENOENT: no such file or directory, open './package.json' 对于此种错误，创建一个空的package.json文件，然后进入到项目目录，执行 npm init --yes 在项目目录里，使其支持 Angular 的 Commit message 格式。 commitizen init cz-conventional-changelog --save --save-exact 以后，凡是用到git commit命令，一律改为使用git cz。这时，就会出现选项，用来生成符合格式的 Commit message。 conventional-changelog: 生成CHANGELOG 如果你的所有 Commit 都符合 Angular 格式，那么发布新版本时， Change log 就可以用脚本自动生成，生成的文档包括以下三个部分。 New features Bug fixes Breaking changes 安装配置 npm install -g conventional-changelog # 生成CHANGELOG conventional-changelog -p angular -i CHANGELOG.md -w # 命令不会覆盖以前的 Change log，只会在CHANGELOG.md的头部加上自从上次发布以来的变动 # 生成所有发布的 Change log conventional-changelog -p angular -i CHANGELOG.md -w -r 0 三、基于Jira的提交规范 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/regular-expression详解.html":{"url":"origin/regular-expression详解.html","title":"正则表达式","keywords":"","body":"正则表达式详解 一、什么是正则表达式？ 正则表达式（regular expression）就是用一个“字符串”来描述一个特征，然后去验证另一个“字符串”是否符合这个特征。比如 表达式“ab+” 描述的特征是“一个 'a' 和 任意个 'b' ”，那么 'ab', 'abb', 'abbbbbbbbbb' 都符合这个特征。 二、正则表达式能干什么？ 验证字符串是否符合指定特征，比如验证是否是合法的邮件地址 用来查找字符串，从一个长的文本中查找符合指定特征的字符串，比查找固定字符串更加灵活方便 用来替换，比普通的替换更强大 三、正则表达式规则 参考链接 http://www.regexlab.com/zh/regref.htm Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ssl-tls.html":{"url":"origin/ssl-tls.html","title":"SSL/TLS基础概念","keywords":"","body":"SSL与TLS 一、简介 SSL(Secrue Socket Layer 安全套接层)： SSL(Secure Socket Layer 安全套接层)是基于HTTPS下的一个协议加密层，最初是由网景公司（Netscape）研发，后被IETF（The Internet Engineering Task Force - 互联网工程任务组）标准化后写入（RFCRequest For Comments 请求注释），RFC里包含了很多互联网技术的规范！ 起初是因为HTTP在传输数据时使用的是明文（虽然说POST提交的数据时放在报体里看不到的，但是还是可以通过抓包工具窃取到）是不安全的，为了解决这一隐患网景公司推出了SSL安全套接字协议层，SSL协议位于TCP/IP协议与各种应用层协议之间，为数据通讯提供安全支持。SSL协议可分为两层： SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。 SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。 TLS(Transport Layer Security安全传输层协议) 由于HTTPS的推出受到了很多人的欢迎，在SSL更新到3.0时，IETF对SSL3.0进行了标准化，并添加了少数机制(但是几乎和SSL3.0无差异)，标准化后的IETF更名为TLS1.0(Transport Layer Security 安全传输层协议)，可以说TLS就是SSL的新版本3.1，并同时发布“RFC2246-TLS加密协议详解” 二、HTTP请求SSL证书认证流程 三、SSL证书 1、证书类型 证书类型 适用网站类型 公信等级 认证强度 安全性 支持的证书品牌 DV域名型 个人网站 一般 CA机构审核个人网站真实性、不验证企业真实性 一般 DigiCert、GeoTrust、GlobalSign、vTrus（国产）、WoSign（国产） OV企业型 政府组织、企业、教育机构等 高 CA机构审核组织及企业真实性 高 DigiCert）、GeoTrust、GlobalSign、CFCA（国产）、vTurs EV企业增强型 大型企业、金融机构等 最高 严格认证 最高 DigiCert、GeoTrust、CFCA（国产） 2、主流Web服务软件支持的证书 一般来说，主流的Web服务软件，通常都基于OpenSSL和Java两种基础密码库。 Tomcat、Weblogic、JBoss等Web服务软件，一般使用Java提供的密码库。通过Java Development Kit （JDK）工具包中的Keytool工具，生成Java Keystore（JKS）格式的证书文件。 Apache、Nginx等Web服务软件，一般使用OpenSSL工具提供的密码库，生成PEM、KEY、CRT等格式的证书文件。 IBM的Web服务产品，如Websphere、IBM Http Server（IHS）等，一般使用IBM产品自带的iKeyman工具，生成KDB格式的证书文件。 微软Windows Server中的Internet Information Services（IIS）服务，使用Windows自带的证书库生成PFX格式的证书文件。 3、证书文件格式 .DER或.CER文件： 这样的证书文件是二进制格式，只含有证书信息，不包含私钥。 CRT： 这样的证书文件可以是二进制格式，也可以是文本格式，一般均为文本格式，功能与 .DER及.CER证书文件相同。 PEM： 这样的证书文件一般是文本格式，可以存放证书或私钥，或者两者都包含。 .PEM 文件如果只包含私钥，一般用.KEY文件代替。 .PFX或.P12文件： 这样的证书文件是二进制格式，同时包含证书和私钥，且一般有密码保护。 四、HTTPS双向认证 OCSP（在线证书状态协议）和CRL（证书吊销列表） 参考： https://help.aliyun.com/zh/api-gateway/user-guide/mutual-tls-authentication https://www.tencentcloud.com/zh/document/product/214/39990 https://www.tencentcloud.com/zh/document/product/214/6155 https://hjk.life/posts/mutual-authentication/ https://cloud.tencent.com/developer/article/1171381 http://www.884358.com/https-auth/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-03-18 11:39:02 "},"origin/ssl-tls-tools.html":{"url":"origin/ssl-tls-tools.html","title":"证书生成工具","keywords":"","body":"SSL证书生成工具 一、简介 数字证书的标准 X.509版本号：指出该证书使用了哪种版本的X.509标准，版本号会影响证书中的一些特定信息 序列号：由CA给予每一个证书分配的唯一的数字型编号，当证书被取消时，实际上是将此证书序列号放入由CA签发的CRL（Certificate Revocation List证书作废表，或证书黑名单表）中。这也是序列号唯一的原因 签名算法标识符：用来指定CA签署证书时所使用的签名算法，常见算法如RSA 签发者信息：颁发证书的实体的 X.500 名称信息。它通常为一个 CA 证书的有效期：证书起始日期和时间以及终止日期和时间；指明证书在这两个时间内有效。 主题信息：证书持有人唯一的标识，在 Internet上应该是唯一的 发布者的数字签名：这是使用发布者私钥生成的签名，以确保这个证书在发放之后没有被撰改过。 证书的公钥：包括证书的公钥、算法(指明密钥属于哪种密码系统)的标识符和其他相关的密钥参数 数字证书的常见格式 .CSR(certificate signing request)文件：证书请求文件，这个并不是证书，而是向证书颁发机构获得签名证书的申请文件。含有公钥信息。生成该文件时需要用到自己的私钥。 CER：存放证书文件可以是二进制编码或者BASE64编码 CRT：证书可以是DER编码，也可以是PEM编码，在linux系统中比较常见 pem：该编码格式在RFC1421中定义，但他也同样广泛运用于密钥管理，实质上是 Base64 编码的二进制内容 PEM 是明文格式的, 以 “-----BEGIN CERTIFICATE----- 开头，已 -----END CERTIFICATE----- 结尾”, 中间是经过 base64 编码的内容, Apache 和 NGINX 服务器偏向于使用这种编码格式，也是 openssl 默认采用的信息存放方式。PEM 其实就是把 DER 的内容进行了一次 base64 编码。 DER：用于二进制DER编码的证书。这些证书也可以用CER或者CRT作为扩展名 JKS：java的密钥存储文件,二进制格式,是一种 Java 特定的密钥文件格式， JKS的密钥库和私钥可以用不同的密码进行保护 p12/PFX：包含所有私钥、公钥和证书。其以二进制格式存储，也称为 PFX 文件，在windows中可以直接导入到密钥区，密钥库和私钥用相同密码进行保护 证数参数含义 C：国家（Country Name） ST： 省份（State or Province Name） L： 城市（Locality Name） O： 公司（Organization Name） OU： 部门（Organizational Unit Name） CN： 产品名（Common Name） emailAddress： 邮箱（Email Address） req_distinguished_name ：根据情况进行修改 alt_names： 127.0.0.1修改为服务器实际的IP或DNS地址，例如：IP.1=127.0.0.1或DNS.1 =broker.xxx.com 二、OpenSSL OpenSSL是TLS/SSL协议的开源实现，提供开发库和命令行程序； OpenSSL一共实现了4种非对称加密算法，包括DH算法、RSA算法、DSA算法和椭圆曲线算法（EC）。DH算法一般用户密钥交换。RSA算法既可以用于密钥交换，也可以用于数字签名，当然，如果你能够忍受其缓慢的速度，那么也可以用于数据加密。DSA算法则一般只用于数字签名。 1.对称加密 openssl enc -ciphername [-in filename] [-out filename] [-pass arg] [-e] [-d] [-a/-base64] [-A] [-k password] [-kfile filename] [-K key] [-iv IV] [-S salt] [-salt] [-nosalt] [-z] [-md] [-p] [-P] [-bufsize number] [-nopad] [-debug] [-none] [-engine id] -in 输入文件 -out 输出文件 -pass 密码 -e encrypt 加密操作 -d decrypt 解密操作 -a/-base64 base64 encode/decode, depending on encryption flag 是否将结果base64编码 -k passphrase is the next argument -kfile passphrase is the first line of the file argument -md 指定密钥生成的摘要算法 默认MD5 -S salt in hex is the next argument 用于加盐加密 -K/-iv key/iv in hex is the next argument 加密所需的key和iv向量 -[pP] print the iv/key (then exit if -P) 是否需要在控制台输出生成的 key和iv向量 -bufsize buffer size 读写文件的I/O缓存，一般不需要指定 -nopad disable standard block padding 禁止标准填充 -engine e use engine e, possibly a hardware device 指定三方加密设备 Cipher Types 以下是部分算法，我们可以选择用哪种算法加密 -aes-128-cbc -aes-128-cbc-hmac-sha1 -aes-128-cfb -aes-128-cfb1 -aes-128-cfb8 -aes-128-ctr -aes-128-ecb -aes-128-gcm -aes-128-ofb ...... # 原始文本 echo \"1234567890abc\" > plain.txt # 加密文本 openssl enc -aes-128-cbc -pbkdf2 -in plain.txt -out encrypt.txt # 解密文本 openssl aes-128-cbc -d -pbkdf2 -in encrypt.txt -out encrypt_decrypt.txt 2.常规操作 ①生成私钥 openssl genrsa [-out filename] [-passout arg] [-des] [-des3] [-idea] [numbits] 选项说明： -out filename ：将生成的私钥保存至filename文件，若未指定输出文件，则为标准输出。 -numbits ：指定要生成的私钥的长度，默认为1024。该项必须为命令行的最后一项参数。 -des|-des3|-idea ：指定加密私钥文件用的算法，这样每次使用私钥文件都将输入密码，太麻烦所以很少使用。 -passout args ：加密私钥文件时，传递密码的格式，如果要加密私钥文件时单未指定该项，则提示输入密码。传递密码的args的格式见一下格式。 　　pass:password ：password表示传递的明文密码 　　env:var ：从环境变量var获取密码值 　　file:filename ：filename文件中的第一行为要传递的密码。若filename同时传递给\"-passin\"和\"-passout\"选项，则filename的第一行为\"-passin\"的值，第二行为\"-passout\"的值 　　stdin ：从标准输入中获取要传递的密码 openssl genrsa -out private.key 2048 ②私钥生成公钥 公钥提取自私钥 openssl rsa -pubout -in private.key -out public.key ③公钥加密 openssl rsautl -encrypt -inkey public.key -pubin -in msg.txt -out rsa_msg.txt ④私钥解密 openssl rsautl -decrypt -inkey private.key -in rsa_msg.txt -out dersa_msg.txt ⑤数字签名: 私钥加密公钥解密 如果是用私钥进行加密，公钥解密叫做数字签名，因为私钥只有一份，用公钥解密出来验证确认是你用这个私钥做的签名，这就是签名和验证。 openssl dgst -sign private.key -sha256 -out sign.msg.txt msg.txt 公钥验证签名，需要对比源文件验证： openssl dgst -verify public.key -sha256 -signature sign.msg.txt msg.txt ⑥查看证书内容 # 查看证书内容 openssl x509 -noout -text -in server.crt # 查看证书的过期时间 openssl x509 -in ca.crt -noout -dates # 查看证书的subject和颁发者 openssl x509 -in server.crt -subject -issuer -noout ⑦去除key文件密码 openssl rsa -in server.key -out server.key 3. 自建CA签名证书 自签名证书分为 自签名私有证书（无法被吊销） 自签名CA证书（可以被吊销） ①生成CA根证书 生成CA私钥（.key）--> 生成CA证书请求（.csr）--> 自签名得到根证书（.crt） 生成CA认证机构的证书密钥key：openssl genrsa -des3 -out ca.key 4096 用私钥ca.key生成CA认证机构的证书ca.crt openssl req -x509 -new -nodes -sha256 \\ -key ca.key \\ -days 36500 \\ -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=Curiouser/OU=devops/CN=test.curiouser.com/CN=localhost/CN=127.0.0.1\" \\ -out ca.crt 查看证书内容：openssl x509 -in ca.crt -noout -text ②用自签根证书给用户证书签名 生成私钥（.key）--> 生成证书请求（.csr）--> 用CA根证书签名得到证书（.crt） # 生成网站的密钥server.key openssl genrsa -des3 -out server.key 4096 # 生成网站证书的请求文件server.csr openssl req -new \\ -subj \"/C=CN/ST=Shanghai/L=Shanghai/O=Curiouser/OU=devops/CN=test.curiouser.com/CN=localhost/CN=127.0.0.1\" \\ -key server.key \\ -out server.csr cert.text authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = localhost IP.2 = 127.0.0.1 DNS.3 = test.curiouser.com # 使用虚拟的CA认证机构的证书ca.crt，来对自己网站的证书请求文件server.csr进行处理，生成签名后的证书server.crt openssl x509 -req \\ -CAcreateserial \\ -CA ca.crt \\ -CAkey ca.key \\ -CAserial serial \\ -extfile cert.ext \\ -in server.csr \\ -out server.crt \\ -days 3650 使用CA验证一下证书是否通过 openssl verify -CAfile ca.crt server.crt openssl verify -CAfile ca.crt client.crt # server.crt: OK 三、Let's Encrypt Let's Encrypt是一个于2015年三季度推出的数字证书认证机构，旨在以自动化流程消除手动创建和安装证书的复杂流程，并推广使万维网服务器的加密连接无所不在，为安全网站提供免费的SSL/TLS证书。 Let's Encrypt由互联网安全研究小组（缩写ISRG）提供服务。主要赞助商包括电子前哨基金会、Mozilla基金会、Akamai以及思科。2015年4月9日，ISRG与Linux基金会宣布合作。 用以实现新的数字证书认证机构的协议被称为自动证书管理环境（ACME）。GitHub上有这一规范的草案，且提案的一个版本已作为一个Internet草案发布。 Let's Encrypt宣称这一过程将十分简单、自动化并且免费 四、证书生成申请工具 1、ACME.sh 简单来说acme.sh 实现了 acme 协议, 可以从 let‘s encrypt 生成免费的证书。acme.sh 有以下特点： 一个纯粹用Shell（Unix shell）语言编写的ACME协议客户端。不依赖于python或官方的Let's Encrypt客户端。 完整的ACME协议实施。 支持ACME v1和ACME v2 支持ACME v2通配符证书 可自动颁发，续订和安装证书。 不需要root/sudoer访问权限。 支持在Docker内使用，支持IPv6 Github：https://github.com/acmesh-official/acme.sh 安装：curl https://get.acme.sh | sh -s email=可用的邮箱地址 申请： Ali_Key=\"阿里云 AccessKey\" \\ Ali_Secret=\"阿里云 AccessSecret\" \\ DINGTALK_WEBHOOK=\"钉钉机器人完整WebHook地址，包含 Token\" \\ DINGTALK_KEYWORD=\"(可选)定义的关键词\" \\ DINGTALK_SIGNING_KEY=\"(可选)加签的密钥secret\" \\ ~/.acme.sh/.acme.sh --server letsencrypt \\ --dns dns_ali \\ --issue \\ -d '*.test.com' \\ --nginx \\ --key-file /usr/local/etc/nginx/ssl/test.com.key \\ --fullchain-file /usr/local/etc/nginx/ssl/test.com.fullchain.cer \\ --reloadcmd \"nginx -s reload\" --set-notify \\ --notify-hook dingtalk \\ --notify-level 2 \\ --notify-mode 0 # --key-file：指定私钥文件路径。 # --fullchain-file：指定证书文件路径。 # --reloadcmd：指定重新加载服务的命令。如果使用其他服务，修改此命令。 # --server letsencrypt : 指定CA为letsencrypt。默认为ZeroSSL，需要邮箱，还经常超时或504 # --dns dns_ali : 指定域名托管运营商。dns_ali调取dnsapi/dns_ali.sh。其他cloudflare = dns.cf.sh # 注意：泛域名*这里一定要加''保证正常解析 # https://github-wiki-see.page/m/acmesh-official/acme.sh/wiki/notify #export BARK_API_URL=\"https://api.day.app/XXXXXXXXXXXXXXXXXXXXXX\" #export BARK_SOUND=\"newmail\" #export BARK_GROUP=ACME #acme.sh --set-notify --notify-hook bark 续期 Ali_Key=\"阿里云 AccessKey\" \\ Ali_Secret=\"阿里云 AccessSecret\" \\ ~/.acme.sh/.acme.sh --server letsencrypt \\ --dns dns_ali \\ --renew \\ -d '*.test.com' \\ --nginx \\ --key-file /usr/local/etc/nginx/ssl/test.com.key \\ --fullchain-file /usr/local/etc/nginx/ssl/test.com.crt 证书文件合并 cat ~/.acme.sh/*.test.com/fullchain.cer ~/.acme.sh/*.test.com/*.test.com.cer > /usr/local/etc/nginx/ssl/test.com.crt cp ~/.acme.sh/*.test.com/*.test.com.key > /usr/local/etc/nginx/ssl/test.com.key 2、Certbot https://certbot.eff.org/ wget https://dl.eff.org/certbot-auto chmod a+x ./certbot-auto ./certbot-auto --help 3、CFSSL CFSSL是CloudFlare开源的一款PKI/TLS工具。 CFSSL 包含一个命令行工具 和一个用于 签名，验证并且捆绑TLS证书的 HTTP API 服务。 使用Go语言编写。 CFSSL包括： 一组用于生成自定义 TLS PKI 的工具 cfssl程序，是CFSSL的命令行工具 multirootca程序是可以使用多个签名密钥的证书颁发机构服务器 mkbundle程序用于构建证书池 cfssljson程序，从cfssl和multirootca程序获取JSON输出，并将证书，密钥，CSR和bundle写入磁盘 PKI借助数字证书和公钥加密技术提供可信任的网络身份。通常，证书就是一个包含如下身份信息的文件： 证书所有组织的信息 公钥 证书颁发组织的信息 证书颁发组织授予的权限，如证书有效期、适用的主机名、用途等 使用证书颁发组织私钥创建的数字签名 Github： https://github.com/cloudflare/cfssl 下载地址： https://pkg.cfssl.org/ export CFSSL_URL=\"https://pkg.cfssl.org/R1.2\" wget \"${CFSSL_URL}/cfssl_linux-amd64\" -O /usr/local/bin/cfssl wget \"${CFSSL_URL}/cfssljson_linux-amd64\" -O /usr/local/bin/cfssljson chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson 4、easy-rsa 安装 yum install easy-rsa apt install easy-rsa apk add easy-rsa brew install easy-rsa docker run -it --rm cmd.cat/easyrsa easyrsa --help 命令 初始化pki：easyrsa init-pki 创建CA：easyrsa build-ca 生成服务器证书请求：easyrsa gen-req server nopass 五、证书部署 1. Nginx server { listen 443 ssl; ssl_certificate cert/cert-file-name.pem; # 证书文件 ssl_certificate_key cert/cert-file-name.key; # 证书私钥文件 ssl_session_timeout 5m; # 会话缓存过期时间 ssl_protocols TLSv1.1 TLSv1.2 TLSv1.3; # 表示使用的TLS协议的类型 ssl_prefer_server_ciphers on; # 设置协商加密算法时，优先使用服务端的加密套件 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; #表示使用的加密套件的类型。 server_name test.curiouser.com; set $app test; if ($time_iso8601 ~ '(\\d{4}-\\d{2}-\\d{2})') { set $tttt $1; } error_log /var/log/nginx/nginx-test-443-error.log; access_log /var/log/nginx/nginx-tes1t-443-access-$tttt.log main; location / { root ~/test; index index.html index.htm; autoindex on; } } # 将所有HTTP请求通过rewrite指令重定向到HTTPS。 server { listen 80; server_name test.curiouser.com; # 需要将yourdomain替换成证书绑定的域名。 rewrite ^(.*)$ https://$host$1; # 将所有HTTP请求通过rewrite指令重定向到HTTPS。 location / { index index.html index.htm; } } 2. Apache ServerName #修改为申请证书时绑定的域名。 DocumentRoot /data/www/hbappserver/public SSLEngine on SSLProtocol all -SSLv2 -SSLv3 # SSL协议支持的协议，去掉不安全的协议。 SSLCipherSuite HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM # 加密套件 SSLHonorCipherOrder on SSLCertificateFile cert/domain_name_public.crt # 证书文件 SSLCertificateKeyFile cert/domain_name.key # 证书密钥文件 SSLCertificateChainFile cert/domain_name_chain.crt # 证书链文件；证书链开头如果有#字符，请删除。 # 设置HTTP请求自动跳转HTTPS。 ServerName #修改为申请证书时绑定的域名。 DocumentRoot /data/www/hbappserver/public # 设置HTTP请求自动跳转HTTPS RewriteEngine on RewriteCond %{SERVER_PORT} !^443$ RewriteRule ^(.*)$ https://%{SERVER_NAME}$1 [L,R] 3. Linux发行版导入自签证书 CentOS/Redhat/Fedora yum install -y ca-certificates cp ca.crt /usr/local/share/ca-certificates/ update-ca-certificates # 或者 cp *.crt /etc/pki/ca-trust/source/anchors/ update-ca-trust extract Ubuntu/Debian apt-get install ca-certificates cp ca.crt /usr/local/share/ca-certificates/ update-ca-certificates # 或者 cp cacert.pem /usr/share/ca-certificates dpkg-reconfigure ca-certificates Alpine apk add --no-cache ca-certificates mv my.crt /usr/local/share/ca-certificates/ update-ca-certificate 4. 命令行工具使用 Curl curl --cacert ./openssl/ca.crt -v https://test.curiouser.com/test 5. 浏览器使用 浏览器Firefox MacOS 将自签CA的证书拖进：钥匙串-->系统-->证书中即可导入 信任 六、证书格式转换 以下证书格式之间是可以互相转换的。 1. 将JKS格式证书转换成PFX格式 使用JDK中自带的Keytool工具，将JKS格式证书文件转换成PFX格式。例如将 server.jks证书文件转换成 server.pfx证书文件： keytool -importkeystore -srckeystore D:\\server.jks -destkeystore D:\\server.pfx -srcstoretype JKS -deststoretype PKCS12 2. 将PFX格式证书转换为JKS格式 使用JDK中自带的Keytool工具，将PFX格式证书文件转换成JKS格式。例如将 server.pfx证书文件转换成 server.jks证书文件： keytool -importkeystore -srckeystore D:\\server.pfx -destkeystore D:\\server.jks -srcstoretype PKCS12 -deststoretype JKS 3. 将PEM/KEY/CRT格式证书转换为PFX格式 使用 OpenSSL工具，将KEY格式密钥文件和CRT格式公钥文件转换成PFX格式证书文件。例如，将您的KEY格式密钥文件（server.key）和CRT格式公钥文件（server.crt）拷贝至OpenSSL工具安装目录，使用OpenSSL工具执行以下命令将证书转换成 server.pfx证书文件： openssl pkcs12 -export -out server.pfx -inkey server.key -in server.crt 4. 将PFX转换为PEM/KEY/CRT 使用 OpenSSL工具，将PFX格式证书文件转化为KEY格式密钥文件和CRT格式公钥文件。例如，将您的PFX格式证书文件拷贝至OpenSSL安装目录，使用OpenSSL工具执行以下命令将证书转换成server.pem证书文件KEY格式密钥文件（server.key）和CRT格式公钥文件（server.crt）： openssl pkcs12 -in server.pfx -nodes -out server.pem openssl rsa -in server.pem -out server.key openssl x509 -in server.pem -out server.crt 说明 此转换步骤是专用于通过Keytool工具生成私钥和CSR申请证书文件的，并且通过此方法您可以在获取到PEM格式证书公钥的情况下分离私钥。在您实际部署数字证书时，请使用通过此转换步骤分离出来的私钥和您申请得到的公钥证书匹配进行部署。 5、其他转换 PEM 转为 DER：openssl x509 -in cacert.crt -outform der -out cacert.der DER 转为 PEM：openssl x509 -in cert.crt -inform der -outform pem -out cacert.pem CER 转化为 PEM：openssl x509 -in .cer -out .pem -outform PEM 将x509转换为PEM：openssl x509 -in certificatename.cer -outform PEM -out certificatename.pem PKCS7转换为PEM：openssl pkcs7 -print_certs -in certificatename.p7b -out certificatename.pem PEM转换为P7B：openssl crl2pkcs7 -nocrl -certfile certificatename.pem -out certificatename.p7b -certfile CACert.cer pfx转换为PEM：openssl pkcs12 -in certificatename.pfx -out certificatename.pem PFX to PKCS#8： PFX to PEM：openssl pkcs12 -in certificatename.pfx -nocerts -nodes -out certificatename.pem PEM to PKCS8：openSSL pkcs8 -in certificatename.pem -topk8 -nocrypt -out certificatename.pk8 P7B to PFX： PKCS#7或P7B格式以Base64 ASCII格式存储，文件扩展名为.p7b或.p7c。一个P7B文件只包含证书和连锁证书（中间CA），不包含私钥。支持P7B文件的最常见的平台是Microsoft Windows和Java Tomcat。 P7B to CER ：openssl pkcs7 -print_certs -in certificatename.p7b -out certificatename.cer CER and Private Key to PFX：openssl pkcs12 -export -in certificatename.cer -inkey privateKey.key -out certificatename.pfx -certfile cacert.cer 七、其他操作 1. openssl命令行获取服务器SSL证书 DOMAIN=192.168.1.1 SSL_PORT=8443 openssl s_client -showcerts -connect $DOMAIN:$SSL_PORT /dev/null|openssl x509 -outform PEM > $DOMAIN.ssl.pem openssl s_client -connect $DOMAIN:$SSL_PORT -showcerts 参考： https://superuser.com/questions/97201/how-to-save-a-remote-server-ssl-certificate-locally-as-a-file 参考 https://blog.csdn.net/yehuozhili/article/details/105214221 https://blog.csdn.net/liwei16611/article/details/83686674 https://www.cnblogs.com/even160941/p/16068449.html https://juejin.cn/post/7163440404480655367#comment https://www.cnblogs.com/handsomeBoys/p/6556336.html https://blog.csdn.net/caijiwyj/article/details/106840234 https://www.jianshu.com/p/2676cdc688f9 https://blog.csdn.net/xcndafad/article/details/122138791 https://ningyu1.github.io/site/post/51-ssl-cert/ https://blog.csdn.net/weixin_30439131/article/details/97570820 https://www.cnblogs.com/haolb123/p/16553055.html https://www.helay.net/bible/detail2022103071.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-21 15:28:37 "},"origin/ceph-rbd单节点安装.html":{"url":"origin/ceph-rbd单节点安装.html","title":"Ceph RBD单节点安装","keywords":"","body":"Prerequisite Hostname OS Ceph版本 allinone.curiouser.com CentOS 7.4.1708 10.2.10（Jewel） 关闭防火墙和SeLinuxsystemctl disable firewalld ; systemctl stop firewalld ; sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config ; setenforce 0 ; sestatus -v SSH免密码登录打通ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa ; ssh-copy-id root@allinone.curiouser.com ; ssh allinone.curiouser.com Hosts绑定IP地址域名解析echo \"192.168.1.21 allinone.curiouser.com\" >> /etc/hosts 创建ceph用户并设置用户密码和为其添加root权限useradd ceph && echo ceph:ceph | chpasswd ; echo \"ceph ALL=(root) NOPASSWD:ALL\" > /etc/sudoers.d/ceph ; chmod 0440 /etc/sudoers.d/ceph 配置Ceph和Epel的yum源仓库 vim /etc/yum.repos.d/ceph.repo [Ceph] name=Ceph packages for $basearch baseurl=http://download.ceph.com/rpm-jewel/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-jewel/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 可以修改ceph源（外国的源总是timeout） export CEPH_DEPLOY_REPO_URL=http://mirrors.163.com/ceph/rpm-jewel/el7 ; export CEPH_DEPLOY_GPG_URL=http://mirrors.163.com/ceph/keys/release.asc (可选)手动安装下载ceph的rpm包（使用ceph-deploy install 安装ceph包网速太慢。）官方下载: http://download.ceph.com/rpm-jewel/el7/x86_64/ 。需要下载的包如下： ``` ceph-10.2.10-0.el7.x86_64.rpmceph-base-10.2.10-0.el7.x86_64.rpmceph-common-10.2.10-0.el7.x86_64.rpmceph-mds-10.2.10-0.el7.x86_64.rpmceph-mon-10.2.10-0.el7.x86_64.rpmceph-osd-10.2.10-0.el7.x86_64.rpmceph-radosgw-10.2.10-0.el7.x86_64.rpmceph-selinux-10.2.10-0.el7.x86_64.rpmrbd-mirror-10.2.10-0.el7.x86_64.rpm yum localinstall -y ./*.rpm ## 一、安装Ceph-Deploy 1. 安装Ceph-deploy ```bash yum install ceph-deploy -y 安装ceph相关的软件 ceph-deploy install $HOSTNAME 二、创建集群配置文件 创建ceph-deploy的集群配置文件夹，路径并切换过去 mkdir my-cluster ;cd my-cluster 用 ceph-deploy 创建集群，用 new 命令、并指定主机作为初始监视器。 ceph-deploy new $HOSTNAME # 该操作会在~/my-cluster下会生成三个文件 -rw-rw-r-- 1 ceph ceph 251 Jan 12 16:34 ceph.conf -rw-rw-r-- 1 ceph ceph 15886 Jan 12 16:30 ceph.log -rw------- 1 ceph ceph 73 Jan 12 16:30 ceph.mon.keyring ceph.conf中默认的osd pool为3，对应了三个node节点。如果只有两个node节点，则需要修改ceph.conf中的默认值 [global] fsid = 25c13add-967e-4912-bb33-ebbc2cb9376d mon_initial_members = allinone.curiouser.com mon_host = 172.16.2.3 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx # =========新增部分============ filestore_xattr_use_omap = true osd pool default size=1 osd max object name len = 256 osd max object namespace len = 256 mon_pg_warn_max_per_osd = 500 三、创建Monitor ceph-deploy mon create $HOSTNAME ; ceph-deploy gatherkeys $HOSTNAME ; ceph mds stat #查看mds节点状态 四、创建OSD 方式一： (可选)手动节点上挂载lvm存储到某个目录下，作为node节点上OSD的数据存储目录 yum install -y lvm2 ; disk=/dev/vdc ; pvcreate ${disk} ; vgcreate ${disk} ; vgcreate -s 16m ceph-osd ${disk} ; PE_Number=`vgdisplay ${disk}|grep \"Free PE\"|awk '{print $5}'` ; lvcreate -l ${PE_Number} -n ceph-osd ceph-osd ; mkfs.xfs /dev/ceph-osd/ceph-osd ; mkdir -p /data/ceph/osd ; chown -R ceph:ceph /data/ceph/osd ; echo \"/dev/ceph-osd/ceph-osd /data/ceph/osd xfs defaults 0 0\" >> /etc/fstab ; mount -a ; df -mh #LV的文件系统格式注意要xfs,CentOS推荐使用xfs的文件系统.如果是ext4，需要在/etc/ceph/ceph.conf 中添加参数用来限制文件名的长度 osd max object name len = 256 osd max object namespace len = 64 # 之后重启osd服务 systemctl restart ceph-osd.target 准备并激活node节点上的OSD #准备Node节点上的OSD ceph-deploy osd prepare $HOSTNAME:/data/ceph/osd #激活Node节点上的OSD ceph-deploy osd activate $HOSTNAME:/data/ceph/osd #查看OSD状态 ceph osd tree 方式二：(不是以目录为OSD数据存储设备，而是直接以硬盘。其实就是省去手动在硬盘上创建分区的操作) #准备Node节点上的OSD ceph-deploy osd prepare $HOSTNAME:/dev/vdc #激活Node节点上的OSD ceph-deploy osd activate $HOSTNAME:/dev/vdc1 #查看OSD状态 ceph osd tree 五、安装验证 #集群健康状态检查 $> ceph health HEALTH_OK $> ceph -s $> systemctl is-enabled ceph-mds.target ceph-mon.target ceph-osd.target ceph-radosgw.target ceph.target 六、其他信息 Ceph相关的SystemD Units ceph-mds.target ceph-mon.target ceph-osd.target ceph-radosgw.target ceph.target 查看、 更新配置 ceph --show-config ceph-deploy --overwrite-conf config push $HOSTNAME 清除卸载Ceph ceph-deploy uninstall $HOSTNAME ceph-deploy purge $HOSTNAME rm -rf /var/lib/ceph/ /var/run/ceph rm -rf /data/ceph/osd/* Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/ceph-filesystem单节点安装.html":{"url":"origin/ceph-filesystem单节点安装.html","title":"Ceph FileSystem单节点安装","keywords":"","body":"Context Hostname OS Ceph版本 allinone.curiouser.com CentOS 7.4.1708 10.2.10（Jewel） 一个cephfs至少要求两个librados存储池，一个为data，一个为metadata。当配置这两个存储池时，注意： 为metadata pool设置较高级别的副本级别，因为metadata的损坏可能导致整个文件系统不用 建议metadata pool使用低延时存储，比如SSD，因为metadata会直接影响客户端的响应速度 Preflight 一个 clean+active 的cluster（Ceph RBD单节点安装） cluster fb506b4e-43b8-4634-acb9-ea3ee5a97b91 health HEALTH_OK monmap e1: 1 mons at {allinone=192.168.1.96:6789/0} election epoch 29, quorum 0 allinone fsmap e4: 1/1/1 up {0=allinone.okd311.curiouser.com=up:active} osdmap e113: 1 osds: 1 up, 1 in flags sortbitwise,require_jewel_osds pgmap v61453: 192 pgs, 3 pools, 2639 MB data, 985 objects 2730 MB used, 94500 MB / 97231 MB avail 192 active+clean 一、操作 部署元数据服务器MDSceph-deploy mds create $HOSTNAME 创建cephfs需要的两个存储池：一个pool用来存储数据，一个pool用来存储元数据ceph osd pool create cephfs_data 64 ceph osd pool create cephfs_metadata 64 创建CephFS ceph fs new cephfs cephfs_metadata cephfs_data ceph fs ls 二、验证 $ ceph mds stat e4: 1/1/1 up {0=allinone.okd311.curiouser.com=up:active} 三、客户端挂载 Kernel方式 #加载rbd内核模块 modprobe rbd lsmod | grep rbd # 获取client.admin用户的秘钥 ceph auth get client.admin # [client.admin] # key = AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== # caps mds = \"allow *\" # caps mon = \"allow *\" # caps osd = \"allow *\" mkdir /mnt/mycephfs mount -t ceph allinone.okd311.curiouser.com:/ /mnt/mycephfs -o name=admin,secret=AQCinINcLykNLhAA7Xr6o+Q2jYeyc5j58JeQeQ== FUSE方式 yum -y install ceph-fuse ceph-fuse -k /etc/ceph/ceph.client.admin.keyring -m 192.168.197.154:6789 ~/mycephfs/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/test-concept-indicator.html":{"url":"origin/test-concept-indicator.html","title":"测试概念指标","keywords":"","body":"性能测试指标 一、概念 1、并发数（Concurrent） 并发数（Concurrent）是指系统同时能处理的请求数量，这个也是反应了系统的负载能力。 指同一时间点对业务功能同时操作的用户数，可以分为两种：一种是严格意义上的并发，即所有的用户在同一时刻做同一件事或操作，这时业务功能一般指同一类型的业务；另外一种并发是广义范围的并发，这种并发与前一种并发的区别是，尽管多个用户对系统发出了请求或者进行了操作，但是这些请求或都操作可以是相同的，也可以是不同的，这时业务功能可能不是同一类型的业务。 2、响应时间（Response Time） 响应时间（response time）是一个系统最重要的指标之一，它的数值大小直接反应了系统的快慢。响应时间是指执行一个请求从开始到最后收到响应数据所花费的总体时间。 3、吞吐量（Throughput） 吞吐量（Throughput）是指单位时间内系统能处理的请求数量，体现系统处理请求的能力，这是目前最常用的性能测试指标。 一段时间内应用系统处理用户的请求数（以下介绍指单位时间内，也可以理解为吞吐率），这个定义考察点一般是系统本身因素；当然也可以用单位时间内流经被测系统的数据流量，一般单位为b/s,即每秒钟流经的字节数，这个定义的考察点既有系统本身因素也有网络，外设等因素，也可以理解为除客户端以外的测试环境及被测系统。 跟吞吐量有关：并发数、响应时间。 吞吐量的常用量化指标：QPS、TPS、HPS QPS（TPS），并发数、响应时间它们三者之间的关系是：QPS（TPS）= 并发数 / 平均响应时间 ①QPS（每秒查询数） QPS：Queries Per Second，意思是每秒查询率，是一台服务器每秒能够响应的查询次数（数据库中的每秒执行查询sql的次数），不能描述增删改. ②TPS（每秒事务数） TPS：Transactions Per Second，意思是每秒事务数，具体事务的定义，都是人为的，可以一个接口、多个接口、一个业务流程等等。一个事务是指事务内第一个请求发送到接收到最后一个请求的响应的过程，以此来计算使用的时间和完成的事务个数。 TPS 总结 利用并发用户数、期望响应时间，可以计算出TPS。 TPS只是用来计算的是期望值，性能测试过程中的TPS无法单独作为性能指标。 TPS数据方位理论值赢在10-100之间，低于10和高于100都说明系统存在瓶颈点。 利用TPS与平均事物响应时间进行对比，可以分析事物数码对执行时间的影响。例：当压力加大，点击率/tps曲线如果变化缓慢或者有平坦趋势，很有可能是服务器开始出现瓶颈。 TPS是从客户端角度审视服务器处理能力，不能证明TPS可以达到什么程度就能支持多少并发，两者没有必然联系。 TPS会受到负载的影响，也会随着负载的增加而逐渐增加，当系统进入繁忙期后，TPS会有所下降。 TPS与响应时间的关系 1、TPS和响应时间在理想状态下的额定值。如果20个入口，并发数只有10的时候，TPS就是10，而响应时间始终都是1，说明并发不够，需要增加并发数达到TPS的峰值。 2、如果增加到100并发，则造成了线程等待，引起平均响应时间从 1 秒变成 3 秒，TPS也从20下降到9；TPS和响应时间都是单独计算出来的，两者不是互相计算出来的。 3、响应时间和TPS在宏观上是反比的关系，但是两者之间没有直接关系。 TPS 性能测试中的作用 反映了系统在同一时间内处理业务的最大能力，这个数据越高，说明处理能力越强，描述（看到系统的TPS随着时间的变化逐渐变大，而在不到多少分钟的时候系统 　　每秒可以处理多少个事物。这里的最高值并不一定代表系统的最大处理能力，TPS会受到负载的影响，也会随着负载增加而逐渐增加，当系统进入繁忙期后，TPS会有所下降。）而在几分钟以后开始出现少量的失败事物） 一个系统的吞吐量（承压能力）与request 对CPU的消耗、外部接口、IO等紧密关联。单个request对CPU消耗越高，外部系统接口、IO营销速度越慢，系统吞吐能力越低，反之越高。 系统吞吐量几个重要参数：TPS、并发数、响应时间（TPS = 并发数 / 平均响应时间） 利用TPS计算系统最高日吞吐量； 找出系统最高TPS和日PV，这两个要素有相对比较稳定的关系。 通过压力测试或者经营评估，得出最高TPS，然后跟进的关系，计算出系统最高日吞吐量。例如：B2B中文和淘宝对客户群不一样，这两个客户群的网络行为不应用，他们之间的TPS和PV关系比例也不一样。 ③HPS（每秒HTTP请求数） HPS：Hits per Second 每秒点击次数，是指在一秒钟的时间内用户对Web页面的链接、提交按钮等点击总和。 它一般和TPS成正比关系，是B/S系统中非常重要的性能指标之一。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-28 11:51:48 "},"origin/wrk.html":{"url":"origin/wrk.html","title":"wrk","keywords":"","body":"命令行测试工具wrk 一、简介 GitHub地址：https://github.com/wg/wrk 常见的测试工具有：Apache ab, Apache JMeter (互联网公司用的较多)，LoadRunner 等 wrk 是一款针对 Http 协议的基准测试工具，它能够在单机多核 CPU 的条件下，使用系统自带的高性能 I/O 机制，如 epoll，kqueue 等，通过多线程和事件模式，对目标机器产生大量的负载。 PS: 其实，wrk 是复用了 redis 的 ae 异步事件驱动框架，准确来说 ae 事件驱动框架并不是 redis 发明的, 它来至于 Tcl 的解释器 jim, 这个小巧高效的框架, 因为被 redis 采用而被大家所熟知。 二、安装 安装 源码编译/Linux git clone https://github.com/wg/wrk.git wrk && \\ cd wrk && \\ make && \\ cp wrk /usr/local/bin MacOS brew install wrk Docker Dockerfile alpine镜像的软件源中有wrk包，版本为：4.1.0 FROM alpine:3.11.5 RUN sed -i \"s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\" /etc/apk/repositories \\ && apk add --no-cache wrk VOLUME [ \"/data\" ] WORKDIR /data ENTRYPOINT [\"wrk\"] Makefile IMAGE_BASE = tools IMAGE_NAME = wrk IMAGE_VERSION = v1 all: build build: docker build --rm -f Dockerfile -t ${IMAGE_BASE}/${IMAGE_NAME}:${IMAGE_VERSION} . 使用命令 # 本地使用docker启动wrk容器 docker run -it tools/wrk:v1 -t10 -c400 -d30s --latency https://www.baidu.com docker run -it -v .:/data tools/wrk:v1 -s test.lua -c400 -d30s --latency https://www.baidu.com # 在K8S集群中临时启动一个pod kubectl run -n test --rm --generator=run-pod/v1 --image tools/wrk:v1 -i -- wrk -t10 -c400 -d30s --latency https://www.baidu.com 三、命令详解 wrk # 参数选项： # -c, --connections 跟服务器建立并保持的TCP连接数量 # -d, --duration 压测时间 # -t, --threads 使用多少个线程进行压测 # -s, --script 指定Lua脚本路径 # -H, --header 为每一个HTTP请求添加HTTP头 # --latency 在压测结束后，打印延迟统计信息 # --timeout 超时时间 # -v, --version 打印正在使用的wrk的详细版本信息 # 代表数字参数，支持国际单位 (1k, 1M, 1G) # 代表时间参数，支持时间单位 (2s, 2m, 2h) 四、压测报告解读 示例：对本地Nginx的Web服务进行10个线程、400个请求连接、长达30s的压力测试 wrk -t10 -c400 -d30s --latency http://localhost:8080/ 30 秒压测过后，生成如下压测报告： Running 30s test @ http://localhost:8080/ 10 threads and 400 connections（共12个测试线程，400个连接） （平均值） （标准差） （最大值）（正负一个标准差所占比例） Thread Stats Avg Stdev Max +/- Stdev （延迟） Latency 22.09ms 4.40ms 115.67ms 90.98% (每秒请求数) Req/Sec 1.09k 456.65 2.08k 59.60% Latency Distribution （延迟分布） 50% 21.14ms 75% 22.36ms 90% 24.83ms 99% 42.64ms 326894 requests in 30.03s, 269.96MB read (32.03s内处理了326894个请求，耗费流量269.96MB) Socket errors: connect 159, read 132, write 0, timeout 0 (发生错误数) Requests/sec: 10884.16 (QPS 10884.16,即平均每秒处理请求数为10884.16) Transfer/sec: 8.99MB (平均每秒流量8.99MB) 五、使用 Lua 脚本进行复杂测试 wrk可以通过编写 Lua 脚本的方式，在运行压测命令时，通过参数 --script 来指定 Lua 脚本，来进行Post请求 等复杂测试 wrk 支持在三个阶段对压测进行个性化，分别是启动阶段、运行阶段和结束阶段。每个测试线程，都拥有独立的Lua 运行环境。 启动阶段: function setup(thread) 在脚本文件中实现 setup 方法，wrk 就会在测试线程已经初始化，但还没有启动的时候调用该方法。wrk会为每一个测试线程调用一次 setup 方法，并传入代表测试线程的对象 thread 作为参数。setup 方法中可操作该 thread 对象，获取信息、存储信息、甚至关闭该线程。 thread.addr - get or set the thread's server address thread:get(name) - get the value of a global in the thread's env thread:set(name, value) - set the value of a global in the thread's env thread:stop() - stop the thread 运行阶段： function init(args) function delay() function request() function response(status, headers, body) init(args): 由测试线程调用，只会在进入运行阶段时，调用一次。支持从启动 wrk 的命令中，获取命令行参数； delay()： 在每次发送请求之前调用，如果需要定制延迟时间，可以在这个方法中设置； request(): 用来生成请求, 每一次请求都会调用该方法，所以注意不要在该方法中做耗时的操作； response(status, headers, body): 在每次收到一个响应时被调用，为提升性能，如果没有定义该方法，那么wrk不会解析 headers 和 body； 结束阶段： function done(summary, latency, requests) done() 方法在整个测试过程中只会被调用一次，我们可以从给定的参数中，获取压测结果，生成定制化的测试报告。 自定义 Lua 脚本中可访问的变量以及方法： 变量：wrk wrk = { scheme = \"http\", host = \"localhost\", port = 8080, method = \"GET\", path = \"/\", headers = {}, body = nil, thread = , } 以上定义了一个 table 类型的全局变量，修改该 wrk 变量，会影响所有请求。 方法： wrk.fomat wrk.lookup wrk.connect 上面三个方法解释如下： function wrk.format(method, path, headers, body) wrk.format returns a HTTP request string containing the passed parameters merged with values from the wrk table. # 根据参数和全局变量 wrk，生成一个 HTTP rquest 字符串。 function wrk.lookup(host, service) wrk.lookup returns a table containing all known addresses for the host and service pair. This corresponds to the POSIX getaddrinfo() function. # 给定 host 和 service（port/well known service name），返回所有可用的服务器地址信息。 function wrk.connect(addr) wrk.connect returns true if the address can be connected to, otherwise it returns false. The address must be one returned from wrk.lookup(). # 测试给定的服务器地址信息是否可以成功创建连接 六、通过 Lua 脚本压测示例 调用 POST 接口： wrk.method = \"POST\" wrk.body = \"foo=bar&baz=quux\" wrk.headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\" 注意: wrk 是个全局变量，这里对其做了修改，使得所有请求都使用 POST 的方式，并指定了 body 和 Content-Type头。 自定义每次请求的参数： request = function() uid = math.random(1, 10000000) path = \"/test?uid=\" .. uid return wrk.format(nil, path) end 在 request 方法中，随机生成 1~10000000 之间的 uid，并动态生成请求 URL. 每次请求前，延迟 10ms: function delay() return 10 end 请求的接口需要先进行认证，获取 token 后，才能发起请求，咋办？ token = nil path = \"/auth\" request = function() return wrk.format(\"GET\", path) end response = function(status, headers, body) if not token and status == 200 then token = headers[\"X-Token\"] path = \"/test\" wrk.headers[\"X-Token\"] = token end end 上面的脚本表示，在 token 为空的情况下，先请求 /auth 接口来认证，获取 token, 拿到 token 以后，将 token 放置到请求头中，再请求真正需要压测的 /test 接口。 压测支持 HTTP pipeline 的服务： init = function(args) local r = {} r[1] = wrk.format(nil, \"/?foo\") r[2] = wrk.format(nil, \"/?bar\") r[3] = wrk.format(nil, \"/?baz\") req = table.concat(r) end request = function() return req end 通过在 init 方法中将三个 HTTP请求拼接在一起，实现每次发送三个请求，以使用 HTTP pipeline。 参考 https://www.cnblogs.com/quanxiaoha/p/10661650.html https://yeqown.github.io/2018/01/16/wrk%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/benchmark-tools-sysbench.html":{"url":"origin/benchmark-tools-sysbench.html","title":"sysbench","keywords":"","body":"基准测试工具SysBench 一、简介 SysBench是一个模块化的、跨平台、多线程基准测试工具，主要用于评估测试各种不同系统参数下的数据库负载情况。它主要包括以下几种方式的测试： cpu性能 磁盘io性能 调度程序性能 内存分配及传输速度 POSIX线程性能 数据库性能(OLTP基准测试) 目前sysbench主要支持 MySQL,pgsql,oracle 这3种数据库。 GitHub：https://github.com/akopytov/sysbench 二、安装 1、包管理器安装 Linux Debian/Ubuntu curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.deb.sh | sudo bash sudo apt -y install sysbench RHEL/CentOS: curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash sudo yum -y install sysbench Fedora: curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash sudo dnf -y install sysbench Arch Linux: sudo pacman -Suy sysbench MacOS # Add --with-postgresql if you need PostgreSQL support brew install sysbench 2、源码安装 ①安装依赖 Debian/Ubuntu apt -y install make automake libtool pkg-config libaio-dev # For MySQL support apt -y install libmysqlclient-dev libssl-dev # For PostgreSQL support apt -y install libpq-dev RHEL/CentOS yum -y install make automake libtool pkgconfig libaio-devel # For MySQL support, replace with mysql-devel on RHEL/CentOS 5 yum -y install mariadb-devel openssl-devel # For PostgreSQL support yum -y install postgresql-devel Fedora dnf -y install make automake libtool pkgconfig libaio-devel # For MySQL support dnf -y install mariadb-devel openssl-devel # For PostgreSQL support dnf -y install postgresql-devel macOS Assuming you have Xcode (or Xcode Command Line Tools) and Homebrew installed: brew install automake libtool openssl pkg-config # For MySQL support brew install mysql # For PostgreSQL support brew install postgresql # openssl is not linked by Homebrew, this is to avoid \"ld: library not found for -lssl\" export LDFLAGS=-L/usr/local/opt/openssl/lib ②编译安装 ./autogen.sh # Add --with-pgsql to build with PostgreSQL support ./configure make -j make install # The above will build sysbench with MySQL support by default. If you have MySQL headers and libraries in non-standard locations (and no `mysql_config` can be found in the `PATH`), you can specify them explicitly with `--with-mysql-includes` and `--with-mysql-libs` options to `./configure`. # To compile sysbench without MySQL support, use `--without-mysql`. If no database drivers are available database-related scripts will not work, but other benchmarks will be functional. 3、Docker Dockerfile FROM debian:latest RUN sed -i 's/deb.debian.org/mirrors.aliyun.com/g' /etc/apt/sources.list \\ && apt update \\ && apt -y install curl \\ && curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.deb.sh | bash \\ && apt update \\ && apt -y install sysbench redis-tools dumb-init \\ && rm -rf /var/lib/apt/lists/* ENTRYPOINT [\"dumb-init\", \"--\"] Makefile REGISTRY = 192.168.1.60 IMAGE_REPO = tools IMAGE_NAME = mid-bf-tools IMAGE_VERSION = $(v) all: build push build: docker build --rm -f Dockerfile -t ${REGISTRY}/${IMAGE_REPO}/${IMAGE_NAME}:${IMAGE_VERSION} . push: docker push ${REGISTRY}/${IMAGE_REPO}/${IMAGE_NAME}:${IMAGE_VERSION} Kubernetes apiVersion: v1 kind: Pod metadata: labels: app: sysbench name: sysbench spec: containers: - image: 192.168.1.60/tools/mid-bf-tools:v1 name: sysbench command: - sysbench - --db-driver=mysql - --report-interval=2 - --mysql-table-engine=innodb - --oltp-table-size=100000 - --oltp-tables-count=24 - --threads=64 - --time=99999 - --mysql-host=galera - --mysql-port=3306 - --mysql-user=sbtest - --mysql-password=password - /usr/share/sysbench/tests/include/oltp_legacy/oltp.lua - run restartPolicy: Never 第三方的docker镜像：https://hub.docker.com/r/severalnines/sysbench 三、使用方法 命令语法 sysbench [选项]... [testname] [命令] 通用选项: --threads=N # 线程的数量，默认是1 --events=N # 限制的最大事件数量，默认是0，不限制 --time=N # 整个测试执行的时间 [10] --forced-shutdown=STRING number of seconds to wait after the --time limit before forcing shutdown, or 'off' to disable [off] --thread-stack-size=SIZE size of stack per thread [64K] --rate=N average transactions rate. 0 for unlimited rate [0] --report-interval=N periodically report intermediate statistics with a specified interval in seconds. 0 disables intermediate reports [0] --report-checkpoints=[LIST,...] dump full statistics and reset all counters at specified points in time. The argument is a list of comma-separated values representing the amount of time in seconds elapsed from start of test when report checkpoint(s) must be performed. Report checkpoints are off by default. [] --debug[=on|off] print more debugging info [off] --validate[=on|off] perform validation checks where possible [off] --help[=on|off] print help and exit [off] --version[=on|off] print version and exit [off] --config-file=FILENAME File containing command line options --tx-rate=N deprecated alias for --rate [0] --max-requests=N deprecated alias for --events [0] --max-time=N deprecated alias for --time [0] --num-threads=N deprecated alias for --threads [1] Pseudo-Random Numbers Generator options: --rand-type=STRING random numbers distribution {uniform,gaussian,special,pareto} [special] --rand-spec-iter=N number of iterations used for numbers generation [12] --rand-spec-pct=N percentage of values to be treated as 'special' (for special distribution) [1] --rand-spec-res=N percentage of 'special' values to use (for special distribution) [75] --rand-seed=N seed for random number generator. When 0, the current time is used as a RNG seed. [0] --rand-pareto-h=N parameter h for pareto distribution [0.2] Log options: --verbosity=N verbosity level {5 - debug, 0 - only critical messages} [3] --percentile=N percentile to calculate in latency statistics (1-100). Use the special value of 0 to disable percentile calculations [95] --histogram[=on|off] print latency histogram in report [off] General database options: --db-driver=STRING specifies database driver to use ('help' to get list of available drivers) [mysql] --db-ps-mode=STRING prepared statements usage mode {auto, disable} [auto] --db-debug[=on|off] print database-specific debug information [off] Compiled-in database drivers: mysql - MySQL driver mysql options: --mysql-host=[LIST,...] MySQL server host [localhost] --mysql-port=[LIST,...] MySQL server port [3306] --mysql-socket=[LIST,...] MySQL socket --mysql-user=STRING MySQL user [sbtest] --mysql-password=STRING MySQL password [] --mysql-db=STRING MySQL database name [sbtest] --mysql-ssl[=on|off] use SSL connections, if available in the client library [off] --mysql-ssl-cipher=STRING use specific cipher for SSL connections [] --mysql-compression[=on|off] use compression, if available in the client library [off] --mysql-debug[=on|off] trace all client library calls [off] --mysql-ignore-errors=[LIST,...] list of errors to ignore, or \"all\" [1213,1020,1205] --mysql-dry-run[=on|off] Dry run, pretend that all MySQL client API calls are successful without executing them [off] Compiled-in tests: fileio - File I/O test cpu - CPU performance test memory - Memory functions speed test threads - Threads subsystem performance test mutex - Mutex performance test command command是sysbench要执行的命令，包括以下三种： prepare 为测试提前准备数据 run 执行正式的测试 cleanup 在测试完成后对数据库进行清理 testname 可以是内置的fileio，memory等，也可以是lua脚本或者是lua脚本的路径 testname 指定了要进行的测试，在老版本的sysbench中，可以通过--test参数指定测试的脚本； 而在新版本中，--test参数已经声明为废弃，可以不使用--test，而是直接指定脚本。测试时使用的脚本为lua脚本，可以使用sysbench自带脚本，也可以自己开发。 测试fileio命令帮助 sysbench --test=fileio help fileio options: --file-num=N 代表生成测试文件的数量，默认为128。 --file-block-size=N 测试时所使用文件块的大小，如果想磁盘针对innodb存储引擎进行测试，可以将其设置为16384， 即innodb存储引擎页的大小。默认为16384。 --file-total-size=SIZE 创建测试文件的总大小，默认为2G大小。 --file-test-mode=STRING 文件测试模式，包含：seqwr(顺序写), seqrewr(顺序读写), seqrd(顺序读), rndrd(随机读), rndwr(随机写), rndrw(随机读写)。 --file-io-mode=STRING 文件操作的模式，sync（同步）,async（异步）,fastmmap（快速mmap）,slowmmap（慢速mmap）， 默认为sync同步模式。 --file-extra-flags=[LIST,...] list of additional flags to use to open files {sync,dsync,direct} [] --file-fsync-freq=N 执行fsync()函数的频率。fsync主要是同步磁盘文件，因为可能有系统和磁盘缓冲的关系。 0代表不使用fsync函数。默认值为100。 --file-fsync-all[=on|off] 每执行完一次写操作，就执行一次fsync。默认为off。 --file-fsync-end[=on|off] 在测试结束时执行fsync函数。默认为on。 --file-fsync-mode=STRING 文件同步函数的选择，同样是和API相关的参数，由于多个操作系统对于fdatasync支持不同， 因此不建议使用fdatasync。默认为fsync。 --file-merged-requests=N 大多情况下，合并可能的IO的请求数，默认为0不合并。 --file-rw-ratio=N 测试时的读写比例，默认时为1.5，即可3：2。 测试memory命令帮助 sysbench --test=memory help memory options: --memory-block-size=SIZE 测试内存块的大小，默认为1K --memory-total-size=SIZE 数据传输的总大小，默认为100G --memory-scope=STRING 内存访问的范围，包括全局和本地范围，默认为global --memory-oper=STRING 内存操作的类型，包括read, write, none，默认为write --memory-access-mode=STRING 内存访问模式，包括seq,rnd两种模式，默认为seq 测试threads命令帮助 $ sysbench --test=threads help threads options: --thread-yields=N 指定每个请求的压力，默认为1000 --thread-locks=N 指定每个线程的锁数量，默认为8 测试mutex命令帮助 sysbench --test=mutex help mutex options: --mutex-num=N 数组互斥的总大小。默认是4096 --mutex-locks=N 每个线程互斥锁的数量。默认是50000 --mutex-loops=N 内部互斥锁的空循环数量。默认是10000 注意 sysbench 1.0版本以上，默认不支持sysbench --test=oltp，使用sysbench oltp_read_write代替 参考：https://github.com/akopytov/sysbench/issues/281 四、示例及结果解读 示例1、测试MySQL读写性能 准备数据 手动在数据库里面创建对应的DB sysbench oltp_read_write \\ --db-driver=mysql \\ --mysql-host=localhost \\ --mysql-db=sysbench \\ --mysql-user=root \\ --mysql-password \\ --rand-type=uniform \\ --table_size=1000000 --tables=2 --threads=1 --events=0 \\ prepare 以上命令会在sysbench库中创建两张包含一百万条记录的表sbtest1和sbtest2 开始测试 sysbench oltp_read_write \\ --threads=16 \\ --histogram=on \\ --report-interval=10 \\ --mysql-host=localhost \\ --mysql-db=sysbench \\ --mysql-user=root \\ --mysql-password= \\ run 测试结果解读 Running the test with following options: Number of threads: 16 Initializing random number generator from current time Initializing worker threads... Threads started! [ 10s ] thds: 16 tps: 1690.78 qps: 33849.48 (r/w/o: 23700.00/6765.32/3384.16) lat (ms,95%): 29.72 err/s: 1.00 reconn/s: 0.00 Latency histogram (values are in milliseconds) value ------------- distribution ------------- count 3.020 | 1 3.130 | 1 3.187 |* 9 3.245 |** 20 3.304 |** 24 3.364 |**** 46 3.425 |****** 75 3.488 |*********** 141 3.551 |**************** 208 3.615 |******************** 258 3.681 |************************ 310 3.748 |*************************** 345 3.816 |******************************* 398 3.885 |******************************** 414 3.956 |******************************** 412 4.028 |************************************ 459 4.101 |**************************************** 512 4.176 |************************************** 488 4.252 |************************************ 457 4.329 |************************************** 486 4.407 |************************************* 470 4.487 |******************************** 407 4.569 |********************************* 422 4.652 |********************************* 417 4.737 |******************************** 411 4.823 |***************************** 365 4.910 |**************************** 360 4.999 |************************* 324 5.090 |************************* 322 5.183 |************************** 327 5.277 |********************* 270 5.373 |*********************** 289 5.470 |****************** 230 5.570 |******************* 240 5.671 |***************** 218 5.774 |**************** 206 5.879 |*************** 190 5.986 |************** 174 6.095 |************ 154 6.205 |************ 156 6.318 |************ 149 6.433 |************* 160 6.550 |************ 155 6.669 |********** 125 6.790 |********** 126 6.913 |********* 115 7.039 |********* 119 7.167 |********* 118 7.297 |******** 100 7.430 |********* 114 7.565 |******** 96 7.702 |******* 84 7.842 |******* 93 7.985 |******* 88 8.130 |******* 88 8.277 |******* 87 8.428 |****** 71 8.581 |****** 71 8.737 |***** 68 8.895 |****** 79 9.057 |***** 58 9.222 |****** 73 9.389 |***** 63 9.560 |**** 49 9.734 |***** 70 9.910 |**** 51 10.090 |*** 42 10.274 |*** 41 10.460 |***** 58 10.651 |***** 64 10.844 |**** 51 11.041 |**** 51 11.242 |**** 53 11.446 |**** 48 11.654 |*** 44 11.866 |**** 55 12.081 |**** 57 12.301 |**** 55 12.524 |**** 52 12.752 |***** 58 12.984 |**** 50 13.219 |**** 48 13.460 |**** 46 13.704 |*** 43 13.953 |**** 54 14.207 |****** 77 14.465 |*** 43 14.728 |**** 56 14.995 |***** 62 15.268 |**** 57 15.545 |**** 53 15.828 |**** 51 16.115 |**** 52 16.408 |**** 51 16.706 |**** 47 17.010 |**** 46 17.319 |*** 33 17.633 |*** 44 17.954 |***** 66 18.280 |*** 37 18.612 |*** 41 18.950 |*** 44 19.295 |**** 46 19.645 |**** 45 20.002 |*** 44 20.366 |** 30 20.736 |*** 37 21.112 |*** 38 21.496 |*** 42 21.886 |*** 33 22.284 |*** 35 22.689 |*** 35 23.101 |** 28 23.521 |*** 35 23.948 |*** 35 24.384 |** 26 24.827 |** 25 25.278 |** 31 25.737 |** 30 # ....省略.... 87.564 | 6 89.155 |* 9 90.775 | 5 92.424 | 6 94.104 | 6 95.814 | 6 97.555 | 4 99.327 | 5 101.132 |* 7 102.969 | 2 104.840 | 6 106.745 | 5 108.685 | 2 110.659 | 4 112.670 | 3 114.717 |* 8 116.802 | 5 118.924 | 3 121.085 | 1 123.285 | 6 125.525 |* 7 127.805 | 4 130.128 | 1 132.492 | 1 SQL statistics: queries performed: read: 237118 # 总select数量 write: 67723 # 总update、insert、delete语句数量 other: 33864 # commit、unlock tables以及其他mutex的数量 total: 338705 transactions: 16927 (1690.41 per sec.) # TPS queries: 338705 (33824.71 per sec.) # QPS ignored errors: 10 (1.00 per sec.) # 忽略的错误 reconnects: 0 (0.00 per sec.) # 重新连接 General statistics: total time: 10.0122s # 测试的总时间 total number of events: 16927 # 总的事件数，一般和TPS一样 Latency (ms): min: 2.99 # 最小响应时间 avg: 9.46 # 平均响应时间 max: 132.49 # 最大响应时间 95th percentile: 29.72 # 95%的响应时间是这个数据 sum: 160046.87 Threads fairness: events (avg/stddev): 1057.9375/8.09 execution time (avg/stddev): 10.0029/0.00 清理测试数据 sysbench oltp_read_write \\ --db-driver=mysql \\ --mysql-host=localhost \\ --mysql-db=sysbench \\ --mysql-user=root \\ --mysql-password \\ cleanup Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/jmeter.html":{"url":"origin/jmeter.html","title":"Jmeter","keywords":"","body":"Jmeter 一、简介 jmeter 是一款专门用于功能测试和压力测试的轻量级测试开发平台。多数情况下是用作压力测试。 接口测试 压力测试 数据库测试 二、安装 1、二进制 2、Docker 三、配置 1、设置中文 设置安装目录的bin/jmeter.properties文件的language=zh_CN，重启即可 2、安装插件管理插件 下载plugins-manager.jar到安装目录的lib/ext中重启，在 3、页面显示设置Jmeter参数 四、测试场景 1、Jmeter抓包录制 抓包方式： badboy： httpL： Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-04-22 23:26:17 "},"origin/apache-benchmark.html":{"url":"origin/apache-benchmark.html","title":"Apache Benchmark","keywords":"","body":"Apache Benchmark 一、简介 ab命令会创建多个并发访问线程，模拟多个访问者同时对某一URL地址进行访问。它的测试目标是基于URL的，因此，它既可以用来测试apache的负载压力，也可以测试nginx、lighthttp、tomcat、IIS等其它Web服务器的压力。ab命令对发出负载的计算机要求很低，它既不会占用很高CPU，也不会占用很多内存。但却会给目标服务器造成巨大的负载，其原理类似CC攻击。自己测试使用也需要注意，否则一次上太多的负载。可能造成目标服务器资源耗完，严重时甚至导致死机。 吞吐率：服务器并发处理能力的量化描述，单位是reqs/s，指的是某个并发用户数下单位时间内处理的请求数。某个并发用户数下单位时间内能处理的最大请求数，称之为最大吞吐率。 并发连接数：某一时刻服务器所接受的请求数（会话数）。 并发用户数：某一时刻服务器所接受的连接数，一个用户可能同时产生多个连接。 用户平均请求等待时间：总请求数 / 并发用户数。 服务器平均请求等待时间：处理完成所有请求数所花费的时间 / 总请求数。 PV： 即page view，页面浏览量，用户每一次对网站中的每个页面访问均被记录1次。用户对同一页面的多次刷新，访问量累计。 UV：即Unique visitor，独立访客，通过客户端的cookies实现。即同一页面，客户端多次点击只计算一次，访问量不累计。 IP： 即Internet Protocol，本意本是指网络协议，在数据统计这块指通过ip的访问量。即同一页面，客户端使用同一个IP访问多次只计算一次，访问量不累计。 TPS：即Transactions Per Second的缩写，每秒处理的事务数目。一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数，最终利用这些信息作出的评估分。 QPS：即Queries Per Second的缩写，每秒能处理查询数目。是一台服务器每秒能够相应的查询次数，是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准。QPS=并发量 / 平均响应时间 RPS：即Requests Per Second的缩写，每秒能处理的请求数目。等效于QPS 二、安装 Linux yum install httpd-tools Macos 自带 三、命令参数 -n requests 执行的请求数，即一共发起多少请求。 -c concurrency 请求并发数。 -t timelimit 测试所进行的最大秒数。其内部隐含值是-n 50000，它对服务器的测试限制在一个固定的总时间以内。默认没有时间限制。 -s timeout 指定每个请求的超时时间，默认是30秒。 -b windowsize 指定tcp窗口的大小，单位是字节。 -B address 指定在发起连接时绑定的ip地址是什么。 -p postfile 指定要POST的文件，同时要设置-T参数。 -u putfile 指定要PUT的文件，同时要设置-T参数。 -T content-type 指定使用POST或PUT上传文本时的文本类型，默认是'text/plain'。 -v verbosity 设置详细模式等级。 -w 将结果输出到html的表中。 -i 使用HEAD方式代替GET发起请求。 -y attributes 以表格方式输出时，设置html表格tr属性。 -z attributes 以表格方式输出时，设置html表格th或td属性。 -C attribute 添加cookie,比如'Apache=1234'。（可重复） -H attribute 为请求追加一个额外的头部，比如'Accept-Encoding: gzip'。（可重复） -A attribute 对服务器提供BASIC认证信任。用户名和密码由一个:隔开，并以base64编码形式发送。无论服务器是否需要(即,是否发送了401认证需求代码)，此字符串都会被发送。 -P attribute 对一个中转代理提供BASIC认证信任。用户名和密码由一个:隔开，并以base64编码形式发送。无论服务器是否需要(即, 是否发送了401认证需求代码)，此字符串都会被发送。 -X proxy:port 指定代理服务器的IP和端口。 -V 打印版本信息。 -k 启用HTTP KeepAlive功能，即在一个HTTP会话中执行多个请求。默认时，不启用KeepAlive功能。 -d 不显示\"percentage served within XX [ms] table\"的消息(为以前的版本提供支持)。 -q 如果处理的请求数大于150，ab每处理大约10%或者100个请求时，会在stderr输出一个进度计数。此-q标记可以抑制这些信息。 -g filename 把所有测试结果写入一个'gnuplot'或者TSV(以Tab分隔的)文件。此文件可以方便地导入到Gnuplot,IDL,Mathematica,Igor甚至Excel中。其中的第一行为标题。 -e filename 产生一个以逗号分隔的(CSV)文件，其中包含了处理每个相应百分比的请求所需要(从1%到100%)的相应百分比的(以微妙为单位)时间。由于这种格式已经“二进制化”，所以比'gnuplot'格式更有用。 -r 当收到错误时不要退出。 -h 输出帮助信息 -Z ciphersuite 指定SSL/TLS密码套件 -f protocol 指定SSL/TLS协议(SSL3, TLS1, TLS1.1, TLS1.2 or ALL) 四、测试结果详解 ab -n 1000 -c 10 https://www.baidu.com/ # 测试服务器的名字 Server Software: # 请求的URL主机名 Server Hostname: www.baidu.com #web服务器监听的端口 Server Port: 443 SSL/TLS Protocol: TLSv1.2,ECDHE-RSA-AES128-GCM-SHA256,2048,128 Server Temp Key: ECDH P-256 256 bits TLS Server Name: www.baidu.com # 请求的URL中的根绝对路径 Document Path: / # HTTP响应数据的正文长度 Document Length: 227 bytes # 并发用户数，这是ab命令中设置的-c参数 Concurrency Level: 10 # 所有这些请求被处理完成所花费的总时间 Time taken for tests: 12.537 seconds # 总请求数量，这是ab命令中设置的-n参数 Complete requests: 1000 # 失败的请求数，这里的失败是指请求的连接服务器、发送数据、接收数据等环节发生异常，以及无响应后超时的情况。对于超时时间的设置可以用ab的-t参数。如果接受到的http响应数据的头信息中含有2xx以外的状态码，则会在测试结果显示另一个名为“Non-2xx responses”的统计项，用于统计这部分请求数，这些请求并不算是失败的请求。 Failed requests: 0 #写入错误 Write errors: 0 # 总的网络传输量(字节数)，包含http的头信息等。使用ab的-v参数即可查看详细的http头信息。 Total transferred: 1110922 bytes # HTML内容传输量，实际的页面传递字节数。也就是减去了Total transferred中http响应数据中头信息的长度。 HTML transferred: 227000 bytes # 每秒处理的请求数，服务器的吞吐量，等于：Complete requests / Time taken for tests(QPS) Requests per second: 79.76 [#/sec] (mean) # 用户平均请求等待时间 Time per request: 125.369 [ms] (mean) # 服务器平均处理时间 Time per request: 12.743 [ms] (mean, across all concurrent requests) # 平均每秒网络上的流量,即每秒收到的速率 Transfer rate: 86.54 [Kbytes/sec] received # 压力测试时的连接处理时间。-- min最小值、mean平均值、[+/-sd]方差、median中位数、maxz最大值 Connection Times (ms) min mean[+/-sd] median max Connect: 49 95 25.8 89 201 # socket链路建立消耗，代表网络状况好 Processing: 12 29 14.9 25 131 # 写入缓冲区消耗+链路消耗+服务器消耗 Waiting: 12 26 12.7 23 130 # 写入缓冲区消耗+链路消耗+服务器消耗+读取数据消耗 Total: 67 124 31.3 116 245 # 单个事务总时间 Percentage of the requests served within a certain time (ms) 50% 116 66% 127 75% 136 80% 142 90% 169 95% 192 98% 213 99% 227 100% 245 (longest request) # 整个场景中所有请求的响应情况。在场景中每个请求都有一个响应时间，其中50％的用户响应时间小于119毫秒，66％的用户响应时间小于141毫秒，最大的响应时间小于281毫秒 五、实操 1、测试试socket出现打开文件过多 socket: Too many open files (24) 解决： ulimit -n ulimit -n 204800 2、测试网址要带URL # 不带路径则无法测试 https://www.baidu.com # 以下路径可以测试 https://www.baidu.com/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-25 10:25:06 "},"origin/sysbench-mysql.html":{"url":"origin/sysbench-mysql.html","title":"MySQL性能测试之sysbench","keywords":"","body":"MySQL性能测试 一、性能指标 TPS ：Transactions Per Second ，即数据库每秒执行的事务数，以 commit 成功次数为准。 QPS ：Queries Per Second ，即数据库每秒执行的 SQL 数（含 insert、select、update、delete 等）。 RT ：Response Time ，响应时间。包括平均响应时间、最小响应时间、最大响应时间、每个响应时间的查询占比。比较需要重点关注的是，前 95-99% 的最大响应时间。因为它决定了大多数情况下的短板。 Concurrency Threads ：并发量，每秒可处理的查询请求的数量。 总结来说，实际就是 2 个维度： 吞吐量 延迟 二、测试工具 mysqlslap mysqlslap可以模拟服务器的负载，并输出计时信息。在MySQL 4.1或者更新的版本中都可以使用。测试时可以执行并发连接数，并指定SQL 语句（可以在命令行上执行，也可以把SQL 语句写入到参数文件中）。如果没有指定SQL 语句，mysqlslap 会自动生成查询schema 的SELECT 语句。 sysbench sysbench是一款多线程系统压测工具。它可以根据影响数据库服务器性能的各种因素来评估系统的性能。例如，可以用来测试文件I/O、操作系统调度器、内存分配和传输速度、POSIX 线程，以及数据库服务器等。sysbench 支持Lua 脚本语言，Lua 对于各种测试场景的设置可以非常灵活。sysbench 是我们非常喜欢的一种全能测试工具，支持MySQL、操作系统和硬件的硬件测试。 MySQL Benchmark Suite （sql-bench） 在MySQL 的发行包中也提供了一款自己的基准测试套件，可以用于在不同数据库服务器上进行比较测试。它是单线程的，主要用于测试服务器执行查询的速度。结果会显示哪种类型的操作在服务器上执行得更快。 　　这个测试套件的主要好处是包含了大量预定义的测试，容易使用，所以可以很轻松地用于比较不同存储引擎或者不同配置的性能测试。其也可以用于高层次测试，比较两个服务器的总体性能。当然也可以只执行预定义测试的子集（例如只测试UPDATE 的性能）。这些测试大部分是CPU 密集型的，但也有些短时间的测试需要大量的磁盘I/O 操作。 　　这个套件的最大缺点主要有：它是单用户模式的，测试的数据集很小且用户无法使用指定的数据，并且同一个测试多次运行的结果可能会相差很大。因为是单线程且串行执行的，所以无法测试多CPU 的能力，只能用于比较单CPU 服务器的性能差别。使用这个套件测试数据库服务器还需要Perl 和BDB 的支持，相关文档请参考. Super Smack Super Smack是一款用于MySQL 和PostgreSQL的基准测试工具，可以提供压力测试和负载生成。这是一个复杂而强大的工具，可以模拟多用户访问，可以加载测试数据到数据库，并支持使用随机数据填充测试表。测试定义在\"smack\"文件中，smack 文件使用一种简单的语法定义测试的客户端、表、查询等测试要素。 等等 三、sysbench测试实例 参考：sysbench 中的测试示例 四、mysqlslap测试实例 mysqlslap是一个mysql官方提供的压力测试工具。以下是比较重要的参数： –defaults-file，配置文件存放位置 –concurrency，并发数 –engines，引擎 –iterations，迭代的实验次数 –socket，socket文件位置 自动测试： –auto-generate-sql，自动产生测试SQL –auto-generate-sql-load-type，测试SQL的类型。类型有mixed，update，write，key，read。 –number-of-queries，执行的SQL总数量 –number-int-cols，表内int列的数量 –number-char-cols，表内char列的数量 –create-schema，指定数据库名称 –query，指定SQL语句，可以定位到某个包含SQL的文件 1、测试同时不同的存储引擎的性能进行对比 mysqlslap -uroot -p123456 \\ -a --concurrency=50,100 --number-of-queries 1000 --iterations=5 --engine=myisam,innodb 2、执行一次测试，分别50和100个并发，执行1000次总查询 mysqlslap -uroot -p123456 \\ -a --concurrency=50,100 --number-of-queries 1000 6 3、50和100个并发分别得到一次测试结果(Benchmark)，并发数越多，执行完所有查询的时间越长。为了准确起见，可以多迭代测试几次: mysqlslap -uroot -p123456 \\ -a --concurrency=50,100 --number-of-queries 1000 --iterations=5 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/redis-benchmark.html":{"url":"origin/redis-benchmark.html","title":"Redis性能测试之redis-benchmark","keywords":"","body":"Redis性能测试工具：redis-benchmark 一、简介 基本命令语法如下： redis-benchmark [option] [option value] 可选参数如下所示： 选项 描述 默认值 -h 指定服务器主机名 127.0.0.1 -p 指定服务器端口 6379 -s 指定服务器 socket -c 指定并发连接数 50 -n 指定请求数 10000 -d 以字节的形式指定 SET/GET 值的数据大小 2 -k 1=keep alive 0=reconnect 1 -r SET/GET/INCR 使用随机 key, SADD 使用随机值 -P 通过管道传输请求 1 -q 强制退出 redis。仅显示 query/sec 值 --csv 以 CSV 格式输出 -l 生成循环，永久执行测试 -t 仅运行以逗号分隔的测试命令列表。 -I Idle 模式。仅打开 N 个 idle 连接并等待。 二、示例 1、测试所有命令 redis-benchmark -n 100000 -q PING_INLINE: 106044.54 requests per second PING_BULK: 104058.27 requests per second SET: 106382.98 requests per second GET: 106496.27 requests per second INCR: 109289.62 requests per second LPUSH: 100806.45 requests per second RPUSH: 105042.02 requests per second LPOP: 104602.52 requests per second RPOP: 88809.95 requests per second SADD: 103842.16 requests per second HSET: 94250.71 requests per second SPOP: 94517.96 requests per second LPUSH (needed to benchmark LRANGE): 93283.58 requests per second LRANGE_100 (first 100 elements): 26795.28 requests per second LRANGE_300 (first 300 elements): 11272.69 requests per second LRANGE_500 (first 450 elements): 7966.22 requests per second LRANGE_600 (first 600 elements): 6308.35 requests per second MSET (10 keys): 77101.00 requests per second 2、测试设置 10 万随机 key 连续 SET 100 万次 redis-benchmark -t set -r 100000 -n 1000000 ====== SET ====== 1000000 requests completed in 10.21 seconds 50 parallel clients # 模拟 50 个客户端 3 bytes payload keep alive: 1 99.38% 3、使用 pipelining进行测试 Redis是一种基于客户端-服务端模型以及请求/响应协议的TCP服务。 这意味着通常情况下一个请求会遵循以下步骤： 客户端向服务端发送一个查询请求，并监听Socket返回，通常是以阻塞模式，等待服务端响应。 服务端处理命令，并将结果返回给客户端。 默认情况下，每个客户端都是在一个请求完成之后才发送下一个请求 服务器几乎是按顺序读取每个客户端的命令。Redis 支持 pipelining，一次请求/响应服务器能实现处理新的请求即使旧的请求还未被响应。这样就可以将多个命令发送到服务器，而不用等待回复，最后在一个步骤中读取该答复。使得可以一次性执行多条命令成为可能。 Redis pipelining 可以提高服务器的 TPS。 # 设置16 条命令使用 pipelining redis-benchmark -r 1000000 -n 2000000 -t get,set,lpush,lpop -P 16 -q SET: 472255.00 requests per second GET: 559753.69 requests per second LPUSH: 569638.31 requests per second LPOP: 571428.56 requests per second # 不使用pipelining redis-benchmark -r 1000000 -n 2000000 -t get,set,lpush,lpop -q SET: 86422.95 requests per second GET: 88409.52 requests per second LPUSH: 87680.84 requests per second LPOP: 80994.61 requests per second 4、使用Lua脚本测试 redis-benchmark -n 100000 -q script load \"redis.call('set','foo','bar')\" script load redis.call('set','foo','bar'): 105042.02 requests per second 三、影响Redis 性能的因素 1、网络带宽和延迟 网络带宽和延迟通常是最大短板。建议在基准测试之前使用 ping 来检查服务端到客户端的延迟。根据带宽，可以计算出最大吞吐量。 比如将 4 KB 的字符串塞入 Redis，吞吐量是 100000 q/s，那么实际需要 3.2 Gbits/s 的带宽，所以需要 10 GBits/s 网络连接， 1 Gbits/s 是不够的。 在很多线上服务中，Redis 吞吐会先被网络带宽限制住，而不是 CPU。 为了达到高吞吐量突破 TCP/IP 限制，最后采用 10 Gbits/s 的网卡， 或者多个 1 Gbits/s 网卡。 2、CPU 由于Redis是单线程模型，Redis 更喜欢大缓存快速 CPU， 而不是多核。 3、连接方式 如果服务器和客户端都运行在同一个机器上面，那么 TCP/IP loopback 和 unix domain sockets 都可以使用。对 Linux 来说，使用 unix socket 可以比 TCP/IP loopback 快 50%。 默认 redis-benchmark 是使用 TCP/IP loopback。 当大量使用 pipelining 时候，unix domain sockets 的优势就不那么明显了。 4、部署环境 Redis 在 VM 上会变慢。虚拟化对普通操作会有额外的消耗，Redis 对系统调用和网络终端不会有太多的 overhead。建议把 Redis 运行在物理机器上， 特别是当你很在意延迟时候。在最先进的虚拟化设备（VMWare）上面，redis-benchmark 的测试结果比物理机器上慢了一倍，很多 CPU 时间被消费在系统调用和中断上面。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/kafka-perf.html":{"url":"origin/kafka-perf.html","title":"Kafka性能测试","keywords":"","body":"kafka性能测试 一、简介 性能测试工具是 Kafka 本身提供的用于生产者性能测试kafka-producer\u0002-perf-test.sh和用于消费者性能测试的kafka-consumer-perf-test.sh kafka-producer\u0002-perf-test.sh命令详解 usage: producer-performance \\ [-h] \\ --topic TOPIC \\ --num-records NUM-RECORDS \\ [--payload-delimiter PAYLOAD-DELIMITER] \\ --throughput THROUGHPUT \\ [--producer-props PROP-NAME=PROP-VALUE [PROP-NAME=PROP-VALUE ...]] \\ [--producer.config CONFIG-FILE] [--print-metrics] \\ [--transactional-id TRANSACTIONAL-ID] [--transaction-duration-ms TRANSACTION-DURATION] (--record-size RECORD-SIZE | --payload-file PAYLOAD-FILE) This tool is used to verify the producer performance. optional arguments: -h, --help show this help message and exit --topic TOPIC produce messages to this topic --num-records NUM-RECORDS number of messages to produce --payload-delimiter PAYLOAD-DELIMITER provides delimiter to be used when --payload-file is provided. Defaults to new line. Note that this parameter will be ignored if --payload-file is not provided. (default: \\n) --throughput THROUGHPUT throttle maximum message throughput to *approximately* THROUGHPUT messages/sec. Set this to -1 to disable throttling. --producer-props PROP-NAME=PROP-VALUE [PROP-NAME=PROP-VALUE ...] kafka producer related configuration properties like bootstrap.servers,client.id etc. These configs take precedence over those passed via --producer.config. --producer.config CONFIG-FILE producer config properties file. --print-metrics print out metrics at the end of the test. (default: false) --transactional-id TRANSACTIONAL-ID The transactionalId to use if transaction-duration-ms is > 0. Useful when testing the performance of concurrent transactions. (default: performance-producer-default-transactional-id) --transaction-duration-ms TRANSACTION-DURATION The max age of each transaction. The commitTransaction will be called after this time has elapsed. Transactions are only enabled if this value is positive. (default: 0) either --record-size or --payload-file must be specified but not both. --record-size RECORD-SIZE message size in bytes. Note that you must provide exactly one of --record-size or --payload-file. --payload-file PAYLOAD-FILE file to read the message payloads from. This works only for UTF-8 encoded text files. Payloads will be read from this file and a payload will be randomly selected when sending messages. Note that you must provide exactly one of --record-size or --payload-file. kafka-consumer-perf-test.sh命令详解 This tool helps in performance test for the full zookeeper consumer Option Description ------ ----------- --bootstrap-server (deprecated) is specified. The server (s) to connect to. --broker-list DEPRECATED, use --bootstrap-server instead; ignored if --bootstrap- server is specified. The broker list string in the form HOST1:PORT1, HOST2:PORT2. --consumer.config Consumer config properties file. --date-format The date format to use for formatting the time field. See java.text. SimpleDateFormat for options. (default: yyyy-MM-dd HH:mm:ss:SSS) --fetch-size The amount of data to fetch in a single request. (default: 1048576) --from-latest If the consumer does not already have an established offset to consume from, start with the latest message present in the log rather than the earliest message. --group The group id to consume on. (default: perf-consumer-50295) --help Print usage information. --hide-header If set, skips printing the header for the stats --messages REQUIRED: The number of messages to send or consume --num-fetch-threads Number of fetcher threads. (default: 1) --print-metrics Print out the metrics. --reporting-interval print progress info. (default: 5000) --show-detailed-stats If set, stats are reported for each reporting interval as configured by reporting-interval --socket-buffer-size The size of the tcp RECV size. (default: 2097152) --threads Number of processing threads. (default: 10) --timeout [Long: milliseconds] The maximum allowed time in milliseconds between returned records. (default: 10000) --topic REQUIRED: The topic to consume from. --version Display Kafka version. 二、生产者写入测试 向一个只有1个分区和1个副本的Topic主题perf-producer-test 中发送 100 万条消息，并且每条消息大小为 1024B 生产者，对应的 acks 参数为1 kafka-producer-perf-test.sh \\ --topic perf-producer-test \\ --num-records 1000000 \\ --record-size 1024 \\ --throughput -1 \\ --print-metrics \\ --producer-props bootstrap.servers=localhost:9092 acks=1 # --nurn-records：用来指定发送消息的总条数 # --record-size：用来设置每条消息的字节数 # --throughput：用来进行限流控制，当设定的值小于0不限流，当设定的值大于0时，当发送的吞吐量大于该值时就会被阻塞一段时间。 # --print-metrics：测试完成后的Metrics信息 # --producer-props：参数用来指定生产者的配置，可同时指定多组配置，各组配置之间以空格分隔，与producer-props 参数对应的还有一个producer-config参数，它用来指定生产者的配置文件 测试结果 67164 records sent, 13430.1 records/sec (13.12 MB/sec), 1461.5 ms avg latency, 2014.0 ms max latency. 92910 records sent, 18430.9 records/sec (18.00 MB/sec), 1718.4 ms avg latency, 2038.0 ms max latency. 120060 records sent, 23741.3 records/sec (23.18 MB/sec), 1304.2 ms avg latency, 1688.0 ms max latency. 110055 records sent, 21853.7 records/sec (21.34 MB/sec), 1428.9 ms avg latency, 1660.0 ms max latency. 119745 records sent, 23949.0 records/sec (23.39 MB/sec), 1281.9 ms avg latency, 1692.0 ms max latency. 110910 records sent, 22182.0 records/sec (21.66 MB/sec), 1383.2 ms avg latency, 1516.0 ms max latency. 135255 records sent, 27051.0 records/sec (26.42 MB/sec), 1191.1 ms avg latency, 1580.0 ms max latency. 122490 records sent, 24498.0 records/sec (23.92 MB/sec), 1214.0 ms avg latency, 1527.0 ms max latency. 1000000 records sent, 22281.639929 records/sec (21.76 MB/sec), 1332.82 ms avg latency, 2038.00 ms max latency, 1357 ms 50th, 1822 ms 95th, 1955 ms 99th, 2010 ms 99.9th. # records sent：表示测试时发送的消息总数 # records sec：表示以每秒发迭的消息数来统计吞吐量，括号中 MB/sec 表示以每秒发送的消息大小来统计吞吐量， # avg latency: 表示消息处理的平均耗时 # max latency：表示消息处理的最大耗时 # 50th,95th,99t,99.9th： 分别表示 50% 95 99% 99.9% 的消息处理耗时。 # 以下为测试完成后的Metrics信息 Metric Name Value app-info:commit-id:{client-id=producer-1} : 66563e712b0b9f84 app-info:start-time-ms:{client-id=producer-1} : 1608628521735 app-info:version:{client-id=producer-1} : 2.5.0 kafka-metrics-count:count:{client-id=producer-1} : 102.000 producer-metrics:batch-size-avg:{client-id=producer-1} : 15555.699 producer-metrics:batch-size-max:{client-id=producer-1} : 15570.000 producer-metrics:batch-split-rate:{client-id=producer-1} : 0.000 producer-metrics:batch-split-total:{client-id=producer-1} : 0.000 producer-metrics:buffer-available-bytes:{client-id=producer-1} : 33554432.000 producer-metrics:buffer-exhausted-rate:{client-id=producer-1} : 0.000 producer-metrics:buffer-exhausted-total:{client-id=producer-1} : 0.000 producer-metrics:buffer-total-bytes:{client-id=producer-1} : 33554432.000 producer-metrics:bufferpool-wait-ratio:{client-id=producer-1} : 0.861 producer-metrics:bufferpool-wait-time-total:{client-id=producer-1} : 37854962935.000 producer-metrics:compression-rate-avg:{client-id=producer-1} : 1.000 producer-metrics:connection-close-rate:{client-id=producer-1} : 0.000 producer-metrics:connection-close-total:{client-id=producer-1} : 0.000 producer-metrics:connection-count:{client-id=producer-1} : 2.000 producer-metrics:connection-creation-rate:{client-id=producer-1} : 0.044 producer-metrics:connection-creation-total:{client-id=producer-1} : 2.000 producer-metrics:failed-authentication-rate:{client-id=producer-1} : 0.000 producer-metrics:failed-authentication-total:{client-id=producer-1} : 0.000 producer-metrics:failed-reauthentication-rate:{client-id=producer-1} : 0.000 producer-metrics:failed-reauthentication-total:{client-id=producer-1} : 0.000 producer-metrics:incoming-byte-rate:{client-id=producer-1} : 114570.421 producer-metrics:incoming-byte-total:{client-id=producer-1} : 5134588.000 producer-metrics:io-ratio:{client-id=producer-1} : 0.224 producer-metrics:io-time-ns-avg:{client-id=producer-1} : 70608.839 producer-metrics:io-wait-ratio:{client-id=producer-1} : 0.561 producer-metrics:io-wait-time-ns-avg:{client-id=producer-1} : 177047.069 producer-metrics:io-waittime-total:{client-id=producer-1} : 25274885534.000 producer-metrics:iotime-total:{client-id=producer-1} : 10079976609.000 producer-metrics:metadata-age:{client-id=producer-1} : 44.621 producer-metrics:network-io-rate:{client-id=producer-1} : 2975.167 producer-metrics:network-io-total:{client-id=producer-1} : 133344.000 producer-metrics:outgoing-byte-rate:{client-id=producer-1} : 23246321.000 producer-metrics:outgoing-byte-total:{client-id=producer-1} : 1041667644.000 producer-metrics:produce-throttle-time-avg:{client-id=producer-1} : 0.000 producer-metrics:produce-throttle-time-max:{client-id=producer-1} : 0.000 producer-metrics:reauthentication-latency-avg:{client-id=producer-1} : NaN producer-metrics:reauthentication-latency-max:{client-id=producer-1} : NaN producer-metrics:record-error-rate:{client-id=producer-1} : 0.000 producer-metrics:record-error-total:{client-id=producer-1} : 0.000 producer-metrics:record-queue-time-avg:{client-id=producer-1} : 1329.508 producer-metrics:record-queue-time-max:{client-id=producer-1} : 2034.000 producer-metrics:record-retry-rate:{client-id=producer-1} : 0.000 producer-metrics:record-retry-total:{client-id=producer-1} : 0.000 producer-metrics:record-send-rate:{client-id=producer-1} : 22429.068 producer-metrics:record-send-total:{client-id=producer-1} : 1000000.000 producer-metrics:record-size-avg:{client-id=producer-1} : 1110.000 producer-metrics:record-size-max:{client-id=producer-1} : 1110.000 producer-metrics:records-per-request-avg:{client-id=producer-1} : 15.000 producer-metrics:request-latency-avg:{client-id=producer-1} : 3.289 producer-metrics:request-latency-max:{client-id=producer-1} : 305.000 producer-metrics:request-rate:{client-id=producer-1} : 1487.617 producer-metrics:request-size-avg:{client-id=producer-1} : 15623.765 producer-metrics:request-size-max:{client-id=producer-1} : 15639.000 producer-metrics:request-total:{client-id=producer-1} : 66672.000 producer-metrics:requests-in-flight:{client-id=producer-1} : 0.000 producer-metrics:response-rate:{client-id=producer-1} : 1487.650 producer-metrics:response-total:{client-id=producer-1} : 66672.000 producer-metrics:select-rate:{client-id=producer-1} : 3169.020 producer-metrics:select-total:{client-id=producer-1} : 142758.000 producer-metrics:successful-authentication-no-reauth-total:{client-id=producer-1} : 0.000 producer-metrics:successful-authentication-rate:{client-id=producer-1} : 0.000 producer-metrics:successful-authentication-total:{client-id=producer-1} : 0.000 producer-metrics:successful-reauthentication-rate:{client-id=producer-1} : 0.000 producer-metrics:successful-reauthentication-total:{client-id=producer-1} : 0.000 producer-metrics:waiting-threads:{client-id=producer-1} : 0.000 producer-node-metrics:incoming-byte-rate:{client-id=producer-1, node-id=node--1} : 12.562 producer-node-metrics:incoming-byte-rate:{client-id=producer-1, node-id=node-2} : 115053.336 producer-node-metrics:incoming-byte-total:{client-id=producer-1, node-id=node--1} : 563.000 producer-node-metrics:incoming-byte-total:{client-id=producer-1, node-id=node-2} : 5134025.000 producer-node-metrics:outgoing-byte-rate:{client-id=producer-1, node-id=node--1} : 2.254 producer-node-metrics:outgoing-byte-rate:{client-id=producer-1, node-id=node-2} : 23345828.974 producer-node-metrics:outgoing-byte-total:{client-id=producer-1, node-id=node--1} : 101.000 producer-node-metrics:outgoing-byte-total:{client-id=producer-1, node-id=node-2} : 1041667543.000 producer-node-metrics:request-latency-avg:{client-id=producer-1, node-id=node--1} : NaN producer-node-metrics:request-latency-avg:{client-id=producer-1, node-id=node-2} : 3.289 producer-node-metrics:request-latency-max:{client-id=producer-1, node-id=node--1} : NaN producer-node-metrics:request-latency-max:{client-id=producer-1, node-id=node-2} : 305.000 producer-node-metrics:request-rate:{client-id=producer-1, node-id=node--1} : 0.045 producer-node-metrics:request-rate:{client-id=producer-1, node-id=node-2} : 1494.039 producer-node-metrics:request-size-avg:{client-id=producer-1, node-id=node--1} : 50.500 producer-node-metrics:request-size-avg:{client-id=producer-1, node-id=node-2} : 15624.232 producer-node-metrics:request-size-max:{client-id=producer-1, node-id=node--1} : 51.000 producer-node-metrics:request-size-max:{client-id=producer-1, node-id=node-2} : 15639.000 producer-node-metrics:request-total:{client-id=producer-1, node-id=node--1} : 2.000 producer-node-metrics:request-total:{client-id=producer-1, node-id=node-2} : 66670.000 producer-node-metrics:response-rate:{client-id=producer-1, node-id=node--1} : 0.045 producer-node-metrics:response-rate:{client-id=producer-1, node-id=node-2} : 1494.073 producer-node-metrics:response-total:{client-id=producer-1, node-id=node--1} : 2.000 producer-node-metrics:response-total:{client-id=producer-1, node-id=node-2} : 66670.000 producer-topic-metrics:byte-rate:{client-id=producer-1, topic=perf-producer-test2} : 23259932.490 producer-topic-metrics:byte-total:{client-id=producer-1, topic=perf-producer-test2} : 1037067350.000 producer-topic-metrics:compression-rate:{client-id=producer-1, topic=perf-producer-test2} : 1.000 producer-topic-metrics:record-error-rate:{client-id=producer-1, topic=perf-producer-test2} : 0.000 producer-topic-metrics:record-error-total:{client-id=producer-1, topic=perf-producer-test2} : 0.000 producer-topic-metrics:record-retry-rate:{client-id=producer-1, topic=perf-producer-test2} : 0.000 producer-topic-metrics:record-retry-total:{client-id=producer-1, topic=perf-producer-test2} : 0.000 producer-topic-metrics:record-send-rate:{client-id=producer-1, topic=perf-producer-test2} : 22428.565 producer-topic-metrics:record-send-total:{client-id=producer-1, topic=perf-producer-test2} : 1000000.000 三、消费者性能测试 消费主题perf-producer-test中的 100 万条消息 kafka-consumer-perf-test.sh \\ --topic perf-producer-test \\ --messages 1000000 \\ --print-metrics \\ --broker-list localhost:9092 测试结果 | start.time | end.time | data.consumed.in.MB | MB.sec | data.consumed.in.nMsg | nMsg.sec | rebalance.time.ms | fetch.time.ms | fetch.MB.sec | fetch.nMsg.sec ================================================================================================== | 2020-12-22 09:28:31:384 | 2020-12-22 09:28:47:769 | 976.5625 | 59.6010 | 1000000 | 61031.4312 | 1608629311854 | -1608629295469 | -0.0000 | -0.0006 # start.time: 起始运行时间 # end.time: 结束运行时 # data.consumed.in.MB: 消息总量（ 单位为 MB ） # MB.sec: 按字节大小计算的消费吞吐量（ MB单位为 MB ）、 # data.consumed.in.nMsg :消费的消息总数 # nMsg.sec: 按消息个数计算的吞吐量 # rebalance.time.ms：再平衡的时间,单位为ms # fetch.time.ms：拉取消息的持续时间，单位为ms # fetch.MB.sec: 每秒拉取消息的字节大小，单位MB # fetch.nMsg.sec: 每秒拉取消息的个数 ,其中 fetch.time. s= end.time - start.time - rebalance.time.ms a Metric Name Value consumer-coordinator-metrics:assigned-partitions:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-coordinator-metrics:commit-latency-avg:{client-id=consumer-perf-consumer-89119-1} : 11.750 consumer-coordinator-metrics:commit-latency-max:{client-id=consumer-perf-consumer-89119-1} : 30.000 consumer-coordinator-metrics:commit-rate:{client-id=consumer-perf-consumer-89119-1} : 0.097 consumer-coordinator-metrics:commit-total:{client-id=consumer-perf-consumer-89119-1} : 4.000 consumer-coordinator-metrics:failed-rebalance-rate-per-hour:{client-id=consumer-perf-consumer-89119-1} : 77.983 consumer-coordinator-metrics:failed-rebalance-total:{client-id=consumer-perf-consumer-89119-1} : 1.000 consumer-coordinator-metrics:heartbeat-rate:{client-id=consumer-perf-consumer-89119-1} : 0.116 consumer-coordinator-metrics:heartbeat-response-time-max:{client-id=consumer-perf-consumer-89119-1} : 9.000 consumer-coordinator-metrics:heartbeat-total:{client-id=consumer-perf-consumer-89119-1} : 5.000 consumer-coordinator-metrics:join-rate:{client-id=consumer-perf-consumer-89119-1} : 0.022 consumer-coordinator-metrics:join-time-avg:{client-id=consumer-perf-consumer-89119-1} : 32.000 consumer-coordinator-metrics:join-time-max:{client-id=consumer-perf-consumer-89119-1} : 32.000 consumer-coordinator-metrics:join-total:{client-id=consumer-perf-consumer-89119-1} : 1.000 consumer-coordinator-metrics:last-heartbeat-seconds-ago:{client-id=consumer-perf-consumer-89119-1} : 1.000 consumer-coordinator-metrics:last-rebalance-seconds-ago:{client-id=consumer-perf-consumer-89119-1} : 16.000 consumer-coordinator-metrics:partition-assigned-latency-avg:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-coordinator-metrics:partition-assigned-latency-max:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-coordinator-metrics:partition-lost-latency-avg:{client-id=consumer-perf-consumer-89119-1} : NaN consumer-coordinator-metrics:partition-lost-latency-max:{client-id=consumer-perf-consumer-89119-1} : NaN consumer-coordinator-metrics:partition-revoked-latency-avg:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-coordinator-metrics:partition-revoked-latency-max:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-coordinator-metrics:rebalance-latency-avg:{client-id=consumer-perf-consumer-89119-1} : 101.000 consumer-coordinator-metrics:rebalance-latency-max:{client-id=consumer-perf-consumer-89119-1} : 101.000 consumer-coordinator-metrics:rebalance-latency-total:{client-id=consumer-perf-consumer-89119-1} : 101.000 consumer-coordinator-metrics:rebalance-rate-per-hour:{client-id=consumer-perf-consumer-89119-1} : 78.086 consumer-coordinator-metrics:rebalance-total:{client-id=consumer-perf-consumer-89119-1} : 1.000 consumer-coordinator-metrics:sync-rate:{client-id=consumer-perf-consumer-89119-1} : 0.022 consumer-coordinator-metrics:sync-time-avg:{client-id=consumer-perf-consumer-89119-1} : 24.000 consumer-coordinator-metrics:sync-time-max:{client-id=consumer-perf-consumer-89119-1} : 24.000 consumer-coordinator-metrics:sync-total:{client-id=consumer-perf-consumer-89119-1} : 1.000 consumer-fetch-manager-metrics:bytes-consumed-rate:{client-id=consumer-perf-consumer-89119-1, topic=perf-producer-test2} : 22454094.6 consumer-fetch-manager-metrics:bytes-consumed-rate:{client-id=consumer-perf-consumer-89119-1} : 22453606.095 consumer-fetch-manager-metrics:bytes-consumed-total:{client-id=consumer-perf-consumer-89119-1, topic=perf-producer-test2}：100602.000 consumer-fetch-manager-metrics:bytes-consumed-total:{client-id=consumer-perf-consumer-89119-1} : 1033000602.000 consumer-fetch-manager-metrics:fetch-latency-avg:{client-id=consumer-perf-consumer-89119-1} : 13.353 consumer-fetch-manager-metrics:fetch-latency-max:{client-id=consumer-perf-consumer-89119-1} : 288.000 consumer-fetch-manager-metrics:fetch-rate:{client-id=consumer-perf-consumer-89119-1} : 21.633 consumer-fetch-manager-metrics:fetch-size-avg:{client-id=consumer-perf-consumer-89119-1, topic=perf-producer-test2} : 1037149.199 consumer-fetch-manager-metrics:fetch-size-avg:{client-id=consumer-perf-consumer-89119-1} : 1037149.199 consumer-fetch-manager-metrics:fetch-size-max:{client-id=consumer-perf-consumer-89119-1, topic=perf-producer-test2} : 1038179.000 consumer-fetch-manager-metrics:fetch-size-max:{client-id=consumer-perf-consumer-89119-1} : 1038179.000 consumer-fetch-manager-metrics:fetch-throttle-time-avg:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-fetch-manager-metrics:fetch-throttle-time-max:{client-id=consumer-perf-consumer-89119-1} : 0.000 consumer-fetch-manager-metrics:fetch-total:{client-id=consumer-perf-consumer-89119-1} : 996.000 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/pxe-kickstart无人值守部署OS.html":{"url":"origin/pxe-kickstart无人值守部署OS.html","title":"PXE网络无人值守自动安装OS","keywords":"","body":"PXE + Kickstart/Autoinstall 一. 网络引导+无人值守部署CentOS、Ubuntu 1、PXE PXE(Pre-boot Execution Environment，预启动执行环境)是由Intel公司开发的最新技术，工作于Client/Server的网络模式，支持工作站通过网络从远端服务器下载映像，并由此支持通过网络启动作系统，在启动过程中，终端要求服务器分配IP地址，再用TFTP（trivial file transfer protocol）或MTFTP(multicast trivial file transfer protocol)协议下载一个启动软件包到本机内存中执行，由这个启动软件包完成终端基本软件设置，从而引导预先安装在服务器中的终端操作系统。 严格来说，PXE 并不是一种安装方式，而是一种引导方式。进行 PXE 安装的必要条件是在要安装的计算机中必须包含一个 PXE 支持的网卡（NIC），即网卡中必须要有 PXE Client。PXE 协议可以使算机通过网络启动。此协议分为 Client端和 Server 端，而PXE Client则在网卡的 ROM 中。当计算机引导时，BIOS 把 PXE Client 调入内存中执行，然后由 PXE Client 将放置在远端的文件通过网络下载到本地运行。运行 PXE 协议需要设置 DHCP 服务器和 TFTP 服务器。DHCP 服务器会给 PXE Client（将要安装系统的主机）分配一个 IP 地址，由于是给 PXE Client 分配 IP 地址，所以在配置 DHCP 服务器时需要增加相应的 PXE 设置。此外，在 PXE Client 的 ROM 中，已经存在了 TFTP Client，那么它就可以通过 TFTP 协议到 TFTP Server 上下载所需的文件了。 2、PXE工作流程 ① PXE Client 从自己的PXE网卡启动，向本网络中的DHCP服务器索取IP ② DHCP 服务器返回分配给客户机的IP 以及PXE文件的放置位置(该文件一般是放在一台TFTP服务器上) ③ PXE Client 向本网络中的TFTP服务器索取pxelinux.0 文件 ④ PXE Client 取得pxelinux.0 文件后之执行该文件 ⑤ 根据pxelinux.0 的执行结果，通过TFTP服务器加载内核和文件系统 ⑥ 进入安装画面, 此时可以通过选择HTTP、FTP、NFS 方式之一进行安装 详细工作流程，请参考下面这幅图： [17-Jun-24 13:58:37] Client 192.168.1.25:2070 /data/TFTP/pxelinux.0, Error 0 at Client, TFTP Aborted [17-Jun-24 13:58:37] Client 192.168.1.25:2071 /data/TFTP/pxelinux.0, 20 Blocks Served [17-Jun-24 13:58:37] Client 192.168.1.25:49152 /data/TFTP/pxelinux.cfg/564d4f19-fe3b-5ae0-f81a-ed57dfa1f0ae, No Such File/No Access [17-Jun-24 13:58:37] Client 192.168.1.25:49153 /data/TFTP/pxelinux.cfg/01-00-0c-29-a1-f0-ae, No Such File/No Access [17-Jun-24 13:58:37] Client 192.168.1.25:49154 /data/TFTP/pxelinux.cfg/C0A80119, No Such File/No Access [17-Jun-24 13:58:37] Client 192.168.1.25:49155 /data/TFTP/pxelinux.cfg/C0A8011, No Such File/No Access [17-Jun-24 13:58:37] Client 192.168.1.25:49156 /data/TFTP/pxelinux.cfg/C0A801, No Such File/No Access [17-Jun-24 13:58:37] Client 192.168.1.25:49157 /data/TFTP/pxelinux.cfg/C0A80, No Such File/No Access [17-Jun-24 13:58:37] Client 192.168.1.25:49158 /data/TFTP/pxelinux.cfg/C0A8, No Such File/No Access [17-Jun-24 13:58:37] Client 192.168.1.25:49159 /data/TFTP/pxelinux.cfg/C0A, No Such File/No Access [17-Jun-24 13:58:37] Client 192.168.1.25:49160 /data/TFTP/pxelinux.cfg/C0, No Such File/No Access [17-Jun-24 13:58:37] Client 192.168.1.25:49161 /data/TFTP/pxelinux.cfg/C, No Such File/No Access [17-Jun-24 13:58:37] Client 192.168.1.25:49162 /data/TFTP/pxelinux.cfg/default, 4 Blocks Served [17-Jun-24 13:58:37] Client 192.168.1.25:49163 /data/TFTP/boot.msg, No Such File/No Access [17-Jun-24 13:58:37] Client 192.168.1.25:49164 /data/TFTP/vesamenu.c32, 110 Blocks Served [17-Jun-24 13:58:37] Client 192.168.1.25:49165 /data/TFTP/pxelinux.cfg/default, 4 Blocks Served [17-Jun-24 13:58:37] Client 192.168.1.25:49166 /data/TFTP/splash.jpg, No Such File/No Access [17-Jun-24 13:58:42] Client 192.168.1.25:49167 /data/TFTP/Ubuntu/20.04.6/vmlinuz, 9718 Blocks Served [17-Jun-24 13:58:54] Client 192.168.1.25:49168 /data/TFTP/Ubuntu/20.04.6/initrd, 62731 Blocks Served 3、CentOS系无人值守Kickstart简介 Kickstart是一种无人值守的安装方式。它的工作原理是在安装过程中记录典型的需要人工干预填写的各种参数，并生成一个名为ks.cfg的文件。如果在安装过程中（不只局限于生成Kickstart安装文件的机器）出现要填写参数的情况，安装程序首先会去查找Kickstart生成的文件，如果找到合适的参数，就采用所找到的参数；如果没有找到合适的参数，便需要安装者手工干预了。所以，如果Kickstart文件涵盖了安装过程中可能出现的所有需要填写的参数，那么安装者完全可以只告诉安装程序从何处取ks.cfg文件，然后就去忙自己的事情。等安装完毕，安装程序会根据ks.cfg中的设置重启系统，并结束安装。 4、Ubuntu系无人值守Autoinstall简介 ToDo 二、PXE基础服务DHCP+TFTP 1、DHCP 服务 ①安装dhcp yum install -y dhcp ②配置dhcp /etc/dhcp/dhcpd.conf Local_ip=$(ip -4 addr | grep inet | grep -E '192(\\.[0-9]{1,3}){3}' | cut -d '/' -f 1 | grep -oE '[0-9]{1,3}(\\.[0-9]{1,3}){3}') && \\ bash -c 'cat >/etc/dhcp/dhcpd.conf ③服务启动验证 服务端口为67 systemctl start dhcpd && \\ systemctl enable dhcpd && \\ systemctl status dhcpd && \\ ss -nulp | grep dhcpd 2、配置TFTP服务 ①安装TFTP yum install -y tftp-server tftp xinetd net-tools ②配置服务 /etc/xinetd.d/tftp bash -c 'cat >/etc/xinetd.d/tftp ③准备pxelinux等相关文件 在 PXE（预启动执行环境）引导过程中，使用 syslinux 引导程序可以加载内核和初始 RAM 文件系统（initrd）以启动安装程序 yum install -y syslinux pxelinux.0：syslinux 的 PXE 引导程序，通常位于 /usr/share/syslinux/ 目录中。 vesamenu.c32：syslinux 的必备文件，通常位于 /usr/share/syslinux/ 目录中。 menu.c32 文本模式：menu.c32 是一个简单的文本模式菜单模块。 兼容性高：由于它是纯文本模式，它可以在大多数硬件上工作，不需要特殊的图形驱动支持。 外观简单：菜单是基于文本的，外观和样式非常简单，没有图形界面。 vesamenu.c3s2 图形模式：vesamenu.c32 是一个图形模式菜单模块，支持使用 VESA 图形模式显示引导菜单。 图形界面：提供更丰富的视觉效果，支持背景图片、图标和更复杂的菜单布局。 依赖图形支持：需要硬件和 BIOS 支持 VESA 图形模式。如果某些旧硬件或 BIOS 不支持 VESA 图形模式，则可能无法使用。 cp /usr/share/syslinux/{pxelinux.0,vesamenu.c32} /var/lib/tftpboot/ tree -phL 2 /var/lib/tftpboot/ ├── [-rw-r--r-- 26K] pxelinux.0 # PXE 引导文件 ├── [-rw-r--r-- 149K] vesamenu.c32 # 系统自带的两种图形模块之一，输出文字带颜色。menu.c32也可以，才50多K，但输出比较单一 ④准备待引导系统的内核与临时文件系统文件 从安装OS介质iso文件中提取内核和 initrd 文件。CentOS/Redhat的在ISO文件isolinux/路径下。Ubuntu/Debian的在ISO文件casper/路径下。 vmlinuz： Linux 内核的压缩可执行映像文件。一个使用虚拟内存的 Linux 内核。 加载内核：vmlinuz 文件包含了操作系统的核心部分，它被加载到内存中并开始执行。 初始化硬件：在加载过程中，内核会初始化硬件设备并配置系统环境，为后续的操作系统启动过程做准备。 initrd.img： initial ramdisk是一个包含临时根文件系统的镜像文件，内核在启动时使用它来挂载根文件系统。 临时根文件系统：initrd.img 文件提供了一个临时的根文件系统，允许内核在启动时加载必要的模块和驱动程序。 加载驱动和模块：在启动过程中，内核会从 initrd.img 中加载所需的驱动程序和模块，以便访问磁盘、网络等设备。 切换根文件系统：在启动完成后，内核会从 initrd.img 切换到实际的根文件系统（通常从硬盘或其他永久存储设备）。 mkdir -p /var/lib/tftpboot/{CentOS,Ubuntu} mkdir -p /mnt/{CentOS,Ubuntu} mount -t iso9660 ubuntu-22.04-live-server-amd64.iso /mnt/Ubuntu mount -t iso9660 CentOS-7.9-x86_64-Everything-2009.iso /mnt/CentOS mkdir -p /var/lib/tftpboot/{CentOS/7.9.2009,Ubuntu/22.04} cp /mnt/Ubuntu/casper/{vmlinuz,initrd} /var/lib/tftpboot/Ubuntu/22.04/ cp /mnt/CentOS/isolinux/{vmlinuz,initrd.img} /var/lib/tftpboot/CentOS/7.9.2009/ umount /mnt/CentOS /mnt/Ubuntu ⑤编写引导配置文件 PXE 配置文件通常位于 TFTP 服务器的配置目录下，例如 /var/lib/tftpboot/pxelinux.cfg/。在该目录下创建或编辑默认配置文件（通常命名为 default 或 01-xx-xx-xx-xx-xx-xx，其中 xx-xx-xx-xx-xx-xx 是 MAC 地址）： mkdir /var/lib/tftpboot/pxelinux.cfg bash -c 'cat >/var/lib/tftpboot/pxelinux.cfg/default Install CentOS 7.9.2009 x86_64 menu default kernel CentOS/7.9.2009/vmlinuz append initrd=CentOS/7.9.2009/initrd.img text ks=http://192.168.1.1/CentOS/CentOS7.9.2009.cfg inst.stage2=http://192.168.1.1:8089/CentOS label 'Ubuntu 22.04 LTS' menu label ^2> Install Ubuntu 22.04 LTS kernel Ubuntu/22.04/vmlinuz append initrd=Ubuntu/22.04/initrd autoinstall ds=nocloud-net;s=http://192.168.1.1/Ubuntu/22.04.autoinstall EOF' ⑤服务启动验证 服务端口为UDP 69 systemctl start xinetd && \\ systemctl enable xinetd && \\ systemctl status xinetd && \\ ss -unlp | grep 69 && \\ netstat -a | grep tftp && \\ netstat -tunap | grep :69 三、CentOS无人值守Kickstart 准备待引导系统的内核与临时文件系统文件 从安装OS介质iso文件中提取内核和 initrd 文件。CentOS/Redhat的在ISO文件isolinux/路径下。Ubuntu/Debian的在ISO文件casper/路径下。 vmlinuz： Linux 内核的压缩可执行映像文件。一个使用虚拟内存的 Linux 内核。 加载内核：vmlinuz 文件包含了操作系统的核心部分，它被加载到内存中并开始执行。 初始化硬件：在加载过程中，内核会初始化硬件设备并配置系统环境，为后续的操作系统启动过程做准备。 initrd.img： initial ramdisk是一个包含临时根文件系统的镜像文件，内核在启动时使用它来挂载根文件系统。 临时根文件系统：initrd.img 文件提供了一个临时的根文件系统，允许内核在启动时加载必要的模块和驱动程序。 加载驱动和模块：在启动过程中，内核会从 initrd.img 中加载所需的驱动程序和模块，以便访问磁盘、网络等设备。 切换根文件系统：在启动完成后，内核会从 initrd.img 切换到实际的根文件系统（通常从硬盘或其他永久存储设备）。 mkdir -p /var/lib/tftpboot/{CentOS,Ubuntu} mkdir -p /mnt/{CentOS,Ubuntu} mount -t iso9660 ubuntu-22.04-live-server-amd64.iso /mnt/Ubuntu mount -t iso9660 CentOS-7.9-x86_64-Everything-2009.iso /mnt/CentOS mkdir -p /var/lib/tftpboot/{CentOS/7.9.2009,Ubuntu/22.04} cp /mnt/Ubuntu/casper/{vmlinuz,initrd} /var/lib/tftpboot/Ubuntu/22.04/ cp /mnt/CentOS/isolinux/{vmlinuz,initrd.img} /var/lib/tftpboot/CentOS/7.9.2009/ umount /mnt/CentOS /mnt/Ubuntu ⑤编写引导配置文件 四、Ubuntu无人值守Autoinstall 五、配置 Nginx server { listen 8089 ; error_log logs/nginx-pxekickstart-error.log; access_log logs/nginx-pxekickstart-access.log json_log; location ~ ^/CentOS/\\.cfg$ { root pxekickstart/CentOS; } location ~ ^/CentOS/(treeinfo|\\.treeinfo)$ { alias /pxekickstart/CentOS/treeinfo; } location /CentOS/LiveOS { alias pxekickstart/CentOS/LiveOS; } } PXE主机： 主机名 IP地址 OS 路由器 pk.tools.curiouser.com 192.168.1.80 CentOS 7.9.2009 192.168.1.1 1、基础准备 上传ISO文件并挂载，关闭Firewall和SELinux OS_distribution=CentOS && \\ OS_arch=x86_64 && \\ OS_version=7.9.2009 && \\ OS_mod=Everything && \\ Local_ip=$(ip -4 addr | grep inet | grep -E '192(\\.[0-9]{1,3}){3}' | cut -d '/' -f 1 | grep -oE '[0-9]{1,3}(\\.[0-9]{1,3}){3}') && \\ ISO_filename=$OS_distribution-${OS_version%%.*}-$OS_arch-$OS_mod-${OS_version##*.}.iso && \\ yum install -y wget && \\ mkdir -p /mnt/{cdrom/$OS_distribution/$OS_version,iso/$OS_distribution} && \\ echo \"/mnt/iso/$OS_distribution/$ISO_filename /mnt/cdrom/$OS_distribution/$OS_version iso9660 defaults,loop 0 0\" >> /etc/fstab && \\ mount -a && \\ df -mh && \\ setenforce 0 && \\ sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config && \\ systemctl stop firewalld && \\ systemctl disable firewalld && \\ systemctl stop firewalld 2、配置HTTD服务 安装服务 yum install -y httpd 配置服务 OS_distribution=CentOS && \\ OS_version=7.9.2009 && \\ ln -s /mnt/cdrom/$OS_distribution/$OS_version /var/www/html/$OS_distribution$OS_version 启动验证服务，服务端口tcp:80 systemctl start httpd && \\ systemctl enable httpd && \\ systemctl status httpd && \\ ss -tnl | grep 80 拷贝文件 cp /mnt/cdrom/$OS_distribution/$OS_version/images/pxeboot/vmlinuz /var/lib/tftpboot/CentOS/vmlinuz-${OS_version%.*} && \\ cp /mnt/cdrom/$OS_distribution/$OS_version/images/pxeboot/initrd.img /var/lib/tftpboot/CentOS/initrd-${OS_version%.*}.img && \\ cp /mnt/cdrom/$OS_distribution/$OS_version/isolinux/{vesamenu.c32,boot.msg,splash.png} /var/lib/tftpboot/ && \\ cp /usr/share/syslinux/{chain.c32,mboot.c32,menu.c32,memdisk} /var/lib/tftpboot/ /var/lib/tftpboot/目录结构 tree -phL 2 /var/lib/tftpboot/ ├── [-rw-r--r-- 84] boot.msg # 窗口提示信息文件,提示信息在菜单出现前出现，显示时间较短，可以添加些艺术字之类的信息。 ├── [-rw-r--r-- 20K] chain.c32 ├── [-rw-r--r-- 33K] mboot.c32 ├── [-rw-r--r-- 26K] memdisk ├── [-rw-r--r-- 54K] menu.c32 # 系统自带的两种图形模块之一，不能自定义背景图片 ├── [-rw-r--r-- 26K] pxelinux.0 ├── [drwxr-xr-x 21] pxelinux.cfg # 启动菜单目录 ├── [-rw-r--r-- 186] splash.png # 背景图片 ├── [-rw-r--r-- 149K] vesamenu.c32 # 系统自带的两种图形模块之一 ├── [drwxr-xr-x 21] CentOS ├── [-rwxr-xr-x 5.9M] vmlinuz-7.5 # CentOS 7.5.1804的内核文件 ├── [-rw-r--r-- 50M] initrd-7.5.img # 这是一个初始化文件，一个最小的系统镜像 创建/var/lib/tftpboot/pxelinux.cfg/default （default文件参数详见：PXE引导配置文件参数详解） 6、准备安装过程使用到的文件 ①内核升级文件 mkdir -p /var/www/html/tools && \\ yum install -y yum-plugin-downloadonly && \\ rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org && \\ rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm && \\ yum makecache && \\ yum --enablerepo=elrepo-kernel install --downloadonly --downloaddir=/var/www/html/tools kernel-ml ②安装oh-my-zsh的脚本 wget https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh -P /var/www/html/tools chmod +x /var/www/html/tools/install.sh 7、创建KS文件 OS_distribution=CentOS && \\ OS_version=7.9.2009 && \\ touch /var/www/html/${OS_distribution}${OS_version}.cfg 方式一：手动编写(KS文件具体参数详情见笔记：Kickstart文件参数详解) 方式二：使用system-config-kickstart图形化界面配置 安装：system-config-kickstart yum install -y system-config-kickstart 8、验证KS文件的语法正确性 yum install -y pykickstart ksvalidator /var/www/html/CentOS7.5.1804.cfg 9、样板KS文件 pxe-kickstart-CentOS7.cfg install text lang en_US.UTF-8 keyboard us auth --useshadow --passalgo=sha512 url --url=\"http://192.168.1.102/CentOS7.9.2009\" rootpw --iscrypted $1$6/87AF3n$eczKeiNRBv7H.GXnur1Ld/ selinux --disabled firewall --disabled network --bootproto=dhcp --device=ens192 --ipv6=auto --activate network --hostname=test reboot timezone Asia/Shanghai --isUtc --nontp bootloader --location=mbr --boot-drive=sda clearpart --all --drives=sda services --enabled=NetworkManager,sshd firstboot --enable ignoredisk --only-use=sda #(可选)autopart --type=lvm --fstype=xfs part /boot --fstype=\"xfs\" --ondisk=sda --size=200 part / --fstype=\"xfs\" --ondisk=sda --size=30720 part /opt --fstype=\"xfs\" --ondisk=sda --size=10240 part /var --fstype=\"xfs\" --grow --ondisk=sda --size=1 %packages @^minimal @core wget curl zsh tree git vim nc unzip net-tools bind-utils %end %post --interpreter=/bin/bash --log=/tmp/post-install.log --erroronfail echo \"====prepare to optimize system parameter====\" >> /tmp/post-install.log sysctl -w fs.inotify.max_user_watches=81920 echo \"fs.inotify.max_user_watches=81920\" >> /etc/sysctl.conf echo \"====prepare to replace yum mirror to ustc====\" >> /tmp/post-install.log mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak cat >> /etc/yum.repos.d/ustc.repo /dev/null yum makecache > /dev/null yum install -y epel-release > /dev/null yum install -y jq > /dev/null echo \"====prepare to update kernel====\" >> /tmp/post-install.log rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org && rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm && rpm -Uvh http://192.168.1.102/kernel-ml-5.17.6-1.el7.elrepo.x86_64.rpm && upgrage_kernel=`awk -F \\' '$1==\"menuentry \" {print $2}' /etc/grub2.cfg | head -n 1` && grub2-set-default 0 echo \"====prepare to install zsh and oh-my-zsh====\" >> /tmp/post-install.log chsh -s /bin/zsh root > /dev/null git config --global http.proxy http://192.168.1.102:1088 git config --global https.proxy http://192.168.1.102:1088 sh -c \"$(wget --timeout=1 --tries=5 --retry-connrefused -O- http://192.168.1.102/install.sh) --skip-chsh\" git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting cat >> /root/.zshrc > /etc/hosts echo \" #######################\" >> /etc/motd echo \" # Keep Your Curiosity #\" >> /etc/motd echo \" #######################\" >> /etc/motd %end 四、自动安装配置脚本 前提：CentOS-7-x86_64-Everything-1804.iso已经放置在/mnt/iso文件夹下 mkdir /mnt/cdrom && \\ echo \"/mnt/iso/CentOS-7-x86_64-Everything-1804.iso /mnt/cdrom iso9660 defaults,loop 0 0\" >> /etc/fstab && \\ mount -a && \\ df -mh && \\ setenforce 0 && \\ sed -i \"s/SELINUX=enforcing/SELINUX=disabled/\" /etc/selinux/config && \\ systemctl stop firewalld && \\ systemctl disable firewalld && \\ systemctl stop firewalld && \\ yum install -y httpd dhcp tftp-server tftp xinetd net-tools syslinux tree && \\ ln -s /mnt/cdrom/ /var/www/html/CentOS7 && \\ systemctl start httpd && \\ systemctl enable httpd && \\ systemctl status httpd && \\ ss -tnl | grep 80 && \\ bash -c 'cat >> /etc/dhcp/dhcpd.conf /etc/xinetd.d/tftp /var/lib/tftpboot/pxelinux.cfg/default 五、测试安装 自动化安装过程中还可以进入一个临时Shell，而postinstall相关命令脚本路径在/mnt/sysimage/tmp路径下 参考 https://docs.centos.org/en-US/centos/install-guide/pxe-server/#chap-installation-server-setup https://blog.csdn.net/yanghua1012/article/details/80426659 https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/6/html/installation_guide/ch-boot-x86#sn-boot-menu-x86 http://www.178linux.com/99307 https://blog.51cto.com/lzhnb/2117618 https://marclop.svbtle.com/creating-an-automated-centos-7-install-via-kickstart-file https://docs.centos.org/en-US/centos/install-guide/Kickstart2/#sect-kickstart-file-create https://www.cnblogs.com/cloudos/p/8143929.html http://bbs.51cto.com/thread-621450-1.html https://wiki.centos.org/zh/HowTos/PXE/PXE_Setup/Menus Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-18 13:54:57 "},"origin/pxe-引导配置文件参数详解.html":{"url":"origin/pxe-引导配置文件参数详解.html","title":"PXE引导配置文件详解","keywords":"","body":"# 默认启动的是 'label ks' 中标记的启动内核 default ks # 显示 'boot: ' 提示符。为 '0' 时则不提示，将会直接启动 'default' 参数中指定的内容。 prompt 1 # 在用户输入之前的超时时间，单位为 1/10 秒，60则为6秒 timeout 60 # 显示某个文件的内容，注意文件的路径。默认是在/var/lib/tftpboot/ 目录下。也可以指定位类似 '/install/boot.msg'这样的，路径+文件名。 display boot.msg # 按下F1键后显示的文件，可以设置F1~F12 F1 boot.msg F2 options.msg F3 general.msg F4 param.msg F5 rescue.msg # label指定你在 'boot:' 提示符下输入的关键字，比如boot: linux[ENTER]，这个会启动'label linux'下标记的kernel和initrd.img 文件。 label linux menu label ^1> Install CentOS 7.9.2009 x86_64 menu default # kernel 指定启动内核文件的路径，路径是相对于/var/lib/tftpboot/的。仅支持从 TFTP 协议服务中加载该文件，不支持HTTP协议等其他协议服务。 kernel images/CentOS-7/vmlinuz-7.9 # append: 指定追加给内核的参数，能够在grub 里使用的追加给内核的参数，在这里也都可以使用。 # initrd: 指定初始RAM磁盘文件的位置。默认根路径：/var/lib/tftpboot/。不支持HTTP协议文件路径。 # ks： 指定CentOS/Redhat发行版本的无人值守 kickstart配置文件的路径。支持从NFS, HTTPS, HTTP, FTP协议服务中获取该文件 append initrd=images/CentOS-7/initrd-7.9.img ks=http://192.168.1.1/CentOS/7.9.2009.cfg label text kernel vmlinuz append initrd=initrd.img text label ks kernel vmlinuz # 告诉系统，从哪里获取ks.cfg文件 append label local localboot 1 label memtest86 kernel memtest append - 示例 default vesamenu.c32 prompt 0 timeout 60 display boot.msg menu background splash.jpg menu title ########## Curiouser PXE Boot Menu ########## label CentOS7.5.1804 menu label ^1> Install CentOS 7.5.1804 x86_64 kernel vmlinuz-7.5 append initrd=initrd-7.5.img text ks=http://192.168.1.80/CentOS7.5.1804.cfg label CentOS7.7.1908 menu label ^2> Install CentOS 7.7.1908 x86_64 menu default kernel vmlinuz-7.7 append initrd=initrd-7.7.img text ks=http://192.168.1.80/CentOS7.7.1908.cfg 参考 https://docs.centos.org/en-US/centos/install-guide/pxe-server/#chap-installation-server-setup https://blog.51cto.com/4690837/2333165 https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/6/html/installation_guide/ch-boot-x86#sn-boot-menu-x86 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-14 18:11:43 "},"origin/pxe-kickstart文件参数详解.html":{"url":"origin/pxe-kickstart文件参数详解.html","title":"CentOS/Readhat: Kickstart无人值守配置文件详解","keywords":"","body":"一、kickstart文件简介 kickstart文件中各部分(section)要遵循一定的顺序。每个部分中的项(Item)并不需要按照一定的顺序排列，除非有其他要求。各部分的顺序如下： 命令部分 %packages部分 %pre, %post, 以及%traceback部分 -- 这些部分的顺序可以任意排列 %packages, %pre, %post以及%traceback部分需要以%end结束。 不要求的项(Item)可以被省略。 省略任何一个被要求的项将会导致安装程序向用户询问相关的问题，就像典型安装过程向用户询问那样。一旦用户给出了答案，安装过程将会继续自动进行，除非又遇到缺失的项。 以(#)开头的行作为注释行被忽略。 如果在kickstart安装中使用了不推荐的命令、选项或者语法，警告日志将会被记录到anaconda日志中。因为在一个或者两个发行版之间这些不推荐的项经常会被删掉，所以检查安装日志以确保没有使用这些项非常必要。当使用ksvalidator的时候，这些不推荐的项会导致错误。 如果选项后接等号（=），则必须指定一个值 引用磁盘的特殊说明： Kickstart一直通过设备节点名(例如 sda)来引用磁盘。Linux内核采用了更加动态的方法，设备名并不会在重启时保持不变。因此，这会使得在Kickstart脚本中引用磁盘变得复杂。为了满足稳定的设备命名，你可以在项(Item)中使用/dev/disk代替设备名。例如，你可以使用： part / --fstype=ext4 --onpart=/dev/disk/by-path/pci-0000:00:05.0-scsi-0:0:0:0-part1 part / --fstype=ext4 --onpart=/dev/disk/by-id/ata-ST3160815AS_6RA0C882-part1 来代替： part / --fstype=ext4 --onpart=sda1 这种方式提供了对磁盘的持久引用，因而比仅仅使用sda更加有意义。 这在大的存储环境中特别有意义。你也可以使用类似于shell的入口来应用磁盘。这种方式主要用来简化大的存储环境中clearpart以及ignoredisk命令的使用。例如，为了替代： ignoredisk --drives=sdaa,sdab,sdac 你可以使用如下的入口： ignoredisk --drives=/dev/disk/by-path/pci-0000:00:05.0-scsi-* 最后，如果想要在任何地方引用已经存在的分区或者文件系统（例如，在part --ondisk=中），你可以通过文件系统标签(label)或者UUID来进行。例如： part /data --ondisk=LABEL=data part /misc --ondisk=UUID=819ff6de-0bd6-4bf4-8b72-dbe41033a85b 二、必需选项 1. bootloader ：设置引导程序 引申Bootloader相关概念：CentOS系统启动流程 # 建议给Bootloader设置密码以防止黑客修改系统启动项或者不授权登录系统 --append= 指定内核参数，要指定多个参数，使用空格分隔。引导程序默认的参数是\"rhgb quiet\"。举例: \"bootloader --location=mbr --append=\"hdd=ide-scsi ide=nodma\" --boot-drive= 指定安装bootloader到哪个磁盘上 --leavebootorder 防止安装程序更改 EFI 或者 ISeries/PSeries 系统中的现有可引导映像。 --driveorder= 指定在 BIOS 引导顺序中的首选驱动器。例如: bootloader --driveorder=sda,hda --location= 指定引导记录的写入位置（在大多数情况下不需要指定这个选项），有效值如下 1.mbr 默认选项。具体要看该驱动器是使用主引导记录（MBR）还是 GUID 分区表（GPT）方案： a.在使用 GPT 格式化的磁盘中，这个选项会在 BIOS 引导分区中安装 stage 1.5 引导装载程序。 b.在使用 MBR 格式化的磁盘中，会在 MBR 与第一个分区之间的空白空间中安装 stage 1.5。 2.partition 在包含内核的分区的第一个扇区中安装引导装载程序 3.none -不安装引导装载程序。 --password= 如果使用GRUB2,则会将使用这个选项指定的密码设定为引导装载程序密码.这应用来限制对GRUB2 shell的访问,并可以跳过任意内核选项.如果指定密码,GRUB2还会询问用户名。该 用户名总是root --iscrypted 通常当使用 --password= 选项指定引导装载程序密码时，会将其以明文方式保存在 Kickstart 文件中。如果要加密此密码，可使用这个选项和一个加密的密码。 请使用 grub2-mkpasswd-pbkdf2 命令生成加密的密码，输入要使用的密码，并将该命令的输出结果（以 grub.pbkdf2 开头的哈希符号）复制到 Kickstart 文件中 --timeout= 指定引导装载程序引导默认选项前等待的时间（以秒为单位）。 --default= 设定引导装载程序配置中的默认引导映像。 --extlinux 使用 extlinux 引导装载程序而不是 GRUB2。这个选项只能用于支持 extlinux 的系统。 --disabled 这个选项是 --location=none 的加强版。--location=none 只是简单地禁用 bootloader 安装，而 --disabled 则不仅禁用 bootloader 安装，也会禁用 bootloader 软件 包的安装，从而节省了空间。 2. keyboard：设置系统键盘类型 --vckeymap= # 指定VConsole应该使用的字符映射表。是字符映射表的文件名，和/usr/lib/kbd/keymaps目录下的文件名除去\".map.gz\"后相同。 --xlayouts=,,..., #指定 X 布局列表，该列表可使用逗号分开，无空格。接受与 setxkbmap(1) 相同格式的值，可以是 layout 格式（比如 cz），也可 #以是 layout (variant) 格式（比如 cz (qwerty)）。 --switch=,,..., #指定布局切换选项（在多个键盘布局间切换的快捷方式）列表。必须使用逗号分开多个选项，无空格。接受值与 setxkbmap(1) 格式相同。 ​ #示例使用 --xlayouts= 选项设置两个键盘布局（English (US) 和 Czech (qwerty)），并允许使用 Alt+Shift 在二者之间进行切换： # keyboard --xlayouts=us,'cz (qwerty)' --switch=grp:alt_shift_toggle 3. lang：设置语言 设置安装过程中使用的语言以及系统的缺省语言 示例：lang en_US 文本模式的安装过程不支持某些语言(主要是中文,日语,韩文和印度的语言).如果用lang命令指定这些语言中的一种,安装过程仍然会使用英语,但是系统会缺省使用指定的语言. 4. part/partition：硬盘分区 install模式必须 part |swap|pv.id|rdid.id options 1.mntpoint:挂载点，是在创建普通分区时指定新分区挂载位置的项；挂载点需要格式正. 例如 /, /usr, /home 2.swap 分区将被用作交换分区。为了自动决定交换分区的大小，可以使用--recommended选项。 3.raid. 表示创建的分区类型为raid型；必须用id号进行唯一区别； 4.pv. 表示所创建的分区类型为LVM型；必须用唯一id号进行区别； --size= 最小分区大小(MB)。这里可以指定一个整数值如500.不要在后面加MB。 --grow 告诉分区增长以填满可用空间(如果有的话)，或者填满设置的最大值。注意，--grow并不支持RAID卷在上的分区。 --maxsize= 分区被设置为grow时的最大分区大小(MB)。指定一个整数值，不要在后面加上MB。 --noformat 告诉安装程序不格式化分区，和--onpart一起使用。 --onpart= or --usepart= 把分区放在已经存在的设备上。使用\"--onpart=LABEL=name\"或者\"--onpart=UUID=name\" 来通过各自的标签(label)或uuid来指定一个分区。Stop (medium size).png Anaconda也许会以特殊的顺序创建分区，所以使用标签比只用分区名要安全些 --ondisk= or --ondrive= 强制在特定的磁盘上创建。 --asprimary 强制分区作为主分区，否则会导致分区失败。 --fsprofile= 为在该分区上创建文件系统的程序指定使用类型。使用类型定义了创建文件系统时各种各样的调整参数。为了让该选项能起作用，文件系统必须支持使用类型的概念，而且必须有 一个配置文件列出所有可用的类型。对于ext2/3/4，配置文件位于/etc/mke2fs.conf。 --fstype= 为分区设置文件系统类型。有效值包括ext4,ext3,ext2,btrfs,swap以及vfat。其它文件系统是否有效取决于传递给anaconda使能其它文件系统的命令行参数。 --fsoptions= 为挂载文件系统指定自由格式的字符串选项。该字符串将会被拷贝到安装系统的/etc/fstab文件中并且应该被引号括起来。 --label= 指定分区上创建的文件系统标签。如果所指定的标签已经被其它文件系统使用，新的标签将会被创建。 --recommended 自动决定分区大小。 --onbiosdisk= 强制在BIOS发现的特定磁盘上创建分区。 --encrypted 说明该分区应该被加密 --passphrase= 指定加密分区时指定的密码短语。如果没有上述--encrypted选项，该选项不起任何作用。如果没有密码短语被指定，将会使用系统范围内的默认密码短语，如果没有默认的， 安装器会停下来提醒。 --escrowcert= 从加载X.509认证。存储用证书加密过的数据加密密钥。只在--encrypted指定时有效。 --backuppassphrase 只在--escrowcert指定时相关。除了存储数据加密密钥之外，产生一个随机密码短语并将其添加到该分区。然后使用--escrowcert指定的证书加密并存储于/root。如果 不止一个LUKS卷使用--backuppassphrase，它们将共享该密码短语。 例： part /boot --fstype=“ext3” --size=100 part swap --fstype=“swap” –size=512 part / --bytes-pre-inode=4096 --fstype=“ext4”--size=10000 part /data --onpart=/dev/sdb1 --noformat part raid.100 --size=2000 part pv.100 --size=1000 5. auth/authconfig：系统授权验证 它只是authconfig程序的封装，因而所有被authconfig程序识别的选项都可以应用于auth命令。想要获取完整的列表，请参考authconfig手册。默认情况下，密码一般会被加密但并不会放在shadow文件中。 --enablemd5,每个用户口令都使用md5加密. --enablenis,启用NIS支持.在缺省情况下,--enablenis使用在网络上找到的域.域应该总是用--nisdomain=选项手工设置. --nisdomain=,用在NIS服务的NIS域名. --nisserver=,用来提供NIS服务的服务器(默认通过广播). --useshadow或--enableshadow,使用屏蔽口令. --enableldap,在/etc/nsswitch.conf启用LDAP支持,允许系统从LDAP目录获取用户的信息(UIDs,主目录,shell 等等).要使用这个选项,必须安装nss_ldap软件包.也必须用--ldapserver=和--ldapbasedn=指定服务器和base DN(distinguished name). --enableldapauth,把LDAP作为一个验证方法使用.这启用了用于验证和更改密码的使用LDAP目录的pam_ldap模块.要使用这个选项,必须安装nss_ldap软件包.也必须用--ldapserver=和--ldapbasedn=指定服务器和base DN. --ldapserver=,如果指定了--enableldap或--enableldapauth,使用这个选项来指定所使用的LDAP服务器的名字.这个选项在/etc/ldap.conf文件里设定. --ldapbasedn=,如果指定了--enableldap或--enableldapauth,使用这个选项来指定用户信息存放的LDAP目录树里的DN.这个选项在/etc/ldap.conf文件里设置. --enableldaptls,使用TLS(传输层安全)查寻.该选项允许LDAP在验证前向LDAP服务器发送加密的用户名和口令. --enablekrb5,使用Kerberos 5验证用户.Kerberos自己不知道主目录,UID或shell.如果启用了Kerberos,必须启用LDAP,NIS,Hesiod或者使用/usr/sbin/useradd命令来使这个工作站获知用户的帐号.如果使用这个选项,必须安装pam_krb5软件包. --krb5realm=,工作站所属的Kerberos 5领域. --krb5kdc=,为领域请求提供服务的KDC.如果的领域内有多个KDC,使用逗号(,)来分隔它们. --krb5adminserver=,领域内还运行kadmind的KDC.该服务器处理改变口令以及其它管理请求.如果有不止一个KDC,该服务器必须是主KDC. --enablehesiod,启用Hesiod支持来查找用户主目录,UID 和 shell.在网络中设置和使用 Hesiod 的更多信息,可以在 glibc 软件包里包括的/usr/share/doc/glibc-2.x.x/README.hesiod里找到.Hesiod是使用DNS记录来存储用户,组和其他信息的 DNS 的扩展. --hesiodlhs,Hesiod LHS(\"left-hand side\")选项在/etc/hesiod.conf里设置.Hesiod 库使用这个选项来决定查找信息时搜索DNS的名字,类似于LDAP对 base DN的使用. --hesiodrhs,Hesiod RHS(\"right-hand side\")选项在/etc/hesiod.conf里设置.Hesiod 库使用这个选项来决定查找信息时搜索DNS的名字,类似于LDAP对base DN的使用. --enablesmbauth,启用对SMB服务器(典型的是Samba或Windows服务器)的用户验证.SMB验证支持不知道主目录,UID 或 shell.如果启用SMB,必须通过启用LDAP,NIS,Hesiod或者用/usr/sbin/useradd命令来使用户帐号为工作站所知.要使用这个选项,必须安装pam_smb软件包. --smbservers=,用来做SMB验证的服务器名称.要指定不止一个服务器,用逗号(,)来分隔它们. --smbworkgroup=,SMB服务器的工作组名称. --enablecache,启用nscd服务.nscd服务缓存用户,组和其他类型的信息.如果选择在网络上用NIS,LDAP或hesiod分发用户和组的信息,缓存就尤其有用. 6. rootpw：设置root密码 rootpw [--iscrypted|--plaintext] [--lock] password --iscrypted #如果该选项存在,口令就会假定已被加密.可使用以下命令生成用随机盐值进行sha512加密后的密码 # python -c 'import crypt,getpass;pw=getpass.getpass();print(crypt.crypt(pw) if (pw==getpass.getpass(\"Confirm: \")) else exit())' --plaintext # 使用不加密的密码 --lock #锁定root用户，root用户将无法登陆Console 三、可选选项 install/upgrade install：默认安装方法。必须从 cdrom、harddrive、nfs、liveimg 或者 url（用于 FTP、HTTP、或者 HTTPS 安装）中指定安装类型 upgrade: 升级现有系统. text/graphical：kickstart安装模式 text：在文本模式下执行kickstart安装. graphical： 在图形模式下执行kickstart安装.kickstart安装默认在图形模式下安装. cdrom/harddrive/url/nfs/liveimg：指定安装类型 cdrom: 从系统上的第一个光盘驱动器CD-ROM/DVD中安装. harddrive [--biospart= | --partition=] [--dir=]: # 从本地驱动器上包含ISO镜像的目录安装，该驱动器必须是vfat或者ext2文件系统。除了改目录之外，还需要以后面的方式提供install.img。 # 一种方式是由boot.iso启动，另一种是在ISO镜像相同的目录中创建一个images/目录，然后将install.img放在那里。 --biospart= 安装用到的BIOS分区（例如82p2）。 --partition= 安装用到的硬盘分区 --dir= 包含ISO镜像和images/install.img的目录 # 例如:harddrive --partition=hdb2 --dir=/tmp/install-tree nfs --server= --dir= [--opts=] # 从指定的NFS服务器安装. --server=,指定NFS服务器（主机名或者IP）。 --dir=,包含安装树的variant目录的目录. --opts=,用于挂载NFS输出的Mount选项(可选). # 例如:nfs --server=nfsserver.example.com --dir=/tmp/install-tree url --url=|--mirrorlist= [--proxy=] [--noverifyssl] # 通过FTP或HTTP从远程服务器上的安装树中安装. --url= 安装用到的URL。支持的协议有HTTP, HTTPS, FTP, file等。 --mirrorlist= 安装用到的镜像URL。在该URL中完成$releasever和$basearch的变量替换 --proxy= 指定安装时用到的HTTP/HTTPS/FTP代理。参数的各个部分用实际值来代替 --noverifyssl 对于HTTPS服务器上的目录树，不用检查服务器的证书以及服务器的主机名匹配证书的域名 # 例如:url --url http:///或url --url ftp://:@/ liveimg --url= [--proxy=] [--checksum=] [--noverifyssl] #使用磁盘映像而不是软件包安装。映像文件可以是取自实时 ISO 映像的 squashfs.img 文件，压缩 tar 文件（.tar、.tbz、.tgz、.txz、.tar.bz2、.tar.gz 或者 .tar.xz） #或者安装介质可以挂载的任意文件系统。支持的文件系统为 ext2、ext3、ext4、vfat 和 xfs。 # Anaconda预期该镜像包含完成系统安装所需的实用程序。因此，创建磁盘镜像最好的方法是使用livemedia-creator。 # 如果该镜像包含/LiveOS/*.img（这是squashfs.img的构成），LiveOS中的第一个*.img将会被挂载，并用来安装目标系统。 --url= 执行安装的位置。支持的协议为 HTTP、HTTPS、FTP 和 file。 --proxy=[protocol://][username[:password]@]host[:port] 指定在安装时用到的HTTP/HTTPS/FTP代理。参数的各个部分用实际值来代替。 --checksum= 可选，镜像文件的sha256校验和。 --noverifyssl 对于HTTPS服务器上的目录树，不用检查服务器的证书以及服务器的主机名匹配证书的域名。 reboot/poweroff/shutdown/halt：安装完成后做什么操作 reboot：重启（缺省选项） poweroff：关闭系统并断电.通常,在手工安装过程中,anaconda会显示一条信息并等待用户按任意键来重新启动系统. shutdown：关闭系统. halt：halt选项基本和shutdown -h命令相同. services：设置禁用或允许列出的服务 --disabled 设置服务为禁用 --enabled 启动服务 例：services --disabled autid,cups,smartd,nfslock 服务之间用逗号隔开，不能有空格 selinux： 在系统里设置SELinux状态.在anaconda里,SELinux缺省为enforcing. selinux [--disabled|--enforcing|--permissive] --enforcing,启用SELinux,实施缺省的targeted policy. 注:如果kickstart文件里没有selinux选项,SELinux将被启用并缺省设置为--enforcing. --permissive,输出基于SELinux策略的警告,但实际上不执行这个策略. --disabled,在系统里完全地禁用 SELinux. user：在系统上创建新用户 user --name= [--groups=] [--homedir=] [--password=] [--iscrypted] [--shell=] [--uid=] --name=,提供用户的名字.这个选项是必需的. --groups=,除了缺省的组以外,用户应该属于的用逗号隔开的组的列表. --homedir=,用户的主目录.如果没有指定,缺省为/home/. --password=,新用户的密码.如果没有指定,这个帐号将缺省被锁住. --iscrypted=,所提供的密码是否已经加密？ --shell=,用户的登录shell.如果不提供,缺省为系统的缺省设置. --uid=,用户的UID.如果未提供,缺省为下一个可用的非系统 UID. network：配置系统的网卡信息 --device= 指定要使用network命令配置或者激活的设备。能够以和 ksdevice启动选项相同的方式指定。例如：network --bootproto=dhcp --device=eth0 对于第一个network命令，如果选项没有被指定，它默认是 1)ksdevice启动选项, 2)为了获得kickstart而激活的设备,或者 3)UI上的选择框。对于如下的network命令，需要--device选项。 --ip= 网络接口的IP地址。 --ipv6= 网络接口的IPv6地址。可以是[/]形式的静态地址，例如，3ffe:ffff:0:1::1/128(如果前缀被省略，会被假定为64)，\"auto\"地址分配基于动态的邻居发现协议，而\"dhcp\"会使用DHCPv6协议。 --gateway= 默认网关，是一个IPv4或者IPv6地址。 --nodefroute 组织设备抓取默认路由。在安装器中使用--activate选项激活其它设备时非常有用，从F16起。 --nameserver= 主域名服务器，是一个IP地址。多个域名服务器必须由逗号分隔。 --nodns 并不配置DNS服务器。 --netmask= 安装系统的网络掩码。 --hostname= 安装系统的主机名。 --ethtool= 指定将要传递给ethtool程序的设备附加的低级别设置。 --essid= 无线网网络ID。 --wepkey= 无线网WEP加密密钥。 --wpakey= 无线网WPA加密密钥(从F16起)。 --onboot= 是否在启动时使能该设备。 --dhcpclass= DHCP类别。 --mtu= 设备的MTU。 --noipv4 在该设备上禁用IPv4。 --noipv6 在该设备上禁用IPv6。 --bondslaves 使用该选项指定的网卡作为多网卡绑定的从网卡，虚拟出的网卡的名字由--device指定。例如--bondslaves=eth0,eth1。自Fedora 19开始。 --bondopts 为--bondslaves和--device选项指定的绑定接口指定一个逗号分隔的参数列表。例如：--bondopts=mode=active-backup,primary=eth1。如果一个选项本身以逗号作为分隔符，那么使用分号作为选项之间的分隔符。 --vlanid 使用--device指定的设备作为父设备来创建的vlan设备的Id(802.1q标签)。例如，network --device=eth0 --vlanid=171将会创建vlan设备eth0.171。从Fedora 19起。 logging：该命令控制anaconda安装过程中的错误日志，并不影响安装系统。 --host=,发送日志信息到给定的远程主机,这个主机必须运行配置为可接受远程日志的syslogd进程. --port=,如果远程的syslogd进程没有使用缺省端口,这个选项必须被指定. --level=,debug,info,warning,error或critical中的一个.指定tty3上显示的信息的最小级别.然而,无论这个级别怎么设置,所有的信息仍将发送到日志文件. zerombr ：清除mbr信息，会同时清空系统用原有分区表 clearpart：在建立新分区前清空系统上原有的分区表，默认不删除分区 --all 擦除系统上原有所有分区； --drives 删除指定驱动器上的分区 --initlabel 初始化磁盘卷标为系统架构的默认卷标 --linux 擦除所有的linux分区 --none（default）不移除任何分区 ​ 例：clearpart --drives=hda,hdb --all --initlabel eula：使用这个选项以非用户互动方式接受终端用户许可证协议（End User License Agreement，EULA）。指定这个选项可防止 Initial Setup 在完成安装并第一次重启系统时提示您接受该许可证。 --agreed（强制） - 接受 EULA。必须总是使用这个选项，否则 eula 命令就毫无意义。 ignoredisk:指定安装程序忽略指定的磁盘。如果您使用自动分区并希望忽略某些磁盘的话，这就很有用 --only-use - 指定安装程序要使用的磁盘列表。忽略其他所有磁盘。 例如1：要在安装过程使用磁盘 sda，并忽略所有其他磁盘：ignoredisk --only-use=sda 要包括不使用 LVM 的多路经设备： ignoredisk --only-use=disk/by-id/dm-uuid-mpath-2416CD96995134CA5D787F00A5AA11017 要包括使用 LVM 的多路径设备： ignoredisk --only-use=disk/by-id/scsi-58095BEC5510947BE8C0360F604351918 --interactive - 允许手动导航高级存储页面。 repo：配置用于软件包安装来源的额外的yum库.可以指定多个repo行. repo --name=repoid [--baseurl=|--mirrorlist=url] [options] --name= - 该库的 id。这个选项是必选项。如果库名称与另一个之前添加的库冲突，则会忽略它。因为这个安装程序使用预先配置的库列表，就是说您无法添加名称与预先配置的库相同的库。 --baseurl= - 程序库的 URL。这里不支持 yum 库配置文件中使用的变量。可以使用这个选项，也可以使用 --mirrorlist，但不能同时使用这两个选项。 --mirrorlist= - URL 指向该程序库的一组镜像。这里不支持 yum 库配置文件中使用的变量。可以使用这个选项，也可以使用 --baseurl，但不能同时使用这两个选项。 --install - 将所安装系统提供的存储库配置保存在/etc/yum.repos.d/目录中。不使用这个选项，在 Kickstart 文件中配置的程序库只能在安装过程中使用，而无法在安装的系统中使用。 --cost= - 为这个库分配的 cost 整数值。如果多个库提供同样的软件包，这个数字就是用来规定那个库优先使用，cost 较低的库比 cost 较高的库优先。 --excludepkgs= - 逗号分开的软件包名称列表，同时一定不能从这个存储库中提取该软件包名称。如果多个库提供同样的软件包，且要保证其来自某个特定存储库。 可接受完整软件包名称（比如 publican）和 globs（比如 gnome-*）。 --includepkgs= - 逗号分开的软件包名称列表，同时一定要从这个存储库中提取 glob。如果多个存储库提供同样的软件包，且要保证其来自某个特定存储库，这个选项就很有用了。 --proxy=[protocol://][username[:password]@]host[:port] - 指定只有这个存储库使用的 HTTP/HTTPS/FTP 代理服务器。 这个设置不会影响其他存储库，也不会影响将 install.img 附加到 HTTP 安装的方法。 --ignoregroups=true - 组成安装树时使用这个选项，且对安装过程本身没有影响。它告诉组合工具在镜像树时不要查看软件包组信息，这样就不会镜像大量无用数据。 --noverifyssl - 连接到 HTTPS 服务器时禁止 SSL 验证。 interactive：在安装过程中使用kickstart文件里提供的信息,但允许检查和修改给定的值.将遇到安装程序的每个屏幕以及kickstart文件里给出的值. autostep：通常 Kickstart 安装会跳过不必要的页面。这个选项可让安装程序浏览所有页面，并摘要显示每个页面。部署系统时不应使用这个选项，因为它会干扰软件包安装 --autoscreenshot 在安装的每一步均截屏。这些截屏将在安装过程中保存在 /tmp/anaconda-screenshots 中，并在安装完成后保存在 /root/anaconda-screenshots 中。 sshpw：安装过程中是否开启SSH与安装进程进行交互与监控 sshpw --username=name password [--iscrypted|--plaintext] [--lock] --username --iscrypted --plaintext --lock autopart：自动生成分区（autopart 选项不能与 part/partition, raid、logvol 或者 volgroup 在同样的 Kickstart 文件中一同使用。） --type= - 选择您要使用的预先定义的自动分区方案之一。可接受以下值： lvm: LVM 分区方案。 btrfs: Btrfs 分区方案。 plain: 不附带 LVM 或者 Btrfs 的常规分区。 thinp: LVM 精简分区方案。 --fstype= - 选择可用文件系统类型之一。可用值为 ext2、ext3、ext4、xfs 和 vfat。默认系统为 xfs。有关使用这些文件系统的详情，请查看 第 6.14.4.1.1 节 “文件系统类型”。 --nolvm - 不使用 LVM 或者 Btrfs 进行自动分区。这个选项等同于 --type=plain。 --encrypted - 加密所有分区。这等同于在手动图形安装过程的起始分区页面中选中 加密分区 复选框。 # 注意：加密一个或多个分区时，Anaconda 尝试收集 256 字节熵，以保证对分区安全加密与安装系统互动可加速此进程（使用键盘输入或移动鼠标）。 # 如果要在虚拟机中安装系统，则可添加 virtio-rng 设备（虚拟随机数生成器）， --passphrase= - 为所有加密设备提供默认的系统范围内的密码短语。 --escrowcert=URL_of_X.509_certificate - 将所有加密卷数据加密密码保存在 /root 中，使用来自 URL_of_X.509_certificate 指定的 URL 的 X.509 证书加密。每个加密卷的密码都作为单独的文件保存。只有指定 --encrypted 时这个选项才有意义。 --backuppassphrase - 为每个加密卷添加随机生成的密码短语。将这些密码保存在 /root 目录下的独立文件中，使用 --escrowcert 指定的 X.509 证书加密。只有指定 --escrowcert 时这个选项才有意义。 --cipher= - 如果指定 Anaconda 默认 aes-xts-plain64 无法满足需要，则可以指定要使用的加密类型。这个选项必须与 --encrypted 选项一同使用，单独使用无效。 《Red Hat Enterprise Linux 7 安全指南》中有可用加密类型列表，但 Red Hat 强烈推荐您使用 aes-xts-plain64 或者 aes-cbc-essiv:sha256。 firewall：配置系统防火墙选项 firewall –enable|--disable [ --trust ] [ --port= ] --enable 拒绝回应输出要求的进入连接，比如 DNS 答复或 DHCP 请求。如果需要访问在这台机器中运行的服务，可以选择通过防火墙允许具体的服务。 --disable 不配置任何iptables防御规则； --trust 在这里列出设备,比如em1,允许所有流量通过该防火墙进出那个设备.要列出一个以上的设备,请使用--trust em1 --trust em2。不要使用逗号分开的格式，比如 --trust em1, em2。 --port 可以用端口:协议（port:protocal）格式指定允许通过防火墙的端口。例如，如果想允许 IMAP 通过您的防火墙，可以指定 imap:tcp。还可以具体指定端口号码，要允许 UDP 分组 在端口 1234 通过防火墙，输入 1234:udp。要指定多个端口，用逗号将它们隔开。 --service= 这个选项提供允许服务通过防火墙的高级方法。有些服务（比如 cups、avahi 等等）需要开放多个端口，或者另外有特殊配置方可工作。 您应该使用 --port 选项指定每个具体端口，或者指定 --service= 并同时打开它们 incoming - 使用以下服务中的一个或多个来替换，从而允许指定的服务通过防火墙。 --ssh --smtp --http --ftp ​ 示例：firewall --enable --trust eth0 --trust eth1 --port=80:tcp group:在系统中生成新组。如果某个使用给定名称或者 GID 的组已存在，这个命令就会失败。另外，该 user 命令可用来为新生成的用户生成新组 group --name=name [--gid=gid] --name= - 提供组名称。 --gid= - 组的 UID。如果未提供，则默认使用下一个可用的非系统 GID。 volgroup：创建逻辑卷组vg volgroup name partition [options] ​ --noformat - 使用现有卷组，且不进行格式化。 --useexisting - 使用现有卷组并重新格式化。如果使用这个选项，请勿指定 partition。例如：volgroup rhel00 --useexisting --noformat --pesize= - 以 KiB 为单位设定卷组物理扩展大小。默认值为 4096 (4 MiB)，最小值为 1024 (1 MiB)。 --reserved-space= - 以 MB 为单位指定在卷组中预留的未使用空间量。只适用于新生成的卷组。 --reserved-percent= - 指定卷组中预留未使用空间的比例。只适用于新生成的卷组。 ​ 注意1 ：不要在逻辑卷和卷组名称中使用小横线（-）。如果使用这个字符，会完成安装，但 /dev/mapper/ 目录列出这些卷和卷组时，小横线会加倍。例如：某个卷组名为 volgrp-01，包含名 为 logvol-01 逻辑卷，该逻辑卷会以 /dev/mapper/volgrp--01-logvol--01 列出。这个限制只适用于新创建的逻辑卷和卷组名。如果您使用 --noformat 选项重复使用现有名称， 它们的名称就不会更改。 注意2: 应该先创建分区，然后创建逻辑卷组，再创建逻辑卷。例如： part pv.01 --size 10000 volgroup volgrp pv.01 logvol / --vgname=volgrp --size=2000 --name=root logvol：创建逻辑卷lv logvol mntpoint --vgname=name --name=name [options] mntpoint — 是该分区挂载的位置，且必须是以下格式之一： 1. /path 例如：/ 或者 /home 2.swap 该分区被用作交换空间。要自动决定 swap 分区的大小，使用 --recommended 选项：swap --recommended 使用 --hibernation 选项自动决定 swap 分区的大小，同时还允许 您的系统有附加空间以便可以休眠：swap --hibernation分配的分区大小将与 --recommended 加上系统 RAM 量相等。 这些选项如下所示： --noformat- 使用现有逻辑卷且不要对其进行格式化。 --useexisting 使用现有逻辑卷并重新格式化它。 --fstype= 为逻辑卷设置文件系统类型。有效值有：xfs、ext2、ext3、ext4、swap 和 vfat。 --fsoptions= 指定在挂载文件系统时所用选项的自由格式字符串。将这个字符串复制到安装的系统的 /etc/fstab 中，并使用括号括起来。 --mkfsoptions= 指定要提供的附加参数，以便在这个分区中建立文件系统。没有对任何参数列表执行任何操作，因此必须使用可直接为 mkfs 程序提供的格式。 就是说可使用逗号分开或双引号分开的多个选项，要看具体文件系统。 --label= 为逻辑卷设置标签。 --grow 会让逻辑卷使用所有可用空间（若有），或使用设置的最大值（如果指定了最大值）。必须给出最小值，可使用 --percent= 选项或 --size= 选项。 --size= 以 MB 单位的逻辑卷大小。这个选项不能与 --percent= 选项一同使用。 --percent= 考虑任何静态大小逻辑卷时的逻辑卷大小，作为卷组中剩余空间的百分比。这个选项不能与 --size= 选项一同使用。 #重要:创建新逻辑卷时，必须使用 --size= 选项静态指定其大小，或使用 --percent= 选项指定剩余可用空间的百分比。不能再同一逻辑卷中同时使用这些选项。 --maxsize= - 当将逻辑卷被设置为可扩充时以 MB 为单位的最大值。在这里指定一个整数值，如500（不要在数字后添加单位）。 --recommended - 创建 swap 逻辑卷时可采用这个选项，以根据您的系统硬件自动决定这个卷的大小。 --resize - 重新定义逻辑卷大小。如果使用这个选项，则必须还指定 --useexisting 和 --size。 --encrypted - 指定该逻辑卷应该用 --passphrase= 选项提供的密码进行加密。如果没有指定密码短语，安装程序将使用 autopart --passphrase 命令指定默认系统级密码， 如果没有设定默认密码则会停止安装，并提示输入密码短语。 # 注意:加密一个或多个分区时，Anaconda 尝试收集 256 字节熵，以保证对分区安全加密与安装系统互动可加速此进程（使用键盘输入或移动鼠标）。如果要在虚拟机中安装系统，则可 # 添加 virtio-rng 设备（虚拟随机数生成器） --passphrase= - 指定在加密这个逻辑卷时要使用的密码短语。必须与 --encrypted 选项一同使用，单独使用这个选项无效。 --cipher= - 指定如果对 Anaconda 默认 aes-xts-plain64 不满意时要使用的加密类型。这个选项必须与 --encrypted 选项一同使用，单独使用无效。 推荐使用 aes-xts-plain64 或者 aes-cbc-essiv:sha256。 --escrowcert=URL_of_X.509_certificate 将所有加密卷数据加密密钥作为文件保存在 /root 中，使用来自 URL_of_X.509_certificate 指定的 URL 的 X.509 证书加密。每个加密卷 的密钥都作为单独的文件保存。只有指定 --encrypted 时这个选项才有意义。 --backuppassphrase - 为每个加密卷添加随机生成的密码短语。将这些密码保存在 /root 目录下的独立文件中，使用 --escrowcert 指定的 X.509 证书加密。只有指定 --escrowcert 时 这个选项才有意义。 --thinpool - 创建精简逻辑卷。（使用 none 挂载点）。 --metadatasize=size - 为新的精简池设备指定元数据大小（单位 MiB）。 --chunksize=size - 为新的精简池设备指定块大小（单位 KiB）。 --thin - 创建精简逻辑卷。（要求使用 --poolname） --poolname=name - 指定在其中创建精简逻辑卷的精简池名称。需要 --thin 选项。 --profile=name 指定与精简逻辑卷配合使用的配置文件名称。如果使用此选项，还要用于给定逻辑的卷元数据中包含该名称。默认情况下，可使用的配置文件为在 /etc/lvm/profile 目录中定 义的 default 和 thin-performance。详情请查看 lvm(8) 手册页。 --cachepvs= - 用逗号分开的物理卷列表，应作为这个卷的缓存使用。 --cachemode= - 指定应使用哪种模式缓存这个逻辑卷 - 可以是 writeback，也可以是 writethrough。 #注意:有关缓存的逻辑卷及其模式的详情，请查看 lvmcache(7) 手册页。 --cachesize= - 附加到该逻辑卷的缓存大小，单位为 MiB。这个选项需要 --cachepvs= 选项。 ​ 注意1: 应该先创建分区，然后创建逻辑卷组，再创建逻辑卷以占据逻辑组里剩余的 90% 空间。例如：： part pv.01 --size 1 --grow volgroup myvg pv.01 logvol / --vgname=myvg --name=rootvol --percent=90 timezone：设置系统的时区 timezone [ --utc ] 例：timezone --utc Asia/Shanghai 软件包的选择 在 Kickstart 文件中使用 %packages 命令列出要安装的软件包。 可以根据环境、组或者其软件包名称指定软件包。 安装程序定义包含相关软件包的几个环境和组。有关环境和组列表请查看安装光盘中的 repodata/*-comps-variant.architecture.xml 文件。 *-comps-variant.architecture.xml 文件包含描述可用环境（使用 标签标记）和组（ 标记）的结构。每个组都有一个 ID、用户可见性数值、名称、描述和软件包列表。如果未安装选择该组，那么就会安装该软件包列表中标记为 mandatory 的软件包；如果未明确指定，也会安装标记为 default 的软件包，而标记为 optional 的软件包必须在明确指定后方可安装。 您可以使用 ID（ 标签）或者名称（ 标签）指定软件包组或者环境。 %packages 部分必须以 %end 命令结尾。 除组外，您还要指定要安装的整体环境 %packages @^Infrastructure Server %end 指定组，每个条目一行，以 @ 符号开始，接着是空格，然后是完整的组名或 *-comps-variant.architecture.xml 中指定的组 id。 %packages @X Window System @Desktop @Sound and Video %end 根据名称指定独立软件包，每行一个条目。您可以在软件包名称中使用星号（*）作为通配符。 %packages sqlite curl aspell docbook* %end 使用小横线（-）开头指定安装中不使用的软件包或组。 %packages -@Graphical Internet -autofs -ipa*fonts %end 常用软件包选择选项 以下选项可用于 %packages。要使用这个选项，请将其添加到软件包选择部分的开始。例如： %packages --multilib --ignoremissing ​ --default 安装默认软件包组。这与在互动安装过程中的 软件包选择 页面中没有其他选择时要安装的软件包组对应。 --excludedocs 不要安装软件包中的任何文档。大多数情况下，这样会排除一般安装在 /usr/share/doc* 目录中的所有文件，但要排除的具体文件取决于各个软件包。 --ignoremissing 忽略所有在这个安装源中缺少的软件包、组及环境，而不是暂停安装询问是应该放弃还是继续安装。 --instLangs= 指定要安装的语言列表。注：这与软件包组等级选择不同。这个选项不会告诉您应该安装哪些软件包组，而是通过设置 RPM 宏控制应该安装独立软件包中的哪些事务文件。 --multilib 为 multilib 软件包配置已安装的系统（即允许在 64 位系统中安装 32 位软件包），并安装在这一部分指定的软件包。通常在 AMD64 和 Intel 64 系统中，只安装用于整个架构 （标记为 x86_64）的软件包以及用于所有架构（标记为 noarch）软件包。使用这个选项时，将自动安装用于 32 位 AMD 系统 Intel（标记为 i686）的软件包。这只适用于在 %packages 部分明确指定的软件包。对于那些仅作为相依性安装而没有在 Kickstart 文件中指定的软件包，将只安装其所需架构版本，即使有更多可用架构也是如此。 --nocore 禁用默认总被安装的 @Core 软件包组。禁用 @Core 软件包组应只用于创建轻量级的容器；用 --nocore 安装桌面或服务器系统将导致系统不可用。 具体软件包组参数项 这个列表中的选项只用于单一软件包组。不是在 Kickstart 文件的 %packages 命令中使用，而是在组名称中添加条目。例如： %packages @Graphical Internet --optional %end ​ --nodefaults 只安装该组的强制软件包，不是默认选择。 --optional 除安装默认选择外，还要安装在 *-comps-variant.architecture.xml 文件组定义中标记为自选的软件包。 注：有些软件包组，比如 Scientific Support，没有指定任何强制或默认软件包 - 只有自选软件包。在这种情况下必须使用 --optional 选项，否则不会安装这个组中的任何软件包。 安装前脚本 ks.cfg文件被解析后马上加入要运行的命令.这个部分必须处于kickstart文件的最后(在命令部分之后)而且 必须用%pre命令开头，%end结尾. 可以在%pre部分访问网络；然而,此时命名服务还未被配置,所以只能使用IP地址. 预安装脚本不会在 chroot 环境中运行 --interpreter= 定义脚本运行解释器.常用的有:/usr/bin/sh, /usr/bin/bash, and /usr/bin/python. --erroronfail 如果脚本失败则显示出错信息并暂停安装。该出错信息可让您进入记录失败原因的位置。 --log= 记录脚本运行过程中的信息到指定路径的文件中。例如：%pre --log=/mnt/sysimage/root/ks-pre.log 示例 %pre #!/bin/sh hds=\"\" mymedia=\"\" for file in /proc/ide/h* do mymedia=`cat $file/media` if [ $mymedia == \"disk\" ] ; then hds=\"$hds `basename $file`\" fi done set $hds numhd=`echo $#` drive1=`echo $hds | cut -d' ' -f1` drive2=`echo $hds | cut -d' ' -f2` ​ #Write out partition scheme based on whether there are 1 or 2 hard drives if [ $numhd == \"2\" ] ; then #2 drives echo \"#partitioning scheme generated in %pre for 2 drives\" > /tmp/part-include echo \"clearpart --all\" >> /tmp/part-include echo \"part /boot --fstype xfs --size 75 --ondisk hda\" >> /tmp/part-include echo \"part / --fstype xfs --size 1 --grow --ondisk hda\" >> /tmp/part-include echo \"part swap --recommended --ondisk $drive1\" >> /tmp/part-include echo \"part /home --fstype xfs --size 1 --grow --ondisk hdb\" >> /tmp/part-include else #1 drive echo \"#partitioning scheme generated in %pre for 1 drive\" > /tmp/part-include echo \"clearpart --all\" >> /tmp/part-include echo \"part /boot --fstype xfs --size 75\" >> /tmp/part-include echo \"part swap --recommended\" >> /tmp/part-include echo \"part / --fstype xfs --size 2048\" >> /tmp/part-include echo \"part /home --fstype xfs --size 2048 --grow\" >> /tmp/part-include fi %end 安装后脚本 post-install 脚本是在 chroot 环境里运行的.因此,某些任务如从安装介质复制脚本或RPM将无法执行. --nochroot 允许指定想在chroot环境之外运行的命令 --interpreter 定义脚本运行解释器.常用的有:/usr/bin/sh, /usr/bin/bash, and /usr/bin/python. --erroronfail 如果脚本失败则显示出错信息并暂停安装。该出错信息可让您进入记录失败原因的位置。 --log= 记录脚本运行过程中的信息到指定路径的文件中。例如：%pre --log=/mnt/sysimage/root/ks-pre.log 示例 %post --nochroot cp /etc/resolv.conf /mnt/sysimage/etc/resolv.conf %end 四、示例 **全新安装系统** install #在文本模式下安装 text #指定安装过程中语言为英语 lang en_US.UTF-8 #指定键盘类型为US布局 keyboard us ​ auth --useshadow --passalgo=sha512 # url --url=\"http://192.168.1.80/CentOS7\" #设置root用户密码 rootpw --iscrypted $1$6/87AF3n$eczKeiNRBv7H.GXnur1Ld/ #开启SELinux selinux --enforcing #关闭防火墙 firewall --disabled #设置网络信息。网卡1设置手动获取IP，不初始化IPV6,不设置为默认路由。网卡2设置DHCP获取IP，不初始化IPV6,设置为默认路由。 network --bootproto=static --ip=192.168.10.6 --device=enp0s3 --activate --nodefroute --noipv6 --nameserver=114.114.114.114 --netmask=24 --gateway=192.168.10.1 network --bootproto=dhcp --device=enp0s8 --activate --nameserver=114.114.114.114 --noipv6 #设置主机名 network --hostname=test #安装完成后重启 reboot #设置时区为上海的时区 timezone Asia/Shanghai --isUtc --nontp #将BootLoader安装在sda磁盘上 bootloader --location=mbr --boot-drive=sda #安装时清理sda磁盘上所有的分区 clearpart --all --drives=sda #将SSH和NetworkManager设置为开机自启动 services --enabled=NetworkManager,sshd firstboot --enable #只使用sda磁盘 ignoredisk --only-use=sda #对sda磁盘进行分区。单位是MB part /boot --fstype=\"xfs\" --ondisk=sda --size=200 part / --fstype=\"xfs\" --ondisk=sda --size=30720 part /opt --fstype=\"xfs\" --ondisk=sda --size=10240 part /var --fstype=\"xfs\" --grow --ondisk=sda --size=1 ​ %packages @^minimal @core %end #安装后要执行的脚本 %post --interpreter=/bin/bash --log=/root/post-install.log mkdir /etc/yum.repos.d/bak mv /etc/yum.repos.d/C* /etc/yum.repos.d/bak cat >> /etc/yum.repos.d/ustc.repo /dev/null yum makecache > /dev/null yum install -y tree vim telnet nc unzip git net-tools wget bind-utils > /dev/null echo \"Set HOSTNAME test\" echo \"Disabled SELinux and Firewall\" echo \"/dev/sda /boot xfs 200MB\" echo \"/dev/sda / xfs 30G\" echo \"/dev/sda /opt xfs 10G\" echo \"/dev/sda /var xfs RemainingCapacity\" echo \"Make Yum Repository To USE USTC Yum Repository \" echo \"Installed Tools : tree vim telnet nc unzip git net-tools wget bind-utils\" echo \" #######################\" >> /etc/motd echo \" # Keep Your Curiosity #\" >> /etc/motd echo \" #######################\" >> /etc/motd %end 五、验证KS文件的语法正确性 yum install -y pykickstart ksvalidator ks文件 六、比较OS不同版本间的KS语法差异 ksverdiff -f RHEL6 -t RHEL7 # -f 指定要比较的第一个发行本，-t 指定要比较的最后一个发行本 参考连接 ★★★★★： https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/installation_guide/sect-kickstart-syntax ★★★★★：https://docs.centos.org/en-US/centos/install-guide/Kickstart2/#sect-kickstart-commands ★★★★★ https://blog.csdn.net/yanghua1012/article/details/80426659 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-14 18:11:44 "},"origin/ubuntu-autoinstall.html":{"url":"origin/ubuntu-autoinstall.html","title":"Ubuntu/Debian: AutoInstall无人值守配置文件详解","keywords":"","body":"Ubuntu/Debian无人值守 Autoinstall 一、简介 preseeds 是基于 debian-installer （又名 d-i ）自动化安装程序的方法。 Ubuntu新 autoinstall 方式的自动安装主要在以下方面不同于 preseeds ： 格式完全不同（ cloud-init config 格式，通常是 yaml 格式，vs debconf-set-selections 格式） 当一个问题的答案不存在于 preseeds 中时， d-i 停止执行并要求用户输入。 autoinstalls 自动安装不是这样的：默认情况下，如果有任何自动安装配置，安装程序会为任何未回答的问题采用默认设置（如没有默认设置则失败）。 用户可以将配置中的特定部分指定为“交互式”，这意味着安装程序仍会停止并询问这些部分。 Ubuntu 20.04 Server 与 23.04 Desktop 之后的版本. 建议使用 Autoinstall 进行无人值守安装 二、Autoinstall配置文件生效的方式 通过 clound-int 提供 autoinstall 相关的配置（推荐） 这种方式直接作用于安装程序，而无需修改安装介质。不过配置文件头得为：#cloud-config #cloud-config autoinstall: version: 1 ... 在安装iso介质中提供 autoinstall.yaml version: 1 .... 24.04 (Noble)之后可以使用 cloud-config 的格式 autoinstall: version: 1 .... 安装程序Subiquity默认搜索配置文件autoinstall.yaml的路径 安装iso介质的根目录，或在 安装系统的根文件系统上。 如果放在其他路径下，也可以通过设置内核参数subiquity.autoinstallpath=path/to/autoinstall.yaml来指定。其中路径是相对于安装系统的根目录的。 配置文件执行的优先顺序 内核命令行参数 subiquity.autoinstallpath指定的配置文件 安装系统的根文件系统 cloud-config 安装 ISO介质的根路径 三、内核参数 DEFAULT vesamenu.c32 #DEFAULT menu.c32 prompt 0 timeout 30 display boot.msg menu background splash.jpg menu title #### Curiouser PXE Boot Menus #### label 'Ubuntu 20.04.6 LTS' menu label ^1> Install Ubuntu 20.04.6 LTS (Focal Fossa) menu default kernel Ubuntu/20.04.6/vmlinuz initrd Ubuntu/20.04.6/initrd # append 追加到内核的参数 append debug root=/dev/ram0 ramdisk_size=2000000 ipv6.disable=1 fsck.mode=skip ip=dhcp url=http://192.168.1.1:8089/Ubuntu/20.04.6/ubuntu-20.04.6-live-server-amd64.iso autoinstall cloud-config-url=http://192.168.1.1:8089/Ubuntu/20.04.6/user-data autoinstall：默认安装程序中有个安全设置，防止带有 autoinstall.yaml 文件的USB驱动器擦除系统。参考：https://canonical-subiquity.readthedocs-hosted.com/en/latest/explanation/zero-touch-autoinstall.html。要绕过此提示，参数 autoinstall 必须在内核命令行设置。 四、Autoinstall配置详解 1、文件内容格式 #cloud-config autoinstall: version: 1 配置项: ... 参数: ... ...... 2、#cloud-config：文件头部 头部标识文件类型为 cloud-config，这是云配置文件格式的标准。 #cloud-config 3、version 配置文件版本字段。目前，该值必须为 1。 version: 1 4、identity：身份设置 用于设置系统的主机名、用户名和密码。密码应该使用加密形式。 identity: hostname: ubuntu-server password: \"$6$rounds=4096$z...$...\" username: ubuntu 密码可以使用以下方式生成 使用 openssl passwd 生成基于 SHA-512的密码 $ openssl passwd -6 #Password: Verifying - Password: $6$rounds=5000$WiA0EmOC.kFC$MOd3X96skVeyDCJb1Hg... (省略) 使用whois包里的 mkpasswd命令生成 5、ssh：SSH设置 ssh: install-server: true allow-pw: true install-server 指定是否安装 SSH 服务器，allow-pw 允许使用密码登录。 6、locale：区域设置 用于设置系统区域和语言 locale: en_US.UTF-8 6、keyboard：键盘布局 用于设置键盘布局和变体 keyboard: layout: us variant: \"\" toggle: null # layout: string，默认 us # variant: string，默认为空 # toggle: string或null，默认为null 7、storage：存储配置 用于配置存储，支持lvm, direct and zfs。建议使用 LVM（逻辑卷管理）自动分区。 storage: layout: name: lvm match: serial: CT* storage: layout: name: direct match: ssd: true 8、network：网络配置 用于配置网络，这里设置 enp0s3 接口使用 DHCP 自动获取 IP 地址。 network: network: version: 2 ethernets: enp0s3: dhcp4: true 9、user-data：用户数据 用于设置用户数据，禁用或启用 root 用户，设置密码不过期 user-data: disable_root: false chpasswd: expire: false 10、early-commands early-commands: - ping -c1 198.162.1.1 11、late-commands：后期命令 用于配置后期的命令，安装完成后执行，这里记录安装成功的日志。 late-commands: - echo \"Installation complete\" > /target/var/log/installer-success.log 12、apt：APT 配置 用于配置 APT 源 apt: primary: - arches: [default] uri: http://archive.ubuntu.com/ubuntu 13、error-commands error-commands: - tar c /var/log/installer | nc 192.168.0.1 1000 五、其他信息 1、安装过程的日志 自动化安装日志输出 tty1 窗口，安装过程可开起 tty2~tty6 窗口 /var/log/curtin-install.log：下载安装日志 六、样例文件 1、Ubuntu 22.04.6 #cloud-config autoinstall: version: 1 locale: en_US.UTF-8 timezone: Asia/Shanghai keyboard: layout: us network: network: version: 2 ethernets: ens32: dhcp4: true storage: layout: name: lvm reset-partition: true identity: realname: 'Ubuntu User' username: ubuntu password: '$6$ph/TO........密码............' hostname: ubuntu packages: - openssh-server - git apt: primary: - arches: [default] uri: https://mirrors.aliyun.com/ubuntu shutdown: reboot 参考 https://ubuntu.com/server/docs/how-to-netboot-the-server-installer-on-amd64 https://discourse.ubuntu.com/t/netbooting-the-live-server-installer/14510 https://askubuntu.com/questions/1292032/how-can-current-ubuntu-versions-be-installed-via-pxe-network-boot-and-an-automat https://www.molnar-peter.hu/en/ubuntu-jammy-netinstall-pxe.html https://louwrentius.com/understanding-the-ubuntu-2004-lts-server-autoinstaller.html https://canonical-subiquity.readthedocs-hosted.com/en/latest/reference/autoinstall-reference.html#version https://hmli.ustc.edu.cn/doc/linux/ubuntu-autoinstall/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-20 16:09:32 "},"origin/netboot.xyz.html":{"url":"origin/netboot.xyz.html","title":"netboot.xyz","keywords":"","body":"基于PXE的网络OS工具Netboot.xyz 一、简介 Github：https://github.com/netbootxyz/netboot.xyz 文档：https://netboot.xyz/docs/ 二、部署 1、docker 2、ansible 3、手动 三、Asuswrt-Merlin https://netboot.xyz/docs/kb/networking/asuswrt-merlin curl -o /jffs/tftproot/netboot.xyz.kpxe https://boot.netboot.xyz/ipxe/netboot.xyz.kpxe curl -o /jffs/tftproot/netboot.xyz.efi https://boot.netboot.xyz/ipxe/netboot.xyz.efi touch /jffs/configs/dnsmasq.conf.add enable-tftp tftp-root=/jffs/tftproot dhcp-match=set:bios,60,PXEClient:Arch:00000 dhcp-boot=tag:bios,netboot.xyz.kpxe,,192.168.1.1 dhcp-match=set:efi32,60,PXEClient:Arch:00002 dhcp-boot=tag:efi32,netboot.xyz.efi,,192.168.1.1 dhcp-match=set:efi32-1,60,PXEClient:Arch:00006 dhcp-boot=tag:efi32-1,netboot.xyz.efi,,192.168.1.1 dhcp-match=set:efi64,60,PXEClient:Arch:00007 dhcp-boot=tag:efi64,netboot.xyz.efi,,192.168.1.1 dhcp-match=set:efi64-1,60,PXEClient:Arch:00008 dhcp-boot=tag:efi64-1,netboot.xyz.efi,,192.168.1.1 dhcp-match=set:efi64-2,60,PXEClient:Arch:00009 dhcp-boot=tag:efi64-2,netboot.xyz.efi,,192.168.1.1 参考： http://www.360doc.com/content/19/0805/08/39899686_853175915.shtml Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:38:44 "},"origin/tool-SublimeText.html":{"url":"origin/tool-SublimeText.html","title":"Sublime Text 3","keywords":"","body":"Sublime Text使用总结 一、简介 Sublime Text是一款具有代码高亮、语法提示、自动完成且反应快速的编辑器软件 Sublime Text具有漂亮的用户界面和强大的功能，例如代码缩略图，Python的插件，代码段等。还可自定义键绑定，菜单和工具栏。Sublime Text 的主要功能包括：拼写检查，书签，完整的 Python API ， Goto 功能，即时项目切换，多选择，多窗口等等。Sublime Text 是一个跨平台的编辑器，同时支持Windows、Linux、Mac OS X等操作系统。 二、安装 Sublime Text官网：http://www.sublimetext.com/3 三、插件管理器Package Control Package Control：https://packagecontrol.io/installation 使用Ctrl + `打开Sublime Text控制台 将下面的代码粘贴到控制台里 Sublime Text 3 import urllib.request,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by) Sublime Text 2 import urllib2,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); os.makedirs( ipp ) if not os.path.exists(ipp) else None; urllib2.install_opener( urllib2.build_opener( urllib2.ProxyHandler()) ); by = urllib2.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); open( os.path.join( ipp, pf), 'wb' ).write(by) if dh == h else None; print('Error validating download (got %s instead of %s), please try manual install' % (dh, h) if dh != h else 'Please restart Sublime Text to finish installation') 给Package Controller设置代理 Sublime Text > Preferences > Package Settings > Package Control > Settings - User 编辑 Package Control.sublime-settings，添加两行: \"http_proxy\": \"http://代理IP地址:3128\", \"https_proxy\": \"http://代理IP地址:3128\", 解决无法安装插件问题 使用第三方Channel，见附件 Preference-->Package Settings-->Package Control-->Settings User { \"bootstrapped\": true, \"channels\": [ \"D:/Sublime Text 3/data/channel_v3.json\" ] } 四、常用快捷键 快捷键 功能 Ctrl+H 查找替换 Ctrl+F 查找内容 Ctrl+Shift+F 在文件夹内查找内容，可进行替换 Ctrl+L 选择一行 Ctrl+Shift+D 复制当前行到下行 Ctrl+K+U 大写光标所在词 Ctrl+K+L 小写光标所在词 Ctrl+Shift+D 复制光标所在整行到下一行 Ctrl+Shift+L 鼠标选中多行（按下快捷键），即可同时编辑这些行 Ctrl+Shift+↑/↓ 可以移动此行代码，与上/下行互换 Ctrl+Shift+←/→ 向右/向右单位性地选中文本 Alt+Shift+1 窗口分屏，恢复默认1屏（非小键盘的数字） Alt+Shift+2 左右分屏-2列 Alt+Shift+3 左右分屏-3列 Alt+Shift+4 左右分屏-3列 Alt+Shift+5 等分4屏 Alt+Shift+8 垂直分屏-2屏 Alt+Shift+9 垂直分屏-3屏 Ctrl+K+B 开启/关闭侧边栏 Ctrl+/ 注释单行 Ctrl+Shift+/ 注释多行 Ctrl+K+K 从光标处开始删除代码至行尾 Ctrl+Shift+K 删除整行 Tab 向右缩进 Shift+Tab 向左缩进 Ctrl+J 合并选中的多行代码为一行 Shift+↑/↓/←/→ 向上/下/左/右选中文本 Ctrl+K+0 展开所有折叠代码 Ctrl+M 光标移动至括号内结束或开始的位置 Ctrl+Shift+M 选择括号内的内容 Ctrl+D 选中光标所占的文本，继续操作则会选中下一个相同的文本 Alt+F3 选中文本按下快捷键，即可一次性选择全部的相同文本进行同时编辑 Ctrl+G 跳转到第几行 Ctrl+Shift+W 关闭所有打开文件 Ctrl+Shift+V 粘贴并格式化 Ctrl+X 删除当前行 Ctrl+Z 撤销 Ctrl+Y 恢复撤销 Ctrl+U 软撤销 Ctrl+T 左右字母互换 Ctrl+Tab 按文件浏览过的顺序，切换当前窗口的标签页 Ctrl+PageDown 向左切换当前窗口的标签页 Ctrl+PageUp 向右切换当前窗口的标签页 Ctrl+W 关闭当前打开文件 Ctrl+Shift+W 关闭所有打开文件 Ctrl+Shift+P 打开命令面板 Ctrl+： 打开搜索框，自动带#，输入关键字，查找文件中的变量名、属性名等。 Ctrl+R 打开搜索框，自动带@，输入关键字，查找文件中的函数名 Ctrl+P 打开搜索框。1、输入当前项目中的文件名，快速搜索文件2、@和关键字，查找文件中函数名3、：和数字，跳转到文件中该行代码4、#和关键字，查找变量名 五、常用插件 插件名 功能 描述 DeleteBlankLines 去除文本中的空白行 Windows: Ctrl+Alt+Backspace --> Delete Blank Lines Ctrl+Alt+Shift+Backspace --> Delete Surplus Blank LinesLinux: Ctrl+Alt+Backspace --> Delete Blank Lines Ctrl+Alt+Shift+Backspace --> Delete Surplus Blank Lines ChineseLocalizations 汉化Sublime Text 请使用主菜单的 帮助/Language 子菜单来切换语言。 目前支持 简体中文 繁体中文 日本語。 要换回英语不需要卸载本插件，请直接从菜单切换英文。 HTML-CSS-JS Prettify HTML/CSS/JS代码格式化 需要安装NodeJS并设置node执行文件的路径右键-->HTML-CSS-JS Prettify-->Set ‘node’ Path GBK Encoding Support 支持gbk编码 Alignment 代码格式的自动对齐 默认快捷键Ctrl+Alt+A Clipboard History 粘贴板历史记录，方便使用复制/剪切的内容 Ctrl+alt+v：显示历史记录Ctrl+alt+d：清空历史记录Ctrl+shift+v：粘贴上一条记录（最旧）Ctrl+shift+alt+v：粘贴下一条记录（最新） ConvertToUTF8 编辑并保存目前编码不被 Sublime Text 支持的文件 IMESupport 支持中文输入法跟随光标 AutoFileName 自动完成文件名的输入，如图片选取 Trailing spaces 检测并一键去除代码中多余的空格 一键删除多余空格：CTRL+SHITF+T（需配置），更多配置请点击标题。快捷键配置：在Preferences / Key Bindings – User加上{ \"keys\": [\"ctrl+shift+t\"], \"command\": \"delete_trailing_spaces\" } FileDiffs 比较当前文件与选中的代码、剪切板中代码、另一文件、未保存文件之间的差别。可配置为显示差别在外部比较工具，精确到行。 右键标签页，出现FileDiffs Menu或者Diff with Tab…选择对应文件比较即可 DocBlockr 生成优美注释 标准的注释，包括函数名、参数、返回值等，并以多行显示，手动写比较麻烦。输入/、/*然后回车，还有很多用法，请参照https://sublime.wbond.net/packages/DocBlockr SideBarEnhancements 增强型侧边栏 Terminal 接使用终端打开你的项目文件夹，并支持使用快捷键。 默认调用系统自带的PowerShell ctrl+shift+t 打开文件所在文件夹，ctrl+shift+alt+t 打开文件所在项目的根目录文件夹 SFTP 快速编辑远程服务器上的文件 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/vscode.html":{"url":"origin/vscode.html","title":"VSCode","keywords":"","body":"VSCode 一、常用设置 1、集成Cygwin中的zsh { \"terminal.integrated.profiles.windows\": { \"Cygwin-zsh\": { \"path\": [ \"C:\\\\Softwares\\\\cygwin\\\\bin\\\\zsh.exe\" ], \"args\": [], \"icon\": \"terminal-bash\" } }, \"terminal.integrated.defaultProfile.windows\":\"Cygwin-zsh\", } 2、集成终端下复制粘贴设置 { \"terminal.integrated.copyOnSelection\": true, \"terminal.integrated.rightClickBehavior\": \"paste\", } 3、Remote SSH插件指定ssh config路径 { \"remote.SSH.configFile\": \"C:\\\\Softwares\\\\cygwin\\\\home\\\\Maker\\\\.ssh\\\\config\", } 4、关闭发生遥测数据 { \"redhat.telemetry.enabled\": false, \"telemetry.telemetryLevel\": \"off\", } 5、控制搜索结果是否显示行号 { \"search.showLineNumbers\": true, } 6、文件自动保存 { \"files.autoSave\": \"afterDelay\", } 7、资源管理器文件夹排列间隔宽度 { \"workbench.tree.indent\": 25, } 二、配置运行Go、Nodejs、Python、Java等程序 .vscode/launch.json { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"vite dev\", \"type\": \"node-terminal\", \"request\": \"launch\", \"command\": \"npm run dev\", \"cwd\": \"${workspaceFolder}/web\" }, { \"name\": \"vite build\", \"type\": \"node-terminal\", \"request\": \"launch\", \"command\": \"npm run build\", \"cwd\": \"${workspaceFolder}/web\" }, { \"name\": \"run go\", \"type\": \"go\", \"request\": \"launch\", \"mode\": \"auto\", \"program\": \"${workspaceFolder}\", \"showLog\": true, } ] } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-02-23 09:52:37 "},"origin/windows-cmd发送SMTP邮件.html":{"url":"origin/windows-cmd发送SMTP邮件.html","title":"CMD发送SMTP邮件","keywords":"","body":"Preflight 邮箱开启POP3/SMTP和IMAP/SMTP服务 一、操作 1. windows开启telnet服务 打开控制面板，找到“打开或关闭windows功能”（在“程序”里面），选中对话框中的Telnet客户端，然后确定，等待完成。这时就开启了telnet功能。 2. 在命令刚窗口输入 telnet smtp.163.com 25 3. 向服务器表明身份 helo 163.com # 如果成功，服务器返回 250 OK 4. 登录认证 auth login # 用户名的Base64加密字符。如果成功，服务器返回一串字符，类似于：334 UGFzc3dvcmQ6（334 是不变的，后面的字母可能会变） ***** # 密码的Base64加密字符，如果登录成功，服务器返回一串字符：235 Authentication successful表示登录成功，如果不能成功登录，请检查账号密码是否正确。 ***** # 对于字符串的Base64加密可使用CMD中的“certutil -encode 包含想要加密字符串的文本文件 Base64加密后输出文本文件” 5. 填写发件人和收件人邮箱地址 mail from: # 若格式不正确，服务器返回501 错误；若格式正确，服务器返回250 Ok。 rcpt to: # 若格式不正确，服务器返回501 错误；若格式正确，服务器返回250 Ok。 6. 编写邮件 data # 服务器返回 354 End data with . To:******@163.com From:******@163.com Subject:test mail From:******@163.com test body 123 . # 服务器返回 250 Ok: queues as ... 表示邮件已经发送 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/windows-小技巧.html":{"url":"origin/windows-小技巧.html","title":"Windows小技巧","keywords":"","body":"1. CMD下的换行符 在CMD下,可以用^作为换行符,类似于Linux下的\\ 2. CMD下查看端口使用情况 netstat -ano |findstr 8080 3. CMD下杀掉进程 taskkill /pid 8080 -t -f 4. CMD下校验文件的MD5、SHA1、SHA256值 certutil -hashfile yourfilename.ext MD5 certutil -hashfile yourfilename.ext SHA1 certutil -hashfile yourfilename.ext SHA256 5. CMD下激活windows系统 使用Docker本地部署KMS服务端 docker run -d -p 1688:1688 --name=kms-server luodaoyi/kms-server 以管理员身份运行CMD 卸载之前的激活密钥 slmgr -upk 设置KMS服务器 slmgr -skms 本地部署的KMS服务端IP地址:1688 常用的KMS服务器 kms.03k.org kms.chinancce.com kms.lotro.cc cy2617.jios.org kms.shuax.com kms.luody.info kms.cangshui.net zh.us.to 122.226.152.230 kms.digiboy.ir kms.library.hk kms.bluskai.com 输入新的密钥 slmgr -ipk 激活密钥 密钥 win10专业版密钥 W269N-WFGWX-YVC9B-4J6C9-T83GX 激活 slmgr -ato 6. PowerShell下载文件 $client = new-object System.Net.WebClient $client.DownloadFile('#1', '#2') # #1为下载链接 #2为文件保存的路径 Note： 一定要在路径中写上保存的新文件的全名（包括后缀） 建议保存的文件格式与下载的文件格式一致 7. 离线安装.NET Framework 3.5 Preflight windows 10 的系统ISO镜像 以管理员身份运行的CMD 将ISO镜像中source/sxs目录拷贝到某个路径下（以桌面为例） 在以管理员身份运行的CMD执行以下命令 dism.exe /online /enable-feature /featurename:netfx3 /Source:C:\\Users\\user\\Desktop\\sxs 8. 添加开机自启动bat脚本 方法一：（推荐） 将脚本放置“C:\\Users\\Curiouser\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup”路径下 方法二： 9. 修改远程桌面的默认端口3389 Windows+R,输入regedit，打开注册表，修改一下注册表的值(十进制)，然后重启远程桌面 HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\Terminal Server\\WinStations\\RDP-Tcp\\PortNumber 防火墙放行新指定的远程桌面端口 10. 防火墙放行指定端口 11. CMD下的用户管理 net user：查看目前系统存在的用户 net user username：查看用户的详细信息 whoami：查看计算机当前登陆的用户 query user：查看已登陆用户的详细信息 logoff+空格+ID号：注销用户 net user 用户名 密码 /add：新增本地用户 net localgroup administrators 用户名 /add：将本地用户加入管理员用户组 net user 用户名 /del：删除用户 runas /user:用户 cmd：以某个用户运行命令 12. Windows软件授权管理工具slmgr命令 13. Window下类Unix终端Cygwin 官网下载地址：https://cygwin.com/install.html 注意：在安装时，会让选择预下载的软件，记得预下载lynx、wget、curl、zsh ①安装apt-cyg包管理器 apt-cyg是Cygwin下类似于apt的包管理器，可安装Github 地址：https://github.com/transcode-open/apt-cyg git clone https://github.com/transcode-open/apt-cyg.git cd apt-cyg install apt-cyg /bin # 或者 lynx -source rawgit.com/transcode-open/apt-cyg/master/apt-cyg > apt-cyg # 先为lynx命令设置代理，不然下载很慢 install apt-cyg /bin # 配置apt-cyg的镜像源 apt-cyg mirror http://mirrors.163.com/cygwin # 更新源 apt-cyg update # 安装软件 apt-cyg install jq vim 参考： https://zhuanlan.zhihu.com/p/66930502 ②安装配置zsh及oh-my-zsh 参考ZSH ③设置默认终端shell $ mkpasswd > /etc/passwd # 然后在/etc/passwd文件中设置当前用户为/bin/zsh ④为lynx命令设置代理 echo -e \"http_proxy:http://localhost:80\\nhttps_proxy:http://localhost:80\" >> /etc/lynx.cfg ⑤mv重命名文件提示“权限不足” 更新cgywin自带的mv apt-cyg install mv 14、Windows下Docker Desktop挂在本地文件或目录 Docker Desktop在Windows下使用使用虚拟机的形式。挂载路径只能是用户家目录下文件或文件夹。特别是在Cygwin中使用docker命令进行挂载时，将用户家目录进行软链在进行挂载 ln -s /cygdrive/c /c docker run -it -v /c/User/用户名/Docker-data:/data nginx sh 15、Windows 查看文件被哪个进程占用 任务管理器 --> 性能 --> 打开资源监视器 --> CPU 下的关联的句柄右侧搜索框内输入文件名称即可查看该文件被那几个程序占用了。 16、Windows下Docker Desktop容器映射端口报错 报错：An attempt was made to access a socket in a way forbidden by its access permissions. 打开Powershell net stop winnat net start winnat 参考：https://github.com/docker/for-win/issues/9272 17、Windows语言栏消失且无法切换输入法 现象：右下角语言栏消失且无法来回切换输入法，原生微软拼音输入法也无法使用，只能输入英文。但在Windows登陆页面的输入法界面可以看到安装的搜狗书法。 原因：手贱禁用掉了系统服务Touch Keyboard and Handwriting Panel Service，导致ctfmon进程无法启动 Point1：手动运行cftmon.exe（Win+R中运行），进程管理器看不到CTF加载程序进程，则说明ctfmon进程无法启动 Point2：检查系统服务中Touch Keyboard and Handwriting Panel Service是否启动，是否禁止运行 18、Windows系统组件修复命令 在系统管理员权限下的CMD中运行以下命令 Dism /Online /Cleanup-Image /CheckHealth 检查映像以查看是否有检测到损坏 Dism /Online /Cleanup-Image /ScanHealth 扫描系统文件并和官方系统文件对比 Dism /Online /Cleanup-Image /RestoreHealth 把不同的系统文件还原成系统官方源文件 sfc /scannow 修复还原 19、Windows包管理器Chocolatey 官网：https://community.chocolatey.org/ 包仓库：https://community.chocolatey.org/packages Github: https://github.com/chocolatey/choco 文档：https://docs.chocolatey.org/en-us/choco/ ①安装 CMD下执行 @\"%SystemRoot%\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\" -NoProfile -InputFormat None -ExecutionPolicy Bypass -Command \"[System.Net.ServicePointManager]::SecurityProtocol = 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\" && SET \"PATH=%PATH%;%ALLUSERSPROFILE%\\chocolatey\\bin\" PowerShell下执行 Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1')) ②相关信息 软件路径：C:\\ProgramData\\chocolatey\\bin 快捷命令 chocolatey = choco别名 cinst = choco install cpush = choco push cuninst = cuninst cup = choco upgrade ③chocolatey配置 # 如果遇到安装软件需要重启操作的话，设置稍后重启，防止无法安装后续软件 choco feature enable --name=\"'exitOnRebootDetected'\" # 安装过程中设置代理 choco install --proxy=127.0.0.1:1088 ④软件管理 自我升级 # 升级chocolatey choco upgrade chocolatey 安装 # 安装软件 choco install -y awk sed nodejs-lts vscode # 或者cinst(choco install的简写) cinst -y awk sed nodejs-lts # 从自定义的源处获取包： choco install {{包名}} --source {{源 URL|别名}} # 允许安装一个包的多个版本 choco install {{包名}} --allow-multiple # 安装一个指定版本的包 choco install {{包名}} --version {{版本号}} # 从一个自定义的配置文件中安装包 choco install {{配置文件的路径}} # 修改安装路径 choco install --install-directory =\"'C:\\Softwares'\" sudo # 安装一个特定的 “nuspec” 或 “nupkg” 文件： choco install {{文件的路径}} # 安装一个非stable的软件 choco install --pre {{包名}} 搜索、列出软件 # 搜索软件 choco search git # 展示源中的软件 choco list --source webpi --page=0 --page-size=25 ⑤备份恢复 备份已安装软件列表 choco list -l --idonly > choco-installed.txt # 导出的软件列表为XML格式，包含了软件的版本信息 choco export -o=\"'c:\\packages.config'\" --include-version-numbers 恢复 choco install packages.config -y ⑥常用软件的安装 choco install --pre --force --ignore-detected-reboot --ignorechecksum --proxy=127.0.0.1:1088 -y \\ wsl-ubuntu-2004 windows-sandbox amd-ryzen-chipset screentogif \\ beyondcompare fsviewer fscapture potplayer wox tiny-pxe-server 7zip winrar teamviewer \\ blender cpu-z.install gpu-z googleearthpro github-desktop \\ googlechrome sourcetree drawio virtualbox nssm postman everything kubernetes-cli kubernetes-helm docker-desktop \\ wakemeonlan wireshark v2ray openvpn \\ winscp.install chocolateygui cygwin cyg-get \\ python golang oracle17jdk maven nodejs-lts nginx jmeter git.install \\ vscode intellijidea-community pycharm-community goland \\ sublimetext3 notepadplusplus.install typora foxitreader \\ ffmpeg opencv openssl.light \\ sudo make vim wget curl jq yq nano sed grep aliyun-cli telnet checksum base64 nmap \\ win32diskimager diskgenius ultraiso etcher \\ wechat dingtalk tencentmeeting xmind wps-office-free telegram.install \\ prometheus-windows-exporter.install sonarqube-scanner.portable kafkaexplorer shellcheck vmrc ssh-copy-id winlogbeat 20、路由管理 添加路由 route add 192.168.1.10 MASK 255.255.255.255 15.16.1.9 # 添加永久路由 route -p add 192.168.1.10 MASK 255.255.255.255 15.16.1.9 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-04-17 13:21:34 "},"origin/windows-server.html":{"url":"origin/windows-server.html","title":"Windows Server管理","keywords":"","body":"Windows Server 管理 一、Windows Server 2012 R2 找回管理员密码 使用系统ISO光盘启动，在安装界面打开CMD（Shift+F10打开），替换放大镜程序 C:\\WINDOWS\\system32\\magnify.exe为C:\\WINDOWS\\system32\\cmd.exe。重启，在锁屏桌面打开放大镜(实际打开的是CMD)，使用net命令重制管理员密码。 进BIOS设置开机从系统ISO光盘启动，启动进入系统安装程序后按Shift+F10进入cmd 在cmd中替换放大镜程序，然后重启系统(从原系统盘启动) X是光盘路径，C盘是系统保留的100M的那个分区，D盘才是我们真正意义上的系统盘（如果有保留的话） # 备份原始放大镜程序 copy D:\\WINDOWS\\system32\\magnify.exe D:\\WINDOWS\\system32\\magnify.exe.bak # 替换放大镜程序 copy D:\\WINDOWS\\system32\\cmd.exe D:\\WINDOWS\\system32\\magnify.exe 在登录页面打开放大镜，然后就在弹出的CMD中使用net命令重置管理员用户 net user Administrator newpassword 然后就可以使用新密码进行登录啦 重制密码登录后再进行1～2操作，恢复原始放大镜程序。 参考 https://blog.csdn.net/njuptxiao/article/details/85098666 https://blog.51cto.com/ilyncsteven/2107216 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/windows-nssm.html":{"url":"origin/windows-nssm.html","title":"Windows进程守护工具NSSM","keywords":"","body":"Windows下的进程守护工具NSSM 一、简介 NSSM(the Non-Sucking Service Manager)是Windows环境下一款免安装的服务管理软件，它可以将应用封装成服务，使之像windows服务可以设置自动启动等。并且可以监控程序运行状态，程序异常中断后自动启动，实现守护进程的功能。不仅支持图形界面操作，也完全支持命令行设置。 同类型的工具还有微软自己的srvany，不过nssm更加简单易用，并且功能强大。它的特点如下： 支持普通exe程序（控制台程序或者带界面的Windows程序都可以） 安装简单，修改方便 可以重定向输出（并且支持Rotation） 可以自动守护封装了的服务，程序挂掉了后可以自动重启 可以自定义环境变量 这里面的每一个功能都非常实用，使用NSSM来封装服务可以大大简化我们的开发流程了。 开发的时候是一个普通程序，降低了开发难度，调试起来非常方便 安装简单，并且可以随时修改服务属性，更新也更加方便 可以利用控制台输出直接实现一个简单的日志系统 不用考虑再加一个服务实现服务守护功能 官方网站：https://nssm.cc/ 下载地址：https://nssm.cc/download （下载解压到windows系统环境Path下，在CMD中可使用即可） 二、配置 使用文档：https://nssm.cc/usage 1、服务的配置 ①安装服务的命令格式 ​ nssm install 服务名 参数项 [...] ②设置服务参数配置的命令格式 nssm set [subparameter] value ③重置服务参数配置的命令格式 nssm reset [subparameter] 三、服务的生命周期管理 1、注册一个服务 # 打开GUI界面配置一个服务 nssm install # 命令行配置一个服务 nssm install 服务名 \"C:\\Program Files\\Java\\jdk1.8.0_251\\bin\\java.exe\" 2、配置一个服务 nssm set 服务名 Application “C:\\Program Files\\Java\\jdk1.8.0_251\\bin\\java.exe” nssm set 服务名 AppDirectory “C:\\Application” nssm set 服务名 AppParameters “-jar test.jar ” nssm set 服务名 DisplayName \"Test\" nssm set 服务名 Description \"测试\" nssm set 服务名 Start SERVICE_AUTO_START 3、列出所有服务 nssm list 4、查看一个服务的配置 nssm get 5、启动一个服务 nssm start 服务名 6、查看服务的状态 nssm status 服务名 7、停止一个服务 nssm stop 服务名 8、重启一个服务 nssm restart 服务名 9、删除一个服务 nssm remove 服务名 10、暂停/继续服务 nssm pause 服务名 nssm continue 服务名 11、手动轮转日志文件 nssm rotate 服务名 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/windows-deployment-service-aik.html":{"url":"origin/windows-deployment-service-aik.html","title":"Windows 无人值守部署服务","keywords":"","body":"Windows 部署服务 一、简介 Windows Server 提供的WDS（Windows Deploy Service）很方便的帮助用户去批量部署windows 操作系统，而且可以和MDT（Microsoft Deployment Toolkit）去结合使用，达到无人值守快速部署Windows系统（Linux下使用的是Kickstart）。 二、Windows Server安装基础服务 DHCP服务 Windows部署服务 三、配置DHCP服务 四、配置Windows部署服务 五、配置Windows部署服务 1、上传Windows ISO镜像文件到服务器中并挂载 2、将ESD转换WIM 参看ESD文件中包括的WIM dism /Get-WimInfo /WimFile:install.esd 转换WIM文件 dism /export-image /SourceImageFile:.\\install.esd /SourceIndex:4 /DestinationImageFile:wndows10-1903.wim /Compress:none /CheckInt egrity /Compress参数项： - fast - recovery - none 参考：https://www.wintips.org/how-to-extract-install-esd-to-install-wim-windows-10-8/ 参考 https://www.jianshu.com/p/3eabf3ae0c27 https://www.dianshouit.com/?thread-63.htm= https://blog.51cto.com/gaowenlong/824781 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/windows-wakeuponlan.html":{"url":"origin/windows-wakeuponlan.html","title":"主机的网络唤醒WOL服务","keywords":"","body":"主机的网络唤醒服务Wake On Lan 一、简介 为了实现远程控制主机的开关机，现在大部分主板及网卡支持网络接收信号唤醒关机状态的主机的功能。 二、BIOS及网卡配置 1、BIOS设置 确保在 BIOS 中的电源管理设置下启用 WOL。 确保在 BIOS 中禁用深度睡眠（不适用于所有系统）。此节能设置会关闭 NIC。 确认系统电源关闭时链路指示灯保持亮起状态。如果没有链路指示灯，则 NIC 无法接收到用于唤醒系统的魔术包。 2、网卡配置允许接收魔幻数据包 注意无线网卡可能无法接收魔幻数据包，所以不支持网络唤醒 Windows 有的网卡需要更新最新驱动才能看到电源管理配置 Linux # 查看网卡是否支持网络唤醒 ethtool eth0 Supports Wake-on: g Wake-on: g # 如果显示g,表示wol功能已经开启 # 使用ethtool设置网卡接受什么形式的网络唤醒 ethtool -s eth0 wol g a -- wake on ARP b -- wake on broadcast messages d -- disable (wake on nothing) f -- wake on filter(s) g -- wake on MagicPacket(tm) m -- wake on multicast messages p -- wake on phy activity s -- enable SecureOn(tm) password for MagicPacket(tm) u -- wake on unicast messages 三、唤醒工具 确认发送魔术包的系统可以对客户端系统执行 ping 命令。 确认魔术包中使用的 MAC 地址与客户端系统上用于以太网 1 的 MAC 地址相匹配。 如果您在魔术包中指定了 IP 地址，则网络交换机可能无法将其正确广播到整个网络。您可能需要更改地址，以将该包广播到整个网络。例如，如果客户端地址为 192.168.1.12，则该包中使用的广播地址将是 192.168.1.255。 命令行 MAOS：brew install wakeonlan Linux：apt/yum install -y wakeonlan 由于主机在关机状态，其他主机无法通过arp协议主动探测其IP地址与MAC地址的映射关系，所以先手动添加ARP记录，再使用唤醒工具发送魔幻数据包。 arp -s IP地址 MAC地址 wakeonlan命令有个zsh插件wakeonlan，创建~/.wakeonlan/Test文件，写入MAC地址 IP地址，即可使用wake Test快捷命令唤醒主机。或者手动使用以下命令 wakeonlan -i 192.168.1.9 -p 9 AA:BB:CC:DD:EE:FF 手机APP IOS：RemoteBoot Python脚本 #!/usr/bin/env python import socket import sys if len(sys.argv) (example: 192.168.1.255 00:11:22:33:44:55)\" sys.exit(1) mac = sys.argv[2] data = ''.join(['FF' * 6, mac.replace(':', '') * 16]) sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1) sock.sendto(data.decode(\"hex\"), (sys.argv[1], 9)) python wake.py 192.168.1.255 00:11:22:33:44:55 Synology NAS 使用IPKGui搜索下载wakelan，默认安装在/opt/bin/路径下 四、魔幻数据包 下图是使用Wireshark抓到的魔幻数据包 参考 https://apple.stackexchange.com/questions/95246/wake-other-computers-from-mac-osx Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/windows-java-exe-install.html":{"url":"origin/windows-java-exe-install.html","title":"如何将 Java代码打包成 EXE 可执行安装软件","keywords":"","body":"如何将 Java代码打包成 EXE 可安装执行软件 一、简介 二、依赖软件 1、exe4j 2、inno setup compiler 3、JRE SpringBoot使用了3.0或者3.0以上，开始最低支持JDK17。 使用 JRE 运行 java jar程序，减少最终安装包的体积。 三、示例步骤 1、使用 IDEA 构建项目的 Artifacts 使用 IDEA 的 Artifacts构建包含项目所有依赖项的 class到 Jar 包。使用 Maven 同样可以实现效果。但是需要繁琐的配置。 IDEA 的Project Structure, 找到Artifacts, 如图添加构建Artifacts的配置 bulid完成后, 在项目中/out文件夹下可以找到jar包。 注意事项： 如果 POM 改过依赖版本，要检查 IDEA 中的 External Libraries中第三方包的版本有没有更新。如果没有，重启 IDEA 后，在 IDEA Maven 工具中重现 Reload project。再次检查版本 查看构建好的Jar包中文件：tar -tvf jar包 2、使用exe4j 使用 exe4j 是无法将 JRE 打到 exe安装包里的 3、 4、 5、 6、 7、 8、 参考 https://blog.csdn.net/wangpaiblog/article/details/119658741 https://blog.csdn.net/orangeTop/article/details/120287684 https://www.cnblogs.com/diysoul/p/14778834.html https://www.ngui.cc/el/2852791.html?action=onClick https://github.com/spring-projects/spring-boot/issues/24889 https://www.eolink.com/news/post/47095.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-07-21 15:55:00 "},"origin/windows-powershell.html":{"url":"origin/windows-powershell.html","title":"PowerShell","keywords":"","body":"PowerShell 一、简介 PowerShell 是基于.NET Framework，因此它支持.NET Framework 中的所有对象类型和方法。这使得它可以访问广泛的系统资源和服务，包括注册表、文件系统、进程和网络。 PowerShell 还支持管道和别名，这使得它非常适合于自动化任务。管道允许你将一个命令的输出作为另一个命令的输入，而别名允许你使用简短的名称来引用命令。 二、自带函数 Get-ChildItem (dir)： 获取指定目录下的文件和文件夹。 Set-Location (cd)： 更改当前目录。 New-Item (mkdir)： 创建新文件夹。 Remove-Item (rmdir)： 删除文件或文件夹。 Copy-Item (copy)： 复制文件或文件夹。 Move-Item (move)： 移动文件或文件夹。 Rename-Item (ren)： 重命名文件或文件夹。 Get-Process： 获取正在运行的进程。 Start-Process： 启动新进程。 Stop-Process： 停止正在运行的进程。 Get-Service： 获取已安装的服务。 Start-Service： 启动服务。 Stop-Service： 停止服务。 Get-EventLog： 获取事件日志。 New-EventLog： 创建新事件日志。 Clear-EventLog： 清除事件日志。 Get-NetworkAdapter： 获取网络适配器。 Enable-NetworkAdapter： 启用网络适配器。 Disable-NetworkAdapter： 禁用网络适配器。 Get-NetIPConfiguration： 获取网络适配器的 IP 配置。 Set-NetIPConfiguration： 设置网络适配器的 IP 配置。 Get-Content： 获取文件的内容。 Set-Content： 设置文件的内容。 Add-Content： 向文件末尾追加内容。 Remove-Content： 清空文件的内容。 Compare-Object： 比较两个对象之间的差异。 Select-Object： 从对象中选择特定的属性。 Where-Object： 根据条件过滤对象。 Sort-Object： 对对象进行排序。 Group-Object： 将对象分组。 ForEach-Object： 对每个对象执行指定的命令。 New-Object： 创建新对象。 Invoke-Command： 在远程计算机上执行命令。 Get-Date： 获取当前日期和时间。 Set-Date： 设置当前日期和时间。 Get-Random： 获取随机数。 ConvertTo-Html： 将对象转换为 HTML 格式。 ConvertFrom-Html： 将 HTML 格式转换为对象。 ConvertTo-Json： 将对象转换为 JSON 格式。 ConvertFrom-Json： 将 JSON 格式转换为对象。 三、常用脚本或命令 1. 获取系统信息脚本 Get-ComputerInfo | Format-List 2. 获取正在运行的进程脚本 Get-Process | Format-Table -Property Name, Id, ProcessName, MainWindowTitle 3. 停止正在运行的服务脚本 Stop-Service -Name \"ServiceName\" 4. 创建新文件夹脚本 New-Item -Path \"C:\\NewFolder\" -ItemType Directory 5. 复制文件脚本 Copy-Item -Path \"C:\\SourceFile.txt\" -Destination \"C:\\DestinationFolder\\DestinationFile.txt\" 6. 重命名文件脚本 Rename-Item -Path \"C:\\OldFileName.txt\" -NewName \"C:\\NewFileName.txt\" 7. 删除文件脚本 Remove-Item -Path \"C:\\FileToDelete.txt\" 8. 创建新注册表项脚本 New-Item -Path \"HKLM:\\Software\\NewKey\" -RegistryHive LocalMachine 9. 设置注册表值脚本 Set-ItemProperty -Path \"HKLM:\\Software\\NewKey\" -Name \"ValueName\" -Value \"ValueData\" 10. 获取注册表值脚本 Get-ItemProperty -Path \"HKLM:\\Software\\NewKey\" -Name \"ValueName\" 11. 删除注册表项脚本 Remove-Item -Path \"HKLM:\\Software\\NewKey\" -Recurse 12. 发送电子邮件脚本 Send-MailMessage -To \"recipient@example.com\" -From \"sender@example.com\" -Subject \"Test Email\" -Body \"This is a test email.\" 13. 下载文件脚本 Invoke-WebRequest -Uri \"https://example.com/file.txt\" -OutFile \"C:\\DownloadedFile.txt\" 14. 解压压缩文件脚本 Expand-Archive -Path \"C:\\CompressedFile.zip\" -Destination \"C:\\ExtractedFiles\" 15. 压缩文件脚本 Compress-Archive -Path \"C:\\FilesToCompress\" -Destination \"C:\\CompressedFile.zip\" 16. 测试网络连通性 测试 IP 地址 Ping 值 测试 IP 地址端口是否打开 测试指定 URL 是否可访问 test.ps1 param( [string]$IPAddress, [int]$Port, [string]$URL ) function Test-Ping { param( [string]$IP ) $pingResult = Test-Connection -ComputerName $IP -Count 1 -ErrorAction SilentlyContinue if ($pingResult) { Write-Output \"$IP 可达！\" } else { Write-Output \"$IP 不可达！\" } } function Test-Port { param( [string]$IP, [int]$Port ) $tcpClient = New-Object System.Net.Sockets.TcpClient try { $tcpClient.Connect($IP, $Port) Write-Output \"$IP 的 $Port 端口可达！\" } catch { Write-Output \"$IP 的 $Port 端口不可达！\" } finally { $tcpClient.Close() } } function Test-URL { param( [string]$URL ) $webRequest = Invoke-WebRequest -Uri $URL -ErrorAction SilentlyContinue if ($webRequest) { Write-Output \"URl可访问！\" } else { Write-Output \"URl不可访问\" } } if ($IPAddress) { Test-Ping -IP $IPAddress } if ($Port) { if (-not $IPAddress) { Write-Error \"测试端口连通性，请至少提供一个 IP 地址.\" } else { Test-Port -IP $IPAddress -Port $Port } } if ($URL) { Test-URL -URL $URL } .\\test.ps1 -IPAdress 192.168.1.1 -Port 8001 -URL https://www.baidu.com Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:33:15 "},"origin/macos-tips.html":{"url":"origin/macos-tips.html","title":"MacOS小技巧","keywords":"","body":"MacOS小技巧 1、DMG 格式文件制作以及 ISO 转换互转 DMG 格式是 Mac OS X 中常用的打包格式 创建 DMG 格式的文件 $ hdiutil create -size 100M -stdinpass -format UDZO -srcfolder folder_to_compress archive_name.dmg UDZO（压缩格式，默认） UDRO（只读格式） UDBZ（Better compressed image） UDRW（可读写格式） UDTO（DVD 格式） 修改 DMG 文件的大小 $ hdiutil resize 150M /path/to/the/diskimage 修改 DMG 格式中的加密口令 $ hdiutil chpass /path/to/the/diskimage 挂载 DMG 格式的文件 $ hdiutil attach archive_name.dmg 它的挂载点在 /Volumes 目录的同名目录下 $ ls -lah /Volumes/archive_name/ 卸载 DMG 文件 $ hdiutil eject /Volumes/archive_name/ 将 ISO 格式的文件转为 DMG 格式的文件 $ hdiutil convert /path/imagefile.iso -format UDRW -o /path/convertedimage.dmg 将 DMG 格式的文件转为 ISO 格式的文件 $ hdiutil convert /path/imagefile.dmg -format UDTO -o /path/convertedimage.cdr $ hdiutil makehybrid /path/convertedimage.cdr -iso -joliet -o /path/convertedimage.iso 2、删除虚拟网络设备 sudo ifconfig utun3 delete 3、路由修改 # 删除路由 ip route delete 172.16.1.2/32 # 添加路由 sudo route add 172.16.1.2/32 -interface utun2 4、HomeBrew安装使用 ①简介 brew 是从下载源码解压然后 ./configure && make install formula：定义了一个软件包。包括了这个软件的，依赖、源码位置及编译方法等 tap：一个包含 formula 的 git 仓库 cask：homebrew 的一个扩展仓库，用来安装 一些带界面的应用软件，下载好后会自动安装 bottle：homebrew 提供的已经编译好的 formula。这些 bottle 可以在这里看到。在大部分的情况下，执行 Homebrew路径 cd \"$(brew --repo)\" Cask仓库路径 cd \"$(brew --repo)\"/Library/Taps/homebrew/homebrew-cask Core仓库路径 cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" ②安装 /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" ③使用 # 安装一个包 brew install # 搜索一个包 brew search brew search /正则表达式/ # 标准格式 brew search /^vi/ #规定了只能是vi开头 brew search /^vi\\\\w$/ # 查看这个包的信息 brew info # 安装图形化的软件 brew cask install # 卸载对应包名字 brew uninstall # 列出哪些包需要更新 brew outdated # 更新过时的包，不带包名就更新所有包 brew upgrade [ package_name ] # 更新HomeBrew自身 brew update # 清除缓存 brew cleanup [包名] # 清理指定包的旧版本 brew cleanup $FORMULA # 查看可清理的旧版本包，不执行实际操作 brew cleanup -n # 列出已经安装的包 brew list # 查看homebrew 的配置 brew config # 添加或者删除仓库 brew [un]tap # 锁定某个包不更新 brew pin $FORMULA # 取消锁定某个包不更新 brew unpin $FORMULA ④使用国内的镜像源 中科大 # Homebrew 源代码仓库 cd \"$(brew --repo)\" git remote set-url origin https://mirrors.ustc.edu.cn/brew.git # Homebrew 核心软件仓库 cd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\" git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git # Homebrew cask 软件仓库，提供 macOS 应用和大型二进制文件 cd \"$(brew --repo)\"/Library/Taps/homebrew/homebrew-cask git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.git # Homebrew cask 其他版本 (alternative versions) 软件仓库，提供使用人数多的、需要的版本不在 cask 仓库中的应用。 cd \"$(brew --repo)\"/Library/Taps/homebrew/homebrew-cask-versions git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask-versions.git # Homebrew 预编译二进制软件包 echo 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles' >> ~/.zshrc source ~/.zshrc ⑤直接从rb文件安装历史版本或官方仓库没有的版本 大部分情况下 homebrew 的仓库只会存在一份最新的软件版本, 有时可能也会同时存在多个版本, 在有些情况下我们可能需要安装某些软件的历史版本。对于 tap 中无多版本的软件, 我们可以通过在 tap 对应的 git 仓库中查看历史 formula 版本, 通过下载到本地进行安装。先通过 formulae.brew.sh找到RB文件的GitHub仓库。然后找到该RB文件历史版本。下载到本地，手动执行以下命令进行安装 brew install ./*.rb 参考：https://juejin.cn/post/7179202980191666233 5、HomeBrew的备份恢复 ①简介 homebrew-bundle - https://github.com/Homebrew/homebrew-bundle Mac 上非常常用的包管理器 Homebrew, 我们经常用它来安装其他的软件包 还有 Homebrew-cask, 可以用来安装图形界面的 App homebrew-bundle 类似 node 中的 package.json 或者 Cocoapods 中的 Podfile 我们将需要的包和 App, 声明在一个 Brewfile 中, 然后执行 brew bundle 即可安装所有包 ②备份内容 brew tap 中的软件库 brew 安装的命令行工具 brew cask 安装的 App Mac App Store 安装的 App ③备份命令 # 执行 brew bundle dump brew bundle dump --describe --force --file=\"~/Desktop/Brewfile\" # 参数说明 --describe：为列表中的命令行工具加上说明性文字。 --force：直接覆盖之前生成的 Brewfile 文件。如果没有该参数，则询问你是否覆盖。 --file=\"~/Desktop/Brewfile\"：在指定位置生成文件。如果没有该参数，则在当前目录生成 Brewfile 文件。 生成的Brewfile 文件内容 ## 该部分是 brew 中的 tap，相当于一个个软件库 tap \"homebrew/bundle\" tap \"homebrew/cask\" ## 该部分是 brew 安装的命令行工具 # Mac App Store command-line interface brew \"mas\" # UNIX shell (command interpreter) brew \"zsh\" # Fish shell like syntax highlighting for zsh brew \"zsh-syntax-highlighting\" ## 该部分是 brew cask 安装的 app cask \"mounty\" cask \"dteoh/sqa/slowquitapps\" ## 该部分是 Mac App Store 安装的 app mas \"ting_en\", id: 734383760 mas \"Xcode\", id: 497799835 ④恢复命令 brew install mas # 批量安装软件 brew bundle --file=\"~/Desktop/Brewfile\" 参考： https://wsgzao.github.io/post/homebrew-bundle/ 6、使用Brew安装的软件信息 ①MySQL 配置文件：/usr/local/etc/my.cnf 日志和底层DB数据文件: /usr/local/var/mysql bin文件路径：/usr/local/Cellar/mysql@mysql版本/mysql版本 brew 启动命令：brew services restart mysql@mysql版本 ②Nginx 主配置文件路径：/usr/local/etc/nginx/nginx.conf bin文件路径: /usr/local/Cellar/nginx/nginx版本号/ 7、stat命令格式化输出 $ echo \"stat -f \"文件: %N%n%n 大小: %Z 字节 类型: %HT%n 所在磁盘分区: %Sd Block编号: %b Inode: %i %n 权限: %Sp(%Mp%Lp) / %Su(%u)-%Sg(%g)%n%n 最近访问时间 : %Sa%n 内容修改时间 : %Sm%n inode修改时间 : %Sc%n 创建时间 : %SB\" -t \"%Y-%m-%d %H:%M:%m\"\" >> ~/.zshrc $ source ~/.zshrc $ stat test.txt 文件: test.txt 大小: 124875 字节 类型: Regular File 所在磁盘分区: disk1s2 Block编号: 256 Inode: 55342799 权限: -rw-r--r--(0644) / curiouser(501)-staff(20) 最近访问时间 : 2021-08-12 19:18:08 内容修改时间 : 2021-08-12 18:47:08 inode修改时间 : 2021-08-12 18:47:08 创建时间 : 2021-07-15 10:19:0721 8、Docker Desktop For Mac ①启动容器报错“no space left on device” 因为MacOS下Docker Desktop的docker是基于虚拟机的。而docker虚拟机的原始存储RAW文件大小超过设置的值后会报上述错误 RAW文件路径：~/Library/Containers/com.docker.docker/Data/vms/0/data/Docker.raw 参考：https://forums.docker.com/t/no-space-left-on-device-error/10894 解决办法： 删除虚悬volumes释放空间：docker volume rm $(docker volume ls -qf dangling=true) 9、Mac 上的“磁盘工具”支持的文件系统格式： Apple 文件系统 (APFS)：macOS 10.13 或后续版本使用的文件系统。 Apple 文件系统 (APFS) 是运行 macOS 10.13 或后续版本的 Mac 电脑所使用的默认文件系统，它具有强加密、空间共享、磁盘快照、快速目录大小统计等特性，以及改进的文件系统基础。虽然 APFS 最适合于新款 Mac 电脑中所用的闪存/SSD 储存，它也可以与使用传统硬盘驱动器 (HDD) 和外置直连储存设备的低版本系统配合使用。macOS 10.13 或后续版本支持 APFS 用于可引导启动的宗卷和数据宗卷。 APFS 按照需求分配容器（分区）中的磁盘空间。单个 APFS 容器包含多个宗卷时，容器的可用空间会共享，并且会自动按需分配到任意单独的宗卷。如果需要，您可以指定每个宗卷的保留大小和配额大小。每个宗卷仅使用整体容器的一部分，这样一来，可用空间即容器的总大小减去该容器中所有宗卷的大小。 为运行 macOS 10.13 或后续版本的 Mac 电脑选取以下其中一种 APFS 格式。 APFS：使用 APFS 格式。如果不需要加密或区分大小写格式，请选取此选项。 APFS（加密）：使用 APFS 格式且加密宗卷。 APFS（区分大小写）：使用 APFS 格式并区分文件和文件夹名称的大小写。例如，名称为“Homework”和“HOMEWORK”的文件夹是两个不同的文件夹。 APFS（区分大小写，加密）：使用 APFS 格式，区分文件和文件夹名称的大小写且加密宗卷。例如，名称为“Homework”和“HOMEWORK”的文件夹是两个不同的文件夹。 您可以轻松添加或删除 APFS 容器中的宗卷。APFS 容器中的每个宗卷都可以拥有其 APFS 格式：APFS、APFS（加密）、APFS（区分大小写）或 APFS（区分大小写，加密）。 Mac OS 扩展：macOS 10.12 或之前版本使用的文件系统。 Mac OS 扩展（日志式）：使用 Mac 格式（日志式 HFS Plus）来保护分层文件系统的完整性。如果不需要加密或区分大小写格式，请选取此选项。 Mac OS 扩展（日志式，加密）：使用 Mac 格式，要求密码，并加密分区。 Mac OS 扩展（区分大小写，日志式）：使用 Mac 格式并区分文件夹名称的大小写。例如，名称为“Homework”和“HOMEWORK”的文件夹是两个不同的文件夹。 Mac OS 扩展（区分大小写，日志式，加密）：使用 Mac 格式，区分文件夹名称的大小写，要求密码，并加密分区。 MS-DOS (FAT) 和 ExFAT：与 Windows 兼容的文件系统。 MS-DOS (FAT)：用于 Windows 宗卷且大小为 32 GB 或不足 32 GB。 ExFAT：用于 Windows 宗卷且大小超过 32 GB。 10、挂载EFI分区 diskutil list # /dev/disk0 # #: TYPE NAME SIZE IDENTIFIER # 0: GUID_partition_scheme *251.0 GB disk0 # 1: EFI 209.7 MB disk0s1 # 2: Apple_HFS Macintosh HD 250.1 GB disk0s2 # 3: Apple_Boot Recovery HD 650.0 MB disk0s3 mkdir /Volumes/EFI sudo mount -t msdos /dev/disk0s1 /Volumes/EFI 11、命令行设置网络代理 sudo networksetup -setwebproxy \"Wi-Fi\" 127.0.0.1 8001 sudo networksetup -setwebproxystate \"Wi-Fi\" on sudo networksetup -setsecurewebproxy \"Wi-Fi\" 127.0.0.1 8001 sudo networksetup -setsecurewebproxystate \"Wi-Fi\" on sudo networksetup -setsocksfirewallproxy \"Wi-Fi\" 127.0.0.1 8002 sudo networksetup -setsocksfirewallproxystate \"Wi-Fi\" on 12、清除 DNS 缓存 MACOS 版本 使用的命令 macOS 12 (Monterey) sudo dscacheutil -flushcache; sudo killall -HUP mDNSResponder macOS 11 (Big Sur) sudo dscacheutil -flushcache; sudo killall -HUP mDNSResponder macOS 10.15 (Catalina) sudo dscacheutil -flushcache; sudo killall -HUP mDNSResponder macOS 10.14 (Mojave) sudo killall -HUP mDNSResponder macOS 10.13 (High Sierra) sudo killall -HUP mDNSResponder macOS 10.12 (Sierra) sudo killall -HUP mDNSResponder OS X 10.11 (El Capitan) sudo killall -HUP mDNSResponder OS X 10.10 (Yosemite) sudo discoveryutil udnsflushcaches OS X 10.9 (Mavericks) sudo killall -HUP mDNSResponder OS X 10.8 (Mountain Lion) sudo killall -HUP mDNSResponder Mac OS X 10.7 (Lion) sudo killall -HUP mDNSResponder Mac OS X 10.6 (Snow Leopard) sudo dscacheutil -flushcache Mac OS X 10.5 (Leopard) sudo lookupd -flushcache Mac OS X 10.4 (Tiger) lookupd -flushcache 13、关掉mds、mds_stores进程 mds_stores是spotlight的后台进程。spotlight为了用户在查询数据的时候能够快速显示查找结果，所以需要对这些文件建立 索引等信息。在建立这些信息的时候，需要对这些文件进行读取分析，并且写入索引等导致磁盘读写非常大。 # 关闭 sudo mdutil -a -i off # 开起 sudo mdutil -a -i on 14、Brew安装旧版软件 如果使用Brew安装指定版本的软件（例如brew install mysql5.6）时，报以下错误。说明该软件的这个版本已不再维护，无法直接安装。但是可通过之前可用的rb文件强制安装。 mysql@5.6 has been disabled because it is not supported upstream! 以下步骤以macOS 12.6 brew 3.6.4安装mysql 5.6版本为例 在网上找到该软件最后可用版本的rb文件 在Homebrew/homebrew-core仓库中查找该软件的最新rb文件。可拼接文件URI链接https://github.com/Homebrew/homebrew-core/blob/master/Formula/软件名@版本号.rb快速找到该文件。之后查看该文件的历史版本，找到最后可用版本的git commit id，之后再次拼接https://github.com/Homebrew/homebrew-core/blob//Formula/软件名@版本号.rb。例如mysql 5.6最后可用版本的rb文件URI：https://github.com/Homebrew/homebrew-core/blob/60fa175e53f36e763315c3d9e66ebcaf1fa21d91/Formula/mysql%405.6.rb class MysqlAT56 :build depends_on \"openssl@1.1\" uses_from_macos \"libedit\" def datadir var/\"mysql\" end # Fixes loading of VERSION file, backported from mysql/mysql-server@51675dd patch :DATA def install # Don't hard-code the libtool path. See: # https://github.com/Homebrew/homebrew/issues/20185 inreplace \"cmake/libutils.cmake\", \"COMMAND /usr/bin/libtool -static -o ${TARGET_LOCATION}\", \"COMMAND libtool -static -o ${TARGET_LOCATION}\" # Fix loading of VERSION file; required in conjunction with patch File.rename \"VERSION\", \"MYSQL_VERSION\" # -DINSTALL_* are relative to `CMAKE_INSTALL_PREFIX` (`prefix`) args = %W[ -DCOMPILATION_COMMENT=Homebrew -DDEFAULT_CHARSET=utf8 -DDEFAULT_COLLATION=utf8_general_ci -DINSTALL_DOCDIR=share/doc/#{name} -DINSTALL_INCLUDEDIR=include/mysql -DINSTALL_INFODIR=share/info -DINSTALL_MANDIR=share/man -DINSTALL_MYSQLSHAREDIR=share/mysql -DMYSQL_DATADIR=#{datadir} -DSYSCONFDIR=#{etc} -DWITH_EDITLINE=system -DWITH_NUMA=OFF -DWITH_SSL=yes -DWITH_UNIT_TESTS=OFF -DWITH_EMBEDDED_SERVER=ON -DWITH_ARCHIVE_STORAGE_ENGINE=1 -DWITH_BLACKHOLE_STORAGE_ENGINE=1 -DENABLED_LOCAL_INFILE=1 -DWITH_INNODB_MEMCACHED=ON ] system \"cmake\", \".\", *std_cmake_args, *args system \"make\" system \"make\", \"install\" # Avoid references to the Homebrew shims directory inreplace bin/\"mysqlbug\", \"#{Superenv.shims_path}/\", \"\" (prefix/\"mysql-test\").cd do system \"./mysql-test-run.pl\", \"status\", \"--vardir=#{Dir.mktmpdir}\" end # Remove the tests directory rm_rf prefix/\"mysql-test\" # Don't create databases inside of the prefix! # See: https://github.com/Homebrew/homebrew/issues/4975 rm_rf prefix/\"data\" # Link the setup script into bin bin.install_symlink prefix/\"scripts/mysql_install_db\" # Fix up the control script and link into bin. inreplace \"#{prefix}/support-files/mysql.server\", /^(PATH=\".*)(\")/, \"\\\\1:#{HOMEBREW_PREFIX}/bin\\\\2\" bin.install_symlink prefix/\"support-files/mysql.server\" libexec.install bin/\"mysqlaccess\" libexec.install bin/\"mysqlaccess.conf\" # Install my.cnf that binds to 127.0.0.1 by default (buildpath/\"my.cnf\").write 本地安装 brew install ./mysql@5.6.rb 15、钥匙串管理 Security是Mac系统中钥匙串和安全模块的命令行管理工具，（图形化工具为Keychain Access.app）。钥匙串（Keychain）实质上就是一个用于存放证书、密钥、密码等安全认证实体的仓库，在计算机中保存为一个.keychain的文件，默认存放在以下目录中（使用这几个目录中的钥匙串时不需要写路径，直接用文件名即可，Security工具会自动搜索）： ~/Library/Keychains/ /Library/Keychains/ /Network/Library/Keychains/ Security命令语法：security [-hilqv] [-p prompt] [command] [command options] [command args] -i 交互模式 -p 进入交互模式，使用自定义提示符 -q 减少信息输出 -v 增加信息输出 -l security退出前，调用/usr/bin/leaks -nocontext检查执行的命令是否有泄漏 子命令列表如下 create-keychain 创建钥匙串并加入搜索列表 list-keychains 显示或设置钥匙串搜索列表 default-keychain 显示或设置默认的钥匙串 login-keychain 显示或设置登录钥匙串 delete-keychain 删除钥匙串并从搜索列表移除 lock-keychain 锁定制定的钥匙串 unlock-keychain 解锁制定的钥匙串 set-keychain-settings 设置钥匙串配置 set-keychain-password 设置钥匙串密码 dump-keychain 显示一个或多个钥匙串的内容 create-keypair 创建非对称密钥对 add-generic-password 向钥匙串中添加通用密码项 find-generic-password 查找通用密码项 delete-generic-password 删除通用密码项 add-internet-password 向钥匙串中添加网络密码项 find-internet-password 查找网络密码项 delete-internet-password 删除网络密码项 add-certificates 向钥匙串种添加证书 find-certificate 查找证书 delete-certificate 从钥匙串种删除证书 find-identity 查找认证实体（证书+私钥） set-identity-preference get-identity-preference create-db export import cms 编码或解码CMS信息（PKCS#7） install-mds 安装/重装MDS 数据库 add-trusted-cert 添加可信证书（只包含公钥，无私钥） remove-trusted-cert 删除可信证书 dump-trusted-setting 显示信任配置 user-trust-settings-enable 显示或管理用户级别的信任配置 trust-settings-export 导出信任配置 trust-settings-import 导入信任配置 verify-cert 验证证书 authorize 授权操作 authorizationdb 变更授权策略数据库 execute-with-privileges 带特权执行工具 leaks 在本进程中运行/usr/bin/leaks error 显示指定错误码的描述信息 create-filevaultmaster-keychain 创建一个带密钥对的钥匙串，用于FileVault恢复（FileVault是苹果系统里的一项保密机制，会自动透明地对主目录的内容进行实时加密和解密） ①添加应用程序密码 语法：security add-generic-password [-a account] [-s service] [-w password] [options...] [-A|-T appPath] [keychain] -a Specify account name (required) -c Specify item creator (optional four-character code) -C Specify item type (optional four-character code) -D Specify kind (default is \"application password\") -G Specify generic attribute (optional) -j Specify comment string (optional) -l Specify label (if omitted, service name is used as default label) -s Specify service name (required) -p Specify password to be added (legacy option, equivalent to -w) -w Specify password to be added -X Specify password data to be added as a hexadecimal string -A Allow any application to access this item without warning (insecure, not recommended!) -T Specify an application which may access this item (multiple -T options are allowed) -U Update item if it already exists (if omitted, the item cannot already exist) security add-generic-password -a 账号 -s 名称 -w 密码 ②查找账号信息 语法：security find-generic-password [-a account] [-s service] [options...] [-g] [keychain...] -a Match \"account\" string -c Match \"creator\" (four-character code) -C Match \"type\" (four-character code) -D Match \"kind\" string -G Match \"value\" string (generic attribute) -j Match \"comment\" string -l Match \"label\" string -s Match \"service\" string -g Display the password for the item found -w Display only the password on stdout security find-generic-password -s 名称 -g 参考 https://git.herrbischoff.com/awesome-macos-command-line/about/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:39:50 "},"origin/linux-小技巧.html":{"url":"origin/linux-小技巧.html","title":"Linux小技巧","keywords":"","body":"1、SSH key加密算法 RSA，DSA，ECDSA，EdDSA和Ed25519都用于数字签名，但只有RSA也可以用于加密。根据数学特性，这四种类型又可以分为两大类，dsa/rsa是一类，ecdsa/ed25519是一类，后者算法更先进。 RSA（Rivest–Shamir–Adleman）：是最早的公钥密码系统之一，被广泛用于安全数据传输。它的安全性取决于整数分解，因此永远不需要安全的RNG（随机数生成器）。与DSA相比，RSA的签名验证速度更快，但生成速度较慢。 DSA（数字签名算法）：是用于数字签名的联邦信息处理标准。它的安全性取决于离散的对数问题。与RSA相比，DSA的签名生成速度更快，但验证速度较慢。如果使用错误的数字生成器，可能会破坏安全性。从OpenSSH 7.0开始，默认情况下SSH不再支持DSA密钥（ssh-dss）。 ECDSA（椭圆曲线数字签名算法）：是DSA（数字签名算法）的椭圆曲线实现。椭圆曲线密码术能够以较小的密钥提供与RSA相对相同的安全级别。它还具有DSA对不良RNG敏感的缺点。dsa因为安全问题，已不再使用了。ecdsa因为政治原因和技术原因，也不推荐使用 EdDSA（爱德华兹曲线数字签名算法）：是一种使用基于扭曲爱德华兹曲线的Schnorr签名变体的数字签名方案。签名创建在EdDSA中是确定性的，其安全性是基于某些离散对数问题的难处理性，因此它比DSA和ECDSA更安全，后者要求每个签名都具有高质量的随机性。 Ed25519：是EdDSA签名方案，但使用SHA-512 / 256和Curve25519；它是一条安全的椭圆形曲线，比DSA，ECDSA和EdDSA 提供更好的安全性，并且具有更好的性能（人为注意）。ed25519是目前最安全、加解密速度最快的key类型，由于其数学特性，它的key的长度比rsa小很多，优先推荐使用。它目前唯一的问题就是兼容性，即在旧版本的ssh工具集中可能无法使用。 ssh-keygen -t ed25519 -C \"curiouser@curiouser.com\" -f ./id_ed25519 如果可以的话，优先选择ed25519，否则选择rsa。 参考： https://security.stackexchange.com/questions/90077/ssh-key-ed25519-vs-rsa https://www.cnblogs.com/librarookie/p/15389876.html 2、bash不显示路径 命令行会变成-bash-3.2$主要原因可能是用户主目录下的配置文件丢失 # 方式一 cp -a /etc/skel/. ~ # 方式二 echo \"export PS1='[\\u@\\h \\W]\\$'\" >> ~/.bash_profile ;\\ source ~/.bash_profile 3、同时监控多个文件 tail -f file1 file2 4、查看网卡 # 方式一 ifconfig -a # 方式二 cat /proc/net/dev 5、cp目录下的带隐藏文件的子目录 cp -R /home/test/* /tmp/test /home/test下的隐藏文件都不会被拷贝，子目录下的隐藏文件倒是会的 cp -R /home/test/. /tmp/test cp的时候有重复的文件需要覆盖时会让不停的输入yes来确认，可以使用yes| yes|cp -r /home/test/. /tmp/test 6、获取出口IP地址 curl http://members.3322.org/dyndns/getip curl cip.cc curl myip.ipip.net curl ifconfig.me curl ipinfo.io curl ipinfo.io/ip curl icanhazip.com curl ifconfig.me/ip ; echo 7、ISO自动挂载 echo \"/mnt/iso/CentOS-7-x86_64-Minimal-1804.iso /mnt/cdrom iso9660 defaults,loop 0 0\" >> /etc/fstab && \\ mount -a && \\ df -mh 8、查看系统版本号和内核信息 cat /proc/version uname -a lsb_release -a cat /etc/redhat-release cat /etc/issue rpm -q redhat-release 9、查看物理CPU个数、核数、逻辑CPU个数 CPU总核数 = 物理CPU个数 * 每颗物理CPU的核数 总逻辑CPU数 = 物理CPU个数 * 每颗物理CPU的核数 * 超线程数 # 查看CPU信息（型号） cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c # 查看物理CPU个数 cat /proc/cpuinfo| grep \"physical id\"| sort| uniq| wc -l # 查看每个物理CPU中core的个数(即核数) cat /proc/cpuinfo| grep \"cpu cores\"| uniq # 查看逻辑CPU的个数 cat /proc/cpuinfo| grep \"processor\"| wc -l 10、Linux缓存 cached是cpu与内存间的，buffer是内存与磁盘间的，都是为了解决速度不对等的问题。buffer是即将要被写入磁盘的，而cache是被从磁盘中读出来的 buff：作为buffer cache的内存，是块设备的读写缓冲区 cache：作为page cache的内存，文件系统的cache。Buffer cache是针对磁盘块的缓存，也就是在没有文件系统的情况下，直接对磁盘进行操作的数据会缓存到buffer cache中。 pagecache：页面缓存（pagecache）可以包含磁盘块的任何内存映射。这可以是缓冲I/O，内存映射文件，可执行文件的分页区域——操作系统可以从文件保存在内存中的任何内容。Page cache实际上是针对文件系统的，是文件的缓存，在文件层面上的数据会缓存到page cache。 dentries：表示目录的数据结构 inodes：表示文件的数据结构 #内核配置接口 /proc/sys/vm/drop_caches 可以允许用户手动清理cache来达到释放内存的作用，这个文件有三个值：1、2、3（默认值为0） #释放pagecache echo 1 > /proc/sys/vm/drop_caches #释放dentries、inodes echo 2 > /proc/sys/vm/drop_caches #释放pagecache、dentries、inodes echo 3 > /proc/sys/vm/drop_caches 11、设置代理 $> bash -c 'cat >> /etc/profile 注意： 当使用“export http_proxy”和“export https_proxy”设置代理时，curl默认所有的请求都是走的代理，请求域名不通过/etc/hosts解析。 所以当有需求curl命令不走代理，通过/etc/hosts解析时，代理设置要通过“export HTTP_PROXY”和“export HTTPS_PROXY”设置。（原因是url.c（版本7.39中的第4337行）处看先检查小写版本，如果找不到，则检查大写。链接：https://stackoverflow.com/questions/9445489/performing-http-requests-with-curl-using-proxy） no_proxy不支持模糊匹配。不支持*.a.com，支持.a.com 12、查看网卡UUID nmcli con | sed -n '1,2p' 13、时间戳与日期 日期与时间戳的相互转换 #将日期转换为Unix时间戳 date +%s #将Unix时间戳转换为指定格式化的日期时间 date -d @1361542596 +\"%Y-%m-%d %H:%M:%S\" date日期操作 date +%Y%m%d #显示前天年月日 date -d \"+1 day\" +%Y%m%d #显示前一天的日期 date -d \"-1 day\" +%Y%m%d #显示后一天的日期 date -d \"-1 month\" +%Y%m%d #显示上一月的日期 date -d \"+1 month\" +%Y%m%d #显示下一月的日期 date -d \"-1 year\" +%Y%m%d #显示前一年的日期 date -d \"+1 year\" +%Y%m%d #显示下一年的日期 获得毫秒级的时间戳 在linux Shell中并没有毫秒级的时间单位，只有秒和纳秒。其实这样就足够了，因为纳秒的单位范围是（000000000..999999999），所以从纳秒也是可以的到毫秒的 current=`date \"+%Y-%m-%d %H:%M:%S\"` #获取当前时间，例：2015-03-11 12:33:41 timeStamp=`date -d \"$current\" +%s` #将current转换为时间戳，精确到秒 currentTimeStamp=$((timeStamp*1000+`date \"+%N\"`/1000000)) #将current转换为时间戳，精确到毫秒 echo $currentTimeStamp 时间+时区 current_timestamp=$(date +%s) new_timestamp=$(( current_timestamp + 8*3600 )) starttime=$(date -d \"@$new_timestamp\" +'%Y-%m-%d %H:%M:%S') 或者 date -d '+8 hours' +'%Y-%m-%d %H:%M:%S' 14、nohup手动后台运行进程并记录进程号 nohup jar -jar jar包 /data/app/logs/app.log 2>&1 & echo $! > /data/app/run.pid # 2>&1是把标准错误2重定向到标准输出1中，而标准输出又导入文件里面，所以标准错误和标准输出都会输出到文件。 # 同时把启动的进程号pid输出到文件 注意： 如果运行时的shell为zsh，将任务放置后台的命令由”&“变为”&!“。 参考：https://stackoverflow.com/questions/19302913/exit-zsh-but-leave-running-jobs-open 15、生成文件的MD值 在网络传输、设备之间转存、复制大文件等时，可能会出现传输前后数据不一致的情况。这种情况在网络这种相对更不稳定的环境中，容易出现。那么校验文件的完整性，也是势在必行的。 在网络传输时，我们校验源文件获得其md5sum，传输完毕后，校验其目标文件，并对比如果源文件和目标文件md5 一致的话，则表示文件传输无异常。否则说明文件在传输过程中未正确传输。 md5值是一个128位的二进制数据，转换成16进制则是32（128/4）位的进制值。 md5校验，有很小的概率不同的文件生成的md5可能相同。比md5更安全的校验算法还有SHA*系列的。 Linux的md5sum命令 md5sum命令用于生成和校验文件的md5值。它会逐位对文件的内容进行校验。是文件的内容，与文件名无关，也就是文件内容相同，其md5值相同。 #md5sum命令的详解 $> md5sum --h Usage: md5sum [OPTION]... [FILE] With no FILE, or when FILE is -, read standard input. -b, --binary 二进制模式读取文件 -c, --check 从文件中读取、校验MD5值 --tag 创建一个BSD-style风格的校验值 -t, --text 文本模式读取文件（默认） #校验文件MD5值使用的参数 The following four options are useful only when verifying checksums: --quiet don't print OK for each successfully verified file --status don't output anything, status code shows success --strict exit non-zero for improperly formatted checksum lines -w, --warn warn about improperly formatted checksum lines --help display this help and exit --version output version information and exit #生成的MD5值重定向到文件中 $>md5sum filename > filename.md5 #生成的MD5值重定向追加到文件中 $> md5sum filename >>filename.md5 #多个文件输出到一个md5文件中，这要使用通配符* $> md5sum *.iso > iso.md5 #同时计算多个文件的MD5值 $> md5sum filetohashA.txt filetohashB.txt filetohashC.txt > hash.md5 #校验MD5:把下载的文件file和该文件的file.md5报文摘要文件放在同一个目录下 $> md5sum -c file.md5 #创建一个BSD风格的校验值 $> md5sum --tag file.md5 MD5 (file.md5) = 9192e127b087ed0ae24bb12070f3051a Python生成MD5值 # 方式一：使用md5包 import md5 src = 'this is a md5 test.' m1 = md5.new() m1.update(src) print m1.hexdigest() # 方式二：使用hashlib（推荐） import hashlib m2 = hashlib.md5() m2.update(src) print m2.hexdigest() # 加密常见的问题： 1：Unicode-objects must be encoded before hashing 　　解决方案：import hashlib 　　　　　　　m2 = hashlib.md5() 　　　　　　　m2.update(src．encode('utf-8')) 　　　　　　　print m2.hexdigest() Java生成MD5值 import java.security.MessageDigest; public static void main(String[] args) { String password = \"123456\"; try { MessageDigest instance = MessageDigest.getInstance(\"MD5\");// 获取MD5算法对象 byte[] digest = instance.digest(password.getBytes());// 对字符串加密,返回字节数组 StringBuffer sb = new StringBuffer(); for (byte b : digest) { int i = b & 0xff;// 获取字节的低八位有效值 String hexString = Integer.toHexString(i);// 将整数转为16进制 // System.out.println(hexString); if (hexString.length() 16、添加用户 useradd (选项) （参数） #选项 －c：加上备注文字，备注文字保存在passwd的备注栏中 －d：指定用户登入时的启始目录 －D：变更预设值 －e：指定账号的有效期限，缺省表示永久有效 －f：指定在密码过期后多少天即关闭该账号 －g：指定用户所属的起始群组 －G：指定用户所属的附加群组 －m：自动建立用户的登入目录 －M：不要自动建立用户的登入目录 －n：取消建立以用户名称为名的群组 －r：建立系统账号 －s：指定用户登入后所使用的shell －u：指定用户ID号 17、su 与 sudo su : switch to another user 切换用户 sudo : superuser do 允许用户使用superuser的身份执行命令 su username ：切换为username，需要输入username密码 su : 切换为root用户，需要输入root密码 su - : 切换为root用户，需要输入root密码，且环境变量也改变 su - -c \"command\" ：使用root身份执行命令，完成后即退出root身份 sudo command : 与su -c相似，需要输入当前用户（superuser，/etc/sudoers中指定）密码 sudo su -：使用当前用户密码实现root身份的切换 su - hdfs -c command 切换用户并以某用户的身份去执行一条命令 su - hdfs test.sh 切换用户并以某用户的身份去执行一个shell文件 18、重新开启SELinux 如果在使用setenforce命令设置selinux状态的时候出现这个提示：setenforce: SELinux is disabled。那么说明selinux已经被彻底的关闭了,如果需要重新开启selinux vi /etc/selinux/config 更改为：SELINUX=1 必须重启linux，不重启是没办法立刻开启selinux的 重启完以后，使用getenforce,setenforce等命令就不会报“setenforce: SELinux is disabled”了。这时，我们就可以用setenforce命令来动态的调整当前是否开启selinux。 19、检查软件是否已安装，没有就自动安装 rpm -qa |grep \"jq\" if [ $? -eq 0 ] ;then echo \"jq hava been installed \" else yum -y install epel-release && yum -y install jq fi 20、使用privoxy代理http，https流量使用socket连接ShadowSocks服务器 echo \"安装ShadowSocks\" && \\ yum -y install epel-release && yum -y install python-pip && \\ pip install shadowsocks && \\ bash -c 'cat > /etc/shadowsocks.json /etc/systemd/system/shadowsocks.service > /etc/profile && \\ echo \"export https_proxy=http://127.0.0.1:8118\" >> /etc/profile && \\ source /etc/profile && \\ curl www.google.com 21、批量打通指定主机SSH免密钥登录脚本 CentOS $> bash -c 'cat > ./HitthroughSSH.sh ./hosts.txt 22、硬盘自动分区，格式化，开机自动挂载到/data disk=/dev/sdc;\\ bash -c \"fdisk ${disk}>/etc/fstab ;\\ sed -i '$ s/$/ \\/data ext4 defaults 0 0/' /etc/fstab ;\\ mkdir /data ;\\ mount -a ;\\ df -h 23、在hosts文件中添加IP地址与主机名的域名映射 ipaddr=$(ip addr | awk '/^[0-9]+: / {}; /inet.*global/ {print gensub(/(.*)\\/(.*)/, \"\\\\1\", \"g\", $2)}'| sed -n '1p') && \\ echo $ipaddr $HOSTNAME >> /etc/hosts 24、Linux禁用透明大页 Redhat sed -i '$a echo nerver > /sys/kernel/mm/redhat_transparent_hugepage/defrag\\necho nerver > /sys/kernel/mm/redhat_transparent_hugepage/enabled' CentOS echo never > /sys/kernel/mm/transparent_hugepage/defrag ;\\ echo never > /sys/kernel/mm/transparent_hugepage/enabled ;\\ sed -i '/GRUB_CMDLINE_LINUX/ s/\"$/ transparent_hugepage=never\"/' /etc/default/grub ;\\ grub2-mkconfig -o /boot/grub2/grub.cfg 25、基础服务软件安装 ①JDK环境 Prerequisite： JDK安装包已下载在内网HTTP服务器中 curl -# http://192.168.1.7:32770/repository/public-resources/jdk-8u241-linux-x64.tar.gz | tar -zxC /opt/ && \\ ln -s `ls /opt |grep jdk1.8.0_241*| sed \"s:^:/opt: \"` /opt/jdk && \\ sed -i '$a export JAVA_HOME=/opt/jdk\\nexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\\nexport PATH=$PATH:$JAVA_HOME/bin' /etc/profile && \\ source /etc/profile && \\ ln -s /opt/jdk/bin/java /usr/bin/java && \\ java -version && \\ javac -version ②安装Tomcat，并由systemctl托管 Prerequisite： 已安装JDK Tomcat安装包已下载在内网HTTP服务器中 wget http://192.168.1.2/tomcat/apache-tomcat-8.5.20.tar.gz;\\ tar -zxvf apache-tomcat-8.5.20.tar.gz -C /opt;\\ rm -rf apache-tomcat-8.5.20.tar.gz;\\ ln -s /opt/apache-tomcat-8.5.20 /opt/tomcat;\\ bash -c 'cat > /lib/systemd/system/tomcat.service ③安装Nginx bash -c 'cat > /etc/yum.repos.d/nginx.repo ④安装最新stable单机Zookeeper Prerequisite： 已安装JDK download_url=`echo https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/``curl -s -L https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/stable/ |grep apache-zookeeper | awk -F \">\" '{print $2}'|awk -F \"\\\"\" '{print $2}' | head -n 1` && \\ curl -# $download_url | tar -zxC /opt/ && \\ ln -s `ls /opt/ |grep apache-zookeeper-* | sed \"s:^:/opt/: \"` /opt/zookeeper && \\ sed -i '$a export ZOOKEEPER_HOME=/opt/zookeeper\\nexport PATH=$PATH:$ZOOKEEPER_HOME/bin' /etc/profile && \\ source /etc/profile && \\ cp /opt/zookeeper/conf/zoo_sample.cfg /opt/zookeeper/conf/zoo.cfg && \\ sed -i -e '/dataDir/d' -e '/dataLogDir/d' /opt/zookeeper/conf/zoo.cfg && \\ sed -i -e '$a dataDir=/data/zookeeper/data\\ndataLogDir=/data/zookeeper/logs\\nserver.1=127.0.0.1:2888:3888\\nautopurge.purgeInterval=24\\nautopurge.purgeInterval=5\\nadmin.enableServer=true\\nadmin.enableServer=true admin.serverPort=9990' /opt/zookeeper/conf/zoo.cfg && \\ mkdir -p /data/zookeeper/{data,logs} && \\ echo \"1\" > /data/zookeeper/data/myid && \\ zkServer.sh start && \\ zkServer.sh status && \\ jps -l # admin server 访问地址：http://主机IP地址:9990/commands ⑤安装最新stable单机的Kafka Prerequisite： 已安装Zookeeper download_d=`echo https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/``curl -sL https://mirrors.tuna.tsinghua.edu.cn/apache/kafka |grep \\`date +%Y\\` |grep \"folder.gif\" | tac | head -n 1 |awk -F \">\" '{print $3}' |awk -F \"/\" '{print $1}'` && \\ download_url=`echo $download_d/``curl -sL $download_d |grep kafka_ | tac | head -n 1 | awk -F \">\" '{print $2}' | awk -F \"\\\"\" '{print $2}'` && \\ curl -# $download_url | tar -zxC /opt/ && \\ ln -s `ls /opt |grep kafka_*| sed \"s:^:/opt/: \"` /opt/kafka && \\ sed -i '$a export KAFKA_HOME=/opt/kafka\\nexport PATH=$PATH:$KAFKA_HOME/bin' /etc/profile && \\ source /etc/profile && \\ cp /opt/kafka/config/server.properties /opt/kafka/config/server_bak.properties && \\ sed -i '/\\#\\ Log\\ directory\\ to\\ use/iLOG_DIR=\\/data\\/kafka\\/logs' /opt/kafka/bin/kafka-run-class.sh && \\ sed -i -e 's/log.dirs=\\/tmp\\/kafka-logs/log.dirs=\\/data\\/kafka\\/data/g' -e 's/log.retention.hours=168/log.retention.hours=12/g' -e '$a auto.create.topics.enable=true\\ndelete.topic.enable=true' /opt/kafka/config/server.properties && \\ mkdir -p /data/kafka/{logs,data} && \\ kafka-server-start.sh -daemon /opt/kafka/config/server.properties && \\ jps -l ⑥安装Hadoop客户端 以hadoop 2.8.3版本为例 wget https://archive.apache.org/dist/hadoop/common/hadoop-2.8.3/hadoop-2.8.3.tar.gz ;\\ tar -xvf hadoop-2.8.3.tar.gz -C /opt ;\\ rm -rf hadoop-2.8.3.tar.gz ;\\ ln -s /opt/hadoop-2.8.3 /opt/hadoop ;\\ sed -i '$a export HADOOP_HOME=/opt/hadoop\\nexport PATH=$PATH:$HADOOP_HOME/bin' /etc/profile ;\\ source /etc/profile #然后在/opt/hadoop-2.8.3/etc/hadoop/core-site.xml配置文件标签中填写HDFS NameNode节点的IP地址及端口号 fs.default.name hdfs://172.16.3.10:9000 hdfs dfs -ls / ⑦安装Maven环境 curl https://mirrors.tuna.tsinghua.edu.cn/apache/maven/binaries/apache-maven-3.2.2-bin.tar.gz -o /opt/apache-maven-3.2.2-bin.tar.gz && \\ tar -zxvf /opt/apache-maven-*.tar.gz -C /opt/ && \\ rm -rf /opt/apache-maven-*.tar.gz && \\ ln -s /opt/apache-maven-3.2.2 /opt/maven && \\ sed -i '$a export M2_HOME=/opt/maven\\nexport PATH=$PATH:$M2_HOME/bin' /etc/profile && \\ source /etc/profile && \\ mvn version ⑧安装NodeJS环境 wget https://nodejs.org/dist/v8.9.4/node-v8.9.4-linux-x64.tar.xz ;\\ tar -xvf node-v8.9.4-linux-x64.tar.xz -C /opt/ ;\\ rm -rf node-v8.9.4-linux-x64.tar.xz ;\\ ln -s /opt/node-v8.9.4-linux-x64 /opt/nodejs ;\\ sed -i '$a export NODEJS_HOME=/opt/nodejs\\nexport PATH=$PATH:$NODEJS_HOME/bin' /etc/profile;\\ source /etc/profile;\\ yum install gcc-c++ make -y;\\ npm config set registry https://registry.npm.taobao.org ;\\ npm config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/ ;\\ npm version 26、安装docker/docker-compose 脚本自动安装 sudo curl -sSL https://get.docker.com | sh CentOS/Redhat 设置新硬盘LVM成docker的数据目录 yum install -y yum-utils epel-rease lvm2 && \\ yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo && \\ yum list docker-ce --showduplicates | sort -r && \\ yum install -y docker-ce docker-compose && \\ mkdir /etc/docker && \\ bash -c 'cat > /etc/docker/daemon.json > /etc/fstab && \\ df -mh && \\ systemctl daemon-reload && \\ systemctl enable docker && \\ systemctl start docker && \\ docker info &&\\ docker info |grep \"Insecure Registries:\" -A 4 && \\ ls /var/lib/docker/ Ubuntu apt-get remove docker docker-engine docker.io containerd runc && \\ apt install -y software-properties-common && \\ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - # X86_64 sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" # arm64 sudo add-apt-repository \\ \"deb [arch=arm64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" apt-get update && \\ apt-get install -y docker-ce && \\ touch /etc/docker/daemon.json && \\ bash -c ' tee /etc/docker/daemon.json 27、字符转换命令expand/unexpand 用于将文件的制表符（Tab）转换为空格符（Space），默认一个Tab对应8个空格符，并将结果输出到标准输出。若不指定任何文件名或所给文件名为”-“，则expand会从标准输入读取数据。 功能与之相反的命令是unexpand，是将空格符转成Tab符。 vi/vim在命令模式下通过设置\":set list\"可显示文件中的制表符“^I” expand命令参数 -i, --initial do not convert tabs after non blanks -t, --tabs=NUMBER have tabs NUMBER characters apart, not 8 -t, --tabs=LIST use comma separated list of explicit tab positions --help display this help and exit --version output version information and exit unexpand命令参数 -a, --all convert all blanks, instead of just initial blanks --first-only convert only leading sequences of blanks (overrides -a) -t, --tabs=N have tabs N characters apart instead of 8 (enables -a) -t, --tabs=LIST use comma separated LIST of tab positions (enables -a) --help display this help and exit --version output version information and exit 实例 将文件中每行第一个Tab符替换为4个空格符，非空白符后的制表符不作转换 #使用\"----\"或\"--\"代表一个制表符，使用\":\"代表一个空格 ----abcd--e $ expand -i -t 4 old-file > new-file ::::abcd--e 注意 不是所有的Tab都会转换为默认或指定数量的空格符，expand会以对齐为原则将Tab符替换为适当数量的空格符，替换的原则是使后面非Tab符处在一个物理Tab边界（即Tab size的整数倍。例如： #使用\"----\"或\"--\"代表一个制表符，使用\":\"代表一个空格 abcd----efg--hi $ expand -t 4 file abcd::::efg::hi 28、修改时区 Docker容器中 添加环境变量：TZ = Asia/Shanghai Linux主机 timedatectl set-timezone \"Asia/Shanghai\" # 设置时区 timedatectl status # 查看当前的时区状态 date -R # 查看时区 或者 cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 29、shell脚本的调试 在脚本运行时添加-x参数 在脚本中开头添加set -x 30、删除“-”开头的文件或文件夹 当直接使用rm -f删除以-开头的文件与文件夹时，rm或其他命令报参数错误，会误认为-后面的内容是命令的参数 rm -rf -- -XGET cd -- -XGET 31、硬盘快速分区 方式一：使用parted命令 parted命令详解：https://www.cnblogs.com/Cherry-Linux/p/10103172.html disk=/dev/vdb && \\ parted -s -a optimal $disk mklabel gpt -- mkpart primary ext4 1 -1 方式二：使用fdisk disk=/dev/vdb && \\ bash -c \"fdisk ${disk} 32、别名传参 别名并不能直接传参，但是可以使用以下方式代替： 方式一：使用functions替代 $ test () { num=${1:-5} dmesg |grep -iw usb|tail -$num } $ test 5 方式二：使用read读取输入，然后使用变量替换命令中的参数 $ alias taila='{ IFS= read -r line_num && tail -n $line_num /var/logs/message ;} 参考： https://askubuntu.com/questions/626458/can-i-pass-arguments-to-an-alias-command https://www.kutu66.com//ubuntu/article_158110 33、Ubuntu/Debian的镜像源URL字段 Nexus设置apt proxy仓库，代理http://archive.ubuntu.com/ubuntu/ deb http://192.168.1.6:8080/repository/apt-ubuntu/ bionic main restricted deb http://192.168.1.6:8080/repository/apt-ubuntu/ bionic-security main restricted deb http://192.168.1.6:8080/repository/apt-ubuntu/ bionic-updates main restricted deb http://192.168.1.6:8080/repository/apt-ubuntu/ bionic-proposed main restricted deb http://192.168.1.6:8080/repository/apt-ubuntu/ bionic-backports main restricted 第一字段，指示包类型。 deb：二进制包 deb-src：源码包 第二字段，指示镜像站点，即「源」！ URL 定位到某个目录，该目录下必有「dists」「pool」两个子目录。如： http://ftp.cn.debian.org/debian/ http://ftp.sjtu.edu.cn/ubuntu/ 第三字段，指示包的「版本类型」，姑且称为「仓库」。 打开某源，进入「dists」子目录可见该源中有哪些仓库，即其下诸子目录。命名形式为「系统发行版名-仓库名」，如 Debian 的「jessie-backports」「stretch-updates」，Ubuntu 的「vivid-updates」「wily-proposed」。无仓库名的即为主仓库。 Debian 的 stable、testing 为链接，指向具体系统发行版，会随时间而变。比如，当前 stable 为 jessie，所以 stable-backports 与 jessie-backports 等效。但本人不建义使用 stable、testing，因为下一个 stable 发布后，你的源便自动指向了一个新版本，然而你并未阅读新版本的发行说明，并未做好升级的准备。 Debian 的仓库自 squeeze 起与 Ubuntu 基本相同。除主仓库外，有： security：Ubuntu 用于指安全性更新。即影响系统安全的 bug 修补。Debian 特殊一些，见下文。 updates：非安全性更新。即不影响到系统安全的 bug 修补。 proposed-updates：预更新。小 beta 版。过后会进入「updates」或「security」。Ubuntu 仅用proposed」，无后缀「updates」。 backports：后备。Debian stable 发布后，Ubuntu 某版本正式发布后，其所有软件版本号便已被冻结，所有软件只修 bug，不增加任何特性。但有人可能需要新特性，甚至某些较新的软件原来根本就没有。该仓库正因此而设，但欠官方维护，且可能在系统正式发布之后过一段时间才有内容。此仓库处于第二优先顺序，而上述几个仓库处于第一优先顺序。安装第二优先顺序的包必须特别指明，见 apt-get(8) aptitude(8) 的 --target-release 选项。 提示：并非所有版本都设有上述全部仓库，请打开源中 dists 目录查看。 后续字段，指示包许可类型。 后续字段排名不分先后，最终结果取其并集。按包本身的许可及所直接依赖的包的许可划分。打开某仓库，可见几个子目录。 Debian 最多有三种 main：本身是自由软件，且所有依赖的包也都是自由软件，此类可称纯自由软件，见 https://www.debian.org/distrib/packages《Debian自由软件指导方针》。 contrib：本身是自由软件，但依赖不纯，即依赖中至少有一例 contrib 或 non-free 者。 non-free：本身并非自由软件，无论依赖如何。当然，该软件是可免费使用或试用的。免费一例 https://packages.debian.org/jessie/unrar，试用xx天一例 https://packages.debian.org/jessie/rar。 Ubuntu 最多有四种 main：官方维护的自由软件。 universe：社区维护的自由软件。 restricted：设备专有驱动。 multiverse：同 Debian 的「non-free」。 某些另类的第三方源，未必遵循上述惯例。总之，打开仓库目录自己看。 特别之处： Debian 安全性更新 不像 Ubuntu 放在「security」仓库，而是放在单独一个源中。各大镜像站通常都把一般的包放在根下来一级的「debian」目录中，而安全性更新则会放在「debian-security」目录中，如果有的话，如 http://ftp.cn.debian.org/debian-security/。 Debian 官方建议，所有安全性更新，只从官方主站更新，勿使用其它镜像站，除非你对镜像站非常有信心，见 https://www.debian.org/security/index.en.html。所以，很多镜像站并不提供安全更新源。 安全性更新的第三字段形式固定为「版本名/updates」，如「wheezy/updates」「jessie/updates」。 Debian 多媒体源 一些多媒体软件因牵涉到版权问题，包括硬件解码器，Debian 官方并未收录，有一网站专门填补该空缺，见 http://www.deb-multimedia.org。 最后忠告： 不要同时启用多个源，同一仓库的源启用一个即可，否则容易引起混乱。以下实例便是列有多套而仅启用一套。 参考 https://forum.ubuntu.org.cn/viewtopic.php?t=366506 34、裸磁盘分区扩容 ①停掉向挂载路径写文件的服务或进程 ② 卸载挂载 umount /data 如果提示umount:/data:target is bus,使用fuser找出正在往挂载路径写文件的进程并kill掉，再次卸载挂载 yum install psmisc -y fuser -mv /data USER PID ACCESS COMMAND /data: root kernel mount /data root 13830 ..c.. bash ③修复分区表 磁盘扩大容量后，分区表中记录的柱头等信息需要更新，否则创建新分区时会报GPT PMBR size mismatch parted -l 在弹出Fix/Ignore?的提示时输入Fix后回车即可。 ④删掉旧分区再重建新分区 fdisk /dev/sdb d # 删除原来的分区/dev/sdb1 n # 创建新的分区 1 # 分区号与旧的保持一致 w # 写入分区表并生效 ⑤调整分区 e2fsck -f /dev/sdb1 检查分区信息 resize2fs /dev/sdb1 调整分区大小 ⑥重新挂载并验证数据是否丢失？容量是否扩容？ 35、MacOS下tar归档文件时,排错._*文件 MacOS下的tar命令，在归档压缩文件或文件夹时，会产生._*的隐藏文件()也一并归档到压缩包中，增加压缩包体积。可以在归档时不包含这些文件 COPYFILE_DISABLE=1 tar czf test.tar /your/files # 去除旧压缩包中的“._*”文件 tar -cf newTar --include='some/path/*' oldTar 参考： https://stackoverflow.com/questions/30962501/how-do-i-delete-a-single-file-from-a-tar-gz-archive https://superuser.com/questions/259703/get-mac-tar-to-stop-putting-filenames-in-tar-archives 36、echo 换行 echo -e \"test\\ndasdasd\" > test 37、dd命令 dd 可从标准输入或文件中读取数据，根据指定的格式来转换数据，再输出到文件、设备或标准输出。 参数说明: if=文件名：输入文件名，默认为标准输入。即指定源文件。 of=文件名：输出文件名，默认为标准输出。即指定目的文件。 ibs=bytes：一次读入bytes个字节，即指定一个块大小为bytes个字节。 obs=bytes：一次输出bytes个字节，即指定一个块大小为bytes个字节。 bs=bytes：同时设置读入/输出的块大小为bytes个字节。 cbs=bytes：一次转换bytes个字节，即指定转换缓冲区大小。 skip=blocks：从输入文件开头跳过blocks个块后再开始复制。 seek=blocks：从输出文件开头跳过blocks个块后再开始复制。 count=blocks：仅拷贝blocks个块，块大小等于ibs指定的字节数。 conv=，关键字可以有以下11种： conversion：用指定的参数转换文件。 ascii：转换ebcdic为ascii ebcdic：转换ascii为ebcdic ibm：转换ascii为alternate ebcdic block：把每一行转换为长度为cbs，不足部分用空格填充 unblock：使每一行的长度都为cbs，不足部分用空格填充 lcase：把大写字符转换为小写字符 ucase：把小写字符转换为大写字符 swap：交换输入的每对字节 noerror：出错时不停止 notrunc：不截短输出文件 sync：将每个输入块填充到ibs个字节，不足部分用空（NUL）字符补齐。 --help：显示帮助信息 --version：显示版本信息 示例： 刻录ISO镜像到硬盘(u盘) sudo dd if=CentOS-7-x86_64-Minimal-2009.iso of=/dev/disk2 bs=1m # 观察刻录进度 sudo watch kill -USR1 $(pgrep ^dd) # -USR1是dd专用的信号，它接收到该信号，就会显示刻录的进度 # 检查刻录是否结束后 sync # 弹出磁盘 umount /dev/disk2 修复无法格式化的U盘 dd if=/dev/zero of=/dev/sdc bs=512 count=1 文件中英文大小写转换 dd if=testfile_2 of=testfile_1 conv=ucase 将本地的/dev/hdb整盘备份到/dev/hdd dd if=/dev/hdb of=/dev/hdd 将备份文件恢复到指定盘 dd if=/root/image of=/dev/hdb 将备份文件恢复到指定盘 dd if=/root/image of=/dev/hdb 备份/dev/hdb全盘数据，并利用gzip工具进行压缩，保存到指定路径 dd if=/dev/hdb | gzip > /root/image.gz 将压缩的备份文件恢复到指定盘 gzip -dc /root/image.gz | dd of=/dev/hdb 备份磁盘开始的512个字节大小的MBR信息到指定文件 dd if=/dev/hda of=/root/image count=1 bs=512 # count=1指仅拷贝一个块；bs=512指块大小为512个字节。 # 恢复 dd if=/root/image of=/dev/hda 备份软盘 dd if=/dev/fd0 of=disk.img count=1 bs=1440k (即块大小为1.44M) 拷贝内存内容到硬盘 dd if=/dev/mem of=/root/mem.bin bs=1024 (指定块大小为1k) 拷贝光盘内容到指定文件夹，并保存为cd.iso文件 dd if=/dev/cdrom(hdc) of=/root/cd.iso 将/dev/hdb全盘数据备份到指定路径的image文件 dd if=/dev/hdb of=/root/image 销毁磁盘数据 利用随机数据填充硬盘来销毁数据 dd if=/dev/urandom of=/dev/hda1 修复硬盘 当硬盘较长时间(一年以上)放置不使用后，磁盘上会产生magnetic flux point，当磁头读到这些区域时会遇到困难，并可能导致I/O错误。当这种情况影响到硬盘的第一个扇区时，可能导致硬盘报废。 dd if=/dev/sda of=/dev/sda 或dd if=/dev/hda of=/dev/hda 38 、生成随机字符串 # 根据时间戳加随机数计算md5值并取前10位 echo $(date +%s)$RANDOM | md5sum | base64 | head -c 10 head -c 16 /dev/random | base64 openssl rand -hex 10 cat /proc/sys/kernel/random/uuid| cksum |cut -f1 -d\" \" | base64 head -n 5 /dev/urandom |sed 's/[^a-Z0-9]//g'|strings -n 4 tr -dc '_A-Z#\\-+=a-z(0-9%^>)]{ 39、ssh目录的权限问题 home目录的权限为700：chmod 700 /home/用户 .ssh目录的权限应为700：chmod 700 ~/.ssh .ssh目录下authorized_keys文件的权限应为600：chmod 600 ~/.ssh/authorized_keys 40、常见包管理器的阿里云镜像源设置 npm npm config set registry https://registry.npm.taobao.org --global npm config set disturl https://npm.taobao.org/dist --global npm config get registry Python mkdir ~/.pip echo -e \"[global]\\nindex-url = https://mirrors.aliyun.com/pypi/simple/\\n[install]\\ntrusted-host=mirrors.aliyun.com\\n\" > ~/.pip/pip.conf 41、使用curl命令发送邮件 curl -s --ssl-reqd --write-out %{http_code} --output /dev/null \\ --url \"smtp://发件人SMTP服务器地址:发件人SMTP服务器端口\" \\ --user \"发件人SMTP服务器用户名:发件人SMTP服务器密码\" \\ --mail-from 发件人邮箱地址 \\ --mail-rcpt 收件人邮箱地址 \\ --upload-file /tmp/emai-data.txt # /tmp/emai-data.txt的内容 FROM: 发件人邮箱地址 To: 收件人邮箱地址 CC: 抄送人邮箱地址 Subject: 主题 MIME-Version: 1.0 Content-Type: multipart/alternative; boundary=\"DELIMETER\" --DELIMETER Content-Type: text/html; charset=\"utf-8\" 测试 --DELIMETER Content-Type: text/plain; name=test.txt Content-Transfer-Encoding: base64 Content-Disposition: attachment; filename=test.txt [base64编码的附件内容] --DELIMETER 参考： https://stackoverflow.com/questions/66801073/sparkpost-sending-email-using-bash-curl-and-smtp-protocol https://blog.edmdesigner.com/send-email-from-linux-command-line/ https://skeletonkey.com/filemaker-18-smtp-curl/ https://www.soliantconsulting.com/blog/html-email-filemaker/ https://stackoverflow.com/questions/44728855/curl-send-html-email-with-embedded-image-and-attachment 42、split按行或大小切割大文件 split命令 可以将一个大文件分割成很多个小文件，有时需要将文件分割成更小的片段，比如为提高可读性，生成日志等。 选项 -a, --suffix-length=N 指定后缀长度(默认为2) --additional-suffix=SUFFIX append an additional SUFFIX to file names -b, --bytes=SIZE put SIZE bytes per output file -C, --line-bytes=SIZE put at most SIZE bytes of lines per output file -d, --numeric-suffixes[=FROM] 使用数字作为后缀(默认起始值为0) -e, --elide-empty-files do not generate empty output files with '-n' --filter=COMMAND write to shell COMMAND; file name is $FILE -l, --lines=NUMBER 值为每一输出档的行数大小。 -n, --number=CHUNKS generate CHUNKS output files; see explanation below -u, --unbuffered immediately copy input to output with '-n r/...' --verbose 在每个输出文件打开前输出文件特征 --help 显示此帮助信息并退出 --version 显示版本信息并退出 SIZE is an integer and optional unit (example: 10M is 10*1024*1024). Units are K, M, G, T, P, E, Z, Y (powers of 1024) or KB, MB, ... (powers of 1000). CHUNKS may be: N split into N files based on size of input K/N output Kth of N to stdout l/N split into N files without splitting lines l/K/N output Kth of N to stdout without splitting lines r/N like 'l' but use round robin distribution r/K/N likewise but only output Kth of N to stdout 实例 使用split命令将date.file文件分割成大小为10KB的小文件： # split -b 10k date.file date.file xaa xab xac xad xae xaf xag xah xai xaj 文件被分割成多个带有字母的后缀文件，如果想用数字后缀可使用-d参数，同时可以使用-a length来指定后缀的长度： # split -b 10k date.file -d -a 3 date.file x000 x001 x002 x003 x004 x005 x006 x007 x008 x009 为分割后的文件指定文件名的前缀： # split -b 10k date.file -d -a 3 split_file date.file split_file000 split_file001 split_file002 split_file003 split_file004 split_file005 split_file006 split_file007 split_file008 split_file009 使用-l选项根据文件的行数来分割文件，例如把文件分割成每个包含10行的小文件： split -l 10 date.file 43、journalctl查看内核/应用日志 Systemd统一管理所有Unit的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。该工具是从message这个文件里读取信息。 ①查看所有日志 journalctl ②查看内核日志（不显示应用日志） journalctl -k ③查看系统本次启动的日志 # 查看系统本次启动的日志 journalctl -b journalctl -b -0 # 查看上一次启动的日志 需更改设置,如上次系统崩溃，需要查看日志时，就要看上一次的启动日志。 journalctl -b -1 ④查看指定时间的日志 journalctl --since=\"2018-10-3018:17:16\" journalctl --since \"20 minago\" journalctl --since yesterday journalctl --since \"2020-09-23 22:50:00\" --until \"2020-09-23 23:20:00\" journalctl --since 09:00 --until\"1 hour ago\" journalctl --since\"15:15\" --until now ⑤日志显示设置 # 显示尾部的最新10行日志 journalctl -n # 显示尾部指定行数的日志 journalctl -n 20 # 实时滚动显示最新日志 journalctl -f # 在标准输出中显示日志（默认情况下，journalctl 会在 pager 内显示输出结果） journalctl -b -0 --no-pager ⑥查看某个Unit的日志 journalctl -u nginx.service # 只显示今天的 journalctl -u nginx.service --since today # 实时滚动显示 journalctl -u nginx.service -f # 合并显示多个Unit的日志 journalctl -u nginx.service -u php-fpm.service --since today ⑦指定用户的日志 journalctl _UID=33 --since today ⑧显示/清理日志存储设置 # 显示日志当前占用的硬盘空间 journalctl --disk-usage # 可以按照日期清理，或者按照允许保留的容量清理 journalctl --vacuum-size=1G journalctl --vacuum-time=1years ⑨指定日志输出格式 journalctl 能够以多种格式进行显示，只须添加 -o 选项即可。-o 选项支持的类型如下： short：这是默认的格式，即经典的 syslog 输出格式。 short-iso： 与 short 类似，强调 ISO 8601 时间戳。 short-precise：与 short 类似，提供微秒级精度。 short-monotonic：与 short 类似，强调普通时间戳。 verbose：全部字段，包括通常被内部隐藏的字段。 export：传输或备份的二进制格式。 json：json 格式，每行一条记录。 json-pretty：阅读的 json 格式。 json-sse：经过包装可以兼容 server-sent 事件的 json 格式。 cat：只显示信息字段本身。 journalctl -u cron -n 1 --no-pager -o json-pretty 44、read命令常用操作 ①提示字符中的换行 read -p $'第一行内容\\n第二行内容:' 变量 ②不显示敏感字符 read -s -p \"请输入密码\" passwd 45、nmcli命令行/numtui字符界面管理网络 参考：https://www.cnblogs.com/liuhedong/p/10695969.html 通常用 con 关键字替换 connection，并用 mod 关键字替换 modify nmtui 是一个基于文本用户界面的，用于控制网络的管理器，当我们执行 nmtui 时，它将打开一个基于文本的用户界面，通过它我们可以添加、修改和删除连接。除此之外，nmtui 还可以用来设置系统的主机名。 安装命令 yum install NetworkManager NetworkManager-tui # 或者 apt install network-manager ①显示网络管理器的整体状态 nmcli general status ②查看网卡设备 $ nmcli dev DEVICE TYPE STATE CONNECTION wlan0 wifi connected **** eth0 ethernet unmanaged -- lo loopback unmanaged -- tun0 tun unmanaged -- p2p-dev-wlan0 wifi-p2p unmanaged -- ③查看附近的WIFI网络 $ nmcli d wifi list IN-USE BSSID SSID MODE CHAN RATE SIGNAL BARS SECURITY * CC:2D:21:4B:53:81 Stark-Industries Infra 4 270 Mbit/s 100 ▂▄▆█ WPA1 WPA2 E8:3F:67:FF:2A:42 HUAWEI-忆 Infra 6 130 Mbit/s 60 ▂▄▆_ WPA2 90:47:3C:3E:32:D1 CMCC-VzjQ Infra 7 130 Mbit/s 60 ▂▄▆_ WPA1 WPA2 E8:3F:67:FF:2A:47 -- Infra 6 130 Mbit/s 57 ▂▄▆_ WPA2 E8:3F:67:FF:2A:44 666666 Infra 6 130 Mbit/s 57 ▂▄▆_ WPA2 8C:FD:18:4A:79:74 CMCC-GNTn Infra 9 130 Mbit/s 55 ▂▄__ WPA1 WPA2 8C:FD:18:4A:79:78 CMCC-GNTn-5G Infra 36 270 Mbit/s 52 ▂▄__ WPA1 WPA2 ④连接WIFI $ nmcli d wifi connect password ⑤连接隐藏WIFI $ nmcli c add type wifi con-name ifname wlan0 ssid $ nmcli c modify wifi-sec.key-mgmt wpa-psk wifi-sec.psk $ nmcli c up ⑥查看网络设备连接状态 nmcli connection show nmcli connection show --active # 以活动的连接进行排序 nmcli connection show --order +active # 将所有连接以名称排序 nmcli connection show --order +name # 将所有连接以类型排序(倒序) nmcli connection show --order -type ⑦固定IP地址 # 列出当前活动的连接 nmcli connection show # 固定IP地址 nmcli con mod ipv4.addresses 192.168.1.4/24 # 设置网关 nmcli con mod ipv4.gateway 192.168.1.1 # 设置手动获取IP，不使用DHCP nmcli con mod ipv4.method manual # 设置DNS nmcli con mod ipv4.dns \"8.8.8.8\" # 生效配置 nmcli con up 46、对bash执行curl的脚本进行传参 curl http://test.com/test/test.sh | bash -s arg1 arg2 bash 47、windows下编写的脚本文件，放到Linux中无法识别格式 在Linux中执行.sh脚本，异常/bin/sh^M: bad interpreter: No such file or directory。windows下编写的脚本文件，放到Linux中无法识别格式，在vi的时候,会在下面显示此文件的格式,比如 \"dos.txt\" [dos] 120L, 2532C 字样,表示是一个[dos]格式文件,如果是MAC系统的,会显示[MAC]。dos格式文件传输到unix系统时,会在每行的结尾多一个^M 用vi打开脚本文件，在命令模式下输入set ff=unix 用命令:set ff?可以看到dos或unix的字样 其他工具去除参考：文本处理的第七章节 48、文件编码格式 查看 brew/yum/apt install -y enca enca 文件名 file 文件名 vim中:set fileencoding 转换 # 将GBK编码的文件转换成UTF-8编码 enconv -L zh_CN -x UTF-8 filename # 将UTF-8 编码的文件转换成GBK编码 iconv -f UTF-8 -t GBK file1 -o file2 vim中:set fileencoding=utf-8 49、Linux安装使用SQLServer客户端sqlcmd 安装 # CentOS/RHEL curl https://packages.microsoft.com/config/rhel/8/prod.repo > /etc/yum.repos.d/msprod.repo sudo yum remove mssql-tools unixODBC-utf16-devel sudo yum install mssql-tools unixODBC-devel # Ubuntu/Debian curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add - curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list | sudo tee /etc/apt/sources.list.d/msprod.list apt-get update apt-get install mssql-tools unixodbc-dev 使用 sqlcmd -S SERVERNAME,49399 -U User -P pwd -d DatabaseName -Q \"SELECT * FROM Test;\" # 如果执行出现“-bash: !”: event not found\",终端shell设置set +H 参考： https://serverfault.com/questions/208265/what-is-bash-event-not-found https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-tools?view=sql-server-ver15#ubuntu 50、NTP同步时间 Windows系统上自带的两个：time.windows.com 和 time.nist.gov MacOS上自带的两个：time.apple.com 和 time.asia.apple.com NTP授时快速域名服务：cn.ntp.org.cn http://www.ntp.org.cn/ yum install ntp -y && \\ cp /etc/ntp.conf /etc/ntp.conf.bak && \\ ntpdate -u NTP服务器 && \\ sed -i '/^server/d' /etc/ntp.conf && \\ echo -e \"server 内网NTP服务器IP地址\\nserver 外网NTP服务器IP地址\" >> /etc/ntp.conf && \\ systemctl enable ntpd && \\ systemctl start ntpd && \\ systemctl status ntpd && \\ ntpstat NTP服务端配置 yum install ntp -y && \\ mv /etc/ntp.conf /etc/ntp.conf.bak && \\ bash -c 'cat > /etc/ntp.conf NTP客户端配置 yum install ntp -y && \\ mv /etc/ntp.conf /etc/ntp.conf.bak && \\ bash -c 'cat > /etc/ntp.conf NTP常用命令 # 从时间服务器更新系统时间 ntpdate -u NTP服务器 # 查询不更新 ntpdate -q NTP服务器 #查看时间同步状态 ntpstat #列出所有作为时钟源校正过本地NTP服务器时钟的上层NTP服务器的列表 ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== *172.16.0.2 LOCAL(0) 11 u 43 64 377 0.623 26.067 8.290 # remote： 远程NTP服务器的IP地址或域名，带 “*” 的表示本地NTP服务器与该服务器同步。 # refid： 远程NTP服务器的上层服务器的IP地址或域名。 # st： 远程NTP服务器所在的层数。 # t： 本地NTP服务器与远程NTP服务器的通信方式，u: 单播； b: 广播； l: 本地。 # when： 上一次校正时间与现在时间的差值。 # poll： 本地NTP服务器查询远程NTP服务器的时间间隔。 # reach： 是一种衡量前8次查询是否成功的位掩码值，377表示都成功，0表示不成功。 # delay： 网络延时，单位是10的-6次方秒。 # offset： 本地NTP服务器与远程NTP服务器的时间偏移。 # jitter： 查询偏差的分布值，用于表示远程NTP服务器的网络延时是否稳定，单位为10的-6次方秒。 ntpdate -d NTP服务器 常见NTP时间服务器 pool.ntp.org # 中国 cn.ntp.org.cn # 中国香港 hk.ntp.org.cn # 美国 us.ntp.org.cn # 阿里云NTP服务器 ntp.aliyun.com ntp1.aliyun.com ntp2.aliyun.com ntp3.aliyun.com ntp4.aliyun.com ntp5.aliyun.com ntp6.aliyun.com ntp7.aliyun.com # 阿里云Time服务器 time1.aliyun.com time2.aliyun.com time3.aliyun.com time4.aliyun.com time5.aliyun.com time6.aliyun.com time7.aliyun.com # 北京大学 s1c.time.edu.cn s2m.time.edu.cn # 清华大学 s1b.time.edu.cn s1e.time.edu.cn s2a.time.edu.cn s2b.time.edu.cn #苹果提供的授时服务器 time1.apple.com time2.apple.com time3.apple.com time4.apple.com time5.apple.com time6.apple.com time7.apple.com #Google提供的授时服务器 time1.google.com time2.google.com time3.google.com time4.google.com Windows下NTP客户端服务配置 运行对话框输入gpedit.msc进入组策略 依次进入 计算机配置 > 管理模板 > 系统 > Windows时间服务 > 时间提供程序 然后进入 控制面板 > 时钟、语言和区域 > 设置时间和日期 > Internet时间 > 更改设置 51、Yum升级内核 内核下载地址：https://elrepo.org/linux/kernel/ kernel-lt（lt=long-term）长期有效 kernel-ml（ml=mainline）主流版本 安装最新内核 rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org && \\ rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm && \\ yum --enablerepo=elrepo-kernel install -y kernel-ml 配置默认内核 # 查看grube启动时当前默认设置的内核 grub2-editenv list # 查看grub2当前支持可启动的内核 awk -F \\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg 0 : CentOS Linux (5.11.8-1.el7.elrepo.x86_64) 7 (Core) 1 : CentOS Linux (3.10.0-1160.el7.x86_64) 7 (Core) 2 : CentOS Linux (0-rescue-7acaacd4599a461f9540eece4c227d87) 7 (Core) # 设置grube启动时使用最新的内核 grub2-set-default 'CentOS Linux (5.11.8-1.el7.elrepo.x86_64) 7 (Core)' # 再次查看grube启动时当前默认设置的内核 grub2-editenv list # 重启生效 reboot now 更新基础软件 # 更新kernel-ml-devel、kernel-ml-headers、kernel-ml-doc、kernel-tools、perf、kernel-ml-headers yum --enablerepo=elrepo-kernel install -y kernel-ml-devel kernel-ml-headers kernel-ml-doc kernel-tools perf python-perf 52、新增磁盘分区不显示设备 对一个磁盘创建了新分区后，fdisk -l 可以显示，但是不显示在/dev/分区号。使用partprobe重新扫描磁盘分区 partprobe 53、APT（Advanced Packaging Tools） 使用APT的操作系统： Ubuntu Debian 包查询 Debian：https://www.debian.org/distrib/packages APT软件包的类型 Main：自由软件及其源代码 Contrib：本身是自由软件，但是需要依赖一些非自由软件运行 Non-Free：收到许可条例限制的软件 说明文档： Debian：https://www.debian.org/doc/manuals/debian-reference/ch02.zh-cn.html Ubuntu: http://manpages.ubuntu.com/manpages/xenial/man8/apt.8.html 常用软件镜像源 中科大 Ubuntu：https://mirrors.ustc.edu.cn/ubuntu/ Debian：https://mirrors.ustc.edu.cn/debian 清华 Ubuntu：https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ Debian：https://mirrors.tuna.tsinghua.edu.cn/debian/ ①查询软件版本 apt-cache madison 包 ②列出软件的所有来源 apt-cache policy 包 # 或者 apt-cache showpkg 包 ③模拟安装软件 apt-get install -s 包 ④安装testing类型仓库里的软件 echo \"deb https://mirrors.tuna.tsinghua.edu.cn/debian testing main contrib non-free \" >> /etc/apt/sources.list apt-get update 54、bash语法变更 从Bash 4.4以后，break关键词不允许出现 for, while ,until 循环中，如果出现将报一下错误： line 0: break: only meaningful in a `for', `while', or `until' loop 参考 https://stackoverflow.com/questions/41532564/bash-function-that-breaks-loop 55、SSH跳板登录 ssh username@目标机器ip -o ProxyCommand=’ssh username@跳板机ip -W %h:%p’ 也可以在配置文件 ~/.ssh/config (若没有则创建)中配置 Host test-ssh-forward HostName 目标机器ip User root ProxyCommand ssh root@跳板机ip -W %h:%p ssh test-ssh-forward 通过中间主机SSH连接 ssh -t reachable_host ssh unreachable_host 56、OpenSSH客户端配置 针对OpenSSH客户端ssh命令的配置有全局配置文件/etc/ssh/ssh_config ，用户级别配置文件~/.ssh/config。可在其中配置常用的SSH主机配置 Host 主机别名 HostName 主机IP地址 User 登录用户 Port 端口 # 默认为22 IdentityFile ssh私钥文件路径 # 默认为~/.ssh/identity 、~/.ssh/id_rsa 、~/.ssh/id_dsa Compression yes # 是否进行压缩 LogLevel INFO 可用参数：https://www.ssh.com/academy/ssh/config 参考： https://blog.csdn.net/MatrixGod/article/details/81905227 57、echo显示带颜色的内容 echo -e \"\\033[文字背景颜色;文字颜色m字符串\\033[控制选项\" echo -e \"\\e[41;36mHello \\e[46;35mWorld\" 文字背景颜色 文字颜色 控制选项 [0m 关闭所有属性 [1m 设置高亮度 [4m 下划线 [5m 闪烁 [7m 反显 [8m 消隐 [30m — \\33[37m 设置前景色 [40m — \\33[47m 设置背景色 [nA 光标上移n行 [nB 光标下移n行 [nC 光标右移n行 [nD 光标左移n行 [y;xH设置光标位置 [2J 清屏 [K 清除从光标到行尾的内容 [s 保存光标位置 [u 恢复光标位置 [?25l 隐藏光标 [?25h 显示光标 参考：https://www.linuxshelltips.com/how-to-change-the-output-color-of-echo-in-linux/ 58、sha256sum、文件内容自动添加隐形换行转义字节 计算文件中字符串的SHA256值时，发现和在在线计算网站中计算字符串的值不一样。是因为在向文件写入字符串时，会自动添加换行字符。如果直接使用sha256sum命令计算文件的hash时，换行字符也算进字符串的一部分。 $ echo \"8\" > test $ hexdump test 0000000 38 0a 0000002 # 0a 为\"\\n\"换行符 $ cat test # zsh下显示带有隐藏换行转义字符的文本内容时 8 $ cat test # zsh下显示不带有隐藏换行转义字符的文本内容时 8% 所以有三种方法解决 Vim 设置禁止自定换行 echo -e 'set noeol\\nset nofixendofline' >> ~/.vimrc # 只有新建文件时才有用，打开已有文件时仍然不能显示出多加的那个空行。 echo显示字符串到文件时添加-n参数不自定添加换行字符 echo -n \"8\" > test sha256sum直接计算字符串 echo -n \"8\" | sha256sum 在线计算加密工具网站：https://crypot.51strive.com/sha256.html 59、wget常用操作 ①使用SSL证书访问HTTPS网站 wget https://www.test.com --ca-certificate=mycertfile.pem 60、crontab下使用date和sudo命令 crontab下使用date命令需要转义%，例如： date +\"\\%Y\\%m\\%d_\\%H:\\%M\" 和 $(date +\"\\%Y\\%m\\%d_\\%H:\\%M\") 直接在crontab里以sudo执行命令无效，会提示 sudo: sorry, you must have a tty to run sudo .需要修改/etc/sudoers，执行visudo或者vim /etc/sudoers 将Defaults requiretty这一行注释掉。因为sudo默认需要tty终端，而crontab里的命令实际是以无tty形式执行的。注释掉\"Defaults requiretty\"即允许以无终端方式执行sudo 但是，这里关于安全性方面有一点需要注意。关于该配置项，说明如下Disable \"`ssh hostname sudo `\", because it will show the password in clear.You have to run \"ssh -t hostname sudo \".该配置的作用是禁止执行\"ssh hostname sudo \"，因为这种方式会将sudo密码以明文显示，你可以运行\"ssh -t hostname sudo \"来替代。开启的情况下，\"ssh hostname sudo \"无法执行成功，关闭了之后，就没有这一层的检查了。 参考：https://blog.csdn.net/kai404/article/details/52169122 61、/etc/crontab文件和crontab -e命令区别 ①格式不同 /etc/crontab # .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * user-name command to be executed crontab -e命令中 50 1 * * * command ②使用范围 修改/etc/crontab只有root用户能用，可以直接给其他用户设置计划任务，而且还可以指定执行shell等等 crontab -e所有用户都可以使用，普通用户也能设置计划任务，自动写入/var/spool/cron/usename 62、常用Cronjob # 每五分钟执行 */5 * * * * CMD # 每小时执行 0 * * * * CMD # 每2小时执行 0 */2 * * * CMD # 每天执行 0 0 * * * CMD # 每周执行 0 0 * * 0 CMD # 每月执行 0 0 1 * * CMD # 每年执行 0 0 1 1 * CMD # 每周六凌晨2点执行 0 2 * * 6 CMD 63、脚本加密shc CFLAGS=-static sh -r -T -e 03/31/2027 -f tesh.sh # CFLAGS=-static 设置进行静态编译链接 # -f 指定脚本文件 # -e 设置脚本在指定日期后失效，日期格式：dd/mm/yyyy # -m 指定过期提示的信息 # -T 设置是否允许二进制可被工具(例如strace, ptrace, truss)调试 # -r 在不同操作系统执行 生成以下文件 tesh.sh tesh.sh.x是加密后可执行的二进制文件 tesh.sh.x.c 是 tesh.sh.x 的源文件（注意是C语言版本的源文件） shc生成的二进制文件只能通过 ./xxx 命令来执行，不能通过 /bin/bash xxx 来执行。 shc加密的脚本在运行时ps -ef可以看到shell的源码 在执行加密脚本的时候，还是会在内存中解密全部的shell代码。解密的思路就是从内存中获取解密后的代码。 shc加密脚本解密可参考：https://cloud.tencent.com/developer/article/1451796 参考： https://linux.die.net/man/1/shc https://www.linuxjournal.com/article/8256 64、节省tar解压大文件中指定文件的速度 tar -zxvf 压缩包 --occurrence 压缩包中的文件路径 # --occurrence参数默认会在解压到第一次匹配的文件后不再处理后续解压。极大节省了解压时间 参考：https://superuser.com/questions/655739/extract-single-file-from-huge-tgz-file 65、seq快速生成序列化数据 ①seq命令格式与参数 seq [选项]... 尾数 seq [选项]... 首数 尾数 seq [选项]... 首数 增量 尾数 选项： -f, --format=格式 使用printf样式的浮点格式 -s, --separator=字符串 使用指定字符串分隔数字(默认使用：\\n) -w, --equal-width 在列前添加0 使得宽度相同【自动补位】 ②生成IP地址 seq -f \"10.1.2.%g\" 2 254 > ip-pools # 10.1.2.2 # 10.1.2.3 # .... # 10.1.2.254 ③指定分隔符 横着输出 seq -s '-' 5 # 1-2-3-4-5 ④默认补位操作 seq -w 1 5 # 01 # 02 # 03 # 04 # 05 66、fuser查看哪些进程使用文件/目录 # 显示哪些进程使用文件/目录 fuser /var/log/daemon.log fuser -v /home/supervisor # 杀死锁定文件的进程 fuser -ki filename # 终止使用特定信号锁定文件的进程 fuser -k -HUP filename # 显示在特定端口上侦听的 PID fuser -v 5601/tcp # 显示使用命名文件系统或块设备的所有进程 fuser -mv /var/www 67、 特殊文件操作 ①快速备份文件 cp filename{,.backup} ②删除文件夹中与特定文件扩展名不匹配的所有文件 rm !(*.foo|*.bar|*.baz) ③将多行字符串传递给文件 # cat >filename ... - overwrite the file # cat >>filename ... - append to a file cat > filename ④使用 vim 编辑远程主机上的文件 vim scp://user@host//etc/fstab 68、lsof # 显示当前使用互联网连接的进程 lsof -P -i -n # 显示使用特定端口号的进程 lsof -i tcp:443 # 列出所有侦听端口以及关联进程的 PID lsof -Pan -i tcp -i udp # 列出所有打开的端口及其所属的可执行文件 lsof -i -P | grep -i \"listen\" # 显示所有开放端口 lsof -Pnl -i # 显示开放端口 (LISTEN) lsof -Pni4 | grep LISTEN | column -t # 列出由特定命令打开的所有文件 lsof -c \"process\" # 查看每个目录的用户活动 lsof -u username -a +D /etc # 显示 10 个最大的打开文件 lsof / | \\ awk '{ if($7 > 1048576) print $7/1048576 \"MB\" \" \" $9 \" \" $1 }' | \\ sort -n -u | tail | column -t # 显示进程的当前工作目录 lsof -p | grep cwd 69、文件目录差异对比 ① JSON 文件差异对比 vimdiff ②根据字符和单词突出显示确切的差异 vimdiff file1 file2 ③比较两个目录树 diff ④比较两个命令的输出 diff 70、监控特定端口的打开连接，包括按 IP 侦听、计数和排序 watch -n 1 \"netstat -plan | grep :443 | awk {'print \\$5'} | cut -d: -f 1 | sort | uniq -c | sort -nk 1\" 71、Tar/Zip加密压缩包 Tar tar压缩并加密 tar -czvf - 待压缩加密的文件或文件夹 | openssl enc -aes-256-cbc -salt -pbkdf2 -k 加密密码 -out 压缩加密后.tar.gz 解密解压 tar包 openssl enc -d -aes-256-cbc -pbkdf2 -in 压缩加密后.tar.gz | tar -x zip 加密 zip -P 加密密码 -r 压缩文件名.zip 要压缩的文件夹 解密 unzip -P 加密密码 压缩文件名.zip 72、tailf自动退出 tail 的--pid参数，监控某一个pid，当检测到pid停止的时候，停止tail 根据进程状态决定是否终止退出 tail -f --pid=$(ps -ef | grep java | grep -v \"grep\" | awk '{ print $2 } ' | sort -nr | head -1) ./nohup.log # MacOS下tail没有--pid参数，可使用 gtail 替代 gtail -f --pid=$(ps -ef | grep java | grep -v \"grep\" | awk '{ print $2 } ' | sort -nr | head -1) ./nohup.log 根据输出日志关键字决定是否终止退出 参考：https://cloud.tencent.com/developer/article/2019300 73、base64 编解码字符末尾“=”的特殊说明 编码 如果要编码的二进制数据不是3的倍数，就用\\x00字节在末尾补足，然后再在编码的末尾加上1到2个等号（=），表示补了多少字节，这样解码的时候就可以自动去掉了。特别注意，Base64编码后的文本的长度总是4的倍数，但是如果再加上1到2个=不就不是4的倍数了吗？所以并不是先编码，再加上1到2个=，而是编码之后，把最后的1到2个字符（这个字符肯定是A）替换成= 解码 与编码相反，首先去除末尾的等号（=），然后比对初始的64字符的数组，把编码后的文本转成各字符在数组里的索引值，再然后转成6比特的二进制数，最后删除多余的\\x00。 标准Base64里是包含 + 和 / 的，在URL里不能直接作为参数，所以出现一种 “url safe” 的Base64编码，其实就是把 + 和 / 替换成 - 和 _ 。 同样的，=也会被误解，所以编码后干脆去掉=，解码时，自动添加一定数量的等号，使得其长度为4的倍数即可正常解码了。 参考：https://www.jianshu.com/p/ccdef9b179e7 74、find高级查找 ①查找指定目录下所有符合以下条件的文件 文件名符合\"nginx*.log\"和\"laralog*.log\"规则的 文件大小超过 100M 的 文件修改时间为三天之前的 find /log/app \\( -name \"nginx*.log\" -o -name \"laralog*.log\" \\) -type f -size +100M -mtime +3 | xargs ls -sh find /log/app \\( -name \"nginx*.log\" -o -name \"laralog*.log\" \\) -type f -size +100M -mtime +3 -empty -exec sh -c '> \"{}\"' \\; -size表示文件大小大于或小于指定单位的文件。+ 表示查找大于指定大小的文件，M 表示单位为兆字节。 -o 表示逻辑 OR，即匹配两个条件之一的文件 75、TCP端口状态 LISTEN： 侦听来自远方的TCP端口的连接请求 SYN-SENT： 再发送连接请求后等待匹配的连接请求 SYN-RECEIVED：再收到和发送一个连接请求后等待对方对连接请求的确认 ESTABLISHED： 代表一个打开的连接 FIN-WAIT-1： 等待远程TCP连接中断请求，或先前的连接中断请求的确认 FIN-WAIT-2： 从远程TCP等待连接中断请求 CLOSE-WAIT： 等待从本地用户发来的连接中断请求 CLOSING： 等待远程TCP对连接中断的确认 LAST-ACK： 等待原来的发向远程TCP的连接中断请求的确认 TIME-WAIT： 等待足够的时间以确保远程TCP接收到连接中断请求的确认 CLOSED： 没有任何连接状态 76、使用 openssl命令行批量查询网站域名证书的有效期 # 子域名列表，以空格分割 sub_domain=\"www mail test gitbok\" # 主域名 main_domain=\"curiouser.com\" for i in $sub_domain ;do echo \"=========================:\"$i.$main_domain echo | openssl s_client -connect \"$i\".$main_domain:443 2>/dev/null | openssl x509 -noout -enddate | awk -F= '{print \"过期日期：\"$2}'; done # 如果在zsh执行的话，for i in $sub_domain 换成 for i in $=sub_domain 77、使用wpa_supplicant设置wifi 创建配置文件 ctrl_interface=/run/wpa_supplicant update_config=1 network={ ssid=\"YOUR_SSID\" psk=\"YOUR_PASSWORD\" } 指定配置文件和无线接口（例如wlan0） wpa_supplicant -B -c /etc/wpa_supplicant/wpa.conf -i wlan0 使用dhclient命令为无线接口分配IP地址，并将其设置为静态IP地址 sudo dhclient -r wlan0 sudo ifconfig wlan0 192.168.0.100 netmask 255.255.255.0 up 验证 ip a ifconfig wlan0 78、查看磁盘vid、pid 如何查看设备的Vendor ID (制造商ID：vid) 和 Product ID (型号ID: pid) Windows 设备管理器 --> 展开磁盘驱动器选项，右键选择属性，在详细信息选项卡中找到硬件ID。 Linux lspci -v MacOS ioreg -c IOBlockStorageDriver -r -w 0 79、dig dig @ # 要查询 example.com 的 A 记录，并且不使用本地缓存 dig a www.baidu.com @8.8.8.8 |grep \"www.baidu.com\" | sed '1,2d' | awk '{print $5}' # 只显示A记录，不显示CNAME dig +short +nocomments +noquestion a www.baidu.com @8.8.8.8 | awk 'match($0, /[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+/) {print substr($0, RSTART, RLENGTH)}' 79、trip https://trippy.cli.rs/#configuration-reference 80、Linux 安装字体 TrueType 字体 (TTF)： TrueType 是最流行且得到广泛支持的字体格式之一。TTF 字体是可缩放的，并且在各种尺寸下都能很好地显示。它们通常用于屏幕和打印目的，因此适用于网页设计、图形设计和一般文本渲染。 OpenType 字体 (OTF)： OpenType 是 TrueType 字体格式的扩展。OTF 字体支持高级排版功能，例如连字、替代字符和样式集。它们可以包含 TrueType 和 PostScript 字体数据，使其适用于不同的平台和应用程序。 PostScript 字体（Type 1）： PostScript 字体通常称为 Type 1 字体，是最早的数字字体格式之一。虽然它们已在很大程度上被 TrueType 和 OpenType 字体取代，但它们仍然在特定的遗留应用程序和打印工作流程中使用。 位图字体 (BDF)：位图字体是使用像素网格创建的，其中每个像素对应于一个特定的字形。这些字体不可缩放，最适合特定的屏幕分辨率。它们通常用于屏幕空间有限的旧系统和终端。 X11 字体格式 (XLFD)： X 逻辑字体描述 (XLFD) 格式在 X Window 系统中用于描述字体。它包括各种属性，如字体系列、样式、大小、粗细等。但是，随着更新的字体技术和格式的出现，XLFD 的相关性已不再那么重要。 Web 字体格式（WOFF、WOFF2）：虽然并非 Linux 独有，但 WOFF（Web 开放字体格式）和 WOFF2 等 Web 字体格式针对 Web 使用进行了优化，提供高效的压缩和更快的加载时间。它们允许网页设计人员在其网站上使用自定义字体，同时确保跨浏览器兼容性。 SVG 字体：可扩展矢量图形 (SVG) 字体使用基于 XML 的描述来定义字体轮廓。虽然它们在不损失质量的情况下提供可扩展性，但在一般文本使用中它们不如 TrueType 或 OpenType 字体常见。 Linux 提供了两种主要的字体安装方式： 系统范围 和 每用户。大多数 Linux 发行版将系统范围的字体存储在 /usr/share/fonts 目录中。 系统范围安装 下载字体文件并复制到/usr/share/fonts/字体文件夹下。 更新字体缓存，以便应用程序可以识别新字体：fc-cache -f -v 查看字体是否已安装：fc-list 用户范围安装 下载字体文件并复制到~/.local/share/fonts/字体文件夹下。 更新字体缓存，以便应用程序可以识别新字体：fc-cache -f -v 查看字体是否已安装：fc-list https://linuxiac.com/how-to-install-fonts-on-linux/ 81、openssl发送 HTTP请求 在没有 curl 和受限的 wget 情况下，使用 openssl 手动构建和发送 HTTPS POST 请求 ddingtoken=.......... payload='{ \"msgtype\": \"markdown\", \"markdown\": { \"title\": \"test\", \"text\": \"11111\" } }' { echo -e \"POST /robot/send?access_token=$ddingtoken HTTP/1.1\\r\\n\\ Host: oapi.dingtalk.com\\r\\n\\ Content-Type: application/json\\r\\n\\ Content-Length: ${#payload}\\r\\n\\ Connection: close\\r\\n\\ \\r\\n\\ $payload\" } | openssl s_client -connect oapi.dingtalk.com:443 -servername oapi.dingtalk.com -ign_eof -quiet > /dev/null 2>&1 # -ign_eof 参数以忽略 EOF 错误 # -servername 参数指定服务器名称， # -quiet 参数使输出安静。 # 请求行：POST /robot/send?access_token=$ddingtoken HTTP/1.1 # 请求头： # Host: oapi.dingtalk.com # Content-Type: application/json # Content-Length: ${#payload}：计算 JSON 数据的字节数 # Connection: close：表示服务器在完成响应后关闭连接 # 请求体：包含在 $payload 变量中的 JSON 数据 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-27 10:57:21 "},"origin/linux-diff.html":{"url":"origin/linux-diff.html","title":"比较两个文件的不同","keywords":"","body":"Linux 比较文件的不同 一、diff比较两个文件的不同 语法 diff [-abBcdefHilnNpPqrstTuvwy][-][-C ][-D ][-I ][-S ][-W ][-x ][-X ][--help][--left-column][--suppress-common-line][文件或目录1][文件或目录2] 参数 - 　指定要显示多少行的文本。此参数必须与-c或-u参数一并使用。 -a或--text 　diff预设只会逐行比较文本文件。 -b或--ignore-space-change 　不检查空格字符的不同。 -B或--ignore-blank-lines 　不检查空白行。 -c 　显示全部内文，并标出不同之处。 -C或--context 　与执行\"-c-\"指令相同。 -d或--minimal 　使用不同的演算法，以较小的单位来做比较。 -D或ifdef 　此参数的输出格式可用于前置处理器巨集。 -e或--ed 　此参数的输出格式可用于ed的script文件。 -f或-forward-ed 　输出的格式类似ed的script文件，但按照原来文件的顺序来显示不同处。 -H或--speed-large-files 　比较大文件时，可加快速度。 -l或--ignore-matching-lines 　若两个文件在某几行有所不同，而这几行同时都包含了选项中指定的字符或字符串，则不显示这两个文件的差异。 -i或--ignore-case 　不检查大小写的不同。 -l或--paginate 　将结果交由pr程序来分页。 -n或--rcs 　将比较结果以RCS的格式来显示。 -N或--new-file 　在比较目录时，若文件A仅出现在某个目录中，预设会显示： Only in目录：文件A若使用-N参数，则diff会将文件A与一个空白的文件比较。 -p 　若比较的文件为C语言的程序码文件时，显示差异所在的函数名称。 -P或--unidirectional-new-file 　与-N类似，但只有当第二个目录包含了一个第一个目录所没有的文件时，才会将这个文件与空白的文件做比较。 -q或--brief 　仅显示有无差异，不显示详细的信息。 -r或--recursive 　比较子目录中的文件。 -s或--report-identical-files 　若没有发现任何差异，仍然显示信息。 -S或--starting-file 　在比较目录时，从指定的文件开始比较。 -t或--expand-tabs 　在输出时，将tab字符展开。 -T或--initial-tab 　在每行前面加上tab字符以便对齐。 -u,-U或--unified= 　以合并的方式来显示文件内容的不同。 -v或--version 　显示版本信息。 -w或--ignore-all-space 　忽略全部的空格字符。 -W或--width 　在使用-y参数时，指定栏宽。 -x或--exclude 　不比较选项中所指定的文件或目录。 -X或--exclude-from 　您可以将文件或目录类型存成文本文件，然后在=中指定此文本文件。 -y或--side-by-side 　以并列的方式显示文件的异同之处。 --help 　显示帮助。 --left-column 　在使用-y参数时，若两个文件某一行内容相同，则仅在左侧的栏位显示该行内容。 --suppress-common-lines 　在使用-y参数时，仅显示不同之处。 实例 样本数据 test-a |------a asd sdf qaz1 adasd |------c 21312 test-b |------a asd sdf qaz1 adad |------b 12312 23121 3432432 1231 |------c 21312 1、比较两个文件下所有文件(包括子文件)的不同 # diff -yr --suppress-common-lines test-a test-b diff -yr --suppress-common-lines test-a/a test-b/a qaz1 adasd | qaz1 adad Only in test-b: b Files test-a/c and test-b/c are identical # test-a/a文件与test-b/a文件存在差异 # 表示b文件只在test-b文件夹中存在 # test-a/c文件与test-b/c文件完全一致 2、比较两个文件下除指定指定文件外其他文件(包括子文件)的不同 # diff -yr --suppress-common-lines -x .git -x .idea test-a test-b Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/linux-baseserver.html":{"url":"origin/linux-baseserver.html","title":"NFS与FTP","keywords":"","body":"一、NFS 1、服务端 ①安装 yum install -y nfs-utils rpcbind ;\\ systemctl enable nfs ;\\ systemctl enable rpcbind ;\\ systemctl start nfs ;\\ systemctl start rpcbind ②配置 配置文件：/etc/exports # 配置格式 要共享的目录 指定客户端IP地址1(权限) 指定客户端IP地址2(权限) /NFS 172.16.2.0/24(ro,rw) /NFS 172.16.2.0/24(ro,rw) 192.168.0.2(ro,rw) 限制挂载客户端 指定ip地址的主机：192.168.0.100 指定子网中的所有主机：192.168.0.0/24 或 192.168.0.0/255.255.255.0 指定域名的主机：nfs.test.com 指定域中的所有主机：*.test.com 所有主机：* 设置挂载权限 ro：共享目录只读； rw：共享目录可读可写； all_squash：所有访问用户都映射为匿名用户或用户组； no_all_squash（默认）：访问用户先与本机用户匹配，匹配失败后再映射为匿名用户或用户组； root_squash（默认）：将来访的root用户映射为匿名用户或用户组； no_root_squash：来访的root用户保持root帐号权限； anonuid=：指定匿名访问用户的本地用户UID，默认为nfsnobody（65534）； anongid=：指定匿名访问用户的本地用户组GID，默认为nfsnobody（65534）； secure（默认）：限制客户端只能从小于1024的tcp/ip端口连接服务器； insecure：允许客户端从大于1024的tcp/ip端口连接服务器； sync：将数据同步写入内存缓冲区与磁盘中，效率低，但可以保证数据的一致性； async：将数据先保存在内存缓冲区中，必要时才写入磁盘； wdelay（默认）：检查是否有相关的写操作，如果有则将这些写操作一起执行，这样可以提高效率； no_wdelay：若有写操作则立即执行，应与sync配合使用； subtree_check（默认） ：若输出目录是一个子目录，则nfs服务器将检查其父目录的权限； no_subtree_check ：即使输出目录是一个子目录，nfs服务器也不检查其父目录的权限，这样可以提高效率； ③生效配置 exportfs -a ④验证查看 showmount -e Server_IP ⑤查看运行状态 nfsstat rpcinfo netstat -tu -4 ⑥查看挂载的客户端 # 查看除本机以外挂载的客户端 netstat -an | grep $(hostname -i):2049 |awk '{print $5}' |grep -v \"$(hostname -i)\" 2、客户端 ①客户端安装 yum install -y nfs-utils rpcbind ②手动挂载 mount -t nfs -o 挂载参数 Server_IP:Share_Dir Mount_Dir mount -t nfs -o vers=3,nolock,proto=tcp,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 \\ 192.168.1.7:/mnt/nfs /mnt/backup ③自动挂载 echo \"Server_IP:Share_Dir Mount_dir nfs 权限 0 1\" >> /etc/fstab mount -a ④挂载参数 参数 说明 参数建议 soft/hard 软挂载方式挂载系统，若NFS请求超时，则客户端向调用程序返回错误；如果使用硬连接方式则客户端一直重新请求直至成功。默认为hard。 对于关键数据业务，不希望业务由于网络延迟或服务器服务重启或短暂的过载等情况而中断，建议挂载时使用hard参数；对于非关键数据业务，希望客户端程序能尽快响应，可以使用soft参数。 timeo=n 客户端重传请求前等待时间。对于基于TCP的NFS服务，默认等待重传时间为60s。使用TCP协议时，NFS Client不执行任何超时backoff。对于UDP协议，client使用一个合适的算法，为常用的请求类型estimate合适的超时时间。但对不常用的请求类型使用timeo设置。如果timeo没有设置，不常用的请求类型1.1秒以后重试。在每次重发后，NFS Client会将timeout时间加倍，直到最大的60秒。 retrans=n 客户端返回错误前的重传次数。默认为重传3次。retrans与soft参数一起使用时才有效。 timeo、retrans这两个参数选择主要取决于网络性能。对于网络吞吐量小，延时高，抖动高，丢包率大的情况，建议将timeo，retrans两个参数值设置大一些。对于网络吞吐量大，延时低，抖动低，丢包率小的情况，建议将timeo，retrans两个参数值设置小一些。具体设置值因网络状况而定。 resize=n 每个READ命令字向服务器读取文件的最大字节数。实际数据小于或等于此值。resize必须是1024倍数的正整数，小于1024时自动设为4096，大于1048576时自动设为1048576。默认时，服务器和客户端进行协商后设置。 通常使用默认值，由客户端和服务器协商设置。对于拥塞的低速网络，可以将该值调小，向服务器发送较短的请求包来提高NFS性能。对于高速网络，可以将该值增大，减少向服务器发送的请求包，获得性能的提升。 wsize=n 每个WRITE命令字向服务器写入文件的最大字节数。实际数据小于或等于此值。resize必须是1024倍数的正整数，小于1024时自动设为4096，大于1048576时自动设为1048576。默认时，服务器和客户端进行协商后设置。 通常使用默认值，由客户端和服务器协商设置。对于拥塞的低速网络，可以将该值调小，向服务器发送较小的请求包来提高NFS性能。对于高速网络，可以将该值增大，减少向服务器发送的请求包，获得性能的提升。 async/sync 同步、异步挂载，客户端默认异步（async）。对于异步挂载，客户端下发的写数据会先缓存在内存中，达到一定大小或者其他条件（与客户端设置，状态有关），再一起发往服务端。而同步挂载，每次下发的写数据马上发到服务端。 建议用默认的异步挂载，可以提高业务性能。对于客户端应用层来说，完全感知不到数据是否发往服务端，只能感知数据已经写成功。同步挂载要求每个请求立即发到服务端，增加了请求连接、发送次数，对于小io场景性能差异很明显（io越小，差异越大，一般都能达到几倍甚至更大的差异）。 注：对于异步挂载，可能在查看io性能显现为性能波动。在客户端写入缓存期间iops可能就低，因为数据还未发到服务端。但是性能是完全正常的，应该说比同步更好。如果是要要追求界面上的平稳，那就建议改为同步挂载。（补充：追求一次小io的数据全部刷到服务端稳定存储时间小，不仅要求客户端同步挂载，还需要服务端共享也是同步导出，否则需要加上手动commit刷盘——flush）。 acregmin/acregmax 设置NFS客户端缓存普通文件属性的最短时间和最长时间，单位为秒。超过此时间后对其进行更新。默认最短时间是3s，最长时间是60s。 acdirmin/acdirmax 设置NFS客户端缓存目录属性的最短时间和最长时间，单位为秒。超过此时间后对其进行更新。默认最短时间是3s，最长时间是60s。 ac/noac 设置是否缓存文件属性。为了提高性能，NFS客户端缓存文件属性（默认ac），然后每隔一段时间去检查文件属性后更新。在缓存有效期内，客户端不检测服务器上文件属性是否改变。默认为ac。 当服务器上共享文件的属性频繁地被多个客户端改变时，建议使用noac选项，或者使用ac并配合使用较小的acregmin/acregmax/acdirmin/acdirmax设置，这样就能获得较好的属性一致性。当服务器上共享文件的属性不会被频繁改变时，例如文件共享为只读，或者网络性能较好，建议使用默认的ac选项，然后根据实际的网络状况来调整acregmin/acregmax/acdirmin/acdirmax设置。 Actimeo 将acregmin/acregmax/acdirmin/acdirmax四个参数设置为相同时间，单位为秒。 bg/fg 设置挂载失败后的行为方式。默认的fg方式将立刻退出返回错误状态，bg方式是退出前将产生一个子进程在后台继续尝试挂载。 sharecache/nosharecache 设置客户端并发挂载同一文件系统时数据缓存和属性缓存的共享方式。设置为sharecache时，多个挂载共享共享同一缓存。设为nosharecache时，每个挂载各有一个缓存。默认为sharecache。 该参数用于客户端多次挂载同一共享目录的情况，建议使用默认的sharecache选项。 resvport/noresvport 设置连接服务器是否使用保密源端口。默认的resvport设置保密端口，noresvport设置为非保密端口。内核2.6.28及以后版本支持。 lookupcache=mode 设置内核管理给定挂载点的目录项缓存方式。其中包括all/none/pos几种方式。客户端缓存LOOKUP命令字请求结果。如果请求的目录项在服务器上，则返回结果为正，否则为负。all的管理方式是在父目录缓存的属性失效前客户端缓存这两种目录项；pos是在父目录缓存的属性失效前客户端缓存结果为正的查询结果，总是重新验证结果为负的查询结果。none总是重新验证目录缓存项。默认为all管理方式，内核2.6.28及以后版本支持。 LOOKUP命令字的作用是将文件名转换文件句柄。对于多个客户端经常创建或删除文件的情况，建议使用none。其它情况选用all或者pos。 intr/nointr 设置是否允许信号中断挂载点的文件操作。如果指定intr，当NFS操作被信号中断时系统返回EINTR。指定nointr，信号不会中断NFS文件操作。默认为nointr。指定intr时，通常同时使用soft选项，防止数据损坏。内核2.6.25及以后不再支持。 cto/nocto 设置是否使用“关闭打开”缓存一致的特性。通常客户端打开文件时检查是否存在以及是否有权限，当关闭文件时刷新更改。当设置为nocto时，客户端使用非标准的试探来检查服务器上文件是否改变，对于只读和文件更改较少时情形有助于提高性能 对于文件内容改变很少的情况，如服务器提供只读共享权限（文件系统以RO权限导出）给客户使用，建议使用nocto选项，这样可以提高性能。对于文件内容经常改变，客户端对文件缓存一致性要求较高，建议使用cto参数。 proto=transport (NFS2/NFS3) 客户端向服务器发起传输请求使用的协议，可以为UDP或者TCP。未指定时，mount命令选择服务器支持的协议。 tcp/udp 等价于proto=tcp和proto=udp选项。 在不稳定的复杂网络环境中建议使用tcp参数，在稳定的网络下可以使用udp参数。NFSv3/NFSv4支持tcp/udp，NFSv2只支持udp。 port=n 指定服务器NFS服务端口。如果NFS服务端口不在port上，则mount请求失败。未指定或设为0，mount命令根据服务器的rpcbind服务选择服务端口。 mountport=n 指定服务器上mountd的端口。如果mountd服务端口不在port上，则mount请求失败。未指定或设为0，mount命令根据服务器的rpcbind服务选择服务端口。该参数用于即使防火墙屏蔽rpcbind协议也能正常mount到服务器。 mountproto=transport 客户端向服务器发起MNT请求时和UMNT使用的协议，可以为udp或者tcp。该参数用于防火墙屏蔽特定的协议也能正常mount到服务器。 mounthost=name 设置开启mountd主机名。未指定时，mount命令认为mountd服务和NFS服务在同一主机上。 mountvers=n 设置连接服务器mountd时的RPC版本号。未指定时使用与请求的NFS版本相适应的版本号。该参数用于多个NFS服务运行于同一远程主机上 namlen=n 设置挂载路径名的最大长度。未指定时，通过与服务器协商设定。大多数情况为255字节 nfsvers/vers=n 设置使用NFS服务的NFS协议版本号。当文件系统为nfs时，Linux客户端支持NFS2和NFS3。如果不支持请求的版本，mount失败。未指定时，客户端先尝试使用NFS3，若失败再与服务器进行协商。 lock/nolock 选择是否使用NLM协议在服务器上锁文件。当选择nolock选项时，锁对于同一主机的应用有效，对不同主机不受锁的影响。默认为lock acl/noacl 设置是否在挂载时使用NFSACL协议。NFSACL协议不属于标准NFS协议，而是Solaris上的实现。未指定时，客户端与服务器协商检查服务器是否支持，如果支持则使用NFSACL rdirplus/nordirplus 设置是否使用NFS3的REAADDIRPLUS请求。默认为readdirplus proto=transpro （NFS4） 客户端向服务器发起传输请求使用的协议，可以为UDP或者TCP。未指定时，选用TCP。 port=n 指定服务器NFS服务端口。如果NFS服务端口不在port上，则mount请求失败。未指定时，客户端使用NFS标准的2049号端口。指定为0时，客户端选用服务器rpcbind服务支持的端口。 clientaddr=n.n.n.n 指定一个IPv4的主机地址使服务器能执行NFS4的回调请求。未指定时，mount命令尝试自己发现一个合适的回调地址。 3、卸载出现“设备正忙”的挂载 # 找到哪个进程正在使用挂载点的文件 lsof | grep '挂载点' # 查看使用挂载点文件的进程命令，如果无关紧要，可以直接杀掉 kill -9 对应的进程号 二、FTP的虚拟用户 vsftpd服务器同时支持匿名用户、本地用户和虚拟用户三类用户账号，使用虚拟用户账号可以提供集中管理的FTP根目录，方便了管理员的管理，同时将用于FTP登录的用户名、密码与系统用户账号区别开，进一步增强了FTP服务器的安全性。 设置不使用ftp所在主机的系统用户及匿名用户登录 限制登录的虚拟用户只在指定的目录下操作 方便添加虚拟用户 1、安装 yum install -y vsftpd vsftpd -version 2、配置 创建系统用户，所有的虚拟用户都对应到这个用户 useradd -d /data/ftp/ -s /sbin/nologin vftpuser 创建虚拟用户的账号密码文本 sehll >vi /etc/vsftpd/vuser.txt testuser 1234test #奇数行是用户名，偶数是密码 #生成虚拟数据库文件（*如果db_load没有安装，yum install db4-utils db4-devel db4-4.3安装才能使用。） db_load –T –t hash –f /etc/vsftpd/vuser.txt /etc/vsftpd/vuser.db #修改虚拟数据库文件vuser.db的权限为 700 chmod 700 /etc/vsftpd/vuser.db #配置PAM文件，目的是对客户端进行验证.编辑/etc/pam.d/vsftpd文件，注释所有内容，后添加： vi /etc/pam.d/vsftpd auth required pam_userdb.so db=/etc/vsftpd/vuser account required pam_userdb.so db=/etc/vsftpd/vuser #不能写成db=/etc/vsftpd/vuser.db 配置vsftp的配置文件/etc/vsftpd/vsftpd.conf anonymous_enable=NO #是否允许匿名用户登录 local_enable=YES #是否允许vsftp所在主机的本地用户登录 listen=YES listen_ipv6=NO guest_enable=YES #激活虚拟账户 guest_username=vftpuser #把虚拟账户绑定为系统账户vftpuser pam_service_name=vsftpd #使用PAM验证，指定PAM配置文件，文件已经在/etc/pam.d/存在（第二步配置的） user_config_dir=/etc/vsftpd/vsftpd_user_conf #设置虚拟用户的配置文件目录，配置文件名与虚拟用户名同名 write_enable=NO 配置虚拟用户的配置文件/etc/vsftpd/vsftpd_user_conf/testuser anon_world_readable_only=NO #浏览FTP目录 anon_upload_enable=YES #允许上传 anon_mkdir_write_enable=YES #建立和删除目录 anon_other_write_enable=YES #改名和删除文件 local_root=/data/ftp/test #指定虚拟用户在系统用户下面的路径，限制虚拟用户家目录，虚拟用户登录后主目录 write_enable=YES #启用/禁止用户的写权限 allow_writeable_chroot=YES download_enable=NO #设置只能上传不能下载 cmds_denied=DELE #禁用掉删除DELE命令 #每行配置项结尾不能有空格 创建配置文件中设置的目录并设置相关权限 mkdir -p /data/ftp/test ;\\ chown -R vftpuser:vftpuser /data/ftp/test ;\\ chmod -R 770 /data/ftp/test 3、启动验证 systemctl start vsftpd systemctl status vsftpd -l ftp 127.0.0.1 > ls > put local_file_path ftp_file_path 4、配置中出现的错误 ①refusing to run with writable root inside chroot () 限定了用户不能跳出其主目录之后，使用该用户登录FTP时往往会遇到这个错误 500 OOPS: vsftpd: refusing to run with writable root inside chroot () 从2.3.5之后，vsftpd增强了安全检查，如果用户被限定在了其主目录下，则该用户的主目录不能再具有写权限了！如果检查发现还有写权限，就会报该错误。 要修复这个错误，可以用命令chmod a-w /home/user去除用户主目录的写权限（注意把目录替换成你自己的）。或者你可以在vsftpd_user_conf下的虚拟用户配置文件中增加下列一项：allow_writeable_chroot=YES ②530 login incrrect无法登陆。 查看日志tail -f /var/log/secure或者systemctl status vsftpd -l PAM unable to dlopen(/lib/security/pam_userdb.so): /lib/security/pam_userdb.so: cannot open shared object file: No such file or directory 原来pam_userdb.so在/lib64/security/pam_userdb.so,修改/etc/pam.d/vsftpd #将下列内容 auth required /lib/security/pam_userdb.so db=/etc/vsftpd/vuser account required /lib/security/pam_userdb.so db=/etc/vsftpd/vuser #替换成 auth required /lib64/security/pam_userdb.so db=/etc/vsftpd/vuser account required /lib64/security/pam_userdb.so db=/etc/vsftpd/vuser #或者 auth required pam_userdb.so db=/etc/vsftpd/vuser account required pam_userdb.so db=/etc/vsftpd/vuser 保存重启vsftpd服务。 ③425 Security: Bad IP connecting 当使用非21端口进行端口转发连接的话，会出现上述情况。 解决方案： 1.#vim /etc/vsftpd/vsftpd.conf 2.添加：pasv_promiscuous=YES 3.保存后退出 4.重启vsftpd #service vsftpd restart #pasv_promiscuous选项参数说明： 此选项激活时，将关闭PASV模式的安全检查。该检查确保数据连接和控制连接是来自同一个IP地址。小心打开此选项。此选项唯一合理的用法是存在于由安全隧道方案构成的组织中。默认值为NO。 合理的用法是：在一些安全隧道配置环境下，或者更好地支持FXP时(才启用它)。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-09-14 16:04:06 "},"origin/linux-daemon.html":{"url":"origin/linux-daemon.html","title":"后台启动进程","keywords":"","body":"Linux如何守护进程 一、问题的由来 Web应用写好后，下一件事就是启动，让它一直在后台运行。 这并不容易。举例来说，下面是一个最简单的Node应用server.js，只有6行。 var http = require('http'); http.createServer(function(req, res) { res.writeHead(200, {'Content-Type': 'text/plain'}); res.end('Hello World'); }).listen(5000); 你在命令行下启动它。 $ node server.js 看上去一切正常，所有人都能快乐地访问 5000 端口了。但是，一旦你退出命令行窗口，这个应用就一起退出了，无法访问了。怎么才能让它变成系统的守护进程（daemon），成为一种服务（service），一直在那里运行呢？ 二、前台任务与后台任务 上面这样启动的脚本，称为\"前台任务\"（foreground job）。它会独占命令行窗口，只有运行完了或者手动中止，才能执行其他命令。 变成守护进程的第一步，就是把它改成\"后台任务\"（background job）。 $ node server.js & 只要在命令的尾部加上符号&，启动的进程就会成为\"后台任务\"。如果要让正在运行的\"前台任务\"变为\"后台任务\"，可以先按ctrl + z，然后执行bg命令（让最近一个暂停的\"后台任务\"继续执行）。 \"后台任务\"有两个特点。 （1）继承当前 session （对话）的标准输出（stdout）和标准错误（stderr）。因此，后台任务的所有输出依然会同步地在命令行下显示。 （2）不再继承当前 session 的标准输入（stdin）。你无法向这个任务输入指令了。如果它试图读取标准输入，就会暂停执行（halt）。 可以看到，\"后台任务\"与\"前台任务\"的本质区别只有一个：是否继承标准输入。所以，执行后台任务的同时，用户还可以输入其他命令。 三、SIGHUP信号 变为\"后台任务\"后，一个进程是否就成为了守护进程呢？或者说，用户退出 session 以后，\"后台任务\"是否还会继续执行？ Linux系统是这样设计的。 用户准备退出 session 系统向该 session 发出SIGHUP信号 session 将SIGHUP信号发给所有子进程 子进程收到SIGHUP信号后，自动退出 上面的流程解释了，为什么\"前台任务\"会随着 session 的退出而退出：因为它收到了SIGHUP信号。 那么，\"后台任务\"是否也会收到SIGHUP信号？ 这由 Shell 的huponexit参数决定的。 $ shopt | grep huponexit 执行上面的命令，就会看到huponexit参数的值。 大多数Linux系统，这个参数默认关闭（off）。因此，session 退出的时候，不会把SIGHUP信号发给\"后台任务\"。所以，一般来说，\"后台任务\"不会随着 session 一起退出。 四、disown 命令 通过\"后台任务\"启动\"守护进程\"并不保险，因为有的系统的huponexit参数可能是打开的（on）。 更保险的方法是使用disown命令。它可以将指定任务从\"后台任务\"列表（jobs命令的返回结果）之中移除。一个\"后台任务\"只要不在这个列表之中，session 就肯定不会向它发出SIGHUP信号。 $ node server.js & $ disown 执行上面的命令以后，server.js进程就被移出了\"后台任务\"列表。你可以执行jobs命令验证，输出结果里面，不会有这个进程。 disown的用法如下。 # 移出最近一个正在执行的后台任务 $ disown # 移出所有正在执行的后台任务 $ disown -r # 移出所有后台任务 $ disown -a # 不移出后台任务，但是让它们不会收到SIGHUP信号 $ disown -h # 根据jobId，移出指定的后台任务 $ disown %2 $ disown -h %2 五、标准 I/O 使用disown命令之后，还有一个问题。那就是，退出 session 以后，如果后台进程与标准I/O有交互，它还是会挂掉。还是以上面的脚本为例，现在加入一行。 var http = require('http'); http.createServer(function(req, res) { console.log('server starts...'); // 加入此行 res.writeHead(200, {'Content-Type': 'text/plain'}); res.end('Hello World'); }).listen(5000); 启动上面的脚本，然后再执行disown命令。 $ node server.js & $ disown 接着，你退出 session，访问5000端口，就会发现连不上。这是因为\"后台任务\"的标准 I/O 继承自当前 session，disown命令并没有改变这一点。一旦\"后台任务\"读写标准 I/O，就会发现它已经不存在了，所以就报错终止执行。 为了解决这个问题，需要对\"后台任务\"的标准 I/O 进行重定向。 $ node server.js > stdout.txt 2> stderr.txt 六、nohup 命令 还有比disown更方便的命令，就是nohup。 $ nohup node server.js & nohup命令对server.js进程做了三件事。 阻止SIGHUP信号发到这个进程。 关闭标准输入。该进程不再能够接收任何输入，即使运行在前台。 重定向标准输出和标准错误到文件nohup.out。 也就是说，nohup命令实际上将子进程与它所在的 session 分离了。 注意，nohup命令不会自动把进程变为\"后台任务\"，所以必须加上&符号。 七、Screen 命令与 Tmux 命令 另一种思路是使用 terminal multiplexer （终端复用器：在同一个终端里面，管理多个session），典型的就是 Screen 命令和 Tmux 命令。 它们可以在当前 session 里面，新建另一个 session。这样的话，当前 session 一旦结束，不影响其他 session。而且，以后重新登录，还可以再连上早先新建的 session。 Screen 的用法如下。 # 新建一个 session $ screen $ node server.js 然后，按下ctrl + A和ctrl + D，回到原来的 session，从那里退出登录。下次登录时，再切回去。 $ screen -r 如果新建多个后台 session，就需要为它们指定名字。 $ screen -S name # 切回指定 session $ screen -r name $ screen -r pid_number # 列出所有 session $ screen -ls 如果要停掉某个 session，可以先切回它，然后按下ctrl + c和ctrl + d。 Tmux 比 Screen 功能更多、更强大，它的基本用法如下。 $ tmux $ node server.js # 返回原来的session $ tmux detach 除了tmux detach，另一种方法是按下Ctrl + B和d ，也可以回到原来的 session。 # 下次登录时，返回后台正在运行服务session $ tmux attach 如果新建多个 session，就需要为每个 session 指定名字。 # 新建 session $ tmux new -s session_name # 切换到指定 session $ tmux attach -t session_name # 列出所有 session $ tmux list-sessions # 退出当前 session，返回前一个 session $ tmux detach # 杀死指定 session $ tmux kill-session -t session-name 八、Node 工具 对于 Node 应用来说，可以不用上面的方法，有一些专门用来启动的工具：forever，nodemon 和 pm2。 forever 的功能很简单，就是保证进程退出时，应用会自动重启。 # 作为前台任务启动 $ forever server.js # 作为服务进程启动 $ forever start app.js # 停止服务进程 $ forever stop Id # 重启服务进程 $ forever restart Id # 监视当前目录的文件变动，一有变动就重启 $ forever -w server.js # -m 参数指定最多重启次数 $ forever -m 5 server.js # 列出所有进程 $ forever list nodemon一般只在开发时使用，它最大的长处在于 watch 功能，一旦文件发生变化，就自动重启进程。 # 默认监视当前目录的文件变化 $ nodemon server.js ＃ 监视指定文件的变化 $ nodemon --watch app --watch libs server.js pm2 的功能最强大，除了重启进程以外，还能实时收集日志和监控。 # 启动应用 $ pm2 start app.js # 指定同时起多少个进程（由CPU核心数决定），组成一个集群 $ pm2 start app.js -i max # 列出所有任务 $ pm2 list # 停止指定任务 $ pm2 stop 0 ＃ 重启指定任务 $ pm2 restart 0 # 删除指定任务 $ pm2 delete 0 # 保存当前的所有任务，以后可以恢复 $ pm2 save # 列出每个进程的统计数据 $ pm2 monit # 查看所有日志 $ pm2 logs # 导出数据 $ pm2 dump # 重启所有进程 $ pm2 kill $ pm2 resurect # 启动web界面 http://localhost:9615 $ pm2 web 十、Systemd 除了专用工具以外，Linux系统有自己的守护进程管理工具 Systemd 。它是操作系统的一部分，直接与内核交互，性能出色，功能极其强大。我们完全可以将程序交给 Systemd ，让系统统一管理，成为真正意义上的系统服务。 参考文章 http://www.ruanyifeng.com/blog/2016/02/linux-daemon.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/linux-文本处理.html":{"url":"origin/linux-文本处理.html","title":"文本处理","keywords":"","body":"一、awk 1、获取匹配关键字后的内容 awk '{ if(match($0,\"关键字\")) {print substr($0,RSTART+RLENGTH) }}'文件 #示例 # 原始文本 2018-07-31T09:33:08.160102Z 1 [Note] A temporary password isgenerated for root@localhost: oco4Pr&a!o;v # 命令 awk '{ if(match($0,\"root@localhost: \")) {print substr($0,RSTAR+RLENGTH) }}' test.log # 结果 oco4Pr&a!o;v 2、去除文本中的空行 awk NF test.txt # NF代表当前行的字段数，空行的话字段数为0,被awk解释为假，因此不进行输出。 3、获取匹配关键字后多少的位字符串 # 样本 a=\"Location: https://allinone.okd311.curiouser.com:8443/oauth/token/implicit#access_token=FBHwgR1jj2coLoYYfG9SdGUke9L9HmAU2IOI9GaMKrQ&expires_in=86400&scope=user%3Afull&token_type=Bearer\" # 获取\"access_token=\"的值 # 方式一 echo $a | grep \"access_token=\" |awk -F\"access_token=\" '/access_token=/{printf substr($2,0,43)}' # 结果： FBHwgR1jj2coLoYYfG9SdGUke9L9HmAU2IOI9GaMKrQ # 方式二 echo $a | grep \"access_token=\" |awk '{ if(match($0,\"access_token=\")) {print substr($0,RSTART+RLENGTH) }}'| awk -F '&' '{print $1}' # 结果： FBHwgR1jj2coLoYYfG9SdGUke9L9HmAU2IOI9GaMKrQ 4、使用多个分隔符分割字符串 # @, 空格和Tab都是字段分隔符 awk -F”[@ /t]\" '{print $1 $2}' 5、为每行增加行号 awk '$0=NR\":\"$0' 文件名 > 新文件名 # $0表示原来每行的内容， # NR表示行号， # 双引号之间表示行号与原来内容之间的delimiter 二、sed sed [-hnV][-e][-f][文本文件] 参数说明： -e 或 --expression= 以选项中指定的script来处理输入的文本文件。 -f 或 --file= 以选项中指定的script文件来处理输入的文本文件。 -h 或 --help 显示帮助。 -n 或 --quiet或--silent 仅显示script处理后的结果。 -V 或 --version 显示版本信息。 动作说明 a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行) c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行) p ：打印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～ s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 0、替换环境变量的值到文件中 如果想替换字符串中的值为环境变量的值，可以使用将匹配规则使用\"\"括起来，在\"\"中直接引用环境变量 echo \"test asdasdas \\$TEST\" > test.txt export aaaaa=1234 sed -i -e \"s/\\$TEST/$aaaaa/g\" test.txt 如果变量的值中包含特殊字符，例如/，与sed匹配模式的关键字符造成歧义。可使用？代替/ echo \"test asdasdas \\$TEST\" > test.txt export aaaaa=/1234 sed -i \"s?\\$TEST?$aaaaa?g\" test.txt 1、新增内容到末尾行的末尾 sed '$ s/$/新增内容/' file_path 2、去除文本中空行和开头\"##\"的行 sed '/^$/d;/^##/d' file_path 3、去除文本中的空行 sed '/^\\s*$/d' test.txt 4、多个匹配规则 sed -i -e '/hah/a lala\\nhehe' -e '/lala/d' test 5、在查找到匹配行后的操作 sed -i '/hah/a lallalla' test #在查找到匹配行后添加一行 sed -i '/hah/a lala\\nhehe' test #在查找到匹配行后添加多行 sed -i '/hah/d' test #删除查找到匹配行 6、在查找匹配行的末首或末尾添加内容 sed -i '/ha/ s/^/la' test #在查找包含\"ha\"的行首追加\"la\",\"laha\" sed -i '/ha/ s/$/la' test #在查找包含\"ha\"的行末追加\"la\"，\"hala\" 7、去掉文本中开头带#号注释的行 sed -i -c -e '/^$/d;/^#/d' file 8、去除文本中的换行符^M Windows下保存的文本文件，上传到Linux/Unix下后总会在末尾多了一个换行符^M，导致一些xml、ini、sh等文件读取错误 sed 's/^M//' 原文件>新文件 # 注意，^M = Ctrl v + Ctrl m，而不是手动输入^M 9、每行前后添加空行 每行后面添加一行空行： sed G tmp 每行前面添加一行空行： sed '{x;p;x;}' tmp 每行后面添加两行空行： sed 'G;G' tmp 每行前面添加两行空行： sed '{x;p;x;x;p;x;}' tmp 每行后面添加三行空行： sed 'G;G;G' tmp 每行前面添加三行空行： sed '{x;p;x;x;p;x;x;p;x}' tmp 依次类推，添加几行空行，就有几个G或者x;p;x 10、如果行后有空行，则删除，然后每行后面添加空行 sed '/^$/d;G' tmp 11、在匹配行前后添加空行 如果一行里面有shui这个单词，那么在他后面会添加一个空行 sed '/shui/G' tmp 如果一行里面有shui这个单词，那么在他前后各添加一个空行 sed '/shui/{x;p;x;G}' tmp 如果一行里面有shui这个单词，那么在他前面添加一个空行 sed '/shui/{x;p;x;}' tmp 在第一行前面添加空行，想在第几行，命令中的1就改成几 sed '1{x;p;x;}' tmp 在第一行后面添加空行，想在第几行，命令中的1就改成几 sed '1G' tmp 12、每几行后面添加一个空行 每两行后面增加一个空行 sed 'N;/^$/d;G' file.txt 每两行前面添加一个空行 sed 'N;/^$/d;{x;p;x;}' tmp 每三行后面增加一个空行 sed 'N;N;/^$/d;G' file.txt 每三行前面增加一个空行 sed 'N;N;/^$/d;{x;p;x;}' tmp 13、以x为开头或以x为结尾的行前后添加空行 以xi为开头的行后面添加空行 sed '/^xi/G;' tmp 以xi为结尾的行前面添加空行 sed '/^xi/{x;p;x;}' tmp 以xi为结尾的行后面添加空行 sed '/xi$/G;' tmp 以xi为结尾的行后面添加空行 sed '/xi$/{x;p;x;}' tmp 三、grep grep [OPTION]... PATTERN [FILE]... -r 是递归查找 -n 是显示行号 -R 查找所有文件包含子目录 -i 忽略大小写 -l 只列出匹配的文件名 -L 列出不匹配的文件名 -w 只匹配整个单词，而不是字符串的一部分 -C 匹配的上下文分别显示[number]行 1、统计某文件夹下文件的个数 ls -l /data|grep \"^-\"|wc -l #不包含子目录 ls -lR /data|grep \"^-\"|wc -l #不含子目录 2、统计某文件夹下目录的个数 ls -l /data |grep \"^ｄ\"|wc -l #不包含子目录 ls -lR /data|grep \"^ｄ\"|wc -l #包含子目录 3、统计某目录(包含子目录)下的所有某种类型的文件 ls -lR /data|grep txt|wc -l 4、去除文本中的空行 grep -v '^\\s*$' test.txt 5、多关键词匹配 grep pattern1 | pattern2 files ：显示匹配 pattern1 或 pattern2 的 grep pattern1 files | grep pattern2 ：显示既匹配 pattern1 又匹配 pattern2 的行。 grep -E '关键词1|关键词2' 6、xargs配合grep查找 find -type f -name '*.php'|xargs grep 'GroupRecord' 7、查找路径下含有某字符串的所有文件 grep -rn \"hello,world!\" * 8、完全匹配关键词 grep -Fx 关键词 9、精准匹配 grep -w 关键词 # 不加-w是默认情况 10、显示文件非#注释内容 # 显示文件非#注释内容 grep -v ^[[:space:]]*# 文本文件 # # 显示文件非#注释内容，不显示空行 egrep -v '#|^$' filename 11、删除空行并重定向到新文件 grep . filename > newfilename 四、egrep 1、只显示文本中的非空行和非注释行 egrep -v '^$|#' file_path 五、cut 1、获取硬盘某个分区的UUID号追加到fstab blkid | grep /dev/sdb5 | cut -d ' ' -f 2 >>/etc/fstab;sed -i '$ s/$/ data ext4 defaults 0 0/' /etc/fstab 六、wc 1、统计某个目录下某种文件内总共多少行 find . -name \"*.java\" -print | xargs cat | wc -l 七、find 精确查找 实时查找 遍历（慢） 支持查找条件较 格式：find [选项]... [查找路径] [查找条件] [处理动作] 查找路径：指定具体目标路径；默认为当前目录 查找条件：可以对文件名、大小、类型、权限等标准进行查找；默认为找出指定路径下的所有文件 处理动作：对符合条件的文件做操作，默认输出至屏幕（print） 查找条件 | 关键字 | 说明 | | --------------- | ------------------------------------------------------------ | | -name | 根据目标文件的名称进行查找,允许使用“*”及“?”通配符必须加上\" \" | | -iname | 不区分名字的大小写 | | -size | 根据目标文件的大小进行查找一般使用“＋”、“-”号设置超过或小于指定的大小作为查找条件常用的容量单位包括 kB（注意 k 是小写）、MB、GB(在没有+ -的情况下写的越小越好) | | -user | 根据文件是否属于目标用户进行查找 | | -type | 根据文件的类型进行查找文件类型包括普通文件（f）、目录（d）、块设备文件（b）、字符设备文件（c）等 | | -a | 和，可以省略 | | -o | 或 | | -inum | 根据文件inode号查找 | | -perm | 按文件权限查找 | | -maxdepth level | 将你的文件以分级的形式查找 | | -mindepth level | 将你的文件以分级的形式查找 | | -mtime | 时间 | | -empty | 空目录 | | -nouser | 无主用户，用户被删除 | 处理动作 | 选项 | 作用 | | ------- | ---------------- | | -print | 默认，不需要输入 | | -ls | 长格式 | | -delete | 删除 | | -ok | 执行一次询问一次 | | -exec | 直接处理，不询问 | 1、显示指定路径的文件，排除文件夹 find . -name '*' -type f -print 2、显示指定路径的文件，排除指定文件夹 find . -name '*' ! -path \"./.git/*\" -type f -print 八、dos2unix dos2unix是将Windows格式文件转换为Unix、Linux格式的实用命令。Windows格式文件的换行符为\\r\\n ,而Unix&Linux文件的换行符为\\n. dos2unix命令其实就是将文件中的\\r\\n 转换为\\n。而unix2dos则是和dos2unix互为孪生的一个命令，它是将Linux&Unix格式文件转换为Windows格式文件的命令。 安装 yum install dos2unix -y #会安装dos2Unix、unix2dos、unix2mac这三条命令 用法 dos2unix [options] [-c convmode] [-o file ...] [-n infile outfile ...] -h 显示命令dos2unix联机帮助信息。 -k 保持文件时间戳不变 -q 静默模式，不输出转换结果信息等 -V 显示命令版本信息 -c 转换模式 -o 在源文件转换，默认参数 -n 保留原本的旧档，将转换后的内容输出到新档案.默认都会直接在原来的文件上修改， 1、一次转换多个文件 $> dos2unix filename1 filename2 filename3 2、默认情况下会在源文件上进行转换，如果需要保留源文件，那么可以使用参数-n dos2unix -n oldfilename newfilename 3、保持文件时间戳不变 $ ls -lrt dosfile -rw-r--r-- 1 root root 67 Dec 26 11:46 dosfile $ dos2unix dosfile dos2unix: converting file dosfile to UNIX format ... $ ls -lrt dosfile -rw-r--r-- 1 root root 65 Dec 26 11:58 dosfile $ dos2unix -k dosfile dos2unix: converting file dosfile to UNIX format ... $ ls -lrt dosfile -rw-r--r-- 1 root root 65 Dec 26 11:58 dosfile 4、静默模式格式化文件 unix2dos -q dosfile 八、Perl 1、统计文件的行数和字符数 perl -lne '$i++; $in += length($_); END { print \"$i lines, $in characters\"; }' 文本文件 2、转换tab为空格 1t = 2sp perl -p -i -e 's/\\t/ /g' 文本文件 3、搜索并替换字符 perl -i -pe's/SEARCH/REPLACE/' 文本文件 九、vscode 1、搜索空行并进行替换 按下ctrl+h键进行正则匹配：^\\s*(?=\\r?$)\\n，然后进行替换操作 2、插入递增数字 安装插件increment-selection 使用 option+cmd+向下键，进行多行编辑 再 cmd+ option + i 插入数字 默认起始数字是0， 如果想改变起始数字，在多行编辑前将第一项改为想要的数字，多行选中后再使用快捷键。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-07-14 18:13:09 "},"origin/ssh-agent.html":{"url":"origin/ssh-agent.html","title":"SSH私钥代理ssh-agent","keywords":"","body":"SSH私钥管理程序 SSH-Agent 一、ssh-agent能干什么？ 多SSH私钥管理 使用不同的密钥连接到不同的主机时，需要手动指定对应的密钥，ssh-agent可以帮助我们选择对应的密钥进行认证，不用手动指定密钥即可进行连接。 避免重复输入加密私钥的密码 当私钥设置了密码，我们又需要频繁的使用私钥进行认证时，ssh-agent可以帮助我们免去重复的输入密码的操作 二、ssh-agent管理 1、安装 yum install -y openssh-clients 2、启动 ssh-agent启动有两种方式 ssh-agent #SHELL 在OS默认shell中再创建一个子shell，在子shell中运行ssh-agent进程，退出子shell自动结束代理。 eval '$(ssh-agent -s)' 单独启动一个代理进程，退出当前shell时最好使用ssh-agent -k关闭对应代理 3、关闭 在当前bash中，使用ssh-agent -k命令可以关闭对应的ssh-agent进程 如果在退出了当前bash以后再使用'ssh-agent -k'命令，是无法关闭对应的ssh-agent进程的，此时使用kill命令 使用 ssh-agent $SHELL 命令启动的ssh-agent也可以使用'ssh-agent -k'命令关闭ssh代理。 4、锁定ssh-agent # 加锁，需要输入加锁密码 ssh-add -x # 解锁，需要输入加锁密码 ssh-add -X 三、私钥管理 ①添加私钥到ssh-agent ssh-add ~/.ssh/id_rsa ②删除ssh-agent管理的私钥 ssh-add -d ～/.ssh/id_rsa # 删除ssh-agent中所有的私钥 ssh-add -D ③查看ssh-agent管理的私钥 ssh-add -l ④查看ssh-agent管理的私钥对应的公钥内容 ssh-add -L Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/linux-htpasswd.html":{"url":"origin/linux-htpasswd.html","title":"htpasswd","keywords":"","body":"一、Overviews htpasswd命令是Apache的Web服务器内置工具，用于创建和更新储存用户名、域和用户基本认证的密码文件,主要用于对基于http用户的认证。 二、安装 yum install -y httpd-tools 三、语法 htpasswd(选项)(参数) 选项 -c：创建一个加密文件 -n：不更新加密文件，只将加密后的用户名密码显示在屏幕上 -m：默认采用MD5算法对密码进行加密 -d：采用CRYPT算法对密码进行加密 -p：不对密码进行进行加密，即明文密码 -s：采用SHA算法对密码进行加密 -b：在命令行中一并输入用户名和密码而不是根据提示输入密码 -D：删除指定的用户 参数 用户：要创建或者更新密码的用户名 密码：用户的新密码 四、常见操作 1、利用htpasswd命令添加用户 htpasswd .passwd -bc www.linuxde.net php # 在bin目录下生成一个.passwd文件，用户名www.linuxde.net，密码：php，默认采用MD5加密方式 2、在原有密码文件中增加下一个用户 htpasswd .passwd -b Jack 123456 #去掉-c选项，即可在第一个用户之后添加第二个用户，依此类推。 3、不更新密码文件，只显示加密后的用户名和密码 htpasswd -nb Jack 123456 # 不更新.passwd文件，只在屏幕上输出用户名和经过加密后的密码 4、利用htpasswd命令删除用户名和密码 htpasswd .passwd -D Jack 5、利用htpasswd命令修改密码 htpasswd .passwd -D Jack htpasswd .passwd -b Jack 123456 # 即先使用htpasswd删除命令删除指定用户，再利用htpasswd添加用户命令创建用户即可实现修改密码的功能。 参考链接 http://man.linuxde.net/htpasswd Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/linux-shyaml.html":{"url":"origin/linux-shyaml.html","title":"YAML文本处理工具shyaml","keywords":"","body":"Linux下YAML文本处理工具shyaml 一、Overviews 通过 shyaml，可以直接获取键、值、键值对或对应的类型 二、安装 pip install shyaml 三、语法 cat | shyaml ACTION KEY [DEFAULT] ACTION get-type：获取相应的类型 get-value：获取值 get-values{,-0}：对序列类型来说，获取值列表 keys{,-0}：返回键列表 values{,-0}：返回值列表 key-values,{,-0}：返回键值对 Note： 结果默认是加\\n换行符，若用-0形式则以NUL字符填充 KEY为要查询的键，如不提供，则使用DEFAULT 四、示例 --- idc_group: name: bx bx: news_bx: news_bx web3_bx: web3_php-fpm_bx 如果要获取idc_group.name的值则可以执行 cat file.yaml | shyaml get-value idc_group.name 想获取idc_group.bx的键值对可执行 cat file.yaml | shyaml key-values idc_group.bx 参考链接 https://www.linuxidc.com/Linux/2016-04/130403.htm https://dev.to/vikcodes/yq-a-command-line-tool-that-will-help-you-handle-your-yaml-resources-better-8j9 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-11-09 17:18:59 "},"origin/linux-jq.html":{"url":"origin/linux-jq.html","title":"JSON文本处理工具jq","keywords":"","body":"Linux下JSON文本处理工具jq 一、Overviews jq 是一款命令行下处理 JSON 数据的工具。其可以接收标准输入，命令管道或者文件中的 JSON 数据，经过一系列的过滤器(filters)和表达式的转后形成我们需要的数据结构并将结果输出到标准输出中。jq 的这种特性使我们可以很容易地在 Shell 脚本中调用它。 文档说明：https://stedolan.github.io/jq/manual/ 工具网站：https://jqplay.org/ 二、安装 yum install -y epel-release ;\\ yum install -y jq 三、jq命令参数 jq [options] [file...] options: -c 使输出紧凑，而不是把每一个JSON对象输出在一行。; -n 不读取任何输入，过滤器运行使用null作为输入。一般用作从头构建JSON数据。; -e set the exit status code based on the output; -s 读入整个输入流到一个数组(支持过滤); -r 如果过滤的结果是一个字符串，那么直接写到标准输出（去掉字符串的引号）; -R read raw strings, not JSON texts; -C 打开颜色显示; -M 关闭颜色显示; -S sort keys of objects on output; --tab use tabs for indentation; --arg a v jq 通过该选项提供了和宿主脚本语言交互的能力。该选项将值(v)绑定到一个变量(a)上。在后面的 filter 中可以直接通过变量引用这个值。例如，filter '.$a'表示查询属性名称等于变量 a 的值的属性。; --argjson a v set variable $a to JSON value ; --slurpfile a f set variable $a to an array of JSON texts read from ; 四、示例 样本数据 {\"snapshots\": [{\"snapshot\": \"AAA-api-frame-2019-02-08-2019.02.24\",\"uuid\": \"aMhlAmhrqng\",\"version_id\": 7050100,\"version\": \"7.5.1\",\"indices\": [\"AAA-api-frame-2019-02-08\"],\"include_global_state\": false,\"state\": \"SUCCESS\",\"start_time\": \"2019-02-24T08:53:01.193Z\",\"end_time\": \"2019-02-24T08:53:01.593Z\",\"duration_in_millis\": 402,\"failures\": [],\"shards\": {\"total\": 1,\"failed\": 0,\"successful\": 1}},{\"snapshot\": \"BBB-api-frame-2019-02-09-2019.02.24\",\"uuid\": \"Wp5MBOFWJA\",\"version_id\": 7050199,\"version\": \"7.5.1\",\"indices\": [\"BBB-api-frame-2019-0209\"],\"include_global_state\": false,\"state\": \"SUCCESS\",\"start_time\": \"2019-02-24T11:04:56.063Z\",\"end_time\": \"2020-02-24T11:04:56.463Z\",\"duration_in_millis\": 399,\"failures\": [],\"test\": \"hah\",\"shards\": {\"total\": 1,\"failed\": 0,\"successful\": 1}}]} { \"snapshots\": [ { \"snapshot\": \"AAA-api-frame-2019-02-08-2019.02.24\", \"uuid\": \"aMhlAmhrqng\", \"version_id\": 7050100, \"version\": \"7.5.1\", \"indices\": [ \"AAA-api-frame-2019-02-08\" ], \"include_global_state\": false, \"state\": \"SUCCESS\", \"start_time\": \"2019-02-24T08:53:01.193Z\", \"end_time\": \"2019-02-24T08:53:01.593Z\", \"duration_in_millis\": 402, \"failures\": [], \"shards\": { \"total\": 1, \"failed\": 0, \"successful\": 1 } }, { \"snapshot\": \"BBB-api-frame-2019-02-09-2019.02.24\", \"uuid\": \"Wp5MBOFWJA\", \"version_id\": 7050199, \"version\": \"7.5.1\", \"indices\": [ \"BBB-api-frame-2019-0209\" ], \"include_global_state\": false, \"state\": \"SUCCESS\", \"start_time\": \"2019-02-24T11:04:56.063Z\", \"end_time\": \"2020-02-24T11:04:56.463Z\", \"duration_in_millis\": 399, \"failures\": [], \"test\": \"hah\", \"shards\": { \"total\": 1, \"failed\": 0, \"successful\": 1 } } ] } 1、输出控制 ①美化输出 $ jq -r '.' test.json ②换行与不换行输出 示例数据 [{\"id\": 16176,\"iid\": 7},{\"id\": 16173,\"iid\": 4}] 默认遍历数组中一个对象属性时会换行显示 $ cat test.json | jq -r '.[] | .id , .iid' 16176 7 16173 4 jq 加-j参数，可不换行输出 $ cat test.json | jq -jr '.[] | .id , .iid' 161767161734 或者 $ cat test.json | jq -r '.[] | \"\\(.id) , \\(.iid)\"' 16176 , 7 16173 , 4 ③输出额外信息 示例数据还使用上一个 $ cat test.json | jq -r '.[] | \" id: \\(.id) , iid: \\(.iid)\"' id: 16176 , iid: 7 id: 16173 , iid: 4 $ cat test.json | jq -jr '.[] | \" \\\"\" , \"IID: \" , .iid , \" ID: \" , .id ,\"\\\"\" ' \"IID: 7 ID: 16176\" \"IID: 4 ID: 16173\" ④格式化输出 $ cat test.json | jq -r '[.id,.iid] as [$id,$iid] | \"\\($id) -|- \\($iid)\"' 1 -|- 11 22 -|- 21 ⑤以Key=value的形式输出 jq -r '.snapshots[].shards|to_entries[]|\"\\(.key | ascii_upcase)=\\(.value)\"' test.json TOTAL=1 FAILED=0 SUCCESSFUL=1 ⑥压缩输出 jq -c '.' test.json 2、访问属性值 ①输出属性的值 $ jq -r '.snapshots[].snapshot' test.json $ jq -r '.snapshots[].snapshot,.snapshots[].end_time' test.json # 如果属性名中有空格，需要加双引号 $ jq -r '.\"with space\"' test.json $ jq -r '\"\\(.\"@timestamp\") \\(.end_time)\"' test.json ②批量访问属性值 $ jq -r '.snapshots[] | [.snapshot,.end_time] test.json 3、操作属性值 ①取值赋予变量 $ cat test.json | jq -r '[.id,.iid] as [$id,$iid] | \"\\($id)|\\($iid)\"' 4、JSON数组的操作 ①遍历访问数组 $ jq -r '.snapshots[]' test.json $ jq -r '.snapshots[] | .snapshot' test.json $ jq -r '.snapshots[].snapshot' test.json ②按索引访问数组 获取snapshot的index $ jq -r '.snapshots[0]' test.json $ jq -r '.snapshots[1].indices[0]' test.json ③数组切片 只取数组指定位置的值 # 从0开始到第一个 $ jq -r '.snapshots[0:1]' test.json # 从头开始到第一个 $ jq -r '.snapshots[:1]' test.json # 倒数一个到最后一个 $ jq -r '.snapshots[-1:]' test.json 5、应用其他环境变量 my_var=\"hello world\" jq -r --arg my_var \"$my_var\" '.foo | \"\\($my_var) \\(.bar)\"' input.json 6、函数操作 ①keys：获取有哪些属性 $ jq -r '.snapshots[] | keys' test.json ②length：获取属性的个数 $ jq -r '.snapshots[] | length' test.json ③min/max：大小的比较 $ jq -r '[.snapshots[].indices[0]] | min' test.json $ jq -r '[.snapshots[].indices[0]] | max' test.json ④ select：筛选过滤 $ jq -r '.snapshots[] | select(.duration_in_millis ⑤select：正则表达式筛选过滤 $ jq -r '.snapshots[] | select(.snapshot|test(\"^BBB.*\") ) | .version_id' test.json ⑥del：删除属性 $ jq -r 'del(.snapshots[].version)' test.json # 删除匹配到的属性 $ jq -r 'del( .snapshots[] | select(.uuid == \"Wp5MBOFWJA\"))' test.json > test-deled.json ⑦map：map属性值进行操作 判断属性值是否存在 $ jq -r '.snapshots | map(has(\"snapshot\"))' test.json 操作数值类型的属性值 $ jq -r '.snapshots | map(.duration_in_millis+2)' test.json ⑧unique：去重属性值 $ jq -r '.snapshots | map(.state) | unique' test.json ⑨重组json结构 $ jq -r '.snapshots | map(.) | .[] | {\"快照名\": .snapshot,\"快照的索引\": .indices}' test.json { \"快照名\": \"AAA-api-frame-2019-02-08-2019.02.24\", \"快照的索引\": [ \"AAA-api-frame-2019-02-08\" ] } { \"快照名\": \"BBB-api-frame-2019-02-09-2019.02.24\", \"快照的索引\": [ \"BBB-api-frame-2019-0209\" ] } ⑩if else 函数判断 $ jq -r ' .snapshots[] | if .snapshot == \"\" then .value |= \"值为空\" else . end | \"\\(.id)= \\(.iid)\" ' 五、操作示例 1、处理 Nginx的 JSON 格式日志 { \"@timestamp\": \"2023-12-07T00:00:18+08:00\", \"app\": \"official\", \"remote_addr\": \"100.94.24.207\", \"referer\": \"\", \"request\": \"GET /health HTTP/1.1\", \"status\": 200, \"bytes\": 13172, \"agent\": \"Go-http-client/1.1\", \"x_forwarded\": \"12.4.18.6, 10.17.8.12\", \"up_addr\": \"127.0.0.1:9000\", \"up_host\": \"\", \"up_resp_time\": 0.124, \"request_time\": 0.123, \"server_name\": \"\" } { \"@timestamp\": \"2023-12-07T00:01:10+08:00\", \"app\": \"official\", \"remote_addr\": \"100.94.24.24\", \"referer\": \"https://www.baidu.com/info\", \"request\": \"POST https://www.baidu.com/info\", \"status\": 200, \"bytes\": 618, \"agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\", \"x_forwarded\": \"10.24.7.21, 192.168.1.2\", \"up_addr\": \"\", \"up_host\": \"\", \"up_resp_time\": 0, \"request_time\": 0, \"server_name\": \"\" } 排除“/health”请求，只显示 app、referer、request、agent、x_forwarded字段，同时以逗号分割x_forwarded字段值，只显示第一个 IP 地址。 jq -r ' select(.request != \"GET /health HTTP/1.1\")| {app: .app ,referer: .referer ,request: .request, agent: .agent ,x_forwarded: (.x_forwarded | split(\",\") | .[0]) }' 输出 { \"app\": \"official\", \"referer\": \"https://www.baidu.com/info\", \"request\": \"POST https://www.baidu.com/info\", \"agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0\", \"x_forwarded\": \"10.24.7.21\" } 参考链接 https://www.baeldung.com/linux/jq-command-json https://www.ibm.com/developerworks/cn/linux/1612_chengg_jq/index.html?ca=drs-&utm_source=tuicool&utm_medium=referral https://stackoverflow.com/questions/56692037/filter-empty-and-or-null-values-with-jq https://justcode.ikeepstudying.com/2018/02/shell%EF%BC%9A%E6%97%A0%E6%AF%94%E5%BC%BA%E5%A4%A7%E7%9A%84shell%E4%B9%8Bjson%E8%A7%A3%E6%9E%90%E5%B7%A5%E5%85%B7jq-linux%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%A7%A3%E6%9E%90json-jq%E8%A7%A3%E6%9E%90-json/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:39:21 "},"origin/yaml-yq.html":{"url":"origin/yaml-yq.html","title":"yaml文本处理工具yq","keywords":"","body":"YAML文本处理工具yq 一、简介 对于k8s工程师，整天接触最多的是k8s资源对象的声明yaml文件。有的时候需要在脚本里面处理它。yq是一个go写的命令行处理工具。 Github：https://github.com/mikefarah/yqs 相关文档：https://mikefarah.gitbook.io/yq/usage/split-into-multiple-files 二、安装 MacOS brew install yq Windows choco install yq Ubuntu and other Linux distros supporting snap packages: snap install yq Go GET GO111MODULE=on go get github.com/mikefarah/yq/v3 Run with Docker Oneshot use: docker run --rm -v \"${PWD}\":/workdir mikefarah/yq yq [flags] FILE... Run commands interactively: docker run --rm -it -v \"${PWD}\":/workdir mikefarah/yq sh It can be useful to have a bash function to avoid typing the whole docker command: yq() { docker run --rm -i -v \"${PWD}\":/workdir mikefarah/yq yq \"$@\" } 三、使用 命令详解 Usage: yq [flags] yq [command] Available Commands: compare yq x [--prettyPrint/-P] dataA.yaml dataB.yaml 'b.e(name==fr*).value' delete yq d [--inplace/-i] [--doc/-d index] sample.yaml 'b.e(name==fred)' help Help about any command merge yq m [--inplace/-i] [--doc/-d index] [--overwrite/-x] [--append/-a] sample.yaml sample2.yaml new yq n [--script/-s script_file] a.b.c newValue prefix yq p [--inplace/-i] [--doc/-d index] sample.yaml a.b.c read yq r [--printMode/-p pv] sample.yaml 'b.e(name==fr*).value' shell-completion Generates shell completion scripts validate yq v sample.yaml write yq w [--inplace/-i] [--script/-s script_file] [--doc/-d index] sample.yaml 'b.e(name==fr*).value' newValue Flags: -C, --colors print with colors -h, --help help for yq -I, --indent int sets indent level for output (default 2) -P, --prettyPrint pretty print -j, --tojson output as json. By default it prints a json document in one line, use the prettyPrint flag to print a formatted doc. -v, --verbose verbose mode -V, --version Print version information and quit Use \"yq [command] --help\" for more information about a command. 原始yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: creationTimestamp: \"2019-06-02T09:30:01Z\" finalizers: - kubernetes.io/pvc-protection name: test namespace: app-test resourceVersion: \"54534390\" selfLink: /api/v1/namespaces/test/persistentvolumeclaims/test uid: c35e6108-46c4-4f71-809d spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: nfs-client volumeMode: Filesystem volumeName: pvc-c35e6108-46c4-4f71-809d status: accessModes: - ReadWriteOnce capacity: storage: 10Gi phase: Bound 示例1 cat test.yaml | yq r - status accessModes: - ReadWriteOnce capacity: storage: 10Gi phase: Bound 示例2 cat test.yaml | yq r - spec.resources requests: storage: 10Gi 示例3 cat test.yaml | yq r - metadata | grep -E 'name:|namespace:' name: test namespace: app-test 原始yaml apiVersion: v1 kind: Secret metadata: name: test namespace: test type: Opaque data: prometheus-additional.yaml: dGVzdAo= 示例1 cat a.yaml | yq r - data.'\"prometheus-additional.yaml\"' | base64 --decode 参考 https://github.com/mikefarah/yq Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-19 21:46:10 "},"origin/linux-xmllint.html":{"url":"origin/linux-xmllint.html","title":"XML文本处理工具xmllint","keywords":"","body":"XML处理工具xmlint 一、简介 xmllint其实是由一个叫libxml2的c语言库函数实现的一个小工具，因此效率比较高，对不同系统的支持度也很好，功能也比较全。他一般属于libxml2-utils这个软件包，因此类似与sudo 的命令就可以安装。 二、安装 APT YUM（Ubuntu/Debian ） apt install -y libxml2-utils yum install -y libxml2-utils Brew（MacOS） brew install xmlstarlet 三、命令详解 1、命令格式 Usage : xmllint [options] XMLfiles ... Parse the XML files and output the result of the parsing --version : display the version of the XML library used --debug : dump a debug tree of the in-memory document --shell : run a navigating shell --debugent : debug the entities defined in the document --copy : used to test the internal copy implementation --recover : output what was parsable on broken XML documents --huge : remove any internal arbitrary parser limits --noent : substitute entity references by their value --noenc : ignore any encoding specified inside the document --noout : don't output the result tree --path 'paths': provide a set of paths for resources --load-trace : print trace of all external entities loaded --nonet : refuse to fetch DTDs or entities over network --nocompact : do not generate compact text nodes --htmlout : output results as HTML --nowrap : do not put HTML doc wrapper --valid : validate the document in addition to std well-formed check --postvalid : do a posteriori validation, i.e after parsing --dtdvalid URL : do a posteriori validation against a given DTD --dtdvalidfpi FPI : same but name the DTD with a Public Identifier --timing : print some timings --output file or -o file: save to a given file --repeat : repeat 100 times, for timing or profiling --insert : ad-hoc test for valid insertions --compress : turn on gzip compression of output --html : use the HTML parser --xmlout : force to use the XML serializer when using --html --nodefdtd : do not default HTML doctype --push : use the push mode of the parser --pushsmall : use the push mode of the parser using tiny increments --memory : parse from memory --maxmem nbbytes : limits memory allocation to nbbytes bytes --nowarning : do not emit warnings from parser/validator --noblanks : drop (ignorable?) blanks spaces --nocdata : replace cdata section with text nodes --format : reformat/reindent the output --encode encoding : output in the given encoding --dropdtd : remove the DOCTYPE of the input docs --pretty STYLE : pretty-print in a particular style 0 Do not pretty print 1 Format the XML content, as --format 2 Add whitespace inside tags, preserving content --c14n : save in W3C canonical format v1.0 (with comments) --c14n11 : save in W3C canonical format v1.1 (with comments) --exc-c14n : save in W3C exclusive canonical format (with comments) --nsclean : remove redundant namespace declarations --testIO : test user I/O support --catalogs : use SGML catalogs from $SGML_CATALOG_FILES otherwise XML Catalogs starting from file:///etc/xml/catalog are activated by default --nocatalogs: deactivate all catalogs --auto : generate a small doc on the fly --xinclude : do XInclude processing --noxincludenode : same but do not generate XInclude nodes --nofixup-base-uris : do not fixup xml:base uris --loaddtd : fetch external DTD --dtdattr : loaddtd + populate the tree with inherited attributes --stream : use the streaming interface to process very large files --walker : create a reader and walk though the resulting doc --pattern pattern_value : test the pattern support --chkregister : verify the node registration code --relaxng schema : do RelaxNG validation against the schema --schema schema : do validation against the WXS schema --schematron schema : do validation against a schematron --sax1: use the old SAX1 interfaces for processing --sax: do not build a tree but work just at the SAX level --oldxml10: use XML-1.0 parsing rules before the 5th edition --xpath expr: evaluate the XPath expression, imply --noout 2、从标准输入输出管道传递处理XML内容 # 从标准输出管道传递 cat settings.xml | xmllint --foramt - # 从标准输入管道传递 xmllint --html --xpath '/html/body/h1[1]' - Dublin EOF 3、输出控制 # 压缩输出 xmllint --noblanks settings.xml > settings-noblank.xml # 格式化输出 xmllint --format settings.xml > settings-format.xml 4、语法schema校验 xmllint --schema settings-1.0.0.xsd settings.xml # xsd文件可以在XML的命名空间中找到下载链接 # 语法校验成功后会输出\"validates\"，不成功会输出\"fails to validate\" # 如果要求只输出校验结果不显示XML内容，可加--noout参数 5、带命名空间的XML查询 例如Maven的settings.xml或pom.xml文件都自带了命名空间。例如： xmllint --xpath \"/*[local-name()='settings']\" settings.xml 四、XML处理 以Maven默认的settings.xml为例 1、查询 xmllint --xpath '//mirror/url' settings.xml 2、 参考 https://www.tutorialspoint.com/unix_commands/xmllint.htm https://linux.die.net/man/1/xmllint https://blog.csdn.net/qmhball/article/details/8955588 https://blog.mythsman.com/post/5d2b5ebf25601931a5f8d885/ https://softwaretester.info/test-xml-command-line-with-xmllint/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/linux-curl.html":{"url":"origin/linux-curl.html","title":"Curl命令详解","keywords":"","body":"一、Curl命令详解 帮助文档：https://curl.se/docs/manpage.html 语法：curl [options] [URL...] 参数： Options: (H) means HTTP/HTTPS only, (F) means FTP only --anyauth 可以使用“任何”身份验证方法 -a, --append FTP/SFTP上传文件时，curl将追加到目标文件，而非覆盖 --basic 使用HTTP基本验证 --cacert FILE 指定CA证书文件(SSL) --capath DIR 指定CA目录 (SSL) -E, --cert CERT[:PASSWD] Client certificate file and password (SSL) --cert-type 指定证书文件类型 (DER/PEM/ENG) (SSL) --ciphers LIST 指定SSL密码 --compressed 响应压缩格式 (deflate/gzip) -K, --config FILE 后接参数文件，参数文件中可以定义HTTP请求的相关的内容（URL、HEAD、DATA） --connect-timeout SECONDS 设置最大请求时间 -C, --continue-at OFFSET 断点续转 -b, --cookie STRING/FILE 设置cookies -c, --cookie-jar FILE 操作结束后把cookie写入到文件中 --create-dirs 建立本地目录层次结构 --crlf 上传时把LF转变成CRLF --crlfile FILE Get a CRL list in PEM format from the given file -d, --data DATA HTTP POST data (H) --data-ascii DATA 以ascii的方式post数据 --data-binary DATA 以二进制的方式post数据 --data-urlencode DATA HTTP POST data url encoded (H) --delegation STRING GSS-API delegation permission --digest 使用HTTP数字身份验证 --disable-eprt 禁止使用EPRT或LPRT --disable-epsv 禁止使用EPSV -D, --dump-header FILE 把header信息写入到文件中 --egd-file FILE 为随机数据(SSL)设置EGD socket路径 --engine ENGINGE 指定加密引擎(SSL). \"--engine list\" for list -f, --fail 连接失败时不显示http错误 -F, --form CONTENT form表单提交 --form-string STRING 模拟http表单提交数据 --ftp-account DATA Account data string (F) --ftp-alternative-to-user COMMAND String to replace \"USER [name]\" (F) --ftp-create-dirs 如果远程目录不存在，创建远程目录 --ftp-method [MULTICWD/NOCWD/SINGLECWD] 控制CWD的使用 --ftp-pasv 使用 PASV/EPSV 代替端口 -P, --ftp-port ADR Use PORT with given address instead of PASV (F) --ftp-skip-pasv-ip Skip the IP address for PASV (F) --ftp-pret Send PRET before PASV (for drftpd) (F) --ftp-ssl-ccc Send CCC after authenticating (F) --ftp-ssl-ccc-mode ACTIVE/PASSIVE Set CCC mode (F) --ftp-ssl-control Require SSL/TLS for ftp login, clear for transfer (F) -G, --get 使用get请求发送 -d参数指定的数据 -g, --globoff 禁用网址序列和范围使用{}和[] -H, --header LINE 增加Head头 -I, --head 只显示文档信息 -h, --help 显示帮助信息 --hostpubmd5 MD5 Hex encoded MD5 string of the host public key. (SSH) -0, --http1.0 强制使用HTTP 1.0协议 --ignore-content-length 忽略的HTTP头信息的长度 -i, --include 输出响应Head头 -k, --insecure 允许curl使用非安全的ssl连接并且传输数据（证书不受信） --interface INTERFACE 使用指定网络接口/地址 -4, --ipv4 解析域名为ipv4地址(域名有多个ip时) -6, --ipv6 解析域名为ipv6地址(域名有多个ip时) -j, --junk-session-cookies 读取文件时忽略session cookie (H) --keepalive-time SECONDS 设置连接的保活时间 --key KEY 私钥文件名(SSL/SSH) --key-type TYPE 私钥文件类型 (DER/PEM/ENG) (SSL) --krb LEVEL 使用指定安全级别的krb (F) --libcurl FILE Dump libcurl equivalent code of this command line --limit-rate RATE 指定最大的传输速率 -l, --list-only 列出ftp目录下的文件名称(F) --local-port RANGE 强制使用本地端口号 -L, --location curl自动重定向（3xx） --location-trusted like --location and send auth to other hosts (H) -M, --manual 显示全手动 --mail-from FROM 指定发信人邮箱(SMTP) --mail-rcpt TO 指定收信人邮箱(SMTP) --mail-auth AUTH Originator address of the original email --max-filesize BYTES 允许下载文件的最大大小 --max-redirs NUM Maximum number of redirects allowed (H) -m, --max-time SECONDS 设置整个操作的允许消耗的最大时间，对于在延时网络下的批量操作有利 --metalink Process given URLs as metalink XML file --negotiate 使用HTTP Negotiate身份验证(H) -n, --netrc 从netrc文件中读取用户名和密码 --netrc-optional 使用 .netrc 或者 URL来覆盖-n --netrc-file FILE 指定.netrc文件 -N, --no-buffer 禁用输出流缓冲区 --no-keepalive 连接不保活 --no-sessionid Disable SSL session-ID reusing (SSL) --noproxy List of hosts which do not use proxy --ntlm 使用 HTTP NTLM 身份验证 -o, --output FILE 将响应数据输出到指定文件，后接文件参数 --pass PASS 私钥密码 (SSL/SSH) --post301 301重定向后不切换至GET请求 (H) --post302 302重定向后不切换至GET请求 (H) --post303 303重定向后不切换至GET请求 (H) -#, --progress-bar 对发送和接收进行简单的进度条展示 --proto PROTOCOLS Enable/disable specified protocols --proto-redir PROTOCOLS Enable/disable specified protocols on redirect -x, --proxy [PROTOCOL://]HOST[:PORT] 设置代理 --proxy-anyauth 选择任一代理身份验证方法 (H) --proxy-basic 在代理上使用基本身份验证 (H) --proxy-digest 在代理上使用数字身份验证 (H) --proxy-negotiate 在代理上使用Negotiate身份验证 (H) --proxy-ntlm 在代理上使用ntlm身份验证 (H) -U, --proxy-user USER[:PASSWORD] 设置代理用户名和密码 --proxy1.0 HOST[:PORT] 使用HTTP/1.0的代理 -p, --proxytunnel Operate through a HTTP proxy tunnel (using CONNECT) --pubkey KEY 公钥文件 (SSH) -Q, --quote CMD 文件传输前，发送命令到服务器 (F/SFTP) --random-file FILE File for reading random data from (SSL) -r, --range RANGE 检索来自HTTP/1.1或FTP服务器字节范围 --raw Do HTTP \"raw\", without any transfer decoding (H) -e, --referer 发送\"Referer Page\"到服务器 -J, --remote-header-name Use the header-provided filename (H) -O, --remote-name 把输出写到文件中，保留远程文件的文件名 --remote-name-all Use the remote file name for all URLs -R, --remote-time 在本地生成文件时，保留远程文件时间 -X, --request COMMAND 指定HTTP请求方法 --resolve HOST:PORT:ADDRESS 强制解析HOST:PORT到某个ADDRESS --retry NUM 传输出现问题时，重试的次数 --retry-delay SECONDS 传输出现问题时，设置重试间隔时间 --retry-max-time SECONDS 传输出现问题时，设置最大重试时间 -S, --show-error 显示错误信息 -s, --silent 静默模式。不输出任何东西 --socks4 HOST[:PORT] 用socks4代理给定主机和端口 --socks4a HOST[:PORT] 用socks4a代理给定主机和端口 --socks5 HOST[:PORT] 用socks5代理给定主机和端口 --socks5-basic socks5代理开启username/password认证 --socks5-gssapi socks5代理开启GSS-API认证 --socks5-hostname HOST[:PORT] SOCKS5 proxy, pass host name to proxy --socks5-gssapi-service NAME SOCKS5 proxy service name for gssapi --socks5-gssapi-nec Compatibility with NEC SOCKS5 server -Y, --speed-limit RATE 如果在speed-time期间，下载比speed-limit这个更慢，则下载废止 -y, --speed-time SECONDS 如果在speed-time期间，下载比speed-limit这个更慢，则下载废止。默认30s --ssl Try SSL/TLS (FTP, IMAP, POP3, SMTP) --ssl-reqd Require SSL/TLS (FTP, IMAP, POP3, SMTP) -2, --sslv2 使用SSLv2的（SSL） -3, --sslv3 使用SSLv3的（SSL） --ssl-allow-beast Allow security flaw to improve interop (SSL) --stderr FILE 指定错误信息输出文件 --tcp-nodelay 使用TCP_NODELAY选项 -t, --telnet-option OPT=VAL Telnet选项设置 --tftp-blksize VALUE 设置TFTP BLKSIZE(必须大于512) -z, --time-cond TIME 传送时间设置 -1, --tlsv1 强制使用TLS version 1.x --tlsv1.0 使用TLSv1.0 (SSL) --tlsv1.1 使用TLSv1.1 (SSL) --tlsv1.2 使用TLSv1.2 (SSL) --trace FILE dump出输入输出数据至文件 --trace-ascii FILE 跟'--trace'一样，但是没有hex输出 --trace-time 跟踪/详细输出时，添加时间戳 --tr-encoding Request compressed transfer encoding (H) -T, --upload-file FILE 上传文件 --url URL URL to work with -B, --use-ascii 使用ASCII文本传输 -u, --user USER[:PASSWORD] 设置服务端用户和密码 --tlsuser USER TLS用户名 --tlspassword STRING TLS密码 --tlsauthtype STRING TLS认证类型(default SRP) --unix-socket FILE Connect through this UNIX domain socket -A, --user-agent STRING 发送用户代理给服务器 (H) -v, --verbose 获取更多输入输出相关的内容，对于debug非常有用 -V, --version 显示当前的curl版本 -w, --write-out FORMAT 指定完成请求以后输出什么信息 --xattr Store metadata in extended file attributes -q If used as the first parameter disables .curlrc 二、实例详解 1、通过-o/-O选项保存下载的文件到指定的文件中 -o：将文件保存为命令行中指定的文件名的文件中 -O：使用URL中默认的文件名保存文件到本地 # 将文件下载到本地并命名为mygettext.html curl -o mygettext.html http://www.gnu.org/software/gettext/manual/gettext.html # 将文件保存到本地并命名为gettext.html curl -O http://www.gnu.org/software/gettext/manual/gettext.html 2、显示response中的Headers或Body -i：显示response header 和 body -I：只显示response header curl -i https://www.baidu.com curl -I https://www.baidu.com 3、同时获取多个文件 curl -O URL1 -O URL2 4、代理的设置 -x：为CURL设置代理 curl -x 192.168.1.2:3128 http://google.com/ 如果curl命令请求不想走系统代理 curl --noproxy http://www.baidu.com 5、允许重定向 -L：允许重定向 curl -L -x 192.168.1.2:3128 http://google.com/ 6、限速 --limit-rate： 对CURL的最大网络使用进行限制 curl --limit-rate 1000B -O http://www.gnu.org/software/gettext/manual/gettext.html 7、添加认证信息 -u: 在访问需要认证的页面时，可通过-u选项提供用户名和密码进行授权 curl -u username:password URL # 通常的做法是在命令行只输入用户名，之后会提示输入密码，这样可以保证在查看历史记录时不会将密码泄露 curl -u username URL 8、获取更多信息 -v 和 -trace：获取更多信息 curl -v -L -x 192.168.1.2:3128 http://google.com/ 9、自定义HTTP请求: POST/PUT/DELETE -X: 可以指定curl发送HTTP请求的方法，例如GET(默认),PUT,POST,DELETE等 -H：添加请求的Header信息 -d/--data: 添加请求的Body curl -XPUT \"http://127.0.0.1:9200/test/test/1\" \\ -H 'Content-Type: application/json' \\ -d ' { \"id\": \"191\", \"prd_id\": \"4\", \"mer_id\": \"1000005\", \"data_status\": \"0\", \"datachange_createtime\": \"1543915326\", \"datachange_lasttime\": \"1543915368\" }' nc -l 8080 & curl -X POST http://localhost:8080 \\ -H \"Content-Type: application/json\" \\ -H \"Connection: close\" \\ --data-binary @测试数据.json nc -l 8080 & curl \"http://localhost:8080\" \\ -H \"Accept: application/json\" \\ -H \"Content-Type: application/json\" \\ --data @- 10、断点续传 -C: 可对大文件使用断点续传功能 curl -C -O http://www.gnu.org/software/gettext/manual/gettext.html 11、模仿浏览器 -A：指定浏览器去访问网站(有些网站需要使用特定的浏览器去访问他们，有些还需要使用某些特定的版本) curl -A \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.0)\" http://google.com/ 12、显示下载进度条 -# ：显示下载进度条 curl -# -O http://www.linux.com/dodo1.JPG 13、伪造referer（盗链） 很多服务器会检查http访问的referer从而来控制访问。比如：你是先访问首页，然后再访问首页中的邮箱页面，这里访问邮箱的referer地址就是访问首页成功后的页面地址，如果服务器发现对邮箱页面访问的referer地址不是首页的地址，就断定那是个盗连了 -e: 设定referer curl -e \"www.linux.com\" http://mail.linux.com # 这样就会让服务器其以为你是从www.linux.com点击某个链接过来的 14、保存与使用Cookie -D: 保存Cookie -b: 使用Cookie # 将网站的cookies信息保存到sugarcookies文件中 curl -D sugarcookies http://localhost/sugarcrm/index.php # 使用上次保存的cookie信息 curl -b sugarcookies http://localhost/sugarcrm/index.php 15、忽略证书不受信问题 -k: 忽略HTTPS证书不受信问题 curl -k https://allinone.okd311.curiouser.com:8443 16、引用环境变量 Shell脚本中使用Curl命令，经常要通过变量替换变量中的值。使用'\"$var\"'进行应用 curl 'https://oapi.dingtalk.com/robot/send?access_token=******' \\ -H 'Content-Type: application/json' \\ -d '{\"msgtype\": \"text\", \"text\": {\"content\": \"消息是: '\"$message\"'\"} }' 17、强制域名解析至指定IP地址 curl --resolve test.test.com:80:127.0.0.1 \"http://test.test.com/\" 18、显示请求的耗时情况 curl 命令提供了 -w 参数，能够帮助分析请求的哪一步耗时比较长，好进一步找到问题的原因。 -w后面指定要显示的内容，后面可通过curl的变量显示某一项耗时。以下为内置 time_namelookup：DNS 域名解析耗时 time_connect：TCP 连接建立的时间，就是三次握手的时间 time_appconnect：SSL/SSH 等上层协议建立连接的时间，比如 connect/handshake 的时间 time_redirect：从开始到最后一个请求事务的时间 time_pretransfer：从请求开始到响应开始传输的时间 time_starttransfer：从请求开始到第一个字节将要传输的时间，这包括time_pretransfer以及服务器计算结果所需的时间。 time_total：这次请求花费的全部时间 url_effective: 最终获取的url地址，尤其是当你指定给curl的地址存在301跳转，且通过-L继续追踪的情形。 time_redirect: 重定向时间，包括到最后一次传输前的几次重定向的DNS解析，连接，预传输，传输时间 num_redirects: 在请求中跳转的次数 ssl_verify_result: ssl认证结果，返回0表示认证成功。 size_request: 请求的大小 $ curl -kls \\ -w \"\\n请求响应状态码 : %{http_code}\\n----------\\n请求信息：\\n 客户端信息: %{local_ip}:%{local_port} \\n 服务器信息: %{remote_ip}:%{remote_port}\\n 发送请求个数: %{num_connects}\\n 请求大小: %{size_request} bytes\\n 重定向URL: %{redirect_url}\\n 响应Header大小: %{size_header} bytes\\n请求耗时统计:\\n DNS解析完成时间: 第%{time_namelookup}秒\\n TCP握手完成时间: 第%{time_connect}秒\\n SSL握手完成时间: 第%{time_appconnect}秒\\n 客户端发送请求开始时间: 第%{time_pretransfer}秒\\n 请求收到第一个字节时间: 第%{time_starttransfer}秒\\n 请求结束时间: 第%{time_total}秒\\n----------\\nTCP和SSL连接耗时: %{time_pretransfer} - %{time_namelookup}\\n服务器处理耗时: %{time_starttransfer} - %{time_pretransfer}\\n响应数据传输耗时: %{time_total} - %{time_starttransfer}\\n共计耗时: %{time_total}秒\\n\" \\ https://google.com 301 Moved 301 Moved The document has moved here. 请求响应状态码 : 301 ---------- 请求信息： 客户端信息: 127.0.0.1:64073 服务器信息: 127.0.0.1:8001 发送请求个数: 1 请求大小: 184 bytes 重定向URL: https://www.google.com/ 响应Header大小: 508 bytes 请求耗时统计: DNS解析完成时间: 第0.000114秒 TCP握手完成时间: 第0.000358秒 SSL握手完成时间: 第0.460882秒 客户端发送请求开始时间: 第0.460956秒 请求收到第一个字节时间: 第0.603045秒 请求结束时间: 第0.603188秒 ---------- TCP和SSL连接耗时: 0.460956 - 0.000114 服务器处理耗时: 0.603045 - 0.460956 响应数据传输耗时: 0.603188 - 0.603045 共计耗时: 0.603188秒 可将-w的输出格式配置写在curl的默认配置文件~/.curlrc中 -kls -w \"\\n请求响应状态码 : %{http_code}\\n----------\\n请求信息：\\n 客户端信息: %{local_ip}:%{local_port} \\n 服务器信息: %{remote_ip}:%{remote_port}\\n 发送请求个数: %{num_connects}\\n 请求大小: %{size_request} bytes\\n 重定向URL: %{redirect_url}\\n 响应Header大小: %{size_header} bytes\\n请求耗时统计:\\n DNS解析完成时间: 第%{time_namelookup}秒\\n TCP握手完成时间: 第%{time_connect}秒\\n SSL握手完成时间: 第%{time_appconnect}秒\\n 客户端发送请求开始时间: 第%{time_pretransfer}秒\\n 请求收到第一个字节时间: 第%{time_starttransfer}秒\\n 请求结束时间: 第%{time_total}秒\\n----------\\nTCP和SSL连接耗时: %{time_pretransfer} - %{time_namelookup}\\n服务器处理耗时: %{time_starttransfer} - %{time_pretransfer}\\n响应数据传输耗时: %{time_total} - %{time_starttransfer}\\n共计耗时: %{time_total}秒\\n\" Chrome控制台时间显示的耗时对应 参考： https://curl.se/docs/manpage.html https://blog.cloudflare.com/a-question-of-timing/ https://cizixs.com/2017/04/11/use-curl-to-analyze-request/ https://blog.csdn.net/weifangan/article/details/80741981 19、新版本Curl不支持旧的TLS版本 创建~/.openssl_allow_tls1.0.cnf openssl_conf = openssl_init [openssl_init] ssl_conf = ssl_sect [ssl_sect] system_default = system_default_sect [system_default_sect] CipherString = DEFAULT@SECLEVEL=1 生效 OPENSSL_CONF=~/.openssl_allow_tls1.0.cnf curl -v https://***** # 或者 export OPENSSL_CONF=~/.openssl_allow_tls1.0.cnf curl -v https://***** unset OPENSSL_CONF 或者直接设置全局的OpenSSL配置文件 /etc/ssl/openssl.cnf openssl_conf = openssl_init [openssl_init] ssl_conf = ssl_sect [ssl_sect] system_default = system_default_sect [system_default_sect] CipherString = DEFAULT@SECLEVEL=1 参考： https://askubuntu.com/questions/1250787/when-i-try-to-curl-a-website-i-get-ssl-error 20、使用SSL证书文件 # 使用client.pem+key.pem curl -k --cert client.pem --key key.pem https://www.xxxx.com # 使用all.pem curl -k --cert all.pem https://www.xxxx.com 21、不显示进度但显示报错 curl -sS https://google.com 22、上传文件 curl --form upload=@localfilename --form press=OK https://google.com 23、显示trace等详细信息 curl --trace-ascii /dev/stdout https://google.com curl --trace-ascii - https://google.com 参考： https://superuser.com/questions/291424/how-do-you-display-post-data-with-curl Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-04-01 21:02:06 "},"origin/linux-rsync.html":{"url":"origin/linux-rsync.html","title":"rsync命令详解","keywords":"","body":"rsync命令详解 一、简介 二、命令参数 1、命令参数 rsync [参数] [源文件] [目标文件] 参数 含义 -v, --verbose 详细模式输出 -q, --quiet 精简输出模式 -c, --checksum 打开校验开关，强制对文件传输进行校验 -a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD -r, --recursive 对子目录以递归模式处理 -R, --relative 使用相对路径信息 -b, --backup 创建备份，也就是对于目的已经存在有同样的文件名时，将老的文件重新命名为~filename。可以使用--suffix选项来指定不同的备份文件前缀。 --backup-dir 将备份文件(如~filename)存放在在目录下。 -suffix=SUFFIX 定义备份文件前缀 -u, --update 仅仅进行更新，也就是跳过所有已经存在于DST，并且文件时间晚于要备份的文件。(不覆盖更新的文件) -l, --links 保留软链结 -L, --copy-links 想对待常规文件一样处理软链结 --copy-unsafe-links 仅仅拷贝指向SRC路径目录树以外的链结 --safe-links 忽略指向SRC路径目录树以外的链结 -H, --hard-links 保留硬链结 -p, --perms 保持文件权限 -o, --owner 保持文件属主信息 -g, --group 保持文件属组信息 -D, --devices 保持设备文件信息 -t, --times 保持文件时间信息 -S, --sparse 对稀疏文件进行特殊处理以节省DST的空间 -n, --dry-run 现实哪些文件将被传输 -W, --whole-file 拷贝文件，不进行增量检测 -x, --one-file-system 不要跨越文件系统边界 -B, --block-size=SIZE 检验算法使用的块尺寸，默认是700字节 -e, --rsh=COMMAND 指定使用rsh、ssh方式进行数据同步 --rsync-path=PATH 指定远程服务器上的rsync命令所在路径信息 -C, --cvs-exclude 使用和CVS一样的方法自动忽略文件，用来排除那些不希望传输的文件 --existing 仅仅更新那些已经存在于DST的文件，而不备份那些新创建的文件 --delete 删除那些DST中SRC没有的文件 --delete-excluded 同样删除接收端那些被该选项指定排除的文件 --delete-after 传输结束以后再删除 --ignore-errors 及时出现IO错误也进行删除 --max-delete=NUM 最多删除NUM个文件 --partial 保留那些因故没有完全传输的文件，以是加快随后的再次传输 --force 强制删除目录，即使不为空 --numeric-ids 不将数字的用户和组ID匹配为用户名和组名 --timeout=TIME IP超时时间，单位为秒 -I, --ignore-times 不跳过那些有同样的时间和长度的文件 --size-only 当决定是否要备份文件时，仅仅察看文件大小而不考虑文件时间 --modify-window=NUM 决定文件是否时间相同时使用的时间戳窗口，默认为0 -T --temp-dir=DIR 在DIR中创建临时文件 --compare-dest=DIR 同样比较DIR中的文件来决定是否需要备份 -P 等同于 --partial --progress 显示备份过程 -z, --compress 对备份的文件在传输时进行压缩处理 --exclude=PATTERN 指定排除不需要传输的文件模式 --include=PATTERN 指定不排除而需要传输的文件模式 --exclude-from=FILE 排除FILE中指定模式的文件 --include-from=FILE 不排除FILE指定模式匹配的文件 --version 打印版本信息 --address 绑定到特定的地址 --config=FILE 指定其他的配置文件，不使用默认的rsyncd. conf文件 --port=PORT 指定其他的rsync服务端口 --blocking-io 对远程shell使用阻塞IO -stats 给出某些文件的传输状态 --progress 在传输时现实传输过程 --log-format=formAT 指定日志文件格式 --password-file=FILE 从FILE中得到密码 --bwlimit=KBPS 限制I/O带宽，KBytes per second -h, --help 显示帮助信息 2、同步模式 ①本地模式 rsync [参数] [源文件] [目标文件] ②通过远程shell访问模式 拉取(pull) ：rsync [参数] 用户@主机:源文件 [目标文件] rsync -av --progress -e 'ssh -p 1022' root@192.168.1.1::www /databack 推送(push) ：rsync [参数] [源文件] 用户@主机:源文件 rsync -avzP /data/test root@192.168.1.1:/data/test ③守护进程模式 拉取 rsync [参数] 用户@主机::源文件 [目标文件] rsync [参数] rsync://用户@主机:端口/源文件 [目标文件] 推送 rsync [参数] [源文件] 用户@主机:源文件 rsync [参数] [源文件] rsync://用户@主机:端口/源文件 [目标文件] 三、实例详解 1、同步本地文件到远程服务器 2、 使用SCP模式同步时 https://superuser.com/questions/138893/scp-to-remote-server-with-sudo Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-12-07 10:24:43 "},"origin/linux-lvm.html":{"url":"origin/linux-lvm.html","title":"LVM原理及使用","keywords":"","body":"LVM原理及使用 一、原理简介 LVM是 Logical Volume Manager(逻辑卷管理)的简写，它由Heinz Mauelshagen在Linux 2.4内核上实现。 LVM将一个或多个硬盘的分区在逻辑上集合，相当于一个大硬盘来使用，当硬盘的空间不够使用的时候，可以继续将其它的硬盘的分区加入其中，这样可以实现磁盘空间的动态管理，相对于普通的磁盘分区有很大的灵活性。 与传统的磁盘与分区相比，LVM为计算机提供了更高层次的磁盘存储。它使系统管理员可以更方便的为应用与用户分配存储空间。在LVM管理下的存储卷可以按需要随时改变大小与移除(可能需对文件系统工具进行升级)。LVM也允许按用户组对存储卷进行管理，允许管理员用更直观的名称(如\"sales'、'development')代替物理磁盘名(如'sda'、'sdb')来标识存储卷 LVM功能实际是通过内核中的dm模块（device mapper）实现，它将一个或多个底层块设备组织成一个逻辑设备的模块，在/dev/目录下以dm-#形式展现 只要是块设备都可以用于创建LVM2。注意分区时ID号要是8e 物理存储介质（The physical media）：指系统的存储设备--硬盘，如：/dev/hda1、/dev/sda等等，是存储系统最低层的存储单元 物理卷PV（physical volume）：物理卷就是指硬盘分区或从逻辑上与磁盘分区具有同样功能的设备(如RAID)，是LVM的基本存储逻辑块，但和基本的物理存储介质（如分区、磁盘等）比较，却包含有与LVM相关的管理参数 卷组VG（volume group）：在较低的逻辑层从多个PV中抽象出来的卷组，由一个或多个物理卷组成 PE（physical extend）：每一个物理卷被划分为称为PE(Physical Extents)的基本单元，具有唯一编号的PE是可以被LVM寻址的最小单元。PE的大小是可配置的，默认为4MB 逻辑卷LV（logical volume）：由多个LV“块”组成可供挂载使用的设备文件 二、使用步骤 1、安装相关软件包 yum install -y lvm2 2、创建PV pvcreate /dev/sdc pvdisplay \"/dev/sdc\" is a new physical volume of \"100.00 GiB\" --- NEW Physical volume --- PV Name /dev/sdc VG Name PV Size 100.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID KiXHSv-PbKj-kOiM-yXhN-ntiw-ULpt-JhvgnB 3、创建VG # vgcreate命令用法 vgcreate -s [N[mgt]] VG名称 PV名称 # -s 指定VG中的PE大小，单位：MB,GB,TB vgcreate -s 16M docker /dev/sdc 4、查看VG vgdisplay --- Volume group --- VG Name docker System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 4 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 2 Act PV 2 VG Size 199.99 GiB PE Size 4.00 MiB Total PE 51198 Alloc PE / Size 38399 / 5、创建LV #lvcreate命令参数 lvcreate -l PE个数 -n LV名称 VG名称 ​ lvcreate -l 6399 -n docker-lib docker 6、查看LV容量 lvdisplay --- Logical volume --- LV Path /dev/docker/docker LV Name docker VG Name docker LV UUID hlbSQl-RfGK-PpUZ-u7Vx-5t3X-WOX7-dxLomX LV Write Access read/write LV Creation host, time node7.test.openshift.com, 2018-09-07 16:11:37 +0800 LV Status available # open 1 LV Size 7、格式化LV mkfs.ext3 LV_Name mkfs.ext4 LV_Name mkfs.xfs LV_Name 8、挂载LV echo \"LV_Name 挂载目录点 文件系统格式 defaults 0 0\" >> /etc/fstab mount -a 三、扩容VG和LV VG已无PE可用 新增硬盘 在线扩容（不卸载,不重启主机） 1、创建PV pvcreate /dev/sdd 2、将PV添加到VG中。之后可看PE数量增加 vgextend VG_Name /dev/sdd 3、扩容LV(之后可看LV容量增加) lvresize -l +6399 LV_Path # 或者 lvresize -L +50G LV_Path 4、检查并修复文件系统 e2fsck -f LV_Name 5、将扩容后的LV完整地扩充到文件系统中 # LV文件系统是ext4时 resize2fs LV_Path # LV文件系统是xfs时 xfs_growfs LV_Path 四、挂载已创建的LVM磁盘 # 查看已有硬盘 fdisk -l # 查看lvm磁盘的lv lvdisplay # 查看lv是否激活 lvscan ACTIVE '/dev/data/data' [ 参考 https://wiki.archlinux.org/index.php/LVM_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87) Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-06-23 11:20:30 "},"origin/linux-交换分区.html":{"url":"origin/linux-交换分区.html","title":"Linux交换分区","keywords":"","body":"交换分区概念及管理 一、什么是交换分区呢？ Linux divides its physical RAM (random access memory) into chucks of memory called pages. Swapping is the process whereby a page of memory is copied to the preconfigured space on the hard disk, called swap space, to free up that page of memory. The combined sizes of the physical memory and the swap space is the amount of virtual memory available. Swap space in Linux is used when the amount of physical memory (RAM) is full. If the system needs more memory resources and the RAM is full, inactive pages in memory are moved to the swap space. While swap space can help machines with a small amount of RAM, it should not be considered a replacement for more RAM. Swap space is located on hard drives, which have a slower access time than physical memory.Swap space can be a dedicated swap partition (recommended), a swap file, or a combination of swap partitions and swap files. Linux内核为了提高读写效率与速度，会将文件在内存中进行缓存，这部分内存就是Cache Memory(缓存内存)。即使你的程序运行结束后，Cache Memory也不会自动释放。这就会导致你在Linux系统中程序频繁读写文件后，你会发现可用物理内存变少。当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间被临时保存到Swap空间中，等到那些程序要运行时，再从Swap分区中恢复保存的数据到内存中。这样，系统总是在物理内存不够时，才进行Swap交换。 二、Swap交换分区对性能的影响 我们知道Linux可以使用文件系统中的一个常规文件或独立分区作为Swap交换空间，相对而言，交换分区要快一些。但是和RAM比较而言，Swap交换分区的性能依然比不上物理内存，目前的服务器上RAM基本上都相当充足，那么是否可以考虑抛弃Swap交换分区，是否不需要保留Swap交换分区呢？这个其实是我的疑问之一。在这篇What Is a Linux SWAP Partition, And What Does It Do?博客中，作者给出了swap交换空间的优劣 Advantages: Provides overflow space when your memory fills up completely Can move rarely-needed items away from your high-speed memory Allows you to hibernate Disadvantages: Takes up space on your hard drive as SWAP partitions do not resize dynamically Can increase wear and tear to your hard drive Does not necessarily improve performance (see below) 其实保留swap分区概括起来可以从下面来看： 首先，当物理内存不足以支撑系统和应用程序（进程）的运作时，这个Swap交换分区可以用作临时存放使用率不高的内存分页，把腾出的内存交给急需的应用程序（进程）使用。有点类似机房的UPS系统，虽然正常情况下不需要使用，但是异常情况下， Swap交换分区还是会发挥其关键作用。 其次，即使你的服务器拥有足够多的物理内存，也有一些程序会在它们初始化时残留的极少再用到的内存分页内容转移到 swap 空间，以此让出物理内存空间。对于有发生内存泄漏几率的应用程序（进程），Swap交换分区更是重要，因为谁也不想看到由于物理内存不足导致系统崩溃。 最后，现在很多个人用户在使用Linux，有些甚至是PC的虚拟机上跑Linux系统，此时可能常用到休眠（Hibernate），这种情况下也是推荐划分Swap交换分区的。 其实少量使用Swap交换空间是不会影响性能，只有当RAM资源出现瓶颈或者内存泄露，进程异常时导致频繁、大量使用交换分区才会导致严重性能问题。另外使用Swap交换分区频繁，还会引起kswapd0进程（虚拟内存管理中, 负责换页的）耗用大量CPU资源，导致CPU飙升。 关于Swap分区的优劣以及是否应该舍弃，我有点恶趣味的想到了这个事情：人身上的两个器官，阑尾和扁桃体。切除阑尾或扁桃体是否也是争论不休。另外，其实不要Swap交换分区，Linux也是可以正常运行的（有人提及过这个问题） 三、Swap分区大小设置建议 系统的Swap分区大小设置多大才是最优呢？ 关于这个问题，应该说只能有一个统一的参考标准，具体还应该根据系统实际情况和内存的负荷综合考虑，像ORACLE的官方文档就推荐如下设置，这个是根据物理内存来做参考的。 RAM Swap Space Up to 512 MB 2 times the size of RAM Between 1024 MB and 2048 MB 1.5 times the size of RAM Between 2049 MB and 8192 MB Equal to the size of RAM More than 8192 MB 0.75 times the size of RAM 另外在其它博客中看到下面一个推荐设置，当然我不清楚其怎么得到这个标准的。是否合理也无从考证。可以作为一个参考。 4G以内的物理内存，SWAP 设置为内存的2倍。 4-8G的物理内存，SWAP 等于内存大小。 8-64G 的物理内存，SWAP 设置为8G。 64-256G物理内存，SWAP 设置为16G。 四、什么时候使用Swap分区空间? 系统在什么情况或条件下才会使用Swap分区的空间呢？ 其实是Linux通过一个参数swappiness来控制的。当然还涉及到复杂的算法。 这个参数值可为 0-100，控制系统 swap 的使用程度。高数值可优先系统性能，在进程不活跃时主动将其转换出物理内存。低数值可优先互动性并尽量避免将进程转换处物理内存，并降低反应延迟。默认值为 60。注意：这个只是一个权值，不是一个百分比值，涉及到系统内核复杂的算法。关于该参数请参考这篇文章[转载]调整虚拟内存，在此不做过多赘述。下面是关于swappiness的相关资料 The Linux 2.6 kernel added a new kernel parameter called swappiness to let administrators tweak the way Linux swaps. It is a number from 0 to 100. In essence, higher values lead to more pages being swapped, and lower values lead to more applications being kept in memory, even if they are idle. Kernel maintainer Andrew Morton has said that he runs his desktop machines with a swappiness of 100, stating that \"My point is that decreasing the tendency of the kernel to swap stuff out is wrong. You really don't want hundreds of megabytes of BloatyApp's untouched memory floating about in the machine. Get it out on the disk, use the memory for something useful.\" Swappiness is a property of the Linux kernel that changes the balance between swapping out runtime memory, as opposed to dropping pages from the system page cache. 有两种临时修改swappiness参数的方法，系统重启后失效 方法1： echo 10 > /proc/sys/vm/swappiness 方法2: sysctl vm.swappiness=10 永久修改swappiness参数的方法就是在配置文件/etc/sysctl.conf里面修改vm.swappiness的值，然后重启系统 echo 'vm.swappiness=10' >>/etc/sysctl.conf 如果有人会问是否物理内存使用到某个百分比后才会使用Swap交换空间，可以明确的告诉你不是这样一个算法，如下截图所示，及时物理内存只剩下8M了，但是依然没有使用Swap交换空间，而另外一个例子，物理内存还剩下19G，居然用了一点点Swap交换空间。 五、交换分区管理 1、查看Swap分区大小 $ free -mh total used free shared buff/cache available Mem: 31G 21G 256M 8.4M 9.6G 9.4G Swap: 4.0G 0B 4.0G $ swapon -s 或者 cat /proc/swaps Filename Type Size Used Priority /dev/vdb partition 4194300 0 -1 2、释放Swap分区空间 swapon -s 3、使用swapoff关闭交换分区 swapoff /dev/vdb 4、使用swapon启用交换分区 swapon /dev/vdb Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/linux-硬盘读写性能测试.html":{"url":"origin/linux-硬盘读写性能测试.html","title":"硬盘工具命令：dd","keywords":"","body":"DD硬盘命令详解 Linux下可使用dd命令来测试硬盘读写速度 一、语法简介 dd if=path/to/input_file of=/path/to/output_file bs=block_size count=number_of_blocks 参数 if=file 　　　　　　　　　　　　　　　　输入文件名，缺省为标准输入 of=file 　　　　　　　　　　　　　　　　输出文件名，缺省为标准输出 ibs=bytes 　　　　　　　　　　　　　　　一次读入 bytes 个字节(即一个块大小为 bytes 个字节) obs=bytes 　　　　　　　　　　　　　　　一次写 bytes 个字节(即一个块大小为 bytes 个字节) bs=bytes 　　　　　　　　　　　　　　　 同时设置读写块的大小为 bytes ，可代替 ibs 和 obs cbs=bytes 　　　　　　　　　　　　　　　一次转换 bytes 个字节，即转换缓冲区大小 skip=blocks 　　　　　　　　　　　　　 从输入文件开头跳过 blocks 个块后再开始复制 seek=blocks 　　　　　　　　　　 从输出文件开头跳过 blocks 个块后再开始复制(通常只有当输出文件是磁盘或磁带时才有效) count=blocks 　　　　　　　　　　　　　仅拷贝 blocks 个块，块大小等于 ibs 指定的字节数 conv=conversion[,conversion...] 用指定的参数转换文件。 iflag=FLAGS　　　　　　　　　　　　　　指定读的方式FLAGS，参见“FLAGS参数说明” oflag=FLAGS　　　　　　　　　　　　　　指定写的方式FLAGS，参见“FLAGS参数说明” #conv 转换参数： ascii 　　　　　　　　　　　　　　　　　转换 EBCDIC 为 ASCII ebcdic 　　　　　　　　　　　　 　 转换 ASCII 为 EBCDIC ibm 　　　　　　　　　　　　　　　　　　转换 ASCII 为 alternate EBCDIC block 　　　　　　　　　　　　　　　　 把每一行转换为长度为 cbs 的记录，不足部分用空格填充 unblock 　　　　　　　　　　　　　　　 使每一行的长度都为 cbs ，不足部分用空格填充 lcase 　　　　　　　　　　　　　　　　 把大写字符转换为小写字符 ucase 　　　　　　　　　　　　　　　　 把小写字符转换为大写字符 swab 　　　　　　　　　　　　　　　　 交换输入的每对字节 noerror 　　　　　　　　　　　　　　　 出错时不停止 notrunc 　　　　　　　　　　　　　　　 不截短输出文件。 sync 　　　　　　　　　　　　　　　　　 把每个输入块填充到ibs个字节，不足部分用空(NUL)字符补齐 FLAGS 参数说明：​ append -append mode (makes sense only for output; conv=notrunc sug-gested) direct　　　　　　　　　　　　　　　 读写数据采用直接IO方式 directory　　　　　　　　　　　　　　读写失败除非是directory dsync　　　　　　　　　　　　　　　　 读写数据采用同步IO sync　　　　　　　　　　　　　　　　　同上，但是针对是元数据 fullblock　　　　　　　　　　　　　　堆积满block（accumulate full blocks of input ）(iflag only) nonblock　　　　　　　　　　　　　　 读写数据采用非阻塞IO方式 noatime　　　　　　　　　　　　　　　 读写数据不更新访问时间 二、DD磁盘读写测试 1、相关参数 time有计时作用，dd用于复制，从if读出，写到of if=/dev/zero（产生字符）不产生IO，因此可以用来测试纯写速度 同理of=/dev/null（回收站、无底洞）不产生IO，可以用来测试纯读速度 将/tmp/test拷贝到/var则同时测试了读写速度 bs是每次读或写的大小，即一个块的大小，count是读写块的数量 当写入到驱动盘的时候，我们简单的从无穷无用字节的源 /dev/zero 读取，当从驱动盘读取的时候，我们读取的是刚才的文件，并把输出结果发送到无用的 /dev/null。在整个操作过程中， DD 命令会跟踪数据传输的速度并且报告出结果。 2、测试磁盘写能力 time dd if=/dev/zero of=/testw.dbf bs=4k count=100000 因为/dev//zero是一个伪设备，它只产生空字符流，对它不会产生IO，所以，IO都会集中在of文件中，of文件只用于写，所以这个命令相当于测试磁盘的写能力。命令结尾添加oflag=direct将跳过内存缓存，添加oflag=sync将跳过hdd缓存。 3、测试磁盘读能力 time dd if=/dev/sdb of=/dev/null bs=4k 因为/dev/sdb是一个物理分区，对它的读取会产生IO，/dev/null是伪设备，相当于黑洞，of到该设备不会产生IO，所以，这个命令的IO只发生在/dev/sdb上，也相当于测试磁盘的读能力。（Ctrl+c终止测试） 4、测试同时读写能力 time dd if=/dev/sdb of=/testrw.dbf bs=4k 在这个命令下，一个是物理分区，一个是实际的文件，对它们的读写都会产生IO（对/dev/sdb是读，对/testrw.dbf是写），假设它们都在一个磁盘中，这个命令就相当于测试磁盘的同时读写能力。 5、测试纯写入性能 time dd if=/dev/zero of=test bs=8k count=10000 oflag=direct 6、测试纯读取性能 dd if=test of=/dev/null bs=8k count=10000 iflag=direct 注意：dd 只能提供一个大概的测试结果，而且是连续 I/O 而不是随机 I/O，理论上文件规模越大，测试结果越准确。 同时，iflag/oflag 提供 direct 模式，direct 模式是把写入请求直接封装成 I/O 指令发到磁盘，非 direct 模式只是把数据写入到系统缓存就认为 I/O 成功，并由操作系统决定缓存中的数据什么时候被写入磁盘。 三、DD刻录功能相关 1、复制磁盘所有扇区数据到文件 sudo dd if=/dev/sdX of=/path/to/output/file bs=4M 2、刻录磁盘文件到U盘 sudo dd if=netboot.xyz.img of=/dev/rdiskN bs=1m # 如果遇到Invalid number '1m', 说明用的是GNU的dd，那参数 bs=1M 四、磁盘读写测试结果 磁盘数据 品牌 类型 容量 主控 存储芯片 缓存 接口 接入方式 接入系统 Kingston SD 256GB USB 3.2 USB 3.0 AsusMerlin Coolfish SSD 128GB USB 2.0 AsusMerlin Surma SSD 128GB PCIE Synology TOSHIBA HDD 1TB SATA Synology 缓存写入性能测试 time dd if=/dev/zero of=test bs=8k count=100000 写入磁盘 写入大小 总耗时 平均速率 系统耗时 Kingston 819200000 bytes(819 MB） 4.7764 s 172 MB/s 1.39s Coolfish 819200000 bytes(819 MB） 16.6004 s 49.3 MB/s 1.05s Surma 819200000 bytes(819 MB） 1.57491 s 520 MB/s 1.49s TOSHIBA 819200000 bytes(819 MB） 1.72799 s 474 MB/s 1.47s 直接写入性能测试 time dd if=/dev/zero of=test bs=8k count=100000 oflag=direct 写入磁盘 写入大小 总耗时 平均速率 系统耗时 Kingston 819200000 bytes(819 MB） 9.4353 s 86.8 MB/s 3.45s Coolfish 819200000 bytes(819 MB） 29.6246 s 27.7 MB/s 1.58s Surma 819200000 bytes(819 MB） 13.4049 s 61.1 MB/s 6.88s TOSHIBA 819200000 bytes(819 MB） 41.5395 s 19.7 MB/s 14.63s 参考链接 http://www.360doc.com/content/15/0906/17/8737500_497292503.shtml Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:41:59 "},"origin/vim-小技巧.html":{"url":"origin/vim-小技巧.html","title":"Vim小技巧","keywords":"","body":"vim常用配置 bash -c 'cat > ~/.vimrc 命令行模式 功能 命令 描述 水平分屏 ：sp 水平分屏打开另一个文件 垂直分屏 ：vsp 垂直分屏打开另一个文件 多屏间切换 Ctrl+W+W 多屏退出 ：qall 排序 ：sort 去除重复行 ：sort u 移动到文本开头 gg 移动到文本结尾 G 移动到行首 0 即行首有空格的情况，会移动到空格之前 移动到行末 $ 即行末有空格的情况，会移动到空格之后 向下翻页 Ctrl+f 向上翻页 Ctrl+b 以word为单位移动 单词数+W/b,B/b,E/e 2w表示向后移动2个word； 2b表示向前移动2个word； 2e表示向后移动2个word(但是会移动到word字符之后) 如果想忽略标点符号的word，就用大写 W B E 行内查找字符 f+字符 向后移动到某字符 F+字符 向前移动到字符a处 全文查找当前光标处的单词 * 向后查找 #+字符 从文件开头到文件尾开始查找匹配字符 ?+字符 从文件尾倒着到文件开头开始查找匹配字符 光标右边最近数字进行自加 Ctrl+A 光标右边最近数字进行自减 Ctrl+X 删除文本中的空行 ：g/^$/d 注释文本行 v进入视图模式，选择要注释的行，然后Ctrl+v进入块选择模式，然后大写I插入#或者/，再ESC退出 ：起始行号,结束行号s/^注释符//g 在10 - 20行添加 // 注释 :10,20s#^#//#g 在10 - 20行添加 # 注释 :10,20s/^/#/g 快速搜索光标所在单词 Shift+* 显示匹配个数 :%s/xxx//gn 插入模式 功能 命令 描述 删除光标前面的单词 Ctrl+W 删除光标前面的一行 Ctrl+U 在光标前面插入一个tab Ctrl+I 将光标以下所有内容向上提 Ctrl+H 将光标以下所有内容向下提 Ctrl+J/M 向下联想 Ctrl+N 向上联想 Ctrl+P 示例 1、行首或行尾加字符 #每行行首加“#” :%s/^/#/g #每行行尾加\" ;\\\" :%s/$/ ;\\\\/g #第二行到第十五行的行首添加“==” :2,15 s/^/==/g #第二行到文本末行的行首添加“==” :2,$ s/^/==/g #第二行到文本首行的行首添加“==” :2,1 s/^/==/g 2、将文本中相同数字进行自增 原始文本 docker save -o 1.tar docker.io/openshiftistio/origin-ansible:0.7.1 ;\\ docker save -o 1.tar docker.io/openshiftistio/origin-ansible:0.8.0 ;\\ docker save -o 1.tar docker.io/skydive/skydive:latest \\ :g/1.tar/ s//\\=line('.').'.tar'/ 效果文本 docker save -o 1.tar docker.io/openshiftistio/origin-ansible:0.7.1 ;\\ docker save -o 2.tar docker.io/openshiftistio/origin-ansible:0.8.0 ;\\ docker save -o 3.tar docker.io/skydive/skydive:latest ;\\ 3、去除文本中的换行符^M Windows下保存的文本文件，上传到Linux/Unix下后总会在末尾多了一个换行符^M，导致一些xml、ini、sh等文件读取错误 进入命令模式 %s/^M//g (注意，^M = Ctrl v + Ctrl m，而不是手动输入^M) # ^M 表示清除成功 4、设置粘贴时换行问题 有些版本vim的默认配置下，在插入模式下粘贴文字会换行加tab缩进，例如 line line line 在命令行中设置或在配置文件中设置 :set paste 参考：https://stackoverflow.com/questions/2514445/turning-off-auto-indent-when-pasting-text-into-vim/2514520 5、空格与 tab的替换 TAB替换为空格： :set ts=4 :set expandtab :%retab! 空格替换为TAB： :set ts=4 :set noexpandtab :%retab! Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-07-21 09:41:54 "},"origin/linux-yum.html":{"url":"origin/linux-yum.html","title":"Yum-RPM包管理","keywords":"","body":"YUM详解 一、Overviews Yum（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。 Yum的关键之处是要有可靠的repository，顾名思义这就是软件的仓库，它可以是http或者ftp站点，也可以是本地的软件池，但是必须包含rpm的header，rmp的header包括了rmp的各种信息，包括描述、功能、提供的文件、依赖性等，正是收集了这些信息，才能自动化的完成余下的任务。 repo文件是yum源（软件仓库）的配置文件，通常一个repo文件定义了一个或者多个软件仓库的细节内容 RPM包的名称规则示例：ttpd-manual- 2.0.40-21.i386.rpm，ttp-manual是软件包的名称，2是主版本号；0是次版本号；40是修正号；21是编译的次数；i386是适合的平台 二、YUM命令详解 yum的命令形式一般是如下：yum [选项] [参数] [package ...] 选项 -h, --help 显示此帮助消息并退出 -t, --tolerant 忽略错误 -C, --cacheonly 完全从系统缓存运行，不升级缓存 -c [config file], --config=[config file] 配置文件路径 -R [minutes], --randomwait=[minutes] 命令最长等待时间 -d [debug level], --debuglevel=[debug level] 调试输出级别 --showduplicates 在 list/search 命令下，显示源里重复的条目 -e [error level], --errorlevel=[error level] 错误输出级别 --rpmverbosity=[debug level name] RPM 调试输出级别 -q, --quiet 静默执行 -v, --verbose 详尽的操作过程 -y, --assumeyes 回答全部问题为是 --assumeno 回答全部问题为否 --version 显示 Yum 版本然后退出 --installroot=[path] 设置安装根目录 --enablerepo=[repo] 启用一个或多个软件源(支持通配符) --disablerepo=[repo] 禁用一个或多个软件源(支持通配符) -x [package], --exclude=[package] 采用全名或通配符排除软件包 --disableexcludes=[repo] 禁止从主配置，从源或者从任何位置排除 --disableincludes=[repo] disable includepkgs for a repo or for everything --obsoletes 更新时处理软件包取代关系 --noplugins 禁用 Yum 插件 --nogpgcheck 禁用 GPG 签名检查 --disableplugin=[plugin] 禁用指定名称的插件 --enableplugin=[plugin] 启用指定名称的插件 --skip-broken 忽略存在依赖关系问题的软件包 --color=COLOR 配置是否使用颜色 --releasever=RELEASEVER 在 yum 配置和 repo 文件里设置 $releasever 的值 --downloadonly 仅下载而不更新 --downloaddir=DLDIR 指定一个其他文件夹用于保存软件包 --setopt=SETOPTS 设置任意配置和源选项 --bugfix Include bugfix relevant packages, in updates --security Include security relevant packages, in updates --advisory=ADVS, --advisories=ADVS Include packages needed to fix the given advisory, in updates --bzs=BZS Include packages needed to fix the given BZ, in updates --cves=CVES Include packages needed to fix the given CVE, in updates --sec-severity=SEVS, --secseverity=SEVS Include security relevant packages matching the severity, in updates 参数 check 检查 RPM 数据库问题 check-update 检查是否有可用的软件包更新 clean 删除缓存数据 deplist 列出软件包的依赖关系 distribution-synchronization 已同步软件包到最新可用版本 downgrade 降级软件包 erase 从系统中移除一个或多个软件包 fs Acts on the filesystem data of the host, mainly for removing docs/lanuages for minimal hosts. fssnapshot Creates filesystem snapshots, or lists/deletes current snapshots. groups 显示或使用、组信息 help 显示用法提示 history 显示或使用事务历史 info 显示关于软件包或组的详细信息 install 向系统中安装一个或多个软件包 langavailable Check available languages langinfo List languages information langinstall Install appropriate language packs for a language langlist List installed languages langremove Remove installed language packs for a language list 列出一个或一组软件包 load-transaction 从文件名中加载一个已存事务 makecache 创建元数据缓存 provides 查找提供指定内容的软件包 reinstall 覆盖安装软件包 repo-pkgs 将一个源当作一个软件包组，这样我们就可以一次性安装/移除全部软件包。 repolist 显示已配置的源 search 在软件包详细信息中搜索指定字符串 shell 运行交互式的 yum shell swap Simple way to swap packages, instead of using shell update 更新系统中的一个或多个软件包 update-minimal Works like upgrade, but goes to the 'newest' package match which fixes a problem that affects your system updateinfo Acts on repository update information upgrade 更新软件包同时考虑软件包取代关系 version 显示机器和/或可用的源版本。 常用命令 清除缓存 yum clean [headers, packages, metadata, dbcache, plugins, expire-cache, rpmdb, all] headers--清除缓存目录(/var/cache/yum)下的 headers packages--清除缓存目录(/var/cache/yum)下的软件包 all--清除所有缓存 yum update与yum upgrade的区别 yum update 只更新软件，不更新内核 yum upgrade 升级所有包，不改变软件设置和系统设置，系统版本升级，内核不改变 三、repo文件 [serverid] #serverid是用于区别各个不同的repository，必须有一个独一无二的名称。若重复了，是前面覆盖后面--还是反过来呢？？？用enabled 测试是后面覆盖前面 name=Some name for this server #name，是对repository的描述，支持像$releasever $basearch这样的变量; name=Fedora Core $releasever - $basearch - Released Updates baseurl=url://path/to/repository/ # 1. 格式: baseurl=url://server1/path/to/repository/, url支持的协议有 http:// ftp:// file://三种。 # 2. baseurl后可以跟多个url，你可以自己改为速度比较快的镜像站，但#baseurl只能有一个 # 3. 其中url指向的目录必须是这个repository header目录的上一级，它也支持$releasever $basearch这样的变量。 gpgcheck=1 exclude=gaim failovermethod=priority #failovermethode有两个选项roundrobin和priority，意思分别是有多个url可供选择时，yum选择的次序，roundrobin是随机选择，如果连接失 败则使用下一个，依次循环，priority则根据url的次序从第一个开始。如果不指明，默认是roundrobin。 enabled=[1 or 0] # 1. 当某个软件仓库被配置成 enabled=0 时，yum 在安装或升级软件包时不会将该仓库做为软件包提供源。使用这个选项，可以启用或禁用软件仓库。 # 2. 通过 yum 的 --enablerepo=[repo_name] 和 --disablerepo=[repo_name] 选项，或者通过 PackageKit 的\"添加/删除软件\"工具，也能够方便地启用和禁用指定的软件仓库 变量解释： # $releasever 发行版的版本，从[main]部分的distroverpkg获取，如果没有，则根据redhat-release包进行判断。 # $arch cpu体系，如i686,athlon等 # $basearch cpu的基本体系组，如i686和athlon同属i386，alpha和alphaev6同属alpha。 四、yum-fastestmirror插件 yum-fastestmirror插件，它会自动选择最快的mirror。它的配置文件/etc/yum/pluginconf.d/fastestmirror.conf，yum镜像的速度测试记录文件/var/cache/yum/x86_64/7/timedhosts.txt** 禁用插件配置 修改插件的配置文件 sed -i 's/enabled=1/enabled=0/' /etc/yum/pluginconf.d/fastestmirror.conf enabled = 1//由1改为0，禁用该插件 修改yum的配置文件 # sed -i 's/plugins=1/plugins=0/' /etc/yum.conf plugins=1 //改为0，不使用插件 五、YUM源的创建 1、使用Nexus的YUN格式仓库作为YUM镜像源 详见：Nexus中yum仓库的配置与使用 2、Createrepo创建本地YUM镜像源 将CentOS版本系统镜像中的Packages并上传到主机上的某一目录下 安装createrepo用来创建软件包的索引。或者将系统镜像中repodata目录放到rpm包路径下 yum install createrepo -y createrepo /data/localrepo/Office 会在创建repodata索引文件夹 在/etc/yum.repos.d/目录下创建repo文件local.repo [local] name=Local Yum Office Repository 仓库名 baseurl=file:///data/localrepo/Office 仓库中rpm包存放路径 gpgcheck=1 是否检查GPG-KEY，0为不检查，1为检查 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 GPG-KEY的存放路径 enabled=1 设置为1表示启用本Repo Createrepo命令参数详解 -u --baseurl 指定Base URL的地址 -o --outputdir 指定元数据的输出位置 -x --excludes 指定在形成元数据时需要排除的包 -i --pkglist 指定一个文件，该文件内的包信息将被包含在即将生成的元数据中，格式为每个包信息独占一行，不含通配符、正则，以及范围表达式。 -n --includepkg 通过命令行指定要纳入本地库中的包信息，需要提供URL或本地路径。 -q --quiet 安静模式执行操作，不输出任何信息。 -g --groupfile 指定本地软件仓库的组划分，范例如下：createrepo -g comps.xml /path/to/rpms注意：组文件需要和rpm包放置于同一路径下。 -v --verbose 输出详细信息。 -c --cachedir 指定一个目录，用作存放软件仓库中软件包的校验和信息。 当createrepo在未发生明显改变的相同仓库文件上持续多次运行时，指定cachedir会明显提高 其性能。 --update 如果元数据已经存在，且软件仓库中只有部分软件发生了改变或增减， 则可用update参数直接对原有元数据进行升级，效率比重新分析rpm包依赖并生成新的元数据要 高很多。 -p --pretty 以整洁的格式输出xml文件。 -d --database 该选项指定使用SQLite来存储生成的元数据，默认项。 3、HTTPD+Createrepo创建YUM镜像源 第二种方法创建的镜像源只能在本地使用，要是能在局域网中提供公共的服务，需要一个能提供HTTP服务的容器，可使用HTTPD（又称Apache），步骤省略。 六、Reposync同步YUM远程仓库的安装包 1、安装 yum install yum-utils -y 2、命令参数 Usage: Reposync is used to synchronize a remote yum repository to a local directory using yum to retrieve the packages. /usr/bin/reposync [options] Options: -h, --help show this help message and exit -c CONFIG, --config=CONFIG config file to use (defaults to /etc/yum.conf) -a ARCH, --arch=ARCH act as if running the specified arch (default: current arch, note: does not override $releasever. x86_64 is a superset for i*86.) --source operate on source packages -r REPOID, --repoid=REPOID secify repo ids to query, can be specified multiple times (default is all enabled) -e CACHEDIR, --cachedir=CACHEDIR directory in which to store metadata -t, --tempcache Use a temp dir for storing/accessing yum-cache -d, --delete delete local packages no longer present in repository -p DESTDIR, --download_path=DESTDIR Path to download packages to: defaults to current dir --norepopath Don't add the reponame to the download path. Can only be used when syncing a single repository (default is to add the reponame) -g, --gpgcheck Remove packages that fail GPG signature checking after downloading -u, --urls Just list urls of what would be downloaded, don't download -n, --newest-only Download only newest packages per-repo -q, --quiet Output as little as possible -l, --plugins enable yum plugin support -m, --downloadcomps also download comps.xml --download-metadata download all the non-default metadata 3、示例 $> bash -c 'cat > ceph.repo 七、下载软件RPM包以及其依赖包 1、yum插件yumdownloadonly yum install yum-plugin-downloadonly && yum install --downloadonly --downloaddir=/root/httpd httpd 2、使用yum-utils的命令yumdownloader yum install -y yum-utils && \\ yumdownloader --resolve docker-ce-20.10.5 附录：常见软件源 1、VirtualBox [virtualbox] name=Oracle Linux / RHEL / CentOS-$releasever / $basearch - VirtualBox baseurl=http://download.virtualbox.org/virtualbox/rpm/el/$releasever/$basearch enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://www.virtualbox.org/download/oracle_vbox.asc 2、Nginx [nginx] name=nginx repo baseurl=http://nginx.org/packages/OS/OSRELEASE/$basearch/ gpgcheck=0 enabled=1 3、EPEL [epel] name=Extra Packages for Enterprise Linux 7 - $basearch #baseurl=http://download.fedoraproject.org/pub/epel/7/$basearch metalink=https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch failovermethod=priority enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 4、Ceph [Ceph] name=Ceph packages for $basearch baseurl=http://download.ceph.com/rpm-jewel/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-jewel/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 5、ELK Stack [ELK-Stack-5.x] name=ELK Stack repository for 5.x packages baseurl=https://mirrors.tuna.tsinghua.edu.cn/elasticstack/5.x/yum/ gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md [ELK-Stack-6.x] name=ELK Stack repository for 6.x packages baseurl=https://mirrors.tuna.tsinghua.edu.cn/elasticstack/6.x/yum/ gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md 6、MySQL [MySQL-Community-5.6] name=MySQL Community 5.6 baseurl=https://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-5.6-community/el/7/x86_64/ enabled=1 gpgcheck=0 [MySQL-Community-5.7] name=MySQL Community 5.7 baseurl=https://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-5.7-community/el/7/x86_64/ enabled=1 gpgcheck=0 [MySQL-Community-8.0] name=MySQL Community 8.0 baseurl=https://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-8.0-community/el/7/x86_64/ enabled=1 gpgcheck=0 [MySQL-Community-Connectors] name=MySQL Community Connectors baseurl=http://mirrors.ustc.edu.cn/mysql-repo/yum/mysql-connectors-community/el/7/x86_64/ enabled=1 gpgcheck=0 [MySQL-Community-Tools] name=MySQL Community Tools baseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-tools-community-el7/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 7、MongoDB bash -c 'cat > /etc/yum.repos.d/mongoDb.repo Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2021-06-10 10:40:14 "},"origin/linux-zsh.html":{"url":"origin/linux-zsh.html","title":"ZSH","keywords":"","body":"Linux zsh && oh-my-zsh 一、简介 Zsh 也许是目前最好用的 shell，是 bash 替代品中较为优秀的一个。 Zsh 官网：http://www.zsh.org/ Zsh具有以下主要优势： 完全兼容bash，之前bash下的使用习惯，shell脚本都可以完全兼容 更强大的tab补全 更智能的切换目录 命令选项、参数补齐 大小写字母自动更正 有着丰富多彩的主题 更强大的alias命令 智能命令错误纠正 集成各种类型的插件 oh-my-zsh 是最为流行的 zsh 配置文件，提供了大量的主题和插件，极大的拓展了 zsh 的功能，推动了 zsh 的流行，有点类似于 rails 之于 ruby。 二、安装zsh CentOS yum install -y zsh Ubuntu apt-get install -y zsh 检查下系统的 shell：$ cat /etc/shells，你会发现多了一个：/bin/zsh 设置用户的默认shell # 给root用户设置 chsh -s /bin/zsh root # 给普通账户设置 chsh -s /bin/zsh 用户名 #　恢复bash chsh -s /bin/bash [user] 三、安装oh-my-zsh oh-my-zsh 帮我们整理了一些常用的 Zsh 扩展功能和主题，我们无需自己去捣搞 Zsh，直接用 oh-my-zsh 就足够了。 oh-my-zsh 官网：https://ohmyz.sh/ oh-my-zsh Github：https://github.com/robbyrussell/oh-my-zsh curl curl -Lo install.sh https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh chmod +x install.sh REPO=\"mirrors/oh-my-zsh\" REMOTE=\"https://gitee.com/mirrors/oh-my-zsh.git\" sh install.sh --skip-chsh # 下载相关插件 git clone https://github.com/zsh-users/zsh-autosuggestions.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting 四、oh-my-zsh配置 oh-my-zsh的配置文件路径为~/.zshrc # If you come from bash you might have to change your $PATH. # export PATH=$HOME/bin:/usr/local/bin:$PATH # 指定oh-my-zsh的安装路径 export ZSH=\"/root/.oh-my-zsh\" # 设置主题。如果设置为\"random\", 每次启动oh-my-zsh会随机加载主题,可使用`echo $RANDOM_THEME`查看每次加载的主题 ZSH_THEME=\"alanpeabody\" # 设置随机的主题 # ZSH_THEME_RANDOM_CANDIDATES=( \"robbyrussell\" \"agnoster\" ) # 大小写是否敏感 # CASE_SENSITIVE=\"true\" # Uncomment the following line to use hyphen-insensitive completion. # Case-sensitive completion must be off. _ and - will be interchangeable. # HYPHEN_INSENSITIVE=\"true\" # 设置是否自动更新 # DISABLE_AUTO_UPDATE=\"true\" # 设置自动更新的天数。默认13天 # export UPDATE_ZSH_DAYS=13 # 设置是否开启`ls`进行颜色显示 # DISABLE_LS_COLORS=\"true\" # 设置是否显示终端标题 # DISABLE_AUTO_TITLE=\"true\" # 设置是否开启语法修正 # ENABLE_CORRECTION=\"true\" # Uncomment the following line to display red dots whilst waiting for completion. # COMPLETION_WAITING_DOTS=\"true\" # Uncomment the following line if you want to disable marking untracked files # under VCS as dirty. This makes repository status check for large repositories # much, much faster. # DISABLE_UNTRACKED_FILES_DIRTY=\"true\" # 历史输入命令的时间展示格式 # HIST_STAMPS=\"mm/dd/yyyy\" # 设置自定义配置文件的路径 # ZSH_CUSTOM=/path/to/new-custom-folder # 设置存储历史命令的默认文件路径 # HISTFILE=~/.zsh_history # 配置要加载的插件（配置的插件要能在 ~/.oh-my-zsh/plugins/* 下找到，自定义的插件目录为 ~/.oh-my-zsh/custom/plugins/ ）.注意：插件安装的越多，zsh的启动速度越慢，选择使用率最高的插件才是最好的选择 plugins=( git zsh-autosuggestions zsh-syntax-highlighting kubectl docker sudo extract ) source /etc/profile source $ZSH/oh-my-zsh.sh # 用户配置 # 设置man文档的环境变量 # export MANPATH=\"/usr/local/man:$MANPATH\" # 设置语言环境变量 # export LANG=en_US.UTF-8 # 设置本地和远程sessions的首选编辑器 # if [[ -n $SSH_CONNECTION ]]; then # export EDITOR='vim' # else # export EDITOR='mvim' # fi # 设置编译标志 # export ARCHFLAGS=\"-arch x86_64\" # 设置别名 alias ll=\"ls -alh\" alias k='kubectl' . ~/.oh-my-zsh/custom/oc_zsh_completion 五、oh-my-zsh常用插件 git：可以使用git缩写，默认自带 git add --all => gaa 查看所有缩写：alias | grep git autojump：快速跳转文件夹 last-working-dir：可以记录上一次退出命令行时候的所在路径，并且在下一次启动命令行的时候自动恢复到上一次所在的路径。 wd：快速地切换到常用的目录 wd add web相当于给当前目录做了一个标识，标识名叫做 web ，我们下次如果再想进入这个目录，只需输入：wd web catimg：将图片的内容输出到命令行 catimg demo.jpg zsh-syntax-highlighting：命令高亮 正确路径自带下划线 安装：git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting zsh-autosuggestions：自动补全可能的路径 安装：git clone git://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM/plugins/zsh-autosuggestions sudo：连按两次Esc添加或去掉sudo extract：功能强大的解压插件 执行x demo.tar.gz git-open：在终端里打开当前项目的远程仓库地址 安装：git clone https://github.com/paulirish/git-open.git $ZSH_CUSTOM/plugins/git-open history：内置，快速搜索history 六、oh-my-zsh常用主题 官方主题：https://github.com/robbyrussell/oh-my-zsh/wiki/Themes 七、升级 omz update Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-06 18:05:07 "},"origin/linux-进程管理工具SystemD.html":{"url":"origin/linux-进程管理工具SystemD.html","title":"Systemd-进程管理","keywords":"","body":"Linux 进程管理工具SystemD 一、简介 SystemD即为system daemon，是linux下的一种init软件，由Lennart Poettering带头开发，并在LGPL 2.1及其后续版本许可证下开源发布，开发目标是提供更优秀的框架以表示系统服务间的依赖关系，并依此实现系统初始化时服务的并行启动，同时达到降低Shell的系统开销的效果，最终代替现在常用的System V与BSD风格init程序。 SystemD是一个专用于 Linux 操作系统的系统与服务管理器。当作为启动进程(PID=1)运行时，它将作为初始化系统运行，也就是启动并维护各种用户空间的服务。 Linux内核加载启动后，用户空间的第一个进程就是初始化进程，这个程序的物理文件约定位于/sbin/init，当然也可以通过传递内核参数来让内核启动指定的程序。这个进程的特点是进程号为1，代表第一个运行的用户空间进程。不同发行版采用了不同的启动程序，主要有以下几种主流选择： 以Ubuntu为代表的Linux发行版采用upstart。 以7.0版本之前的CentOS为代表的System V init。 CentOS 7.0版本开始的Systemd。 为了与传统的 SysV 兼容，如果将 systemd 以 init 名称启动，并且\"PID≠1\"，那么它将执行 telinit 命令并将所有命令行参数原封不动的传递过去。 这样对于普通的登录会话来说，无论是调用 init 还是调用 telinit 都是等价的。 当作为系统实例运行时，systemd将会按照system.conf配置文件以及system.conf.d配置目录中的指令工作；当作为用户实例运行时，systemd 将会按照user.conf配置文件 以及 user.conf.d配置目录中的指令工作。 systemd将各种系统启动和运行相关的对象，表示为各种不同类型的单元(unit)，并提供了处理不同单元之间依赖关系的能力。大部分单元都静态的定义在单元文件中，但是有少部分单元则是动态自动生成的：其中一部分来自于其他传统的配置文件(为了兼容性)，而另一部分则动态的来自于系统状态或可编程的运行时状态。单元既可以处于活动(active)状态，也可以处于停止(inactive)状态，当然也可以处于启动中(activating)或停止中(deactivating)的状态。还有一个特殊的失败(failed)状态，意思是单元以某种方式失败了(进程崩溃了、或者触碰启动频率限制、或者退出时返回了错误代码、或者遇到了操作超时之类的故障)。当进入失败(failed)状态时，导致故障的原因将被记录到日志中以方便日后排查。需要注意的是，不同的单元可能还会有各自不同的\"子状态\"，但它们都被映射到上述五种状态之一。 历史上，Linux 的启动一直采用init进程。这种方法有两个缺点 启动时间长。init进程是串行启动，只有前一个进程启动完，才会启动下一个进程。 启动脚本复杂。init进程只是执行启动脚本，不管其他事情。脚本需要自己处理各种情况，这往往使得脚本变得很长。 当前系统/etc/inittab这个文件的内容，这个文件是systme V init的标准配置文件，如今变成了 # inittab is no longer used when using systemd. # # ADDING CONFIGURATION HERE WILL HAVE NO EFFECT ON YOUR SYSTEM. # # Ctrl-Alt-Delete is handled by /etc/systemd/system/ctrl-alt-del.target # # systemd uses 'targets' instead of runlevels. By default, there are two main targets: # # multi-user.target: analogous to runlevel 3 # graphical.target: analogous to runlevel 5 # # To set a default target, run: # systemctl set-default TARGET.target 在systemd掌权后，inittab不再起作用，也没有了“运行级”的概念。现在起作用的配置文件是/etc/systemd/system/default.target这个文件了。此文件的内容如下： # This file is part of systemd. # # systemd is free software; you can redistribute it and/or modify it # under the terms of the GNU Lesser General Public License as published by # the Free Software Foundation; either version 2.1 of the License, or # (at your option) any later version. [Unit] Description=Multi-User System Documentation=man:systemd.special(7) Requires=basic.target Conflicts=rescue.service rescue.target After=basic.target rescue.service rescue.target AllowIsolate=yes 作为系统初始化系统，systemd 的最大特点有两个： 令人惊奇的激进的并发启动能力，极大地提高了系统启动速度； 用 CGroup 统计跟踪子进程，干净可靠。 此外，和其前任不同的地方在于: systemd 已经不仅仅是一个初始化系统了 Systemd 出色地替代了 sysvinit 的所有功能。因为 init 进程是系统所有进程的父进程这样的特殊性，systemd 非常适合提供曾经由其他服务提供的功能，比如定时任务 (以前由 crond 完成) ；会话管理 (以前由 ConsoleKit/PolKit 等管理) 有助于标准化 Linux 的管理！如果所有的 Linux 发行版都采纳了 systemd，那么系统管理任务便可以很大程度上实现标准化 此外 systemd 有个很棒的承诺：接口保持稳定，不会再轻易改动 根据 Linux 惯例，字母d是守护进程（daemon）的缩写。 Systemd 这个名字的含义，就是它要守护整个系统 systmed是一个用户空间的程序，属于应用程序，不属于Linux内核范畴，Linux内核的主要特征在所有发行版中是统一的，厂商可以自由改变的是用户空间的应用程序 二、基本概念 1. Unit单元 系统初始化要做很多工作，如挂在文件系统，启动sshd服务，配置交换分区，这都可以看做是一个配置单元，Systemd 可以管理所有系统资源。不同的资源统称为 Unit（单位）。systemd把配置单元分成分成12种 Service unit：系统服务 Target unit：多个 Unit 构成的一个逻辑分组，可以当成是SystemV中的运行级。 Device Unit：硬件设备 Mount Unit：文件系统的挂载点，systemd据此进行自动挂载，为了与SystemV兼容，目前systemd自动处理/etc/fstab并转化为mount Automount Unit：自动挂载点 Path Unit：文件或路径 Scope Unit：不是由 Systemd 启动的外部进程 Slice Unit：进程组 Snapshot Unit：Systemd 快照，可以切回某个快照 Socket Unit：进程间通信的 socket Swap Unit：配置swap交换分区文件 Timer Unit：定时器。用来定时触发用户定义的操作，它可以用来取代传统的atd，crond等。 每一个配置单元都有一个对应的配置文件，系统管理员的任务就是编写和维护这写不同的配置文件，比如一个MySql服务对应一个mysql.service文件。 2. 依赖关系 systemd并不能完全解除各个单元之间的依赖关系，如物理设备单元准备就绪之前，不可能执行挂载单元。为此需要定义各个单元之间的依赖关系。有依赖的地方就会有出现死循环的可能，比如A依赖于B，B依赖于C，C依赖于A，那么导致死锁。systemd为此提供了两种不同程度的依赖关系，一个是require，一个是want，出现死循环时，systemd会尝试忽略want类型的依赖，如仍不能解锁，那么systemd报错。 如前所述，在 Systemd 中，所有的服务都并发启动，比如 Avahi、D-Bus、livirtd、X11、HAL 可以同时启动。乍一看，这似乎有点儿问题，比如 Avahi 需要 syslog 的服务，Avahi 和 syslog 同时启动，假设 Avahi 的启动比较快，所以 syslog 还没有准备好，可是 Avahi 又需要记录日志，这岂不是会出现问题？ Systemd 的开发人员仔细研究了服务之间相互依赖的本质问题，发现所谓依赖可以分为三个具体的类型，而每一个类型实际上都可以通过相应的技术解除依赖关系。 并发启动原理之一：解决 socket 依赖 绝大多数的服务依赖是套接字依赖。比如服务 A 通过一个套接字端口 S1 提供自己的服务，其他的服务如果需要服务 A，则需要连接 S1。因此如果服务 A 尚未启动，S1 就不存在，其他的服务就会得到启动错误。所以传统地，人们需要先启动服务 A，等待它进入就绪状态，再启动其他需要它的服务。Systemd 认为，只要我们预先把 S1 建立好，那么其他所有的服务就可以同时启动而无需等待服务 A 来创建 S1 了。如果服务 A 尚未启动，那么其他进程向 S1 发送的服务请求实际上会被 Linux 操作系统缓存，其他进程会在这个请求的地方等待。一旦服务 A 启动就绪，就可以立即处理缓存的请求，一切都开始正常运行。 那么服务如何使用由 init 进程创建的套接字呢？ Linux 操作系统有一个特性，当进程调用 fork 或者 exec 创建子进程之后，所有在父进程中被打开的文件句柄 (file descriptor) 都被子进程所继承。套接字也是一种文件句柄，进程 A 可以创建一个套接字，此后当进程 A 调用 exec 启动一个新的子进程时，只要确保该套接字的 close_on_exec 标志位被清空，那么新的子进程就可以继承这个套接字。子进程看到的套接字和父进程创建的套接字是同一个系统套接字，就仿佛这个套接字是子进程自己创建的一样，没有任何区别。 这个特性以前被一个叫做 inetd 的系统服务所利用。Inetd 进程会负责监控一些常用套接字端口，比如 Telnet，当该端口有连接请求时，inetd 才启动 telnetd 进程，并把有连接的套接字传递给新的 telnetd 进程进行处理。这样，当系统没有 telnet 客户端连接时，就不需要启动 telnetd 进程。Inetd 可以代理很多的网络服务，这样就可以节约很多的系统负载和内存资源，只有当有真正的连接请求时才启动相应服务，并把套接字传递给相应的服务进程。 和 inetd 类似，systemd 是所有其他进程的父进程，它可以先建立所有需要的套接字，然后在调用 exec 的时候将该套接字传递给新的服务进程，而新进程直接使用该套接字进行服务即可。 并发启动原理之二：解决 D-Bus 依赖 D-Bus 是 desktop-bus 的简称，是一个低延迟、低开销、高可用性的进程间通信机制。它越来越多地用于应用程序之间通信，也用于应用程序和操作系统内核之间的通信。很多现代的服务进程都使用D-Bus 取代套接字作为进程间通信机制，对外提供服务。比如简化 Linux 网络配置的 NetworkManager 服务就使用 D-Bus 和其他的应用程序或者服务进行交互：邮件客户端软件 evolution 可以通过 D-Bus 从 NetworkManager 服务获取网络状态的改变，以便做出相应的处理。 D-Bus 支持所谓\"bus activation\"功能。如果服务 A 需要使用服务 B 的 D-Bus 服务，而服务 B 并没有运行，则 D-Bus 可以在服务 A 请求服务 B 的 D-Bus 时自动启动服务 B。而服务 A 发出的请求会被 D-Bus 缓存，服务 A 会等待服务 B 启动就绪。利用这个特性，依赖 D-Bus 的服务就可以实现并行启动。 并发启动原理之三：解决文件系统依赖 系统启动过程中，文件系统相关的活动是最耗时的，比如挂载文件系统，对文件系统进行磁盘检查（fsck），磁盘配额检查等都是非常耗时的操作。在等待这些工作完成的同时，系统处于空闲状态。那些想使用文件系统的服务似乎必须等待文件系统初始化完成才可以启动。但是 systemd 发现这种依赖也是可以避免的。 Systemd 参考了 autofs 的设计思路，使得依赖文件系统的服务和文件系统本身初始化两者可以并发工作。autofs 可以监测到某个文件系统挂载点真正被访问到的时候才触发挂载操作，这是通过内核 automounter 模块的支持而实现的。比如一个 open()系统调用作用在\"/misc/cd/file1\"的时候，/misc/cd 尚未执行挂载操作，此时 open()调用被挂起等待，Linux 内核通知 autofs，autofs 执行挂载。这时候，控制权返回给 open()系统调用，并正常打开文件。 Systemd 集成了 autofs 的实现，对于系统中的挂载点，比如/home，当系统启动的时候，systemd 为其创建一个临时的自动挂载点。在这个时刻/home 真正的挂载设备尚未启动好，真正的挂载操作还没有执行，文件系统检测也还没有完成。可是那些依赖该目录的进程已经可以并发启动，他们的 open()操作被内建在 systemd 中的 autofs 捕获，将该 open()调用挂起（可中断睡眠状态）。然后等待真正的挂载操作完成，文件系统检测也完成后，systemd 将该自动挂载点替换为真正的挂载点，并让 open()调用返回。由此，实现了那些依赖于文件系统的服务和文件系统本身同时并发启动。 当然对于\"/\"根目录的依赖实际上一定还是要串行执行，因为 systemd 自己也存放在/之下，必须等待系统根目录挂载检查好。 不过对于类似/home 等挂载点，这种并发可以提高系统的启动速度，尤其是当/home 是远程的 NFS 节点，或者是加密盘等，需要耗费较长的时间才可以准备就绪的情况下，因为并发启动，这段时间内，系统并不是完全无事可做，而是可以利用这段空余时间做更多的启动进程的事情，总的来说就缩短了系统启动时间。 3. Target和runlevel systemd使用target取代了systemV的运行级的概念，Sysvinit 运行级别和 systemd 目标的对应表 Sysvinit 运行级别 Systemd 目标 备注 0 runlevel0.target, poweroff.target 关闭系统。 1, s, single runlevel1.target, rescue.target 单用户模式。 2, 4 runlevel2.target, runlevel4.target, multi-user.target 用户定义/域特定运行级别。默认等同于 3。 3 runlevel3.target, multi-user.target 多用户，非图形化。用户可以通过多个控制台或网络登录。 5 runlevel5.target, graphical.target 多用户，图形化。通常为所有运行级别 3 的服务外加图形化登录。 6 runlevel6.target, reboot.target 重启 emergency emergency.target 紧急 Shell 三、Systemd包含的命令 Systemd 是一个完整的软件包，Systemd 并不是一个命令，而是一组命令，涉及到系统管理的方方面面。安装完成后有很多物理文件组成，大致分布为，配置文件位于/etc/systemd这个目录下，配置工具命令位于/bin，和/sbin这两个目录下，预先准备的备用配置文件位于/lib/systemd目录下，还有库文件和帮助手册等等。这是一个庞大的软件包。详情使用rpm -ql systemd即可查看。 1. 电源管理工具 Systemd 可用于管理系统。 关机不是每个登录用户在任何情况下都可以执行的，一般只有管理员才可以关机。正常情况下系统不应该允许 SSH 远程登录的用户执行关机命令。否则其他用户正在工作，一个用户把系统关了就不好了。为了解决这个问题，传统的 Linux 系统使用 ConsoleKit 跟踪用户登录情况，并决定是否赋予其关机的权限。现在 ConsoleKit 已经被 systemd 的 logind 所替代。 logind 不是 pid-1 的 init 进程。它的作用和 UpStart 的 session init 类似，但功能要丰富很多，它能够管理几乎所有用户会话(session)相关的事情。logind 不仅是 ConsoleKit 的替代，它可以： 维护，跟踪会话和用户登录情况。如上所述，为了决定关机命令是否可行，系统需要了解当前用户登录情况，如果用户从 SSH 登录，不允许其执行关机命令；如果普通用户从本地登录，且该用户是系统中的唯一会话，则允许其执行关机命令；这些判断都需要 logind 维护所有的用户会话和登录情况。 Logind 也负责统计用户会话是否长时间没有操作，可以执行休眠/关机等相应操作。 为用户会话的所有进程创建 CGroup。这不仅方便统计所有用户会话的相关进程，也可以实现会话级别的系统资源控制。 负责电源管理的组合键处理，比如用户按下电源键，将系统切换至睡眠状态。 多席位(multi-seat) 管理。如今的电脑，即便一台笔记本电脑，也完全可以提供多人同时使用的计算能力。多席位就是一台电脑主机管理多个外设，比如两个屏幕和两个鼠标/键盘。席位一使用屏幕 1 和键盘 1；席位二使用屏幕 2 和键盘 2，但他们都共享一台主机。用户会话可以自由在多个席位之间切换。或者当插入新的键盘，屏幕等物理外设时，自动启动 gdm 用户登录界面等。所有这些都是多席位管理的内容。ConsoleKit 始终没有实现这个功能，systemd 的 logind 能够支持多席位。 # 重启系统 $ systemctl reboot # 关闭系统，切断电源 $ systemctl poweroff # CPU停止工作 $ systemctl halt # 暂停系统 $ systemctl suspend # 让系统进入冬眠状态 $ systemctl hibernate # 让系统进入交互式休眠状态 $ systemctl hybrid-sleep # 启动进入救援状态（单用户状态） $ systemctl rescue 2. 服务消耗分析工具 systemd-analyze命令用于查看启动耗时 # 查看启动耗时 $ systemd-analyze # 查看每个服务的启动耗时 $ systemd-analyze blame # 显示瀑布状的启动过程流 $ systemd-analyze critical-chain # 显示指定服务的启动流 $ systemd-analyze critical-chain atd.service 3. 主机信息信息管理工具 hostnamectl命令用于查看当前主机的信息。 # 显示当前主机的信息 $ hostnamectl # 设置主机名。 $ hostnamectl set-hostname rhel7 4. 本地化设置管理工具 localectl命令用于查看本地化设置。 # 查看本地化设置 $ localectl # 设置本地化参数。 $ localectl set-locale LANG=en_GB.utf8 $ localectl set-keymap en_GB 5. 时区管理工具 timedatectl命令用于查看当前时区设置。 # 查看当前时区设置 $ timedatectl # 显示所有可用的时区 $ timedatectl list-timezones # 设置当前时区 $ timedatectl set-timezone America/New_York $ timedatectl set-time YYYY-MM-DD $ timedatectl set-time HH:MM:SS 6. 登陆会话管理工具 loginctl命令用于查看当前登录的用户。 # 列出当前session $ loginctl list-sessions # 列出当前登录用户 $ loginctl list-users # 列出显示指定用户的信息 $ loginctl show-user test 三、Unit 每一个 Unit 都有一个配置文件，告诉 Systemd 怎么启动这个 Unit 。 Systemd 默认从目录/etc/systemd/system/读取配置文件。但是，里面存放的大部分文件都是符号链接，指向目录/usr/lib/systemd/system/，真正的配置文件存放在那个目录。 配置文件的后缀名，就是该 Unit 的种类，比如sshd.socket。如果省略，Systemd 默认后缀名为.service，所以sshd会被理解成sshd.service。 1. Unit配置文件结构 配置文件分成几个区块。每个区块的第一行，是用方括号表示的区别名，比如[Unit]。注意： 配置文件的区块名和字段名，都是大小写敏感的。 每个区块内部是一些等号连接的键值对，键值对的等号两侧不能有空格。 [Unit] Description=ATD daemon [Service] Type=forking ExecStart=/usr/bin/atd [Install] WantedBy=multi-user.target Unit 配置文件的完整字段清单，请参考官方文档。 [Unit]区块 [Unit]区块通常是配置文件的第一个区块。用来定义 Unit 的元数据，以及配置与其他 Unit 的关系。它的主要字段如下。 Description：简短描述 Documentation：文档地址 Requires：当前 Unit 依赖的其他 Unit，如果它们没有运行，当前 Unit 会启动失败 Wants：与当前 Unit 配合的其他 Unit，如果它们没有运行，当前 Unit 不会启动失败 BindsTo：与Requires类似，它指定的 Unit 如果退出，会导致当前 Unit 停止运行 Before：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之后启动 After：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之前启动 Conflicts：这里指定的 Unit 不能与当前 Unit 同时运行 Condition...：当前 Unit 运行必须满足的条件，否则不会运行 Assert...：当前 Unit 运行必须满足的条件，否则会报启动失败 [Service]区块 [Service]区块用来定义如何启动当前服务，只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。 Type：定义启动时的进程行为。它有以下几种值。 Type=simple：默认值，执行ExecStart指定的命令，启动主进程 Type=forking：以 fork 方式从父进程创建子进程，创建后父进程会立即退出 Type=oneshot：一次性进程，Systemd 会等当前服务退出，再继续往下执行 Type=dbus：当前服务通过D-Bus启动 Type=notify：当前服务启动完毕，会通知Systemd，再继续往下执行 Type=idle：若有其他任务执行完毕，当前服务才会运行 ExecStart：启动当前服务的命令 ExecStartPre：启动当前服务之前执行的命令 ExecStartPost：启动当前服务之后执行的命令 ExecReload：重启当前服务时执行的命令 ExecStop：停止当前服务时执行的命令 ExecStopPost：停止当其服务之后执行的命令 RestartSec：自动重启当前服务间隔的秒数 Restart：定义何种情况 Systemd 会自动重启当前服务，可能的值包括always（总是重启）、on-success、on-failure、on-abnormal、on-abort、on-watchdog TimeoutSec：定义 Systemd 停止当前服务之前等待的秒数 Environment：指定环境变量 [Install]区块 [Install]通常是配置文件的最后一个区块，用来定义如何启动，以及是否开机启动。它的主要字段如下。 WantedBy：它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入/etc/systemd/system目录下面以 Target 名 +.wants后缀构成的子目录中 RequiredBy：它的值是一个或多个 Target，当前 Unit 激活时，符号链接会放入/etc/systemd/system目录下面以 Target 名 + .required后缀构成的子目录中 Alias：当前 Unit 可用于启动的别名 Also：当前 Unit 激活（enable）时，会被同时激活的其他 Unit 2. Unit管理 3.2.1 systemctl enable 命令用于在上面两个目录之间，建立符号链接关系。如果配置文件里面设置了开机启动，systemctl enable命令相当于激活开机启动。 $ systemctl enable clamd@scan.service # 等同于 $ ln -s '/usr/lib/systemd/system/clamd@scan.service' '/etc/systemd/system/multi-user.target.wants/clamd@scan.service' 3.2.2 systemctl disable 用于在两个目录之间，撤销符号链接关系，相当于撤销开机启动。 systemctl disable clamd@scan.service 3.2.3 systemctl list-unit-files systemctl list-unit-files会显示每个配置文件的状态。而每个配置文件的状态，一共有四种 enabled：已建立启动链接 disabled：没建立启动链接 static：该配置文件没有[Install]部分（无法执行），只能作为其他配置文件的依赖 masked：该配置文件被禁止建立启动链接 # 列出所有配置文件 $ systemctl list-unit-files # 列出指定类型的配置文件 $ systemctl list-unit-files --type=service 3.2.4 systemctl list-units 查看当前系统的所有 Unit # 列出正在运行的 Unit $ systemctl list-units # 列出所有Unit，包括没有找到配置文件的或者启动失败的 $ systemctl list-units --all # 列出所有没有运行的 Unit $ systemctl list-units --all --state=inactive # 列出所有加载失败的 Unit $ systemctl list-units --failed # 列出所有正在运行的、类型为 service 的 Unit $ systemctl list-units --type=service 3.2.5 systemctl status systemctl status命令用于查看系统状态和单个 Unit 的状态 # 显示系统状态 $ systemctl status # 显示单个 Unit 的状态 $ sysystemctl status httpd httpd.service - The Apache HTTP Server Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled) Active: active (running) since 金 2014-12-05 12:18:22 JST; 7min ago Main PID: 4349 (httpd) Status: \"Total requests: 1; Current requests/sec: 0; Current traffic: 0 B/sec\" CGroup: /system.slice/httpd.service ├─4349 /usr/sbin/httpd -DFOREGROUND ├─4350 /usr/sbin/httpd -DFOREGROUND ├─4351 /usr/sbin/httpd -DFOREGROUND ├─4352 /usr/sbin/httpd -DFOREGROUND ├─4353 /usr/sbin/httpd -DFOREGROUND └─4354 /usr/sbin/httpd -DFOREGROUND 12月 05 12:18:22 localhost.localdomain systemd[1]: Starting The Apache HTTP Server... 12月 05 12:18:22 localhost.localdomain systemd[1]: Started The Apache HTTP Server. 12月 05 12:22:40 localhost.localdomain systemd[1]: Started The Apache HTTP Server. # 显示远程主机的某个 Unit 的状态 $ systemctl -H root@rhel7.example.com status httpd.service 3.2.6 查询Unit状态 # 显示某个 Unit 是否正在运行 $ systemctl is-active application.service # 显示某个 Unit 是否处于启动失败状态 $ systemctl is-failed application.service # 显示某个 Unit 服务是否建立了启动链接 $ systemctl is-enabled application.service 3.2.7 Unit状态管理 # 立即启动一个服务 $ systemctl start apache.service # 立即停止一个服务 $ systemctl stop apache.service # 重启一个服务 $ systemctl restart apache.service # 杀死一个服务的所有子进程 $ systemctl kill apache.service # 重新加载一个服务的配置文件 $ systemctl reload apache.service # 重载所有修改过的配置文件 $ systemctl daemon-reload # 显示某个 Unit 的所有底层参数 $ systemctl show httpd.service # 显示某个 Unit 的指定属性的值 $ systemctl show -p CPUShares httpd.service # 设置某个 Unit 的指定属性 $ systemctl set-property httpd.service CPUShares=500 3.2.8 查询Unit间的依赖关系 Unit 之间存在依赖关系：A 依赖于 B，就意味着 Systemd 在启动 A 的时候，同时会去启动 B。 # 列出一个 Unit 的所有依赖。 $ systemctl list-dependencies nginx.service # 上面命令的输出结果之中，有些依赖是 Target 类型（详见下文），默认不会展开显示。如果要展开 Target，就需要使用--all参数。 $ systemctl list-dependencies --all nginx.service 四、Unit的日志管理 Systemd 统一管理所有 Unit 的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。journalctl功能强大，用法非常多。 # 查看所有日志（默认情况下 ，只保存本次启动的日志） $ journalctl # 查看内核日志（不显示应用日志） $ journalctl -k # 查看系统本次启动的日志 $ journalctl -b $ journalctl -b -0 # 查看上一次启动的日志（需更改设置） $ journalctl -b -1 # 查看指定时间的日志 $ journalctl --since=\"2012-10-30 18:17:16\" $ journalctl --since \"20 min ago\" $ journalctl --since yesterday $ journalctl --since \"2015-01-10\" --until \"2015-01-11 03:00\" $ journalctl --since 09:00 --until \"1 hour ago\" # 显示尾部的最新10行日志 $ journalctl -n # 显示尾部指定行数的日志 $ journalctl -n 20 # 实时滚动显示最新日志 $ journalctl -f # 查看指定服务的日志 $ journalctl /usr/lib/systemd/systemd # 查看指定进程的日志 $ journalctl _PID=1 # 查看某个路径的脚本的日志 $ journalctl /usr/bin/bash # 查看指定用户的日志 $ journalctl _UID=33 --since today # 查看某个 Unit 的日志 $ journalctl -u nginx.service $ journalctl -u nginx.service --since today # 实时滚动显示某个 Unit 的最新日志 $ journalctl -u nginx.service -f # 合并显示多个 Unit 的日志 $ journalctl -u nginx.service -u php-fpm.service --since today # 查看指定优先级（及其以上级别）的日志，共有8级 # 0: emerg # 1: alert # 2: crit # 3: err # 4: warning # 5: notice # 6: info # 7: debug $ journalctl -p err -b # 日志默认分页输出，--no-pager 改为正常的标准输出 $ journalctl --no-pager # 以 JSON 格式（单行）输出 $ journalctl -b -u nginx.service -o json # 以 JSON 格式（多行）输出，可读性更好 $ journalctl -b -u nginx.serviceqq -o json-pretty # 显示日志占据的硬盘空间 $ journalctl --disk-usage # 指定日志文件占据的最大空间 $ journalctl --vacuum-size=1G # 指定日志文件保存多久 $ journalctl --vacuum-time=1years 五、Target 启动计算机的时候，需要启动大量的 Unit。如果每一次启动，都要一一写明本次启动需要哪些 Unit，显然非常不方便。Systemd 的解决方案就是 Target。简单说，Target 就是一个 Unit 组，包含许多相关的 Unit 。启动某个 Target 的时候，Systemd 就会启动里面所有的 Unit。从这个意义上说，Target 这个概念类似于\"状态点\"，启动某个 Target 就好比启动到某种状态。 传统的init启动模式里面，有 RunLevel 的概念，跟 Target 的作用很类似。不同的是，RunLevel 是互斥的，不可能多个 RunLevel 同时启动，但是多个 Target 可以同时启动。 1. 与传统 RunLevel 的对应关系如下: Traditional runlevel New target name --> Symbolically linked to Runlevel 0 runlevel0.target -> poweroff.target Runlevel 1 runlevel1.target -> rescue.target Runlevel 2 runlevel2.target -> multi-user.target Runlevel 3 runlevel3.target -> multi-user.target Runlevel 4 runlevel4.target -> multi-user.target Runlevel 5 runlevel5.target -> graphical.target Runlevel 6 runlevel6.target -> reboot.target 2. 与init进程的主要差别如下: 默认的 RunLevel（在/etc/inittab文件设置）现在被默认的 Target 取代，位置是/etc/systemd/system/default.target，通常符号链接到graphical.target（图形界面）或者multi-user.target（多用户命令行）。 启动脚本的位置，以前是/etc/init.d目录，符号链接到不同的 RunLevel 目录 （比如/etc/rc3.d、/etc/rc5.d等），现在则存放在/lib/systemd/system和/etc/systemd/system目录。 3. Target管理 # 查看当前系统的所有 Target $ systemctl list-unit-files --type=target # 查看一个 Target 包含的所有 Unit $ systemctl list-dependencies multi-user.target # 查看启动时的默认 Target $ systemctl get-default # 设置启动时的默认 Target $ sudo systemctl set-default multi-user.target # 切换 Target 时，默认不关闭前一个 Target 启动的进程， # systemctl isolate 命令改变这种行为， # 关闭前一个 Target 里面所有不属于后一个 Target 的进程 $ systemctl isolate multi-user.target 六、常见服务的Unit配置 1. Zookeeper bash -c ' cat > /usr/lib/systemd/system/zookeeper.service 2. Kafka bash -c ' cat > /usr/lib/systemd/system/kafka.service 3. Tomcat bash -c ' cat > /usr/lib/systemd/system/tomcat.service 参考 http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-part-two.html https://blog.51cto.com/andyxu/2122109?source=dra Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/linux-zombie-orphaned-process.html":{"url":"origin/linux-zombie-orphaned-process.html","title":"僵尸进程与孤儿进程","keywords":"","body":"孤儿进程orphaned与僵尸进程Zombie 一、概念 ​ 我们知道在unix/linux中，正常情况下，子进程是通过父进程创建的，子进程在创建新的进程。子进程的结束和父进程的运行是一个异步过程,即父进程永远无法预测子进程 到底什么时候结束。 当一个 进程完成它的工作终止之后，它的父进程需要调用wait()或者waitpid()系统调用取得子进程的终止状态。 　　孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。 　　僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。 二、僵尸进程 在linux系统中，当用ps命令观察进程的执行状态时，经常看到某些进程的状态栏为defunct，这就是所谓的“僵尸”进程。“僵尸”进程是一个早已死亡的进程，但在进程表（processs table）中仍占了一个位置（slot）。由于进程表的容量是有限的，所以，defunct进程不仅占用系统的内存资源，影响系统的性能，而且如果其数目太多，还会导致系统瘫痪。 1、僵尸进程的产生原因 我们知道，每个进程在进程表里都有一个进入点（entry），核心程序执行该进程时使用到的一切信息都存储在进入点。当用ps命令察看系统中的进程信息时，看到的就是进程表中的相关数据。 所以，当一个父进程以fork()系统调用建立一个新的子进程后，核心进程就会在进程表中给这个子进程分配一个进入点，然后将相关信息存储在该进入点所对应的进程表内。这些信息中有一项是其父进程的识别码。 而当这个子进程结束的时候（比如调用exit命令结束），其实他并没有真正的被销毁，而是留下一个称为僵尸进程（Zombie）的数据结构（系统调用exit的作用是使进程退出，但是也仅仅限于一个正常的进程变成了一个僵尸进程，并不能完全将其销毁）。此时原来进程表中的数据会被该进程的退出码（exit code）、执行时所用的CPU时间等数据所取代，这些数据会一直保留到系统将它传递给它的父进程为止。由此可见，defunct进程的出现时间是在子进程终止后，但是父进程尚未读取这些数据之前。 此时，该僵尸子进程已经放弃了几乎所有的内存空间，没有任何可执行代码，也不能被调度，仅仅在进程列表中保留一个位置，记载该进程的退出状态信息供其他进程收集，除此之外，僵尸进程不再占有任何存储空间。他需要他的父进程来为他收尸，如果他的父进程没有安装SIGCHLD信号处理函数调用wait 或 waitpid() 等待子进程结束，也没有显式忽略该信号，那么它就一直保持僵尸状态，如果这时候父进程结束了，那么init进程会自动接手这个子进程，为他收尸，他还是能被清除掉的。但是如果父进程是一个循环，不会结束，那么子进程就会一直保持僵尸状态，这就是系统中为什么有时候会有很多的僵尸进程。 2、如何杀死僵尸进程 如上可知，僵尸进程一旦出现之后，很难自己消亡，会一直存在下去，直至系统重启。虽然僵尸进程几乎不占系统资源，但是，这样下去，数量太多了之后，终究会给系统带来其他的影响。因此，如果一旦见到僵尸进程，我们就要将其杀掉。如何杀掉僵尸进程呢？ 有同学可能会说，很简单嘛，直接使用kill命令就好啊。或者，实在不行，加一个-9的后缀（kill -9），肯定杀掉！ 请注意：defunct状态下的僵尸进程是不能直接使用kill -9命令杀掉的，否则就不叫僵尸进程了。那么，该如何杀呢？ 方法有二： 重启服务器电脑，这个是最简单，最易用的方法，但是如果你服务器电脑上运行有其他的程序，那么这个方法，代价很大。所以，尽量使用下面一种方法。 找到该defunct僵尸进程的父进程，将该进程的父进程杀掉，则此defunct进程将自动消失。 问题又来了，如何找到defunct僵尸进程的父进程呢？ 很简单，一句命令就够了：ps -ef | grep defunct_process_pid。 3、如何预防僵尸进程 以上介绍的只是在发现了僵尸进程之后，如何去杀死它。那么，有同学可能会说了，这个是治标不治本的。真正的办法是，不让它产生，问题才能彻底解决。OK，那我们就来介绍一下，如何预防僵尸进程的产生。 在父进程创建子进程之前，就向系统申明自己并不会对这个子进程的exit动作进行任何关注行为，这样的话，子进程一旦退出后，系统就不会去等待父进程的操作，而是直接将该子进程的资源回收掉，也就不会出现僵尸进程了。具体的办法就是，在父进程的初始化函数中，调用这个函数：signal(SIGCHLD,SIG_IGN)； 如果上述语句没来得及调用，也有另外一个办法。那就是在创建完子进程后，用waitpid等待子进程返回，也能达到上述效果； 如果上述两个办法都不愿意采用，那还有一招：在父进程创建子进程的时候，连续调用两次fork()，而且使紧跟的子进程直接退出，使其孙子进程成为孤儿进程，从而init进程将代替父进程来接手，负责清除这个孤儿进程。于是，父进程就无需进行任何的清理行为，系统会自动处理； 三、问题及危害 　　unix提供了一种机制可以保证只要父进程想知道子进程结束时的状态信息， 就可以得到。这种机制就是: 在每个进程退出的时候,内核释放该进程所有的资源,包括打开的文件,占用的内存等。 但是仍然为其保留一定的信息(包括进程号the process ID,退出状态the termination status of the process,运行时间the amount of CPU time taken by the process等)。直到父进程通过wait / waitpid来取时才释放。 但这样就导致了问题，如果进程不调用wait / waitpid的话， 那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程. 此即为僵尸进程的危害，应当避免。 　　孤儿进程是没有父进程的进程，孤儿进程这个重任就落到了init进程身上，init进程就好像是一个民政局，专门负责处理孤儿进程的善后工作。每当出现一个孤儿进程的时候，内核就把孤 儿进程的父进程设置为init，而init进程会循环地wait()它的已经退出的子进程。这样，当一个孤儿进程凄凉地结束了其生命周期的时候，init进程就会代表党和政府出面处理它的一切善后工作。因此孤儿进程并不会有什么危害。 　　任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理。这是每个 子进程在结束时都要经过的阶段。如果子进程在exit()之后，父进程没有来得及处理，这时用ps命令就能看到子进程的状态是“Z”。如果父进程能及时 处理，可能用ps命令就来不及看到子进程的僵尸状态，但这并不等于子进程不经过僵尸状态。 如果父进程在子进程结束之前退出，则子进程将由init接管。init将会以父进程的身份对僵尸状态的子进程进行处理。 四、僵尸进程危害场景 　　例如有个进程，它定期的产 生一个子进程，这个子进程需要做的事情很少，做完它该做的事情之后就退出了，因此这个子进程的生命周期很短，但是，父进程只管生成新的子进程，至于子进程 退出之后的事情，则一概不闻不问，这样，系统运行上一段时间之后，系统中就会存在很多的僵死进程，倘若用ps命令查看的话，就会看到很多状态为Z的进程。 严格地来说，僵死进程并不是问题的根源，罪魁祸首是产生出大量僵死进程的那个父进程。因此，当我们寻求如何消灭系统中大量的僵死进程时，答案就是把产生大 量僵死进程的那个元凶枪毙掉（也就是通过kill发送SIGTERM或者SIGKILL信号啦）。枪毙了元凶进程之后，它产生的僵死进程就变成了孤儿进 程，这些孤儿进程会被init进程接管，init进程会wait()这些孤儿进程，释放它们占用的系统进程表中的资源，这样，这些已经僵死的孤儿进程 就能瞑目而去了。 参考： https://blog.csdn.net/LEON1741/article/details/78142269 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/linux-proc.html":{"url":"origin/linux-proc.html","title":"proc文件详解","keywords":"","body":"Proc文件系统 一、简介 Linux系统上的/proc目录是一种文件系统，即proc文件系统。与其它常见的文件系统不同的是，/proc是一种伪文件系统（也即虚拟文件系统），存储的是当前内核运行状态的一系列特殊文件，用户可以通过这些文件查看有关系统硬件及当前正在运行进程的信息，甚至可以通过更改其中某些文件来改变内核的运行状态。 基于/proc文件系统如上所述的特殊性，其内的文件也常被称作虚拟文件，并具有一些独特的特点。例如，其中有些文件虽然使用查看命令查看时会返回大量信息，但文件本身的大小却会显示为0字节。此外，这些特殊文件中大多数文件的时间及日期属性通常为当前系统时间和日期，这跟它们随时会被刷新（存储于RAM中）有关。 为了查看及使用上的方便，这些文件通常会按照相关性进行分类存储于不同的目录甚至子目录中，如/proc/scsi目录中存储的就是当前系统上所有SCSI设备的相关信息，/proc/N中存储的则是系统当前正在运行的进程的相关信息，其中N为正在运行的进程（可以想象得到，在某进程结束后其相关目录则会消失）。 大多数虚拟文件可以使用文件查看命令如cat、more或者less进行查看，有些文件信息表述的内容可以一目了然，但也有文件的信息却不怎么具有可读性。不过，这些可读性较差的文件在使用一些命令如apm、free、lspci或top查看时却可以有着不错的表现。 二、常见文件 1 10824 11989 12474 15454 4283 5481 7368 9174 cpuinfo kpagecount softirqs 10 10827 11992 12476 15461 4288 5497 8 9328 crypto kpageflags stat 10005 10828 11993 12477 15462 4289 5541 8192 9362 devices loadavg swaps 10026 10829 11994 12479 15463 430 555 8204 9382 diskstats locks synobios 10136 10863 11995 12481 15464 4376 562 8214 9385 dma mdstat syno_cpu_arch 10137 10864 11996 12483 15538 4522 5698 8215 9386 driver megaraid syno_loadavg 10246 11 11997 12567 15577 4523 580 8235 9387 execdomains meminfo syno_platform 10295 11242 11998 12568 15610 4524 6025 8246 9388 fb misc synotty 10297 11274 11999 12777 15887 4651 6026 8251 9389 filesystems modules sys 10365 11297 12 12812 16 5 6027 8254 9394 fs mounts sysrq-trigger 10369 11328 12001 12873 16104 5007 603 8257 9397 interrupts mpt sysvipc 10371 11366 12002 12904 16124 5010 6057 8258 9400 iomem mtrr thread-self 10372 11394 12022 13 16131 5013 607 8455 9619 ioports net timer_list 10606 11400 12067 13471 16133 5016 612 8457 9620 irq pagetypeinfo tty 10738 11425 12104 13681 16139 5289 6149 9 acpi kallsyms partitions uptime 10773 11704 12106 13685 16195 5394 6179 9075 buddyinfo kcore sched_debug version 10777 11712 12314 14 16324 5398 6290 9077 bus keys schedstat vmallocinfo 10821 11715 12420 15 16380 5405 690 9078 cgroups key-users scsi vmstat 10822 11801 12470 15084 16384 5407 7 9079 cmdline kmsg self zoneinfo 10823 11805 12472 15099 16387 5436 7342 9080 consoles kpagecgroup slabinfo 1、进程相关文件 /proc目录中包含许多以数字命名的子目录，这些数字表示系统当前正在运行进程的进程号，里面包含对应进程相关的多个信息文件。每个数字目录下是当前进程本身相关的信息文件。（以1号初始化进程为例） dr-xr-xr-x root 0 Feb 4 09:34 attr -r-------- root 0 Feb 4 09:34 auxv -r--r--r-- root 0 Feb 4 09:20 cgroup --w------- root 0 Feb 4 09:34 clear_refs -r--r--r-- root 0 Feb 4 09:34 cmdline # 启动当前进程的完整命令，但僵尸进程目录中的此文件不包含任何信息 -rw-r--r-- root 0 Feb 4 09:34 comm -rw-r--r-- root 0 Feb 4 09:34 coredump_filter lrwxrwxrwx root 0 Feb 4 09:34 cwd -> / # 指向当前进程运行目录的一个符号链接 -r-------- root 0 Feb 4 09:34 environ # 当前进程的环境变量列表，彼此间用空字符（NULL）隔开；变量用大写字母表示，其值用小写字母表示 lrwxrwxrwx root 0 Feb 4 09:34 exe -> /usr/sbin/init #指向启动当前进程的可执行文件（完整路径）的符号链接，通过/proc/N/exe可以启动当前进程的一个拷贝 dr-x------ root 0 Feb 4 09:34 fd # 这是个目录，包含当前进程打开的每一个文件的文件描述符，这些文件描述符是指向实际文件的一个符号链接； dr-x------ root 0 Feb 4 09:34 fdinfo -r-------- root 0 Feb 4 09:34 io -r--r--r-- root 0 Feb 4 09:34 limits # 当前进程所使用的每一个受限资源的软限制、硬限制和管理单元；此文件仅可由实际启动当前进程的UID用户读取 dr-x------ root 0 Feb 4 09:34 map_files -r--r--r-- root 0 Feb 4 09:34 maps # 当前进程关联到的每个可执行文件和库文件在内存中的映射区域及其访问权限所组成的列表 -rw------- root 0 Feb 4 09:34 mem # 当前进程所占用的内存空间，由open、read和lseek等系统调用使用，不能被用户读取； -r--r--r-- root 0 Feb 4 09:34 mountinfo -r--r--r-- root 0 Feb 4 09:34 mounts -r-------- root 0 Feb 4 09:34 mountstats dr-xr-xr-x root 0 Feb 4 09:34 net dr-x--x--x root 0 Feb 4 09:34 ns -rw-r--r-- root 0 Feb 4 09:34 oom_adj -r--r--r-- root 0 Feb 4 09:34 oom_score -rw-r--r-- root 0 Feb 4 09:34 oom_score_adj -r-------- root 0 Feb 4 09:34 pagemap -r-------- root 0 Feb 4 09:34 personality lrwxrwxrwx root 0 Feb 4 09:34 root -> / # 指向当前进程运行根目录的符号链接；在Unix和Linux系统上，通常采用chroot命令使每个进程运行于独立的根目录 -rw-r--r-- root 0 Feb 4 09:34 sched -r--r--r-- root 0 Feb 4 09:34 schedstat -r--r--r-- root 0 Feb 4 09:34 smaps -r-------- root 0 Feb 4 09:34 stack -r--r--r-- root 0 Feb 4 09:20 stat # 当前进程的状态信息，包含一系统格式化后的数据列，可读性差，通常由ps命令使用 -r--r--r-- root 0 Feb 4 09:34 statm # 当前进程占用内存的状态信息，通常以“页面”（page）表示 -r--r--r-- root 0 Feb 4 09:20 status # 与stat所提供信息类似，但可读性较好，如下所示，每行表示一个属性信息 -r-------- root 0 Feb 4 09:34 syscall dr-xr-xr-x root 0 Feb 4 09:34 task # 包含由当前进程所运行的每一个线程的相关信息，每个线程的相关信息文件均保存在一个由线程号（tid）命名的目录中，这类似于其内容类似于每个进程目录中的内容；（内核2.6版本以后支持此功能） -r--r--r-- root 0 Feb 4 09:34 wchan 2、/proc/sys目录 与/proc下其它文件的“只读”属性不同的是，管理员可对/proc/sys子目录中的许多文件内容进行修改以更改内核的运行特性，事先查看某文件是否“可写入”。写入操作通常使用类似于echo DATA > /path/to/your/filename的格式进行。需要注意的是，即使文件可写，其一般也不可以使用编辑器进行编辑。 3、其他目录文件 /proc/apm：高级电源管理（APM）版本信息及电池相关状态信息，通常由apm命令使用； /proc/buddyinfo：用于诊断内存碎片问题的相关信息文件； /proc/cmdline：在启动时传递至内核的相关参数信息，这些信息通常由lilo或grub等启动管理工具进行传递； syno_hdd_powerup_seq=1 HddHotplug=0 syno_hw_version=DS918+ vender_format_version=2 console=ttyS0,115200n8 withefi quiet syno_hdd_detect=0 root=/dev/md0 sn=1780PDN998701 mac1=001132112233 mac2=001132112231 netif_num=2 /proc/cpuinfo：处理器的相关信息的文件； /proc/crypto：系统上已安装的内核使用的密码算法及每个算法的详细信息列表； name : crc32c driver : crc32c-generic module : kernel priority : 0 type : digest blocksize : 32 digestsize : 4 ………… /proc/devices：系统已经加载的所有块设备和字符设备的信息，包含主设备号和设备组（与主设备号对应的设备类型）名； Character devices: 1 mem 4 /dev/vc/0 4 tty 4 ttyS ………… Block devices: 1 ramdisk 2 fd 8 sd ………… /proc/diskstats：每块磁盘设备的磁盘I/O统计信息列表；（内核2.5.69以后的版本支持此功能） /proc/dma：每个正在使用且注册的ISA DMA通道的信息列表； /proc/execdomains：内核当前支持的执行域（每种操作系统独特“个性”）信息列表； /proc/fb：帧缓冲设备列表文件，包含帧缓冲设备的设备号和相关驱动信息； /proc/filesystems：当前被内核支持的文件系统类型列表文件，被标示为nodev的文件系统表示不需要块设备的支持；通常mount一个设备时，如果没有指定文件系统类型将通过此文件来决定其所需文件系统的类型； nodev sysfs nodev rootfs nodev proc iso9660 ext3 ………… /proc/interrupts：X86或X86_64体系架构系统上每个IRQ相关的中断号列表；多路处理器平台上每个CPU对于每个I/O设备均有自己的中断号； CPU0 CPU1 CPU2 CPU3 0: 11 0 0 0 IO-APIC 2-edge timer 3: 0 0 0 0 IO-APIC 3-edge serial 4: 206 0 0 0 IO-APIC 4-edge serial 8: 0 0 0 0 IO-APIC 8-fasteoi rtc0 9: 0 0 0 0 IO-APIC 9-fasteoi acpi 87: 7594 15750 1492 9913 PCI-MSI 311296-edge 0000:00:13.0 88: 3137 0 67388 0 PCI-MSI 1048576-edge 0000:02:00.0 89: 34 0 0 0 PCI-MSI 327680-edge xhci_hcd 90: 1 0 0 0 PCI-MSI 1572864-edge eth0 91: 67 0 13 38093 PCI-MSI 1572865-edge eth0-rx-0 92: 1058 715 235 881 PCI-MSI 1572866-edge eth0-rx-1 93: 5258 0 470 1026 PCI-MSI 1572867-edge eth0-tx-0 94: 1949 0 0 36357 PCI-MSI 1572868-edge eth0-tx-1 95: 166 0 0 0 PCI-MSI 32768-edge i915 NMI: 102 96 80 81 Non-maskable interrupts LOC: 3399022 3276720 3075216 3057331 Local timer interrupts SPU: 0 0 0 0 Spurious interrupts PMI: 102 96 80 81 Performance monitoring interrupts /proc/iomem：每个物理设备上的记忆体（RAM或者ROM）在系统内存中的映射信息； 00000100-00000fff : reserved 00001000-0003efff : System RAM 0003f000-0003ffff : ACPI Non-volatile Storage 00040000-0009ffff : System RAM 000a0000-000bffff : PCI Bus 0000:00 000c0000-000dffff : PCI Bus 0000:00 000c0000-000c7fff : Video ROM /proc/ioports：当前正在使用且已经注册过的与物理设备进行通讯的输入-输出端口范围信息列表；如下面所示，第一列表示注册的I/O端口范围，其后表示相关的设备； 0000-001f : dma1 0020-0021 : pic1 0040-0043 : timer0 0050-0053 : timer1 0060-006f : keyboard /proc/kallsyms：模块管理工具用来动态链接或绑定可装载模块的符号定义，由内核输出；（内核2.5.71以后的版本支持此功能）；通常这个文件中的信息量相当大； /proc/kcore：系统使用的物理内存，以ELF核心文件（core file）格式存储，其文件大小为已使用的物理内存（RAM）加上4KB；这个文件用来检查内核数据结构的当前状态，因此，通常由GBD通常调试工具使用，但不能使用文件查看命令打开此文件； /proc/kmsg：此文件用来保存由内核输出的信息，通常由/sbin/klogd或/bin/dmsg等程序使用，不要试图使用查看命令打开此文件； /proc/locks：保存当前由内核锁定的文件的相关信息，包含内核内部的调试数据；每个锁定占据一行，且具有一个惟一的编号；如下输出信息中每行的第二列表示当前锁定使用的锁定类别，POSIX表示目前较新类型的文件锁，由lockf系统调用产生，FLOCK是传统的UNIX文件锁，由flock系统调用产生；第三列也通常由两种类型，ADVISORY表示不允许其他用户锁定此文件，但允许读取，MANDATORY表示此文件锁定期间不允许其他用户任何形式的访问； 1: POSIX ADVISORY WRITE 20975 00:1f:9017052 0 EOF 2: POSIX ADVISORY WRITE 20975 00:1f:9016783 0 EOF 3: POSIX ADVISORY WRITE 20975 00:1f:9016653 0 EOF 4: POSIX ADVISORY WRITE 20975 00:1f:9016501 0 EOF 5: POSIX ADVISORY WRITE 20975 00:1f:9016442 0 EOF 6: POSIX ADVISORY WRITE 20975 00:1f:9016361 0 EOF 7: POSIX ADVISORY WRITE 20975 00:1f:9016114 0 EOF 8: POSIX ADVISORY WRITE 20975 00:1f:9016042 0 EOF 9: POSIX ADVISORY WRITE 20975 00:1f:9015963 0 EOF 10: POSIX ADVISORY WRITE 20975 00:1f:28255 0 EOF /proc/mdstat：保存RAID相关的多块磁盘的当前状态信息，在没有使用RAID机器上，其显示unused devices: ： Personalities : [linear] [raid0] [raid1] [raid10] [raid6] [raid5] [raid4] md4 : active raid1 sdf5[0] 483555456 blocks super 1.2 [1/1] [U] md2 : active raid1 sdb3[0] 10094080 blocks super 1.2 [1/1] [U] md3 : active raid1 sde3[0] 971940544 blocks super 1.2 [1/1] [U] md1 : active raid1 sdb2[0] sde2[1] sdf2[2] 2097088 blocks [16/3] [UUU_____________] md0 : active raid1 sdb1[0] sde1[1] sdf1[2] 2490176 blocks [16/3] [UUU_____________] unused devices: /proc/meminfo：系统中关于当前内存的利用状况等的信息，常由free命令使用；可以使用文件查看命令直接读取此文件，其内容显示为两列，前者为统计属性，后者为对应的值； MemTotal: 8058608 kB MemFree: 138636 kB MemAvailable: 484704 kB Buffers: 6000 kB Cached: 590724 kB SwapCached: 68220 kB Active: 4024568 kB Inactive: 3306388 kB Active(anon): 3719356 kB Inactive(anon): 3106256 kB Active(file): 305212 kB Inactive(file): 200132 kB Unevictable: 2528 kB Mlocked: 2528 kB SwapTotal: 6930348 kB SwapFree: 6119948 kB Dirty: 624 kB Writeback: 0 kB AnonPages: 6706328 kB Mapped: 223620 kB Shmem: 91104 kB Slab: 208120 kB SReclaimable: 113072 kB SUnreclaim: 95048 kB KernelStack: 28144 kB PageTables: 81552 kB NFS_Unstable: 0 kB Bounce: 0 kB WritebackTmp: 0 kB CommitLimit: 10959652 kB Committed_AS: 21421636 kB VmallocTotal: 34359738367 kB VmallocUsed: 0 kB VmallocChunk: 0 kB DirectMap4k: 14124 kB DirectMap2M: 8263680 kB /proc/mounts：在内核2.4.29版本以前，此文件的内容为系统当前挂载的所有文件系统，在2.4.19以后的内核中引进了每个进程使用独立挂载名称空间的方式，此文件则随之变成了指向/proc/self/mounts（每个进程自身挂载名称空间中的所有挂载点列表）文件的符号链接；/proc/self是一个独特的目录。如下所示，其中第一列表示挂载的设备，第二列表示在当前目录树中的挂载点，第三点表示当前文件系统的类型，第四列表示挂载属性（ro或者rw），第五列和第六列用来匹配/etc/mtab文件中的转储（dump）属性； /dev/md0 / ext4 rw,relatime,barrier,data=ordered 0 0 none /dev devtmpfs rw,nosuid,noexec,relatime,size=4016084k,nr_inodes=1004021,mode=755 0 0 none /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0 none /proc proc rw,nosuid,nodev,noexec,relatime 0 0 none /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0 /tmp /tmp tmpfs rw,relatime 0 0 /run /run tmpfs rw,nosuid,nodev,relatime,mode=755 0 0 /dev/shm /dev/shm tmpfs rw,nosuid,nodev,relatime 0 0 none /sys/fs/cgroup tmpfs rw,relatime,size=4k,mode=755 0 0 cgmfs /run/cgmanager/fs tmpfs rw,relatime,size=100k,mode=755 0 0 cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset,release_agent=/run/cgmanager/agents/cgm-release-agent.cpuset,clone_children 0 0 cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu,release_agent=/run/cgmanager/agents/cgm-release-agent.cpu 0 0 cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct,release_agent=/run/cgmanager/agents/cgm-release-agent.cpuacct 0 0 cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio,release_agent=/run/cgmanager/agents/cgm-release-agent.blkio 0 0 cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory,release_agent=/run/cgmanager/agents/cgm-release-agent.memory 0 0 cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices,release_agent=/run/cgmanager/agents/cgm-release-agent.devices 0 0 cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer,release_agent=/run/cgmanager/agents/cgm-release-agent.freezer 0 0 none /proc/bus/usb devtmpfs rw,nosuid,noexec,relatime,size=4016084k,nr_inodes=1004021,mode=755 0 0 none /sys/kernel/debug debugfs rw,relatime 0 0 securityfs /sys/kernel/security securityfs rw,relatime 0 0 /dev/md2 /volume1 btrfs rw,relatime,synoacl,space_cache=v2,auto_reclaim_space,metadata_ratio=50,subvolid=257,subvol=/@syno 0 0 /dev/vg1000/lv /volume3 btrfs rw,relatime,relatime_period=30,synoacl,space_cache=v2,auto_reclaim_space,metadata_ratio=50,subvolid=257,subvol=/@syno 0 0 /proc/modules：当前装入内核的所有模块名称列表，可以由lsmod命令使用，也可以直接查看；如下所示，其中第一列表示模块名，第二列表示此模块占用内存空间大小，第三列表示此模块有多少实例被装入，第四列表示此模块依赖于其它哪些模块，第五列表示此模块的装载状态（Live：已经装入；Loading：正在装入；Unloading：正在卸载），第六列表示此模块在内核内存（kernel memory）中的偏移量； xt_ipvs 2202 0 - Live 0xffffffffa0fe0000 ip_vs_rr 1447 0 - Live 0xffffffffa0fdc000 ip_vs 127371 3 xt_ipvs,ip_vs_rr, Live 0xffffffffa0fb3000 xt_mark 1317 0 - Live 0xffffffffa0faf000 iptable_mangle 1656 0 - Live 0xffffffffa0fab000 br_netfilter 13589 0 - Live 0xffffffffa0fa3000 bridge 55340 1 br_netfilter, Live 0xffffffffa0f8d000 stp 1693 1 bridge, Live 0xffffffffa0f89000 aufs 194415 0 - Live 0xffffffffa0f4d000 macvlan 13776 0 - Live 0xffffffffa0f45000 veth 5094 0 - Live 0xffffffffa0f40000 xt_conntrack 3401 5 - Live 0xffffffffa0f3c000 xt_addrtype 2893 1 - Live 0xffffffffa0f38000 ipt_MASQUERADE 1213 25 - Live 0xffffffffa0f34000 xt_REDIRECT 1486 0 - Live 0xffffffffa0f30000 /proc/partitions：块设备每个分区的主设备号（major）和次设备号（minor）等信息，同时包括每个分区所包含的块（block）数目 major minor #blocks name 1 0 655360 ram0 1 1 655360 ram1 1 2 655360 ram2 1 3 655360 ram3 1 4 655360 ram4 1 5 655360 ram5 1 6 655360 ram6 1 7 655360 ram7 1 8 655360 ram8 1 9 655360 ram9 1 10 655360 ram10 1 11 655360 ram11 1 12 655360 ram12 1 13 655360 ram13 1 14 655360 ram14 1 15 655360 ram15 8 16 15638616 sdb 8 17 2490240 sdb1 8 18 2097152 sdb2 8 19 10095104 sdb3 8 20 65536 sdb4 8 21 64512 sdb5 /proc/slabinfo :在内核中频繁使用的对象（如inode、dentry等）都有自己的cache，即slab pool，而/proc/slabinfo文件列出了这些对象相关slap的信息 /proc/stat：实时追踪自系统上次启动以来的多种统计信息； “cpu”行后的八个值分别表示以1/100（jiffies）秒为单位的统计值（包括系统运行于用户模式、低优先级用户模式，运系统模式、空闲模式、I/O等待模式的时间等）； “intr”行给出中断的信息，第一个为自系统启动以来，发生的所有的中断的次数；然后每个数对应一个特定的中断自系统启动以来所发生的次数； “ctxt”给出了自系统启动以来CPU发生的上下文交换的次数。 “btime”给出了从系统启动到现在为止的时间，单位为秒； “processes (total_forks) 自系统启动以来所创建的任务的个数； “procs_running”：当前运行队列的任务的数目； “procs_blocked”：当前被阻塞的任务的数目； cpu 223139 48 44015 1586826 69922 0 2477 0 0 0 cpu0 56431 23 11065 395817 17523 0 649 0 0 0 cpu1 54109 10 11450 397915 17649 0 596 0 0 0 cpu2 55572 10 10954 397559 16719 0 695 0 0 0 cpu3 57026 3 10544 395533 18031 0 535 0 0 0 intr 12065678 11 0 0 0 206 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 31886 66307 34 1 32453 2413 5989 32388 166 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ctxt 20744413 btime 1612399810 processes 37878 procs_running 1 procs_blocked 0 softirq 10803666 2 4776615 1052 818173 113621 0 2279 1548289 0 3543635 /proc/swaps：当前系统上的交换分区及其空间利用信息，如果有多个交换分区的话，则会每个交换分区的信息分别存储于/proc/swap目录中的单独文件中，而其优先级数字越低，被使用到的可能性越大 Filename Type Size Used Priority /dev/md1 partition 2097084 0 -1 /dev/zram0 partition 1208316 207996 1 /dev/zram1 partition 1208316 207660 1 /dev/zram2 partition 1208316 207612 1 /dev/zram3 partition 1208316 207976 1 /proc/uptime：系统上次启动以来的运行时间，如下所示，其第一个数字表示系统运行时间，第二个数字表示系统空闲时间，单位是秒； 4578.13 14964.34 /proc/version：当前系统运行的内核版本号 Linux version 4.4.59+ (root@build3) (gcc version 4.9.3 20150311 (prerelease) (crosstool-NG 1.20.0) ) #24922 SMP PREEMPT Thu Mar 28 11:07:03 CST 2019 /proc/vmstat：当前系统虚拟内存的多种统计数据，信息量可能会比较大，这因系统而有所不同，可读性较好（2.6以后的内核支持此文件） nr_anon_pages 22270 nr_mapped 8542 nr_file_pages 47706 nr_slab 4720 nr_page_table_pages 897 nr_dirty 21 nr_writeback 0 ………… /proc/zoneinfo：内存区域（zone）的详细信息列表 Node 0, zone DMA pages free 1208 min 28 low 35 high 42 active 439 inactive 1139 scanned 0 (a: 7 i: 30) spanned 4096 present 4096 nr_anon_pages 192 nr_mapped 141 nr_file_pages 1385 nr_slab 253 nr_page_table_pages 2 nr_dirty 523 nr_writeback 0 nr_unstable 0 nr_bounce 0 protection: (0, 0, 296, 296) pagesets all_unreclaimable: 0 prev_priority: 12 start_pfn: 0 ………… 参考 http://bbs.chinaunix.net/thread-2002769-1-1.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/7zip.html":{"url":"origin/7zip.html","title":"7Zip解压缩","keywords":"","body":"一、简介 二、安装 cygwin apt-cyg install p7zip 三、压缩命令参数 Windows下 7z.exe a -t7z C:\\\\Users\\\\Maker\\\\Desktop\\\\test.7z \\ -xr\\!.git \\ -xr\\!logs \\ -mmt \\ C:\\\\Users\\\\Maker\\\\Desktop\\\\test1 \\ C:\\\\Users\\\\Maker\\\\Desktopp\\\\test2 > null # a 压缩 # -xr\\!.git 排除.git文件夹 # -xr\\!logs 排除logs文件夹 # -mnt 使用多线程 # -t7z 指定压缩格式，选项：zip、7z（默认）、rar、cab、gzip、bzip2、tar 或其它格式。 # > null 静默输出压缩过程 参考： https://www.cnblogs.com/qanholas/archive/2011/10/03/2198487.html https://superuser.com/questions/97342/7zip-command-line-exclude-folders-by-wildcard-pattern https://stackoverflow.com/questions/3774278/extracting-a-7-zip-file-silently-command-line-option https://www.shuzhiduo.com/A/kvJ3waMOJg/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-04-11 22:03:04 "},"origin/linux-io-monitor.html":{"url":"origin/linux-io-monitor.html","title":"磁盘I/O监控","keywords":"","body":"Linux 磁盘 IO 监控 一、简介 二、iostat 三、iotop 四、lsof 五、pidstat 参考：https://jaminzhang.github.io/os/Linux-IO-Monitoring-and-Deep-Analysis/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-05-09 10:55:09 "},"origin/linux-ps.html":{"url":"origin/linux-ps.html","title":"ps","keywords":"","body":"Linux ps 一、简介 在Linux系统中，ps(process status)命令常常用来捕获系统在某一时间的进程状态 ps命令支持的语法格式： UNIX 风格，选项可以组合在一起，并且选项前必须有“-”连字符 BSD 风格，选项可以组合在一起，但是选项前不能有“-”连字符 GNU 风格的长选项，选项前有两个“-”连字符 Linux进程状态: 状态码 进程状态 含义 R 运行（runnable (on run queue)） 正在运行或在运行队列中等待 S 中断（sleeping） 休眠中, 受阻, 在等待某个条件的形成或接受到信号 D 不可中断（uninterruptible sleep ）(usually IO) 收到信号不唤醒和不可运行, 进程必须等待直到有中断发生 Z 僵死（a defunct (”zombie”) process） 进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放 T 停止（traced or stopped） 进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行 显示进程信息格式 ps aux最初用到Unix Style中，而ps -ef被用在System V Style中，两者输出略有不同。现在的大部分Linux系统都是可以同时使用这两种方式的 au(x)输出格式 $ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 194364 7256 ? Ss 09:27 0:25 /usr/lib/systemd/systemd .... USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND 用户名 进程ID 进程占用的CPU百分比 占用CPU百分比 该进程使用的虚拟內存量（KB） 该进程占用的固定內存量（KB）（驻留中页的数量） 进程在那个终端上运行。若与终端无关，则显示? 若为pts/0等，则表示由网络连接主机进程。 进程的状态 进程启动时间 该进程实际使用CPU运行的时间 所执行的命令及参数 ps -ef $ ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 09:27 ? 00:00:25 /usr/lib/systemd/systemd .... UID PID PPID C STIME TTY CMD 用户ID 进程ID 父进程ID 占用CPU百分比 进程启动时间 进程在那个终端上运行。若与终端无关，则显示? 若为pts/0等，则表示由网络连接主机进程。 所执行的命令及参数 STAT状态位常见的状态字符 状态字符 含义 D 无法中断的休眠状态（通常 IO 的进程） R 正在运行可中在队列中可过行的 S 处于休眠状态 T 停止或被追踪 X 死掉的进程 （基本很少见） Z 僵尸进程 优先级高的进程 N 优先级较低的进程 L 有些页被锁进内存 s 进程的领导者（在它之下有子进程） l 多线程，克隆线程（使用 CLONE_THREAD, 类似 NPTL pthreads） + 位于后台的进程组 二、命令参数 ps [options] [--help] -a 显示所有终端机下执行的进程，除了阶段作业领导者之外。 a 显示现行终端机下的所有进程，包括其他用户的进程。 -A 显示所有进程。 -c 显示CLS和PRI栏位。 c 列出进程时，显示每个进程真正的指令名称，而不包含路径，参数或常驻服务的标示。 -C 　指定执行指令的名称，并列出该指令的进程的状况。 -d 　显示所有进程，但不包括阶段作业领导者的进程。 -e 　此参数的效果和指定\"A\"参数相同。 e 　列出进程时，显示每个进程所使用的环境变量。 -f 　显示UID,PPIP,C与STIME栏位。 f 　用ASCII字符显示树状结构，表达进程间的相互关系。 -g 　此参数的效果和指定\"-G\"参数相同，当亦能使用阶段作业领导者的名称来指定。 g 　显示现行终端机下的所有进程，包括群组领导者的进程。 -G 　列出属于该群组的进程的状况，也可使用群组名称来指定。 h 　不显示标题列。 -H 　显示树状结构，表示进程间的相互关系。 -j或j 　采用工作控制的格式显示进程状况。 -l或l 　采用详细的格式来显示进程状况。 L 　列出栏位的相关信息。 -m或m 　显示所有的执行绪。 n 　以数字来表示USER和WCHAN栏位。 -N 　显示所有的进程，除了执行ps指令终端机下的进程之外。 -p 　指定进程识别码，并列出该进程的状况。 p 　此参数的效果和指定\"-p\"参数相同，只在列表格式方面稍有差异。 r 　只列出现行终端机正在执行中的进程。 -s 　指定阶段作业的进程识别码，并列出隶属该阶段作业的进程的状况。 s 　采用进程信号的格式显示进程状况。 S 　列出进程时，包括已中断的子进程资料。 -t 　指定终端机编号，并列出属于该终端机的进程的状况。 t 　此参数的效果和指定\"-t\"参数相同，只在列表格式方面稍有差异。 -T 　显示现行终端机下的所有进程。 -u 　此参数的效果和指定\"-U\"参数相同。 u 　以用户为主的格式来显示进程状况。 -U 　列出属于该用户的进程的状况，也可使用用户名称来指定。 U 　列出属于该用户的进程的状况。 v 　采用虚拟内存的格式显示进程状况。 -V或V 　显示版本信息。 -w或w 　采用宽阔的格式来显示进程状况。　 x 　显示所有进程，不以终端机来区分。 X 　采用旧式的Linux i386登陆格式显示进程状况。 -y 配合参数\"-l\"使用时，不显示F(flag)栏位，并以RSS栏位取代ADDR栏位。 三、示例 1. 查看CPU占用最多的前10个进程 ps auxw|head -1;ps auxw|sort -rn -k3|head -10 2.显示所有当前进程 ps -ax -a参数，-a 代表 all。 -x参数会显示没有控制终端的进程 3. 查看虚拟内存使用最多的前10个进程 ps auxw | head -1 ; ps auxw|sort -rn -k5|head -10 4. 根据用户名过滤进程 ps -u root $ ps -u root PID TTY TIME CMD 1 ? 00:00:25 systemd 2 ? 00:00:00 kthreadd 3 ? 00:00:07 ksoftirqd/0 6 ? 00:00:02 kworker/u40:0 8 ? 00:00:05 migration/0 5. 根据内存使用来升序排序 ps -aux --sort -pmem USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND 1000130+ 13437 16.7 9.2 13303016 5703272 ? Ssl 09:29 131:50 /usr/lib/jvm/jre/bin/java .... 1000540+ 25444 8.7 4.1 9575924 2533824 ? Ssl 09:30 68:56 /usr/share/elasticsearch/ ..... 6. 根据 CPU 使用来升序排序 ps -aux --sort -pcpu $ USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 7602 48.8 0.3 4883792 231504 ? Ssl 09:29 385:36 /usr/bin/hyperkube kubelet .... root 3282 25.0 1.4 4151700 895280 ? Ssl 09:28 198:12 openshift start master api 7. 格式化输出 指定用户（真实或有效的UID）创建的进程 ps -U pid_user -u pid_user u -U 参数按真实用户 ID(RUID) 筛选进程，它会从用户列表中选择真实用户名或 ID。真实用户即实际创建该进程的用户 -u 参数用来筛选有效用户 ID（EUID） u 参数用来决定以针对用户的格式输出，由 User, PID, %CPU, %MEM, VSZ, RSS, TTY, STAT, START, TIME 和 COMMAND这几列组成 $ pid_user=ceph && ps -U $pid_user -u $pid_user u USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND ceph 1474 0.2 0.0 398604 40772 ? Ssl 09:27 2:10 /usr/bin/ceph-mon -f --cluster ceph --id ... ceph 1478 0.0 0.0 393520 34540 ? Ssl 09:27 0:21 /usr/bin/ceph-mds -f --cluster ceph --id ... ceph 1845 0.1 0.3 1104056 219284 ? Ssl 09:27 0:59 /usr/bin/ceph-osd -f --cluster ceph --id ... 参考 https://blog.csdn.net/ShiXueTanLang/article/details/80781089 https://www.cnblogs.com/jiqing9006/p/10036676.html https://www.cnblogs.com/xiangtingshen/p/10920236.html http://www.sohu.com/a/238434503_100180425 https://www.cnblogs.com/fps2tao/p/10313337.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/iperf.html":{"url":"origin/iperf.html","title":"iperf","keywords":"","body":"网络工具 iperf 一、简介 二、安装 apt install -y iperf iperf3 yum install -y iperf iperf3 三、命令详解 四、实际应用 1、网络测速 在要测试机上启动一个 iperf3 服务器(192.168.1.7)，等待来自客户端的连接。 iperf3 -s -p 111 在另一台计算机上作为客户端，运行以下命令，客户端将开始向服务器发送数据，同时测量网络的带宽和其他性能指标。 iperf3 -c -p 111 Server listening on TCP port 111 TCP window size: 85.3 KByte (default) [ 4] local 192.168.1.7 port 111 connected with 192.168.1.35 port 54716 (peer 2.1.5) [ ID] Interval Transfer Bandwidth [ 4] 0.0-10.0 sec 1.10 GBytes 941 Mbits/sec 知识回顾之二进制： 1 Bytes = 8 bits 1 KB(Kilobyte) = 1024 Bytes 1 MB(Megabyte) = 1024 *1024 Bytes Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:47:03 "},"origin/network-tcpdump.html":{"url":"origin/network-tcpdump.html","title":"tcpdump网络抓包","keywords":"","body":"Tcpdump 一、简介 tcpdump就是：dump the traffic on a network，根据使用者的定义对网络上的数据包进行截获的包分析工具。 tcpdump是一个用于截取网络分组，并输出分组内容的工具。凭借强大的功能和灵活的截取策略，使其成为类UNIX系统下用于网络分析和问题排查的首选工具 tcpdump 支持针对网络层、协议、主机、网络或端口的过滤，并提供and、or、not等逻辑语句来帮助你去掉无用的信息 tcpdump需要有root权限。 tcpdump使用了独立于系统的libpcap的接口。libpcap是linux平台下的网络数据包捕获函数包 tcpdump调用libpcap的接口在linux系统链路层抓包。而linux本身指定的许多访问控制规则都是基于三层或三层以上的过滤规则，所以tcpdump可以抓取过滤规则之前的数据包。 tcpdump可以使用带-w参数的tcpdump 截获数据并保存到文件中，然后再使用Wireshark进行解码分析 tcpdump工作在网卡的混杂模式 二、使用 1、命令格式 tcpdump [-aAbdDefhHIJKlLnNOpqStuUvxX#] [ -B size ] [ -c count ] [ -C file_size ] [ -E algo:secret ] [ -F file ] [ -G seconds ] [ -i interface ] [ -j tstamptype ] [ -M secret ] [ --number ] [ -Q in|out|inout ] [ -r file ] [ -s snaplen ] [ --time-stamp-precision precision ] [ --immediate-mode ] [ -T type ] [ --version ] [ -V file ] [ -w file ] [ -W filecount ] [ -y datalinktype ] [ -z postrotate-command ] [ -Z user ] [ expression ] 2、命令选项 ①抓包选项 -A: 以ASCII码方式显示每一个数据包(不会显示数据包中链路层头部信息). 在抓取包含网页数据的数据包时, 可方便查看数据(nt: 即Handy for capturing web pages). -C file-size: (nt: 此选项用于配合-w file 选项使用) 该选项使得tcpdump 在把原始数据包直接保存到文件中之前, 检查此文件大小是否超过file-size. 如果超过了, 将关闭此文件,另创一个文件继续用于原始数据包的记录. 新创建的文件名与-w 选项指定的文件名一致, 但文件名后多了一个数字.该数字会从1开始随着新创建文件的增多而增加. file-size的单位是百万字节(nt: 这里指1,000,000个字节,并非1,048,576个字节, 后者是以1024字节为1k, 1024k字节为1M计算所得, 即1M=1024 ＊ 1024 ＝ 1,048,576) -c：指定要抓取的包数量。 -d : 以容易阅读的形式,在标准输出上打印出编排过的包匹配码, 随后tcpdump停止.(nt | rt: human readable, 容易阅读的,通常是指以ascii码来打印一些信息. compiled, 编排过的. packet-matching code, 包匹配码,含义未知, 需补充) -dd: 以C语言的形式打印出包匹配码. -ddd: 以十进制数的形式打印出包匹配码(会在包匹配码之前有一个附加的'count'前缀). -i interface：指定tcpdump需要监听的接口。默认会抓取第一个网络接口 -N: 不列出域名 -n：对地址以数字方式显式，否则显式为主机名，也就是说-n选项不做主机名解析。 -nn：除了-n的作用外，还把端口显示为数值，否则显示端口服务名。 -P：指定要抓取的包是流入还是流出的包。可以给定的值为in、out和inout，默认为\"inout\"。 -p: 不让网络界面进入混杂模式 -O: 不将数据包编码最佳化 -s len：设置tcpdump的数据包抓取长度为len。 ​ 如果不设置默认将会是65535字节。对于要抓取的数据包较大时，长度设置不够可能会产生包截断，若出现包截断，输出行中会出现\"[|proto]\"的标志(proto实际会显示为协议名)。但是抓取len越长，包的处理时间越长，并且会减少tcpdump可缓存的数据包的数量，从而会导致数据包的丢失，所以在能抓取我们想要的包的前提下，抓取长度越小越好。 -S： 打印TCP 数据包的顺序号时, 使用绝对的顺序号, 而不是相对的顺序号. ​ 相对顺序号可理解为, 相对第一个TCP 包顺序号的差距,比如, 接受方收到第一个数据包的绝对顺序号为232323, 对于后来接收到的第2个,第3个数据包, tcpdump会打印其序列号为1, 2分别表示与第一个数据包的差距为1 和 2. 而如果此时-S 选项被设置, 对于后来接收到的第2个, 第3个数据包会打印出其绝对顺序号:232324, 232325 -y datalinktype：设置tcpdump 只捕获数据链路层协议类型是datalinktype的数据包 ②输出选项 -e：输出的每行中都将包括数据链路层头部信息，例如源MAC和目标MAC。 -q：快速打印输出。即打印很少的协议相关信息，从而输出行都比较简短。 -t：在每行输出中不打印时间戳 -tt：不对每行输出的时间进行格式处理(nt: 这种格式一眼可能看不出其含义, 如时间戳打印成1261798315) -ttt：tcpdump 输出时, 每两行打印之间会延迟一个段时间(以毫秒为单位) -tttt：在每行打印的时间戳之前添加日期的打印 -X：输出包的头部数据，会以16进制和ASCII两种方式同时输出。 -XX：输出包的头部数据，会以16进制和ASCII两种方式同时输出，更详细。 -v：当分析和打印的时候，产生详细的输出。 -vv：产生比-v更详细的输出。 -vvv：产生比-vv更详细的输出。 ③其他功能性选项 -D：列出可用于抓包的接口。将会列出接口的数值编号和接口名，它们都可以用于\"-i\"后。 -F：从文件中读取抓包的表达式。若使用该选项，则命令行中给定的其他表达式都将失效。 -w：将抓包数据输出到文件中而不是标准输出。可以同时配合”-G time\"选项使得输出文件每time秒就自动切换到另一个文件。可通过\"-r\"选项载入这些文件以进行分析和打印。 -r：从给定的数据包文件中读取数据。使用\"-\"表示从标准输入中读取 3、条件表达式定义过滤规则 表达式用于决定哪些数据包将被过滤打印. 如果不给定条件表达式, 网络上所有被捕获的包都会被打印,否则, 只有满足条件表达式的数据包被打印 表达式格式： tcpdump 命令选项 proto dir type proto(protocol协议类型的过滤器)：根据协议进行过滤 tcp： udp： icmp： ether： fddi： tr： wlan： ip： ip6： arp： rarp： decnet： 若未给定协议类型，则匹配所有可能的类型 dir（direction数据流向类型的过滤器）：根据数据流向进行过滤 src： dst： src or dst： src and dst： type（关键词类型的过滤器）： host： net： port： portrange： ether： gateway： 例如：host 192.168.201.128 , net 128.3, port 20, portrange 6000-6008’ 4、过滤规则的组合 表达式单元之间可以使用操作符\" and / && / or / || / not / ! \"进行连接，从而组成复杂的条件表达式。 and：所有的条件都需要满足，也可以表示为 && or：只要有一个条件满足就可以，也可以表示为 || not：取反，也可以使用 ! 而在单个过滤器里，常常会判断一条件是否成立，这时候，就要使用下面两个符号 =：判断二者相等 ==：判断二者相等 !=：判断二者不相等 使用括号\"()\"可以改变表达式的优先级，但需要注意的是括号会被shell解释，所以应该使用反斜线\"\"转义为\"()\"，在需要的时候，还需要包围在引号中。 当你使用这两个符号时，tcpdump 还提供了一些关键字的接口来方便我们进行判断，比如 if：表示网卡接口名 proc：表示进程名 pid：表示进程id svc：表示 service class dir：表示方向，in 和 out eproc：表示 effective process name epid：表示 effective process ID 三、抓包解读 输出格式 $ tcpdump -S -e -nn -i eth0 dst 192.168.1.7 and port 5000 06:24:16.887580 dc:a6:32:c4:88:d2 > 00:11:32:11:22:33, ethertype IPv4 (0x0800), length 338: 192.168.1.18.54832 > 192.168.1.7.5000: Flags [P.], seq 2391754816:2391755088, ack 1379501343, win 2051, options [nop,nop,TS val 238444658 ecr 19745762], length 272 第一列：时分秒毫秒 21:26:49.013621 第二列：源MAC地址 第三列：目标MAC地址 第四列：网络协议及其长度 第五列：源IP地址.端口号 第六列：目标IP地址.端口号 第七列：数据包内容，包括Flags 标识符，seq 号，ack 号，win 窗口，数据长度 length，其中 [P.] 表示 PUSH 标志位为 1， Flags 标识符 [S] : SYN（发起连接标志） [P] : PSH（传送数据标志） [F] : FIN （关闭连接标志） [R] : RST（重置连接） [.] : 没有 Flag，由于除了 SYN 包外所有的数据包都有ACK，所以一般这个标志也可表示 ACK 组合表示 [S.]: SYN同步标识，以及确认S的ACK [P.]: 推送数据, 和 ACK [R.]: RST, 连接重置 [F.]: FIN 结束连接 [DF]: Don't Fragment（不要碎裂），当DF=0时，表示允许分片，一般-v时才有这个标识 [FP.]: 标记FIN、PUSH、ACK组合，这样做是为了提升网络效率，减少数据来回确认等 四、示例 1、监听多个端口、多个网卡 # 监听多个端口 tcpdump port 22 or port 23 or port 24 # 监听所有网卡 tcpdump -i any port 22 or port 23 2、找出一段时间内发包数最多的 IP $ tcpdump -nnn -t -c 200 | cut -f 1,2,3,4 -d '.' | sort | uniq -c | sort -nr | head -n 20 200 packets captured 244 packets received by filter 0 packets dropped by kernel 103 IP 192.168.1.18 72 IP 220.64.11.92 25 IP 192.168.1.7 3、抓取 HTTP GET 请求 $ tcpdump -s 0 -A -vv 'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x47455420' $ tcpdump -vv -A -l -i6 | grep 'GET' 4、抓取 HTTP POST请求 $ tcpdump -s 0 -A -vv 'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x504f5354' $ tcpdump -vv -A -l -i6 | grep 'POST' 5、抓取 HTTP请求中的 User-Agent和Host $ tcpdump -nn -A -i6 -l | egrep -i 'User-Agent:|Host:' 6、抓取 HTTP请求中的主机名和路径 $ tcpdump -i6 -v -n -l | egrep -i \"POST /|GET /|Host:\" .Ke..B]'POST /webapi/entry.cgi HTTP/1.1 Host: 192.168.1.7:5000 7、抓取 HTTP 有效数据包 抓取 80 端口的 HTTP 有效数据包，排除 TCP 连接建立过程的数据包（SYN / FIN / ACK） $ tcpdump 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)>2)) != 0)' 参考 https://baijiahao.baidu.com/s?id=1671144485218215170&wfr=spider&for=pc https://www.jianshu.com/p/d9162722f189 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-04-09 16:23:24 "},"origin/nc.html":{"url":"origin/nc.html","title":"nc","keywords":"","body":"nc命令详解 一、简介 nc是netcat的简写，有着网络界的瑞士军刀美誉。因为它短小精悍、功能实用，被设计为一个简单、可靠的网络工具 二、语法参数详解 nc [-hlnruz] [-g] [-G] [-i] [-o] [-p] [-s] [-v...] [-w] [主机名称] [通信端口...] 参数说明 -g 设置路由器跃程通信网关，最多可设置8个。 -G 设置来源路由指向器，其数值为4的倍数。 -h 在线帮助。 -i 设置时间间隔，以便传送信息及扫描通信端口。 -l 用于指定nc将处于侦听模式。指定该参数，则意味着nc被当作server，侦听并接受连接，而非向其它地址发起连接。 -n 直接使用IP地址，而不通过域名服务器。 -o 指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存。 -p 设置本地主机使用的通信端口。 -r 乱数指定本地与远端主机的通信端口。 -s 设置本地主机送出数据包的IP地址。 -u 使用UDP传输协议。 -v 输出交互或出错信 -w 设置等待连线的时间。 -z 使用0输入/输出模式，只在扫描通信端口时使用。 三、常用操作 1、机器之间传输文件 首先，在A机上监听指定端口，并将该端口得到的数据重定向到文件 nc -lp 9999 > test.txt 然后，在B机上向该端口写入文件 nc 9999 唯一的缺点是没有进度显示，因此你并不能准确的知道是否已经完成。 2、机器之间网络测速 本地监听一个端口用于接收请求，把请求信息转到到空洞文件中，减少输出 nc -l 8888 >/dev/null 发送测试数据，把无限个0发送给到8888端口 nc 192.168.1.3 8888 然后可以使用dstat查看当前网速 3、TCP端口扫描 # 扫描指定TCP范围端口 nc -v -z -w2 192.168.1.3 1-100 # 扫描指定UDP范围端口 nc -u -z -w2 192.168.1.3 1-1000 # 扫描指定端口 nc -nvv 192.168.1.3 80 参考： https://blog.csdn.net/chy555chy/article/details/118397479 https://blog.csdn.net/yexiangCSDN/article/details/82864469 https://www.runoob.com/linux/linux-comm-nc.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-31 16:17:24 "},"origin/linux-hping.html":{"url":"origin/linux-hping.html","title":"hping","keywords":"","body":"网络分析工具hping 一、简介 官方网站：http://www.hping.org/ GitHub：https://github.com/antirez/hping 二、安装 包管理器 Brew brew install hping # hping二进制可执行命令安装/usr/local/sbin,不是在/usr/local/bin。要将/usr/local/sbin加到PATH中 APT apt install hping3 YUM yum install hping3 APK apk add hping3 源码安装 源码安装文档：https://github.com/antirez/hping/blob/master/INSTALL 所需依赖：Libpcap、Tcl/Tk yum install libpcap-devel tc-devel ln -s /usr/include/pcap-bpf.h /usr/include/net/bpf.h wget http://www.hping.org/hping3-20051105.tar.gz tar zxvf hping3-20051105.tar.gz cd hping3-20051105 ./configure make make install 三、命令参数 hping3 host [options] 参数 -H --help 显示帮助。 -v -VERSION 版本信息。 -c --count count 发送数据包的次数 关于countreached_timeout 可以在hping2.h里编辑。 -i --interval 包发送间隔时间（单位是毫秒）缺省时间是1秒,此功能在增加传输率上很重要,在idle/spoofing扫描时此功能也会被用到,你可以参考hping-howto获得更多信息-fast 每秒发10个数据包。 -n -nmeric 数字输出，象征性输出主机地址。 -q -quiet 退出。 -I --interface interface-name 无非就是eth0之类的参数。 -v --verbose 显示很多信息，TCP回应一般如：len=46 ip=192.168.1.1 flags=RADF seq=0 ttl=255 id=0 win=0 rtt=0.4ms tos=0 iplen=40 seq=0 ack=1380893504 sum=2010 urp=0 -D --debug 进入debug模式当你遇到麻烦时，比如用HPING遇到一些不合你习惯的时候，你可以用此模式修改HPING，（INTERFACE DETECTION,DATA LINK LAYER ACCESS,INTERFACE SETTINGS,.......） -z --bind 快捷键的使用。 -Z --unbind 消除快捷键。 -O --rawip RAWIP模式，在此模式下HPING会发送带数据的IP头。 -1 --icmp ICMP模式，此模式下HPING会发送IGMP应答报，你可以用--ICMPTYPE --ICMPCODE选项发送其他类型/模式的ICMP报文。 -2 --udp UDP模式，缺省下，HPING会发送UDP报文到主机的0端口，你可以用--baseport --destport --keep选项指定其模式。 -9 --listen signatuer hping的listen模式，用此模式，HPING会接收指定的数据。 -a --spoof hostname 伪造IP攻击，防火墙就不会记录你的真实IP了，当然回应的包你也接收不到了。 -t --ttl time to live 可以指定发出包的TTL值。 -H --ipproto 在RAW IP模式里选择IP协议。 -w --WINID UNIX ,WINDIWS的id回应不同的，这选项可以让你的ID回应和WINDOWS一样。 -r --rel 更改ID的，可以让ID曾递减输出，详见HPING-HOWTO。 -F --FRAG 更改包的FRAG，这可以测试对方对于包碎片的处理能力，缺省的“virtual mtu”是16字节。 -x --morefrag 此功能可以发送碎片使主机忙于恢复碎片而造成主机的拒绝服务。 -y -dontfrag 发送不可恢复的IP碎片，这可以让你了解更多的MTU PATH DISCOVERY。 -G --fragoff fragment offset value set the fragment offset -m --mtu mtu value 用此项后ID数值变得很大，50000没指定此项时3000-20000左右。 -G --rroute 记录路由，可以看到详悉的数据等等，最多可以经过9个路由，即使主机屏蔽了ICMP报文。 -C --ICMPTYPE type 指定ICMP类型，缺省是ICMP echo REQUEST。 -K --ICMPCODE CODE 指定ICMP代号，缺省0。 --icmp-ipver 把IP版本也插入IP头。 --icmp-iphlen 设置IP头的长度，缺省为5（32字节）。 --icmp-iplen 设置IP包长度。 --icmp-ipid 设置ICMP报文IP头的ID，缺省是RANDOM。 --icmp-ipproto 设置协议的，缺省是TCP。 -icmp-cksum 设置校验和。 -icmp-ts alias for --icmptype 13 (to send ICMP timestamp requests) --icmp-addr Alias for --icmptype 17 (to send ICMP address mask requests) -s --baseport source-port hping 用源端口猜测回应的包，它从一个基本端口计数，每收一个包，端口也加1，这规则你可以自己定义。 -p --deskport [+][+]deskport 设置目标端口，缺省为0，一个加号设置为:每发送一个请求包到达后，端口加1，两个加号为：每发一个包，端口数加1。 --keep 上面说过了。 -w --win 发的大小和windows一样大，64BYTE。 -O --tcpoff Set fake tcp data offset. Normal data offset is tcphdrlen / 4. -m --tcpseq 设置TCP序列数。 -l --tcpck 设置TCP ack。 -Q --seqnum 搜集序列号的，这对于你分析TCP序列号有很大作用。 四、示例 1、端口扫描 Hping3也可以对目标端口进行扫描。Hping3支持指定TCP各个标志位、长度等信息 hping3 -I eth0 -S 192.168.10.1 -p 80 hping3支持非常丰富的端口探测方式，nmap拥有的扫描方式hping3几乎都支持（除开connect方式，因为Hping3仅发送与接收包，不会维护连接，所以不支持connect方式探测）。而且Hping3能够对发送的探测进行更加精细的控制，方便用户微调探测结果。当然，Hping3的端口扫描性能及综合处理能力，无法与Nmap相比。一般使用它仅对少量主机的少量端口进行扫描。 2、Idle扫描 Idle扫描（Idle Scanning）是一种匿名扫描远程主机的方式，该方式也是有Hping3的作者Salvatore Sanfilippo发明的，目前Idle扫描在Nmap中也有实现。 该扫描原理是：寻找一台idle主机（该主机没有任何的网络流量，并且IPID是逐个增长的），攻击端主机先向idle主机发送探测包，从回复包中获取其IPID。冒充idle主机的IP地址向远程主机的端口发送SYN包（此处假设为SYN包），此时如果远程主机的目的端口开放，那么会回复SYN/ACK，此时idle主机收到SYN/ACK后回复RST包。然后攻击端主机再向idle主机发送探测包，获取其IPID。那么对比两次的IPID值，我们就可以判断远程主机是否回复了数据包，从而间接地推测其端口状态。 3、拒绝服务攻击 使用Hping3可以很方便构建拒绝服务攻击。比如对目标机发起大量SYN连接，伪造源地址为192.168.10.99，并使用1000微秒的间隔发送各个SYN包。 hping3 -I eth0 -a192.168.10.99 -S 192.168.10.33 -p 80 -i u1000 4、数据包跟踪 sudo hping3 --traceroute -V -1 www.baidu.com 参考 https://wangchujiang.com/linux-command/c/hping3.html https://mochazz.github.io/2017/07/23/hping3/#ICMP%E6%B5%8B%E8%AF%95 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/ipvsadm.html":{"url":"origin/ipvsadm.html","title":"ipvsadm","keywords":"","body":"ipvs管理工具ipvsadm List IPVS services ipvsadm -l -n Set up a new IPVS service ipvsadm -A -t 1.2.3.4:80 -s rr Change scheduling mode to Round Robin ipvsadm -E -t 1.2.3.4:80 -s rr Change scheduling mode to Weighted Round Robin ipvsadm -E -t 1.2.3.4:80 -s wrr Delete virtual service ipvsadm -d -t 1.2.3.4:80 -r 172.17.0.3 Delete real server ipvsadm -D -t 1.2.3.4:80 Show stats ipvsadm -L -n --stats --rate Clear the whole table ipvsadm -C Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-05-09 10:55:25 "},"origin/linux-iptables.html":{"url":"origin/linux-iptables.html","title":"iptables详解","keywords":"","body":"iptables 一、简介 iptables其实不是真正的防火墙，我们可以把它理解成一个客户端代理，用户通过iptables这个代理，将用户的安全设定执行到对应的netfilter安全框架\"中，netfilter位于内核空间。 自1995年ipfwadm开始进入1.2.1的核心，Linux的防火墙实现有很长的时间了。Ipfwadm实现了标准的tcp/ip包过滤功能，比如过滤源地址与目的地址以及端口过滤。早在1999年第一个稳定的2.2.0核心中防火墙的实现被ipchains替代了，ipchains的新功能包括支持规则链，碎片包控制，较好的网络地址翻译功能（NAT）以及其他一些有用的改进。我们需要明白Linux防火墙包括核心级代码（通常是可加载核心模块或者核心源程序的补丁）和用户级代码（一个配置的工具，比如/usr/bin/ipchains，这是用来插入包规则到核心空间的）因此无论如何，只要新的linux防火墙代码被引入，核心和用户空间的有关代码都要改写。 2001年2.4的核心完成了，iptables出现了。它引入了很多重要的改进，比如基于状态的防火墙，基于任何TCP标记和MAC地址的包过滤，更灵活的配置和记录功能，强大而且简单的NAT功能和透明代理功能，通过速度限制实现DoS的阻止。 然而，最重要变化是引入了模块化的架构方式。比如，ipchains和ipfwadm兼容模式是通过一个核心模块的设置实现的，该模块能够在运行的核心中插入，以便提供相应的通讯功能。在附加的变化中，用户自定义编码功能已经成为了可能，比如过滤一定范围的端口，根据TTL值和包的到达时间进行判断，对自定义的协议进行状态监视，对随机的数据包进行监视等，这些目前都还不是iptable的一部分，但是在未来将被实现。很多很有趣的新模块已经完成了。编写一个可加载的核心模块来创建核心级代码，通过用户级代码实现控制过滤器的行为。 二、基础知识 1、iptables和netfilter的关系 iptables其实是一个命令行工具，位于用户空间，我们用这个工具操作真正的框架netfilter。 netfilter/iptables（下文中简称为iptables）组成Linux平台下的包过滤防火墙，与大多数的Linux软件一样，这个包过滤防火墙是免费的，它可以代替昂贵的商业防火墙解决方案，完成封包过滤、封包重定向和网络地址转换（NAT）等功能。 2、iptables传输数据包的过程 ① 当一个数据包进入网卡时，它首先进入PREROUTING链，内核根据数据包目的IP判断是否是发往本机的（本机的任何一个接口IP地址），那么这个网络包将被认为是流向本机的。 ② 如果数据包就是进入本机的，它就会沿着图向下移动，到达INPUT链。数据包到了INPUT链后，任何进程都会收到它。本机上运行的程序可以发送数据包，这些数据包会经过OUTPUT链，然后到达POSTROUTING链输出。 ③ 如果数据包是要转发出去的，且内核允许转发，数据包就会流向FORWARD链进行处理（是否转发或拦截），然后到达POSTROUTING链（是否修改数据包的地 址等）进行处理。 报文的链流向： 到本机某进程的报文：PREROUTING --> INPUT 由本机转发的报文：PREROUTING --> FORWARD --> POSTROUTING 由本机的某进程发出报文（通常为响应报文）：OUTPUT --> POSTROUTING 报文到达链后匹配表的优先顺序： raw ==> mangle ==> nat ==> filter 3、链chains 链的作用：容纳各种防火墙规则 链的分类依据：处理数据包的不同时机 内置链： PREROUTING：在进行路由选择前处理数据包（做目标地址转换） INPUT：处理入站数据包 FORWARD：处理转发数据包 OUTPUT：处理出站数据包 POSTROUTING：在进行路由选择后处理数据包（对数据链进行源地址修改转换） 4、表tables filter表：负责过滤功能，防火墙；内核模块：iptables_filter nat表：网络地址转换功能；内核模块：iptable_nat mangle表：拆解报文，做出修改，并重新封装的功能；内核模块：iptable_mangle raw表：关闭nat表上启用的连接追踪机制；内核模块：iptable_raw 三、iptables规则 规则（rules）其实就是网络管理员预定义的条件 规则一般的定义为“指定什么样的数据包头符合什么的条件，就怎么样处理这个数据包” 规则的作用：对数据包进行过滤或处理 规则存储在内核空间的信息 包过滤表中，这些规则分别指定了源地址、目的地址、传输协议（如TCP、UDP、ICMP）和服务类型（如HTTP、FTP和SMTP）等。当数据包与规 则匹配时，iptables就根据规则所定义的方法来处理这些数据包，如放行（accept）、拒绝（reject）和丢弃（drop）等。 配置iptables防火墙的主要工作就是添加、修改和删除这些规则。 1、规则定义 ⓪语法格式 iptables [ -t 表名 ] 命令选项 ［链名］［匹配规则］［-j 处理动作］ # 表名 必须是 raw， nat，filter，mangle 中的一个。默认指filter表 # 不指定链名时，默认指表内的所有链 # 除非设置链的默认策略，否则必须指定匹配条件 # 命令选项、链名、处理动作使用大写字母，其余均为小写 ①命令选项 规则管理命令选项： -A：在指定链的末尾添加（append）一条新的规则 -D：删除（delete）指定链中的某一条规则，可以按规则序号和内容删除 -I ： 在指定链中插入（insert）一条新的规则，默认在第一行添加 -R：修改、替换（replace）指定链中的某一条规则，可以按规则序号和内容替换 查看命令选项： -L ：列出（list）指定链中所有的规则进行查看 --line-numbers：查看规则时，显示规则的序号 -n ：使用数字形式（numeric）显示输出结果 -v：以更详细的方式显示规则信息 链管理命令（这都是立即生效的）： -E ：重命名用户定义的链，不改变链本身 -F ：清空（flush） -N：新建（new-chain）一条用户自己定义的规则链 -X ：删除指定表中用户自定义的规则链（delete-chain） -P ：设置指定链的默认策略（policy） -Z ：将所有表的所有链的字节和数据包计数器清零 ②匹配规则 1) 通用匹配 协议匹配: -p [协议名] 地址匹配 -s [源地址] -d [目标地址] 接口匹配 -i [入站网卡] -o [出站网卡] 2) 隐含匹配 端口匹配 -sport [源端口] -dport [目标端口] TCP连接标记匹配：--tcp-flags [列表1：检查范围] [列表2：被设置的标记] 有两个参数列表，列表内部用逗号为分隔符，两个列表之间用空格分开 LIST1用作参数检查，LIST2用作参数匹配。 可用标志有： SYN( 同步; 表示开始会话请求 ), ACK（应答）, FIN（结束; 结束会话）， RST(复位;中断一个连接) PSH（推送; 数据包立即发送）， URG（紧急 ）， ALL（指选定所有的标记）， NONE（指未选定任何标记） iptables -A INPUT -p tcp –tcp-flags SYN,FIN,ACK SYN # 表示SYN、ACK、FIN的标志都检查，但是只有SYN匹配 iptables -A FROWARD -p tcp –tcp-flags ALL SYN,ACK # 表示ALL（SYN，ACK，FIN，RST，URG，PSH）的标志都检查，但是只有设置了SYN和ACK的匹配。 ICMP类型匹配：--icmp-type [ICMP类型] 3) 显式匹配(-m): iptables可使用额外的扩展模块进行显示条件匹配，详情参考第四章节 ③处理动作 动作也可以分为基本动作和扩展动作 ACCEPT：允许数据包通过。 DROP：直接丢弃数据包，不给任何回应信息，这时候客户端会感觉自己的请求泥牛入海了，过了超时时间才会有反应。 REJECT：拒绝数据包通过，必要时会给数据发送端一个响应的信息，客户端刚请求就会收到拒绝的信息。 SNAT：源地址转换，解决内网用户用同一个公网地址上网的问题。 MASQUERADE：地址伪装。是SNAT的一种特殊形式，适用于动态的、临时会变的ip上。 DNAT：目标地址转换。 REDIRECT：在本机做端口映射 MASK : 做防火墙标记 RETURN : 返回调用链 LOG：在/var/log/messages文件中记录日志信息，然后将数据包传递给下一条规则，也就是说除了记录以外不对数据包做任何其他操作，仍然让下一条规则去匹配。 --log-level LEVEL : 日志的等级 --log-prefix FREFIX : 日志的提示语句的前缀 2、规则管理 ①查看规则 # 查看表中的规则 iptables -L -t filter # 查看链中的规则 iptables -L INPUT iptables -L FORWARD --line-numbers # 查看POSTROUTING链nat表中的规则 iptables -L POSTROUTING -t nat iptables -L PREROUTING -t nat --line-numbers ②删除规则 # 删除所有的规则 iptables -F # 删除链中指定的规则 iptables -D FORWARD 1 # 删除链中指定表的规则 iptables -D POSTROUTING -t nat 1 ③添加规则 # 在指定链的末尾添加（append）一条新的规则 iptables -A FORWARD -s 10.8.0.10 -d 192.168.1.5 -j DROP # 在指定链中插入（insert）一条新的规则，默认在第一行添加 iptables -I FORWARD -s 10.8.0.10 -d 192.168.1.5 -j DROP 3、规则的匹配顺序 ①按自上而下的顺序依次匹配，匹配即停止（LOG策略例外） ②若找不到相匹配的规则，则按该链的默认策略处理 四、iptables的显示扩展模块 官方文档：https://ipset.netfilter.org/iptables-extensions.man.html 必须使用-m选项手动加载模块, 其扩展模块路径为:/lib64/xtables,其中大写的为目标扩展,小写的为规则扩展 1、模块管理 ①查看Iptables已加载的模块 cat /proc/net/ip_tables_matches # 查看内核已编译的模块 ②查看内核支持的模块 ls /lib/modules/$(uname -r)/kernel/net/netfilter/* # 或者 ls /lib/modules/nf_* # 或者 ls /usr/lib/iptables/ ③加载模块 modprobe 模块名 CentOS/Redhat 编辑/etc/sysconfig/iptables-config IPTABLES_MODULES=\"模块1 模块2\" 然后重启iptables systemctl restart iptables 例如手动加载注释模块 OS=$(uname -r) insmod /lib/modules/${OS}/kernel/net/netfilter/xt_comment.ko iptables -D INPUT -p tcp -m multiport --dports 26721:26729,22313 -m comment --comment \"bt_port\" -j ACCEPT 2、模块语法 -m 扩展模块 --模块参数 查看模块的支持的参数： iptables -m --help # 如果不显示模块的详细参数，则说明该模块没有加载，无法使用 3、常见条件匹配模块示例 ①multiport：多端口匹配模块 -m multiport --sport | --dport [端口列表] iptables -A INPUT -p tcp -m multiport --dport 22,25,80,443 -j ACCEPT ②iprange：IP范围匹配模块 -m iprange --src-range [IP范围] iptables -A FORWARD -p tcp -m iprange --src-range 192.168.1.10-192.168.1.20 -j ACCEPT -m iprange --dst-range [IP范围] iptables -t nat -A POSTROUTING -s 10.11.2.2/32 -p tcp -m multiport --dport 22,4000 -m iprange --dst-range 192.168.10.1-192.168.10.9 -j SNAT --to 192.168.1.2 ③mac：MAC地址匹配模块 -m mac --mac-range [MAC地址] iptables -A INPUT -m mac --mac-source 00:01:02:03:04:cc -j DROP 将来自macblock集合中的源MAC地址、目的端口为21033的TCP流量拒绝掉 ipset create macblock hash:mac ipset add macblock 00:11:22:33:44:55 iptables -I INPUT -p tcp --dport 12033 -m set --match-set macblock srcmac -j DROP tcpdump监控流量是否生效 tcpdump -i enp0s5 ether src host 00:11:22:33:44:55 and port 12033 设置定时移除拒绝MAC地址 apt install -y at echo \"ipset del 00:11:22:33:44:55\" | at now + 5 minutes ④state：报文状态匹配模块 --state [报文状态]：多个state可以使用,号分隔 支持配置的报文状态： ESTABLISHED：第一个成功穿越防火墙的报文之后所有的报文； NEW：一个连接的第一个报文，例如TCP连接中的SYN报文； RELATED：伴随连接的报文，即某个已经处于ESTABLISHED的连接所产生的报文，这些报文不属于第一条连接，但是的确是由第一条连接产生的； INVALID：如果一个包没有办法被识别，或者这个包没有任何状态，那么这个包的状态就是INVALID，我们可以主动屏蔽状态为INVALID的报文 UNTRACKED：报文的状态为untracked时，表示报文未被追踪，当报文的状态为Untracked时通常表示无法找到相关的连接。 iptables -A INPUT -d 172.168.100.67 -p tcp -m multiport --dport 22,80 -m state --state NEW,ESTABLISHED -j ACCEPT ⑤string：字符串匹配模块 --algo {匹配算法: bm|kmp} --string \"字符串\" iptables -I OUTPUT -s 192.168.1.0/24 -m string --algo kmp --string \"qq\" -j REJECT #使用kmp算法限制拒绝源地址192.168.1.0/24带有\"qq\"字符串的请求 iptables -I INPUT -m string --string \"export/*\" --algo bm -j DROP ⑥limit：连接数匹配模块 —limit: 平均速率，单位：个数/second ，个数/minute，个数/hour --limit-burst: 峰值数量，默认5个 iptables -I INPUT -d 172.16.100.7 -p tcp --dport 22 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT # 当达到100个连接后，才启用上述25/minute限制 ⑦connlimit：连接数匹配模块 --connlimit-upto n : 当现在的连接数量低于或等于这个数量(n),就匹配 --connlimit-above n : 当现有的连接数量大于这个数量, 就匹配 iptables -A INPUT -d 172.16.36.61 -p tcp --dport 22 -m connlimit --connlimit-above 2 -j REJECT ⑧time：时间限制匹配模块 -m time --datestart --datestop --timestart --timestop iptables -A INPUT -p tcp --dport 21 -s 192.168.1.0/24 -m time ! --weekdays 6,7 -m time --timestart 8:30 --timestop 18:00 -m connlimit --connlimit-above 5 -j ACCET # 在工作时间，即周一到周五的8:30-18:00，开放本机的ftp服务给 192.168.1.0网络中的主机访问；并且数据下载请求的次数每分钟不得超过 5 个； ⑨set: 地址集合模块 普通的iptables链是线性的存储和过滤，在进行规则匹配时，是从规则列表中从头到尾一条一条进行匹配。这像是在链表中搜索指定节点费力 ipset 提供了把这个 O(n) 的操作变成 O(1) 的方法：就是把要处理的 IP 放进一个集合，对这个集合设置一条 iptables 规则。存储在带索引的数据结构中,这种结构即使集合比较大也可以进行高效的查找 ipset是iptables的扩展，允许创建管理匹配整个地址集合的规则。命令详解参考附录第2章节 -m set –match-set 地址集合名称 iptables -I INPUT -m set –match-set 集合名称 src -p tcp -j DROP ⑩Owner：用户模块 –uid-owner 和 –gid-owner 这两个选项用来匹配数据包由哪个用户和哪个用户组所产生，它们的功能由 xt_owner.ko 模块提供，且仅适用在 OUTPUT 链 POSROUTING 链中。 –uid-owner userid | username 该选项用来匹配数据包由哪个用户产生，因此可以利用这来限制该用户只能访问某些特定的网络服务， –gid-owner groupid | groupname 该选项和 –uid-owner 类似，它用来匹配某个用户组所产生的数据包。 groupadd no-internet iptables -I OUTPUT 1 -m owner --gid-owner no-internet -j DROP useradd -g no-internet username 参考：https://serverfault.com/questions/550276/how-to-block-internet-access-to-certain-programs-on-linux 五、iptables的内核调优 1、iptables的conntrack连接追踪优化 conntrack是netfilter的核心。有许多增强的功能，例如，地址转换（NAT），基于内容的业务识别（l7， layer-7 module）都是基于连接跟踪。 nf_conntrack模块在kernel 2.6.15（2006-01-03发布） 被引入，支持ipv4和ipv6，取代只支持ipv4的ip_connktrack，用于跟踪连接的状态，供其他模块使用。 iptables的连接追踪表最大容量是/proc/sys/ipv4/ip_conntrack_max设置的, 链接达到各种状态的超时后,会从表中删除,当模板满载时, 后续的链接可能会超时 跟踪的连接用哈希表存储，每个桶（bucket）里都是1个链表，默认长度为4KB。netfilter的哈希表存储在内核空间，这部分内存不能swap 哈希表大小 ：64位的最大连接数/8； 32位的最大连接数/4 在64位下，当CONNTRACK_MAX为 1048576，HASHSIZE 为 262144 时，最多占350多MB 连接跟踪调优计算公式 CONNTRACK_MAX（最大几率的连接条数） = 内存个数*1024*1024*1024/16384/2 = *** Buckets（哈希表大小） = CONNTRACK_MAX / 4 = ***（Byte字节） 跟踪数暂用最内存大小 = CONNTRACK_MAX * 300（Byte字节）= ***（Byte字节） 异常现象： 丢包 可调优参数 哈希表桶大小 注：net.netfilter.nf_conntrack_buckets 不能直接改（报错） # 临时生效 echo 262144 > /sys/module/nf_conntrack/parameters/hashsize # 重启永久生效 新建文件：/etc/modprobe.d/iptables.conf options nf_conntrack hashsize = 32768 最大追踪连接数 注：加大max值, 也会加大内存的压力 # 临时生效 sysctl -w net.nf_conntrack_max = 393216 sysctl -w net.netfilter.nf_conntrack_max = 393216 # 永久生效 echo \"net.nf_conntrack_ma=393216\" >> /etc/sysctl.conf echo \"net.netfilter.nf_conntrack_max=393216\" >> /etc/sysctl.conf sysctl -p 响应时间 net.netfilter.nf_conntrack_tcp_timeout_close_wait: # CLOSE_WAIT是被动方收到FIN发ACK，然后会转到LAST_ACK发FIN，除非程序写得有问题，正常来说这状态持续时间很短。默认 60 秒 # 临时生效 sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300 sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=120 sysctl -w net.netfilter.nf_conntrack_tcp_timeout_close_wait=60 sysctl -w net.netfilter.nf_conntrack_tcp_timeout_fin_wait=120 # 永久生效 echo \"net.netfilter.nf_conntrack_tcp_timeout_established=300\" >> /etc/sysctl.conf echo \"net.netfilter.nf_conntrack_tcp_timeout_time_wait=120\" >> /etc/sysctl.conf echo \"net.netfilter.nf_conntrack_tcp_timeout_close_wait=60\" >> /etc/sysctl.conf echo \"net.netfilter.nf_conntrack_tcp_timeout_fin_wait=120\" >> /etc/sysctl.conf sysctl -p 六、iptables应用 https://github.com/trimstray/iptables-essentials 1、防火墙 ①防止ACK欺骗 拒绝TCP标记为SYN/ACK但连接状态为NEW的数据包， iptables -A INPUT -p tcp --tcp-flags SYN,ACK SYN,ACK -m state --state NEW -j DROP ②防止TCP Null扫描 iptables -A INPUT -p tcp --tcp-flags ALL NONE -j DROP ③防止Xmas扫描 iptables -A INPUT -p tcp --tcp-flags ALL FIN,URG,PSH -j DROP ④限流/防止端口DoS攻击 iptables -A INPUT -p tcp --dport 80 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT # -m limit: 启用limit扩展，限制速度。 # --limit 25/minute: 允许最多每分钟25个连接 # --limit-burst 100: 当达到100个连接后，才启用上述25/minute限制 ⑤限制主机服务时间 在工作时间，即周一到周五的8:30-18:00，开放本机的ftp服务给 192.168.1.0网络中的主机访问；并且数据下载请求的次数每分钟不得超过 5 个； iptables -A INPUT -p tcp --dport 21 -s 192.168.1.0/24 -m time ! --weekdays 6,7 -m time --timestart 8:30 --timestop 18:00 -m connlimit --connlimit-above 5 -j ACCET ⑥丢弃无效数据包 iptables -A INPUT -m conntrack --ctstate INVALID -j DROP ⑦关键词屏蔽 iptables -I FORWARD -p udp --dport 53 -m string --string \"tencent\" -m time --timestart 8:15 --timestop 20:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP iptables -I FORWARD -p udp --dport 53 -m string --string \"TENCENT\" -m time --timestart 8:15 --timestop 20:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP iptables -I FORWARD -p udp --dport 53 -m string --string \"qq.com\" -m time --timestart 8:15 --timestop 20:30 --days Mon,Tue,Wed,Thu,Fri,Sat -j DROP iptables -I FORWARD -s 10.113.0.0/24 -m string --string \"ay2000.net\" -j DROP # 关键词屏蔽 iptables -I FORWARD -s 10.113.0.0/24 -m string --string \"eroticism\" -j DROP ⑧防止外网使用内网IP欺骗 iptables -t nat -A PREROUTING -i eth0 -s 10.0.0.0/8 -j DROP iptables -t nat -A PREROUTING -i eth0 -s 172.16.0.0/12 -j DROP iptables -t nat -A PREROUTING -i eth0 -s 192.168.0.0/16 -j DROP ⑨禁Ping # 允许本机ping别的主机；但不开放别的主机ping本机； iptables -A OUTPUT -p icmp --icmp-type 8 -j ACCEPT iptables -A INPUT -p icmp --icmp-type 0 -j ACCEPT 2、NAT网络地址转换 iptable上中包含一个NAT表，其中有两条缺省的PREROUTING和 POSTROUTING 链，在这两条链上配置规则可以实现NAT功能。 ①SNAT源地址目标转换 概念 SNAT(Source Network Address Translation)是指在数据包从网卡发送出去的时候，把数据包中的源地址部分替换为指定的IP。 适用于由局域网中的主机发起连接的情况。报文在经过NAT路由器时，将IP报文中的源IP地址转换为一个有效的广域网地址；在服务器给一个在私有网络中的主机返回响应报文时，目的IP地址就是这个局域网对外的广域网地址。报文到达NAT路由器的时候，路由器要将该报文分发给对应的主机，将IP报文的目的IP地址转换为私有网络地址 涉及到iptables中的链表 POSTROUTING链中的nat表 应用场景 局域网主机共享单个公网IP地址接入Internet 做法： 设置能上外网的那一台主机(192.168.1.2)的iptables，一旦接收到来自局域网(192.168.1.0/24)的数据，修改数据包的源IP地址为本机IP地址，然后就转发出去。 前提 有公网IP地址绑定 内核设置net.ipv4.ip_forward=1，开起路由转发功能 # 查看内核是否启用路由转发功能 sysctl -a | grep \"ip_forward\" 或者 sysctl net.ipv4.ip_forward # “net.ipv4.ip_forward=1”即可表示成功开启 # 临时设置内核启用路由转发功能。重启失效 sysctl -w net.ipv4.ip_forward=1 或者 echo 1 >/proc/sys/net/ipv4/ip_forward # 永久设置内核启用路由转发功能。 echo \"net.ipv4.ip_forward = 1\" >> /etc/sysctl.conf && sysctl -p Iptables 设置 iptables -t nat -A POSTROUTING -s 192.168.1.0/24 -j SNAT --to 192.168.1.2 iptables -t nat -A POSTROUTING -s 192.168.1.26/32 -p tcp --dport 3306 -d 192.168.1.88/32 -j SNAT --to 192.168.1.2 局域网内想上外网的主机或者路由器想添加自定义路由策略 route add 0.0.0.0 gw 192.168.1.2 ②DNAT目标网络地址转换 DNAT(Destination Network Address Translation)是又称为“端口转发”，适用于由广域网上的主机发起连接的情况。当广域网的主机访问NAT路由器的广域网端口时，可以将NAT路由器的广域网的端口映射到局域网内的某个IP地址的某个端口，这样就可以实现广域网主机访问局域网内的资源。 iptables -t nat -I PREROUTING -d 公网IP -p tcp -m tcp --dport 公网port -j DNAT --to-destination 10.10.223.12-10.10.223.20:8080（内网） # 对于在云上绑定公网IP地址的ECS主机做DNAT，公网IP地址要写成ECS的内网地址。因为云厂商的公网IP也是使用SNAT实现的，通过公网IP访问ECS的所有流量请求，已经将目标地址改成ECS的内网地址啦。 # 例如阿里云上一台ECS绑定弹性公网IP地址123.11.12.13，内网地址为192.168.1.8，还有一台ECS，内网地址为192.168.1.9，上面部署了MYSQL。想通过弹性公网IP地址访问MySQL。则在192.168.1.8这台ECS中做DNAT的时候，可以这样配置 # iptables -t nat -I PREROUTING -d 192.168.1.8 -p tcp --dport 33306 -j DNAT --to-destination 192.168.1.9:3306 附录 1、TCP连接状态 https://blog.mimvp.com/article/44678.html 2、使用SystemD管理iptables规则 bash -c 'cat > /etc/systemd/system/iptables-nat.service 3、ipset命令 官网：https://ipset.netfilter.org/ 文档：https://ipset.netfilter.org/ipset.man.html#lbBF ipset默认可以存储65536个元素，使用maxelem指定数量 不支持0.0.0.0/0 ，可以替换为 0.0.0.0/1，128.0.0.0/1 需要内核版本高于2.6.32 ①安装ipset命令 yum install -y ipset apt install -y ipset apk add -y ipset ②ipset语法规则 # 创建集合 ipset (create | -N) 集合名称 集合存储方法:记录类型1[,数据类型2[,数据类型3]] [ 集合存储方法:记录类型支持的参数 ] # 支持的集合储存方法 - bitmap：仅支持ip、port、mac记录类型 - hash：仅支持net、iface、mac、ip、port、mark记录类型 - list: 仅支持集合间的继承关联关系 # 支持的记录类型 - ip：IP地址，例如1.2.3.4 - net：IP地址网络段，例如1.2.3.0/24 - mac：MAC地址，例如1A:2B:3C:4D:5E:6F - port：协议类型:端口，例如[udp/tcp]:23、[udp/tcp]:21-23 - iface：网卡，例如eth0 - mark: 例如0x63，值在0~4294967295之间 # 例如：ipset create whitelist hash:ip,port # 查看集合存储方法:记录类型支持的参数 ipset help 集合存储方法:记录类型 # 例如：ipset help hash:ip,port # 集合中添加记录 ipset (add | -A) 集合名称 记录 # 例如：ipset add whitelist 192.168.1.7,tcp:21-22 # 查看集合。不加集合名称是查看所有的集合 ipset [list | -L) [集合名称] # 例如：ipset -L whitelist # 删除集合中的记录 ipset (del | -D) 集合名称 记录 [ DEL-OPTIONS ] # 例如：ipset del whitelist 192.168.1.7,tcp:21 # 删除集合，不能有任何下游依赖 。不加集合名称是删除所有集合 ipset (destroy | -X) [集合名称] # 例如：ipset destroy whitelist # 清空集合，不加集合名称是清空所有 ipset (flush | -F) [集合名称] # 例如：ipset flush # 将ipset规则保存到文件，不加集合名词是保存所有集合，不加-f是输出记录到标准输出 ipset (save | -S) [集合名称] [-f 文件名] # 例如：ipset save whitelist -f iptbales-whitelist-ip.txt # 导入ipset规则，不加-f是从标准输入读取规则 ipset (restore | -R) [-f 文件名] # 例如：ipset restore -f iptbales-whitelist-ip.txt # 重命名集合 ipset (rename | -E) 旧集合名称 新集合名称 # 例如：ipset rename whitelist blacklist # 测试一个ip是不是在集合中（要是ip在集合中返回0，如果ip不在集合中则返回非0） ipset (test | -T) 集合名称 ip地址 # 例如：ipset test blacklist 192.168.1.7,tcp 4、攻击处理脚本 https://github.com/ppabc/cc_iptables/tree/master 参考 https://github.com/ppabc/cc_iptables/tree/master https://www.jianshu.com/p/ee4ee15d3658 http://www.zsythink.net/archives/1199/ https://www.linuxidc.com/Linux/2018-08/153378.htm https://blog.csdn.net/u014721096/article/details/78626729 https://www.jianshu.com/p/586da7c8fd42 http://www.stearns.org/modwall/archives/tcpchk.v0.1.1 https://blog.51cto.com/woyaoxuelinux/1906316 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-04-30 17:55:23 "},"origin/mysql-basic.html":{"url":"origin/mysql-basic.html","title":"MySQL","keywords":"","body":"MySQL 一、简介 逻辑架构图 每个虚线框为一层，总共三层。 第一层，服务层(为客户端服务):为请求做连接处理，授权认证，安全等。 第二层，核心层:查询解析，分析，优化，缓存，提供内建函数（例如：日期，时间，数学和加密）;所有跨存储引擎的功能都在这一层实现：存储过程，触发器，视图。 第三层，存储引擎层，不光做存储和提取数据，而且针对特殊数据引擎还要做事务处理。第二层通过API与存储引擎通信，但存储引擎不会解析SQL，不同存储引擎之间不进行通信。 1、对于Select语句，在解析查询之前，服务器会先检查查询缓存。 2、通过读写锁实现并发控制。读写锁分为两类：共享锁（读锁）和排他锁（写锁）。读锁是共享的，多个客户端在同一时间可以同时读取同一资源。写锁是排他的的，也就是在一个写锁会阻塞读锁和写锁的，确保在给定的时间里，只有一个用户能写入，其他用户则禁止读和写。 3、锁粒度：表锁和行级锁。行级锁只能在存储引擎中实现。 二、主从复制 MySQL内建的复制功能是构建大型，高性能应用程序的基础。将Mysql的数据分布到多个系统上去，这种分布的机制，是通过将Mysql的某一台主机的数据复制到其它主机（slaves）上，并重新执行一遍来实现的。复制过程中一个服务器充当主服务器，而一个或多个其它服务器充当从服务器。主服务器将更新写入二进制日志文件，并维护文件的一个索引以跟踪日志循环。这些日志可以记录发送到从服务器的更新。当一个从服务器连接主服务器时，它通知主服务器从服务器在日志中读取的最后一次成功更新的位置。从服务器接收从那时起发生的任何更新，然后封锁并等待主服务器通知新的更新。 请注意当你进行复制时，所有对复制中的表的更新必须在主服务器上进行。否则，你必须要小心，以避免用户对主服务器上的表进行的更新与对从服务器上的表所进行的更新之间的冲突。 mysql支持的复制类型： 基于语句的复制： 在主服务器上执行的SQL语句，在从服务器上执行同样的语句。MySQL默认采用基于语句的复制，效率比较高。 一旦发现没法精确复制时， 会自动选着基于行的复制。 基于行的复制：把改变的内容复制过去，而不是把命令在从服务器上执行一遍. 从mysql5.0开始支持 混合类型的复制: 默认采用基于语句的复制，一旦发现基于语句的无法精确的复制时，就会采用基于行的复制。 复制解决的问题 MySQL复制技术有以下一些特点： 数据分布 (Data distribution ) 负载平衡(load balancing) 备份(Backups) 高可用性和容错行 (High availability and failover ) 复制如何工作 整体上来说，复制有3个步骤： master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events）； slave将master的binary log events拷贝到它的中继日志(relay log)； slave重做中继日志中的事件，将改变反映它自己的数据。 ​ 该过程的第一部分就是master记录二进制日志。在每个事务更新数据完成之前，master在二日志记录这些改变。MySQL将事务串行的写入二进制日志，即使事务中的语句都是交叉执行的。在事件写入二进制日志完成后，master通知存储引擎提交事务。 ​ 下一步就是slave将master的binary log拷贝到它自己的中继日志。首先，slave开始一个工作线程——I/O线程。I/O线程在master上打开一个普通的连接，然后开始binlog dump process。Binlog dump process从master的二进制日志中读取事件，如果已经跟上master，它会睡眠并等待master产生新的事件。I/O线程将这些事件写入中继日志。 ​ SQL slave thread（SQL从线程）处理该过程的最后一步。SQL线程从中继日志读取事件，并重放其中的事件而更新slave的数据，使其与master中的数据一致。只要该线程与I/O线程保持一致，中继日志通常会位于OS的缓存中，所以中继日志的开销很小。 ​ 此外，在master中也有一个工作线程：和其它MySQL的连接一样，slave在master中打开一个连接也会使得master开始一个线程。复制过程有一个很重要的限制——复制在slave上是串行化的，也就是说master上的并行更新操作不能在slave上并行操作。 复制配置 有两台MySQL数据库服务器Master和slave，Master为主服务器，slave为从服务器，初始状态时，Master和slave中的数据信息相同，当Master中的数据发生变化时，slave也跟着发生相应的变化，使得master和slave的数据信息同步，达到备份的目的。 要点： 负责在主、从服务器传输各种修改动作的媒介是主服务器的二进制变更日志，这个日志记载着需要传输给从服务器的各种修改动作。因此，主服务器必须激活二进制日志功能。从服务器必须具备足以让它连接主服务器并请求主服务器把二进制变更日志传输给它的权限。 三、集群模式 MySQL Cluster 是MySQL 适合于分布式计算环境的高实用、可拓展、高性能、高冗余版本，其研发设计的初衷就是要满足许多行业里的最严酷应用要求，这些应用中经常要求数据库运行的可靠性要达到99.999%。MySQL Cluster允许在无共享的系统中部署“内存中”数据库集群，通过无共享体系结构，系统能够使用廉价的硬件，而且对软硬件无特殊要求。此外，由于每个组件有自己的内存和磁盘，不存在单点故障。 实际上，MySQL集群是把一个叫做NDB的内存集群存储引擎集成与标准的MySQL服务器集成。它包含一组计算机，每个都跑一个或者多个进程，这可能包括一个MySQL服务器，一个数据节点，一个管理服务器和一个专有的一个数据访问程序。 MySQL Cluster能够使用多种故障切换和负载平衡选项配置NDB存储引擎，但在Cluster 级别上的存储引擎上做这个最简单。以下为MySQL集群结构关系图， MySQL从结构看，由3类节点(计算机或进程)组成，分别是： 管理节点:用于给整个集群其他节点提供配置、管理、仲裁等功能。理论上通过一台服务器提供服务就可以了。 数据节点:MySQL Cluster的核心，存储数据、日志，提供数据的各种管理服务。2个以上 时就能实现集群的高可用保证，DB节点增加时，集群的处理速度会变慢。 SQL节点(API):用于访问MySQL Cluster数据，提供对外应用服务。增加 API 节点会提高整个集群的并发访问速度和整体的吞吐量，该节点 可以部署在Web应用服务器上，也可以部署在专用的服务器上，也开以和DB部署在 同一台服务器上。 NDB引擎 MySQL Cluster 使用了一个专用的基于内存的存储引擎——NDB引擎，这样做的好处是速度快， 没有磁盘I/O的瓶颈，但是由于是基于内存的，所以数据库的规模受系统总内存的限制， 如果运行NDB的MySQL服务器一定要内存够大，比如4G, 8G, 甚至16G。NDB引擎是分布式的，它可以配置在多台服务器上来实现数据的可靠性和扩展性，理论上 通过配置2台NDB的存储节点就能实现整个数据库集群的冗余性和解决单点故障问题。 缺陷 基于内存，数据库的规模受集群总内存的大小限制 基于内存，断电后数据可能会有数据丢失，这点还需要通过测试验证。 多个节点通过网络实现通讯和数据同步、查询等操作，因此整体性受网络速度影响，因此速度也比较慢 2.2 优点 多个节点之间可以分布在不同的地理位置，因此也是一个实现分布式数据库的方案 扩展性很好，增加节点即可实现数据库集群的扩展。 冗余性很好，多个节点上都有完整的数据库数据，因此任何一个节点宕机都不会造成服务中断。 实现高可用性的成本比较低，不象传统的高可用方案一样需要共享的存储设备和专用的软件才能实现，NDB 只要有足够的内存就能实现。 四、存储引擎 1、在文件系统中，MySQL会将每一个数据库保存为数据目录下的一个子目录。创建表时，MySQL会在数据库子目录下创建一个和表同名的.frm文件保存表的定义。 2、MySQL使用文件系统的目录和文件来保存数据库和表的定义，大小写敏感和具体的平台密切相关。在Windows中，大小写是不敏感的；但在类Unix中则大小写敏感。 3、不同的存储引擎保存数据和索引的方式是不同的，但表的定义则是在MySQL的服务层统一处理的。 4、MySQL在5.1及之前的版本中，MyISAM是默认的存储引擎，之后默认的是InnoDB。MyISAM存储引擎不支持事务和行级锁。所以不要再默认使用MyISAM，而应默认使用InnoDB。 5、MySQL内建的其他存储引擎： Archive引擎，只支持Insert和Select操作。但支持行级锁和专用的缓冲区 Blackhole引擎 CSV引擎，可以将普通的CSV文件作为MySQL的表处理，但这种表不支持索引。CSV引擎可以在数据库运行时拷入或拷出文件。可以将Excel文件存储为CSV文件，然后复制到MySQL数据目录中，就能在MySQL中打开使用。同样，如果将数据写入到一个CSV引擎表，其他的外部程序也能立即从表的数据文件中读取CSV格式的数据。 Federated引擎,是访问其他MySQL服务器的一个代理，它会创建一个到远程MySQL服务器的客户端连接，并将查询传输到远程服务器执行，然后提取或者发送需要的数据。 Memory引擎。所有的数据都保存在内存中，Memory表的结构在重启后还是会保留，但数据会丢失。 Merge引擎。MyISAM引擎的一个变种。Merge表是有多个MyISAM表合并而来的虚拟表 NDB引擎。MySQL集群的存储引擎。 五、多版本并发控制MVCC 1、MVCC的实现是通过保存数据在某个时间点的快照来实现。 2、不同存储引擎的MVCC实现是不同的。典型的有乐观（optimistic）控制和悲观（pessimistic）控制 3、MVCC只在REPEATABLE READ 和READ COMMITTED两个隔离级别下工作。 其他两个隔离级别都不和MVCC兼容。是因为，READ UNCOMMITED总是读取最新的数据行，而不是读取符合当前事务版本的数据行，而SERIALIZABLE则会在所有读取的行都加锁。 4、InnoDB的MVCC通过在每行记录后面保存两个隐藏的列来实现的，一列保存了行的创建时间，一列保存行的过期时间（或删除时间），存储的并不是实际的时间值，而是系统版本号（System Version Number）。没开始一个新的事务，系统版本号会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行对比。 六、示例数据 Title DB Download HTML Setup Guide employee data (large dataset, includes data and test/verification suite) GitHub View world database TGZ \\ Zip View world_x database TGZ \\ Zip View sakila database TGZ \\ Zip View airportdb database (large dataset, intended for MySQL on OCI and HeatWave) TGZ \\ Zip View menagerie database TGZ \\ Zip https://dev.mysql.com/doc/index-other.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-06-29 16:16:44 "},"origin/mysql-common-operations.html":{"url":"origin/mysql-common-operations.html","title":"常见操作及SQL","keywords":"","body":"MySQL 常见操作 1、相关网站 官方文档：https://dev.mysql.com/doc/refman/5.7/en/ 下载地址：https://dev.mysql.com/downloads/ 2、常用服务端配置 [mysqld] datadir=/data/mysql/data # 为MySQL客户程序与服务器之间的本地通信指定一个套接字文件(Linux下默认是/var/lib/mysql/mysql.sock文件) socket=/data/mysql/data/mysql.sock symbolic-links=0 log-error=/data/mysql/logs/mysqld.log pid-file=/data/mysql/data/mysqld.pid lower_case_table_names=0 federated init_connect='SET NAMES utf8' character_set_server=utf8 transaction-isolation=READ COMMITTED # key_buffer是用于索引块的缓冲区大小，增加它可得到更好处理的索引(对所有读和多重写)。 索引块是缓冲的并且被所有的线程共享，key_buffer的大小视内存大小而定。 key_buffer=384M # 为所有线程打开表的数量。增加该值能增加mysqld要求的文件描述符的数量。可以避免频繁的打开数据表产生的开销 table_cache=512 # 每个需要进行排序的线程分配该大小的一个缓冲区。增加这值加速ORDER BY或GROUP BY操作。注意：该参数对应的分配内存是每连接独占！如果有100个连接，那么实际分配的总共排序缓冲区大小为100×6=600MB sort_buffer_size=2M # 读查询操作所能使用的缓冲区大小。和sort_buffer_size一样，该参数对应的分配内存也是每连接独享。 read_buffer_size=2M # 指定MySQL查询结果缓冲区的大小 query_cache_size=32M # 在使用行指针排序之后，随机读用的。 read_rnd_buffer_size=8M # MyISAM表发生变化时重新排序所需的缓冲 myisam_sort_buffer_size=64M # 最大并发线程数，取值为服务器逻辑CPU数量×2，如果CPU支持H.T超线程，再×2 thread_concurrency=8 #缓存可重用的线程数 thread_cache=8 # 避免MySQL的外部锁定，减少出错几率增强稳定性。 skip-locking # back_log参数的值指出在MySQL暂时停止响应新请求之前的短时间内多少个请求可以被存在堆栈中。如果系统在一个短时间内有很多连接，则需要增大该参数的值，该参数值指定到来的TCP/IP连接的侦听队列的大小。试图设定back_log高于你的操作系统的限制将是无效的。默认值为50。对于Linux系统推荐设置为小于512的整数。 back_log=384 # MySQL服务器同时处理的数据库连接的最大数量(默认设置是100)。超过限制后会报 Too many connections 错误 max_connections=n # 用来存放索引区块的RMA值(默认设置是8M)，增加它可得到更好处理的索引(对所有读和多重写) key_buffer_size=n #每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区。如果你做很多顺序扫描，你可能想要增加该值。默认数值是131072(128K) record_buffer=131072 # 服务器在关闭它之前在一个连接上等待行动的秒数。 wait_timeout=3 # 服务器在关闭它前在一个交互连接上等待行动的秒数。一个交互的客户被定义为对 mysql_real_connect()使用 CLIENT_INTERACTIVE 选项的客户。默认数值是28800，可以把它改为3600。 interactive_timeout=3600 # 禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求！ skip-name-resolve skip-innodb # 关闭不需要的表类型 skip-bdb # 开启Binlog log-bin=/data/mysql/data/mysql-bin.log expire-logs-days=14 max-binlog-size=500M server-id=1 binlog_format=ROW binlog_row_image=FULL # 开启慢查询日志 slow_query_log=1 slow_query_log_file=/data/mysql/logs/slowquery.log long_query_time=2 [mysqldump] # 服务器和客户端之间最大能发送的可能信息包 max_allowed_packet=16M [mysql] socket=/data/mysql/data/mysql.sock default-character-set=utf8 [client] default-character-set=utf8 socket=/data/mysql/data/mysql.sock 3、初始化数据目录 mkdir -p /data/mysql/{logs,data} &&\\ chown -R mysql:mysql /data/mysql &&\\ mysqld --initialize --user=mysql 4、修改root用户本地连接密码 ALTER USER 'root'@'localhost' IDENTIFIED BY '***'; flush privileges; 或者 set password for 'root'@'localhost'=password('***'); flush privileges; 5、添加远程登录用户 默认只允许root帐户在本地登录，如果要在其它机器上连接mysql，必须修改root允许远程连接，或者添加一个允许远程连接的帐户，为了安全起见，添加一个新的帐户： GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '***' WITH GRANT OPTION; flush privileges; 6、修改用户密码 ①用set password命令 SET PASSWORD FOR 'root'@'localhost' = PASSWORD('***'); flush privileges; ②用mysqladmin mysqladmin -uroot -p*** password 1234abcd 格式：mysqladmin -u用户名 -p旧密码 password 新密码 ③update更新user表 where user = 'root'; flush privileges; # mysql 5.7 use mysql update user set authentication_string = PASSWORD('1234abcd') where user = 'root'; flush privileges; 7、跳过密码验证重置root密码 修改my.cnf，追加skip-grant-tables，重启mysql服务，然后就可以不输入密码直接登录 8、表复制 复制表结构及数据到新表 CREATE TABLE 新表 SELECT * FROM 旧表 只复制表结构到新表 CREATE TABLE 新表 SELECT * FROM 旧表 WHERE 1=2 即:让WHERE条件不成立. 方法二:(低版本的mysql不支持，mysql4.0.25 不支持，mysql5已经支持了) CREATE TABLE 新表 LIKE 旧表 复制旧表的数据到新表(假设两个表结构一样) INSERT INTO 新表 SELECT * FROM 旧表 复制旧表的数据到新表 (假设两个表结构不一样) INSERT INTO 新表(字段1,字段2,…….) SELECT 字段1,字段2,…… FROM 旧表 参考: https://www.cnblogs.com/lxboy2009/p/7234535.html 9、show status命令 show status like '%下面变量%'; Aborted_clients 由于客户没有正确关闭连接已经死掉，已经放弃的连接数量。 Aborted_connects 尝试已经失败的MySQL服务器的连接的次数。 Connections 试图连接MySQL服务器的次数。 Created_tmp_tables 当执行语句时，已经被创造了的隐含临时表的数量。 Delayed_insert_threads 正在使用的延迟插入处理器线程的数量。 Delayed_writes 用INSERT DELAYED写入的行数。 Delayed_errors 用INSERT DELAYED写入的发生某些错误(可能重复键值)的行数。 Flush_commands 执行FLUSH命令的次数。 Handler_delete 请求从一张表中删除行的次数。 Handler_read_first 请求读入表中第一行的次数。 Handler_read_key 请求数字基于键读行。 Handler_read_next 请求读入基于一个键的一行的次数。 Handler_read_rnd 请求读入基于一个固定位置的一行的次数。 Handler_update 请求更新表中一行的次数。 Handler_write 请求向表中插入一行的次数。 Key_blocks_used 用于关键字缓存的块的数量。 Key_read_requests 请求从缓存读入一个键值的次数。 Key_reads 从磁盘物理读入一个键值的次数。 Key_write_requests 请求将一个关键字块写入缓存次数。 Key_writes 将一个键值块物理写入磁盘的次数。 Max_used_connections 同时使用的连接的最大数目。 Not_flushed_key_blocks 在键缓存中已经改变但是还没被清空到磁盘上的键块。 Not_flushed_delayed_rows 在INSERT DELAY队列中等待写入的行的数量。 Open_tables 打开表的数量。 Open_files 打开文件的数量。 Open_streams 打开流的数量(主要用于日志记载） Opened_tables 已经打开的表的数量。 Questions 发往服务器的查询的数量。 Slow_queries 要花超过long_query_time时间的查询数量。 Threads_connected 当前打开的连接的数量。 Threads_running 不在睡眠的线程数量。 Uptime 服务器工作了多少秒。 10、查看状态或监控信息 # 如果是root帐号，你能看到所有用户的当前连接。如果是其它普通帐号，只能看到自己占用的连接。 # show processlist;只列出前100条， show processlist; # 如果想全列出请使用 show full processlist; # 最大使用连接数 show status like 'Max_used_connections' # 当前打开的连接数 show status like 'Threads_connected' # 未从缓冲池读取的次数 show status like 'Innodb_buffer_pool_reads' # 从缓冲池读取的次数 show status like 'Innodb_buffer_pool_read_requests' # 缓冲池的总页数 show status like 'Innodb_buffer_pool_pages_total' # 缓冲池空闲的页数 show status like 'Innodb_buffer_pool_pages_free' # 缓存命中率计算 （1-Innodb_buffer_pool_reads/Innodb_buffer_pool_read_requests）*100% # 缓存池使用率为 ((Innodb_buffer_pool_pages_total-Innodb_buffer_pool_pages_free）/Innodb_buffer_pool_pages_total）*100% # 锁等待个数 show status like 'Innodb_row_lock_waits' # 平均每次锁等待时间 show status like 'Innodb_row_lock_time_avg' # 查看是否存在表锁(有数据代表存在锁表，空为无表锁) show open TABLES where in_use > 0; # insert 数量 show status like 'Com_insert' # delete 数量 show status like 'Com_delete' # update 数量 show status like 'Com_update' # select 数量 show status like 'Com_select' # 发送吞吐量 show status like 'Bytes_sent' # 接收吞吐量 show status like 'Bytes_received' # 总吞吐量：Bytes_sent+Bytes_received show variables like 'long_query_time' 11、查看支持的引擎 show engines； 12、清空表中数据 # truncate语句直接清空表中数据 truncate 表名； # drop语句清空表中数据 delete from 表名; 13、导入导出数据到CSV 导出 select * from Test.User into outfile 'UserData.csv' fields terminated by '@' # 字段间以@分割 optionally enclosed by \"\" escaped by \"Curiouser\" lines terminated by '\\n'; # 数据行之间以\\n分割 如果在导出期间出现以下错误 The MySQL server is running with the --secure-file-priv option so it cannot execute this statement 可以用sql>show variables like '%secure%'查看secure-file-priv当前的值。导出的数据文件必须是这个值的指定路径才可以。默认有可能是NULL就代表禁止导出。可在配置文件中设置此变量的值,然后重启服务。 [mysqld] secure-file-priv=/home/Curiouser/Desktop 导入 sql>load data infile '/var/lib/mysql-files/UserData.csv' into table User fields terminated by '@' optionally enclosed by \"\" escaped by \"\" lines terminated by '\\n'; 当CSV文件中的每一行记录的列数小于数据库表时，使用下列语句： sql>load data infile '/var/lib/mysql-files/UserData.csv' into table User fields terminated by '@' lines terminated by '\\n' (Id,Name,Addr,Phone); 14、MySQL字符集的设置 设置MySQL默认字符集 基于session会话 set character_set_client=utf8mb4; # 主要用来设置客户端使用的字符集。 set character_set_database=utf8mb4; # 主要用来设置默认创建数据库的编码格式，如果在创建数据库时没有设置编码格式，就按照这个格式设置。 set character_set_server=utf8mb4; # 服务器安装时指定的默认编码格式，这个变量建议由系统自己管理，不要人为定义 set character_set_connection=utf8mb4; # 主要用来设置连接数据库时的字符集，如果程序中没有指明连接数据库使用的字符集类型则按照这个字符集设置。 set character_set_results=utf8mb4; # 数据库给客户端返回时使用的编码格式，如果没有指明，使用服务器默认的编码格式。 set character_set_system=utf8mb4; # 数据库系统使用的编码格式，这个值一直是utf8，不需要设置，它是为存储系统元数据的编码格式。 set collation_connection=utf8mb4; set collation_server=utf8mb4; set collation_database=utf8mb4; 基于全局global # 设置全局的数据库字符编码 set global character_set_database=utf8mb4; set global character_ser_server=utf8mb4; 永久性改变,在配置文件中修改数据库的字符编码(需重启服务) [mysqld] character-set-server=utf8mb4 [client] default-character-set=utf8mb4 [mysql] default-character-set=utf8mb4 字符集的查看 查看MySQL支持的字符集 show charset; +----------+---------------------------------+---------------------+--------+ | Charset | Description | Default collation | Maxlen | +----------+---------------------------------+---------------------+--------+ | big5 | Big5 Traditional Chinese | big5_chinese_ci | 2 | | dec8 | DEC West European | dec8_swedish_ci | 1 | | cp850 | DOS West European | cp850_general_ci | 1 | | hp8 | HP West European | hp8_english_ci | 1 | | koi8r | KOI8-R Relcom Russian | koi8r_general_ci | 1 | | latin1 | cp1252 West European | latin1_swedish_ci | 1 | | latin2 | ISO 8859-2 Central European | latin2_general_ci | 1 | | swe7 | 7bit Swedish | swe7_swedish_ci | 1 | | ascii | US ASCII | ascii_general_ci | 1 | | ujis | EUC-JP Japanese | ujis_japanese_ci | 3 | | sjis | Shift-JIS Japanese | sjis_japanese_ci | 2 | | hebrew | ISO 8859-8 Hebrew | hebrew_general_ci | 1 | | tis620 | TIS620 Thai | tis620_thai_ci | 1 | | euckr | EUC-KR Korean | euckr_korean_ci | 2 | | koi8u | KOI8-U Ukrainian | koi8u_general_ci | 1 | | gb2312 | GB2312 Simplified Chinese | gb2312_chinese_ci | 2 | | greek | ISO 8859-7 Greek | greek_general_ci | 1 | | cp1250 | Windows Central European | cp1250_general_ci | 1 | | gbk | GBK Simplified Chinese | gbk_chinese_ci | 2 | | latin5 | ISO 8859-9 Turkish | latin5_turkish_ci | 1 | | armscii8 | ARMSCII-8 Armenian | armscii8_general_ci | 1 | | utf8 | UTF-8 Unicode | utf8_general_ci | 3 | | ucs2 | UCS-2 Unicode | ucs2_general_ci | 2 | | cp866 | DOS Russian | cp866_general_ci | 1 | | keybcs2 | DOS Kamenicky Czech-Slovak | keybcs2_general_ci | 1 | | macce | Mac Central European | macce_general_ci | 1 | | macroman | Mac West European | macroman_general_ci | 1 | | cp852 | DOS Central European | cp852_general_ci | 1 | | latin7 | ISO 8859-13 Baltic | latin7_general_ci | 1 | | utf8mb4 | UTF-8 Unicode | utf8mb4_general_ci | 4 | | cp1251 | Windows Cyrillic | cp1251_general_ci | 1 | | utf16 | UTF-16 Unicode | utf16_general_ci | 4 | | utf16le | UTF-16LE Unicode | utf16le_general_ci | 4 | | cp1256 | Windows Arabic | cp1256_general_ci | 1 | | cp1257 | Windows Baltic | cp1257_general_ci | 1 | | utf32 | UTF-32 Unicode | utf32_general_ci | 4 | | binary | Binary pseudo charset | binary | 1 | | geostd8 | GEOSTD8 Georgian | geostd8_general_ci | 1 | | cp932 | SJIS for Windows Japanese | cp932_japanese_ci | 2 | | eucjpms | UJIS for Windows Japanese | eucjpms_japanese_ci | 3 | | gb18030 | China National Standard GB18030 | gb18030_chinese_ci | 4 | +----------+---------------------------------+---------------------+--------+ 查看MySQL已配置的默认字符设置 show variables like '%character%'; +--------------------------+----------------------------+ | Variable_name | Value | +--------------------------+----------------------------+ | character_set_client | utf8mb4 | | character_set_connection | utf8mb4 | | character_set_database | latin1 | | character_set_filesystem | binary | | character_set_results | utf8mb4 | | character_set_server | latin1 | | character_set_system | utf8 | | character_sets_dir | /usr/share/mysql/charsets/ | +--------------------------+----------------------------+ 查看库已配置的字符设置 show create database test1; +----------+----------------------------------------------------------------------+ | Database | Create Database | +----------+----------------------------------------------------------------------+ | test1 | CREATE DATABASE `test1` /*!40100 DEFAULT CHARACTER SET utf8mb4 */ | +----------+----------------------------------------------------------------------+ 查看表已配置的字符设置 show table status from 库名 like '表名'; 查看表中字段已配置的字符设置 show full columns from 表名; 查看库中所有表的字符集设置 select TABLE_NAME,TABLE_COLLATION from information_schema.`TABLES`; 查看所有库所有表中的字段的字符集设置 select TABLE_SCHEMA ,TABLE_NAME,COLUMN_NAME,COLLATION_NAME from information_schema.`COLUMNS` 修改字符集 修改库的字符集 alter database 库名 default character set 字符集; 修改表的字符集 alter table 表名 convert to character set 字符集; 修改字段的字符集 alter table 表名 modify 字段名 字段属性 character set 字符集； 15、Mysql大小写敏感设置 MySQL在Windows下都不区分大小写。 Linux下MySQL安装完后是默认区分表名的大小写，但不区分列名的大小写。数据库名、表名、列名、别名大小写规则是这样的： 数据库名与表名是严格区分大小写 表的别名是严格区分大小写 列名与列的别名在所有的情况下均是忽略大小写 变量名也是严格区分大小写 修改大小写敏感设置 在/etc/my.cnf中的[mysqld]后添加添加lower_case_table_names=1（0:区分大小写，1:不区分大小写)，重启MYSQL服务 为0时 表示区分大小写，使用CREATE TABLE或CREATE DATABASE语句指定的大小写字母在硬盘上保存表名和数据库名。名称比较对大小写敏感。在大小写不敏感的操作系统如windows或Mac OS x上我们不能将该参数设为0，如果在大小写不敏感的文件系统上将--lowercase-table-names强制设为0，并且使用不同的大小写访问MyISAM表名，可能会导致索引破坏。 为1时 表示将名字转化为小写后存储，名称比较对大小写不敏感。MySQL将所有表名转换为小写在存储和查找表上。该行为也适合数据库名和表的别名。该值为Windows的默认值。 为2时 表名和数据库名在硬盘上使用CREATE TABLE或CREATE DATABASE语句指定的大小写字母进行保存，但MySQL将它们转换为小写在查找表上。名称比较对大小写不敏感，即按照大小写来保存，按照小写来比较。注释：只在对大小写不敏感的文件系统上使用! innodb表名用小写保存。如果你使用innodb表，为了避免避免大小写敏感问题，可以把lower_case_table_names=1把lower_case_table_names从0改变为1在你把lower_case_table_names设置为1时，在restart你的mysqld之前，请把数据库名和表名更改为小写 如果想在查询时区分字段值的大小写，则字段值需要设置BINARY属性 创建时设置 CREATE TABLE T( A VARCHAR(10) BINARY ); 使用alter修改 ALTER TABLE `tablename` MODIFY COLUMN `cloname` VARCHAR(45) BINARY; 16、查看数据库或表容量大小 查看所有数据库容量大小 select table_schema as '数据库', sum(table_rows) as '记录数', sum(truncate(data_length/1024/1024, 2)) as '数据容量(MB)', sum(truncate(index_length/1024/1024, 2)) as '索引容量(MB)' from information_schema.tables group by table_schema order by sum(data_length) desc, sum(index_length) desc; 查看所有数据库各表容量大小 select table_schema as '数据库', table_name as '表名', table_rows as '记录数', truncate(data_length/1024/1024, 2) as '数据容量(MB)', truncate(index_length/1024/1024, 2) as '索引容量(MB)' from information_schema.tables order by data_length desc, index_length desc; 查看指定数据库容量大小 select table_schema as '数据库', sum(table_rows) as '记录数', sum(truncate(data_length/1024/1024, 2)) as '数据容量(MB)', sum(truncate(index_length/1024/1024, 2)) as '索引容量(MB)' from information_schema.tables where table_schema='mysql'; 查看指定数据库各表容量大小 select table_schema as '数据库', table_name as '表名', table_rows as '记录数', truncate(data_length/1024/1024, 2) as '数据容量(MB)', truncate(index_length/1024/1024, 2) as '索引容量(MB)' from information_schema.tables where table_schema='mysql' order by data_length desc, index_length desc; 17、开起慢查询日志 参考：采集MySQL慢查询日志到Elasticsearch中附录的第二小节 MySQL可以将执行超过指定时间的DQL、DML、DDL等语句记录下来。默认慢查询日志记录是关闭的 log_slow_queries ：表示是否开启慢查询日志，5.6以前的版本使用此参数指定是否开启慢查询日志，5.6以后的版本使用slow_query_log取代此参数，如果你使用的mysql版本刚好是5.5，那么你可以看到这两个参数同时存在，此时我们不用同时设置它们，设置这两个参数中的任何一个，另一个也会自动保持一致。 log_output : 表示当慢查询日志开启以后，以哪种方式存放，log_output可以设置为4种值，\"FILE\"、\"TABLE\"、\"FILE,TABLE\"、\"NONE\"。此值为\"FILE\"表示慢查询日志存放于指定的文件中，此值为\"TABLE\"表示慢查询日志存放于mysql库的slow_log表中，此值为\"FILE,TABLE\"表示将慢查询日志同时存放于指定的文件与slow_log表中，一般不会进行这样的设置，因为这样会徒增很多IO压力，如果开启，建议设置为\"table\",此值为\"NONE\"时表示不记录查询日志，即使slow_query_log设置为ON，如果log_output设置为NONE，也不会记录慢查询日志，其实，log_output不止用于控制慢查询日志的输出，查询日志的输出也是由此参数进行控制，也就是说，log_output设置为file，就表示查询日志和慢查询日志都存放到对应的文件中，设置为table，查询日志和慢查询日志就都存放在对应的数据库表中。 slow_query_log ：表示是否开启慢查询日志，此参数与log_slow_queries的作用没有区别，5.6以后的版本使用此参数替代log_slow_queries。 slow_query_log_file ：当使用文件存储慢查询日志时(log_output设置为\"FILE\"或者\"FILE,TABLE\"时)，指定慢查询日志存储于哪个日志文件中，默认的慢查询日志文件名为\"主机名-slow.log\"，慢查询日志的位置为datadir参数所对应的目录位置，一般情况下为 /var/lib/mysql long_query_time ：表示\"多长时间的查询\"被认定为\"慢查询\"，此值得默认值为10秒，表示超过10秒的查询被认定为慢查询。 log_queries_not_using_indexes ：表示如果运行的sql语句没有使用到索引，是否也被当做慢查询语句记录到慢查询日志中，OFF表示不记录，ON表示记录。 log_throttle_queries_not_using_indexes ：5.6.5版本新引入的参数，当log_queries_not_using_inde设置为ON时，没有使用索引的查询语句也会被当做慢查询语句记录到慢查询日志中，使用log_throttle_queries_not_using_indexes可以限制这种语句每分钟记录到慢查询日志中的次数，因为在生产环境中，有可能有很多没有使用索引的语句，此类语句频繁的被记录到慢查询日志中，可能会导致慢查询日志快速不断的增长，管理员可以通过此参数进行控制。 min_examined_row_limit ：扫描记录少于改值的SQL不记录到慢查询日志，结合去记录没有使用索引的SQL语句的例子，有可能存在某一个表，数据量维持在几行左右，且没有建立索引。这种表即使不建立索引，查询也很快，扫描记录很小，如果确定有这种表，则可以通过此参数设置，将这个SQL不记录到慢查询日志。 log_slow_admin_statements：记录超时的管理操作SQL到慢查询日志，比如ALTER/ANALYZE TABLE log_slow_slave_statements：在从服务器上开启慢查询日志 log_timestamps(5.7+)： 写入时区信息。可根据需求记录UTC时间或者服务器本地系统时间 查询慢日志是否开起等其他参数 # 查询慢日志是否开启 show variables like 'slow_query%'; # 查询多少秒的查询视为慢查询 show variables like 'long_query_time%'; # 查询慢查询日志输出到哪儿。 show variables like 'log_output%'; MySQL 8 set global log_output=‘FILE’; – 开启慢日志,纪录到 mysql.slow_log 表 set global long_query_time=2; – 设置超过0.1秒的查询为慢查询 set global slow_query_log=‘ON’;-- 打开慢日志记录 18、慢查询日志统计分析工具mysqldumpslow 通过mysqldumpslow命令我们可以更加方便的从不同的维度对慢日志进行排序、查找、统计。但是mysqldumpslow只能作用于慢查询日志文件 -s：排序规则参数 c: 执行计数 l: 锁定时间 r: 返回记录 t: 执行时间 al:平均锁定时间 ar:平均返回记录数 at:平均执行时间 -t 是top n的意思，返回多少条数据。 -g 可以跟上正则匹配模式，大小写不敏感。 # 得到返回记录最多的20个sql mysqldumpslow -s r -t 20 sqlslow.log # 得到平均访问次数最多的20条sql mysqldumpslow -s ar -t 20 sqlslow.log # 得到平均访问次数最多,并且里面含有ttt字符的20条sql mysqldumpslow -s ar -t 20 -g \"ttt\" sqldlow.log # 如果出现 -bash: mysqldumpslow: command not found 错误，请执行\"ln -s /usr/local/mysql/bin/mysqldumpslow /usr/bin\" # 如果出现如下错误，Died at /usr/bin/mysqldumpslow line 161, <> chunk 405659.说明你要分析的sql日志太大了，请拆分后再分析 19、锁表锁库操作 如果对mysql进行操作时，网络中断或SQL异常 , 可能会导致表或者库卡死 , 锁死，无法进行后续操作 （如果用 navicat 等工具连接操作, 操作都会在转圈圈，无法完成操作）。此时要找出造成锁库锁表的SQL语句的进程，将其杀死，中断其操作执行，即可解决锁库锁表。 ①查看锁死SQL的进程ID # 进程ID在trx_mysql_thread_id那一列 select id, db, user, host, command, time, state, info from information_schema.processlist where command != 'Sleep' order by time desc 或者 SELECT concat( 'kill ', id, ';' ) FROM information_schema.PROCESSLIST WHERE Command = 'Sleep' AND Time > 10 AND db = 'test' ORDER BY Time DESC; # sql语句在info列，进程ID在ID列 show processlist; ②杀死相关进程 kill -9 进程号 20、统计DB所有表的实际条数 如果从information_schema.tables 表统计获取各表的信息话，对于非事务性表，table_rows 这个值是精确的，对于事务性引擎，这个值通常是估算的。例如 MyISAM，存储精确的数目。对于其它存储引擎，比如 InnoDB ，本值是一个大约的数，与实际值相差可达 40 到 50% 。在这些情况下，使用 SELECT COUNT(*) 来获得准确的数目。对于在 information_schema 数据库中的表， Rows 值为 NULL 。 SELECT CONCAT( 'SELECT \"', TABLE_NAME, '\", COUNT(*) FROM ', TABLE_SCHEMA, '.', TABLE_NAME, ' UNION ALL' ) EXEC_SQL FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'DB名字'; 上述SQL会输出用于统计指定DB中所有表行数的SQL语句，复制以后，删除最后一行末尾的UNION ALL，然后执行即可获取所有表的实际条数 参考：https://commandnotfound.cn/sql/7/345/MySQL-%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8B%E5%90%84%E4%B8%AA%E8%A1%A8%E7%9A%84%E8%A1%8C%E6%95%B0%E4%BF%A1%E6%81%AF 21、Ubuntu20 APT安装MySQL5.7 wget https://dev.mysql.com/get/mysql-apt-config_0.8.12-1_all.deb dpkg -i mysql-apt-config_0.8.12-1_all.deb # ---会弹出CUI对话页面,安装后续提示确定即可---> Ubuntu Bionic，Ok--->这届 bash -c 'cat > /etc/apt/sources.list.d/mysql.list 22、设置MySQL8默认证方式 MySQL 8.0 调整了账号认证方式，把 caching_sha2_password 插件认证方式作为默认首选，这就导致很多需要使用密码登录的客户端，远程登录 MySQL 时报错： MySQL said: Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen( /usr/local/lib/plugin/caching_sha2_password.so, 2): not found MySQL添加以下配置到mysqld.conf中并重启 default_authentication_plugin=mysql_native_password 对于已有用户的认证方式变更 ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY '**密码**'; Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-04-29 11:14:58 "},"origin/mysql-user-privileges.html":{"url":"origin/mysql-user-privileges.html","title":"用户权限管理","keywords":"","body":"MySQL的用户及权限管理 一、用户管理 创建用户 方式1：CREATE USER命令 必须要拥有CREATE USER权限。 CREATE USER user[IDENTIFIED BY [PASSWORD] 'password'], [user[IDENTIFIED BY [PASSWORD] 'password']]... CREATE USER 'name'@'%' IDENTIFIED BY 'pwd'; 方式2：INSERT方式 必须拥有mysql.user表的INSERT权限。另外，ssl_cipher、x509_issuer、x509_subject等必须要设置值 INSERT INTO mysql.user(Host,User,Password,ssl_cipher,x509_issuer,x509_subject) VALUES('%','name',PASSWORD('pwd'),'','','') 删除用户 方式1：DROP USER命令 需要拥有DROP USER权限。 DROP USER user[,user]… user是需要删除的用户，由用户名(User)和主机名(Host)构成。 DROP USER name@'1.1.1.1' 方式2：DELETE方式 DELETE FROM mydb.user WHERE Host = '% AND User = 'admin'; 重命名用户 rename user 'jack'@'%' to 'jim'@'%'; 二、权限管理 1、用户权限验证过程 第一阶段：服务器首先会检查你是否允许连接。因为创建用户的时候会加上主机限制，可以限制成本地、某个IP、某个IP段、以及任何地方等，只允许你从配置的指定地方登陆。 第二阶段：如果你能连接，Mysql会检查你发出的每个请求，看你是否有足够的权限实施它。比如你要更新某个表、或者查询某个表，Mysql会查看你对哪个表或者某个列是否有权限。 2、用户授权原则 1、只授予能满足需要的最小权限，防止用户干坏事。比如用户只是需要查询，那就只给select权限就可以了，不要给用户赋予update、insert或者delete权限。 2、创建用户的时候限制用户的登录主机，一般是限制成指定IP或者内网IP段。 3、初始化数据库的时候删除没有密码的用户。安装完数据库的时候会自动创建一些用户，这些用户默认没有密码。 4、为每个用户设置满足密码复杂度的密码。 5、定期清理不需要的用户。回收权限或者删除用户。 3、GRANT命令授权语法 GRANT priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level TO user [auth_option] [, user [auth_option]] ... [REQUIRE {NONE | tls_option [[AND] tls_option] ...}] [WITH {GRANT OPTION | resource_option} ...] object_type: { TABLE | FUNCTION | PROCEDURE } priv_level: { * | *.* | db_name.* | db_name.tbl_name | tbl_name | db_name.routine_name } user由用户名(User)和主机名(Host)构成,中间用@隔开，最好加上单引号 auth_option: { IDENTIFIED BY 'auth_string' | IDENTIFIED WITH auth_plugin | IDENTIFIED WITH auth_plugin BY 'auth_string' | IDENTIFIED WITH auth_plugin AS 'auth_string' | IDENTIFIED BY PASSWORD 'auth_string' } tls_option: { SSL | X509 | CIPHER 'cipher' | ISSUER 'issuer' | SUBJECT 'subject' } resource_option: { | MAX_QUERIES_PER_HOUR count | MAX_UPDATES_PER_HOUR count | MAX_CONNECTIONS_PER_HOUR count | MAX_USER_CONNECTIONS count } Mysql权限层级 user表：全局层级 存储用户记录的表。关键字段有Host、User、Password。 创建对所有表有SELECT操作权限的用户 GRANT SELECT ON *.* TO name@'1.1.1.1' IDENTIFIED BY 'pwd'; db表：数据库层级 存储该用户对一个数据库所有的操作权限。关键字段有Host、User、Db。 授予所有权限 GRANT ALL ON mydb.* TO name@'1.1.1.1' IDENTIFIED BY 'pwd'; tables_priv表：表层级 记录了对一个表的单独授权记录.关键字段有Host、User、Db、Table_name、Table_priv、Column_priv。 当授权all在某张表的时候，Table_priv会有如下授权记录： Select,Insert,Update,Delete,Create,Drop,References,Index,Alter,Create View,Show view,Trigger。 单独授权表的某一列，会记录在此表的Column_priv里 GRANT UPDATE(age) ON mydb.user TO name@'1.1.1.1'; GRANT SELECT(birthday) ON mydb.user TO name@'1.1.1.1'; 此时会在另一张表columns_priv表中留下单独授权记录 columns_priv表：列层级 记录对表的某一列的授权记录。关键字段Host、User、Db、Table_name、Column_name。 procs_priv表：子程序层级 可以对存储过程和存储函数进行权限设置。关键字段Host、User、proc_priv MySQL 5.7的权限列表 官方文档：https://dev.mysql.com/doc/refman/5.7/en/privileges-provided.html Privilege Grant Table Column Context ALL [PRIVILEGES\\] Synonym for “all privileges” Server administration ALTER Alter_priv Tables ALTER ROUTINE Alter_routine_priv Stored routines CREATE Create_priv Databases, tables, or indexes CREATE ROUTINE Create_routine_priv Stored routines CREATE TABLESPACE Create_tablespace_priv Server administration CREATE TEMPORARY TABLES Create_tmp_table_priv Tables CREATE USER Create_user_priv Server administration CREATE VIEW Create_view_priv Views DELETE Delete_priv Tables DROP Drop_priv Databases, tables, or views EVENT Event_priv Databases EXECUTE Execute_priv Stored routines FILE File_priv File access on server host GRANT OPTION Grant_priv Databases, tables, or stored routines INDEX Index_priv Tables INSERT Insert_priv Tables or columns LOCK TABLES Lock_tables_priv Databases PROCESS Process_priv Server administration PROXY See proxies_priv table Server administration REFERENCES References_priv Databases or tables RELOAD Reload_priv Server administration REPLICATION CLIENT Repl_client_priv Server administration REPLICATION SLAVE Repl_slave_priv Server administration SELECT Select_priv Tables or columns SHOW DATABASES Show_db_priv Server administration SHOW VIEW Show_view_priv Views SHUTDOWN Shutdown_priv Server administration SUPER Super_priv Server administration TRIGGER Trigger_priv Tables UPDATE Update_priv Tables or columns USAGE Synonym for “no privileges” Server administration 4、查看用户权限 查看当前用户的权限 show grants; 查看某个用户的所有权限 show grants for 'jack'; show grants for admin'@'localhost'; 5、REVOKE命令回收权限语法 REVOKE priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level FROM user [, user] ... REVOKE ALL [PRIVILEGES], GRANT OPTION FROM user [, user] ... REVOKE PROXY ON user FROM user [, user] .. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/mysql-mode-only-full-groupby.html":{"url":"origin/mysql-mode-only-full-groupby.html","title":"MySQL SQL Mode : ONLY_FULL_GROUP_BY","keywords":"","body":"MySQL 5.7 GroupBy的语义检查 this is incompatible witn select_mode=only_full_group_mode ONLY_FULL_GROUP_BY是MySQL提供的一个sql_mode，通过这个sql_mode来提供SQL语句GROUP BY合法性的检查，在MySQL的sql_mode是非ONLY_FULL_GROUP_BY语义时。一条select语句，MySQL允许target list中输出的表达式是除聚集函数或group by column以外的表达式，这个表达式的值可能在经过group by操作后变成undefined，例如： mysql> create database test charset utf8mb4; mysql> use test; mysql> create table tt(id int,count int); mysql> insert into tt values(1,1),(1,2),(2,3),(2,4); mysql> select * from tt group by id; +------+-------+ | id | count | +------+-------+ | 1 | 1 | | 2 | 3 | +------+-------+ 2 rows in set (0.00 sec) 而对于语义限制都比较严谨的多家数据库，如SQLServer、Oracle、PostgreSql都不支持select target list中出现语义不明确的列，这样的语句在这些数据库中是会被报错的，所以从MySQL 5.7版本开始修正了这个语义，就是我们所说的ONLY_FULL_GROUP_BY语义，例如查看MySQL 5.7默认的sql_mode如下： mysql> select @@global.sql_mode; ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION 去掉ONLY_FULL_GROUP_BY模式，如下操作： mysql> set global sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'; #或者 mysql> set @@sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'; 上面是改变了全局sql_mode，对于新建的数据库有效。对于已存在的数据库，则需要在对应的数据下执行： mysql> set sql_mode ='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'; #再或者 在my.cnf 里面设置 sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION' 在sql_mode 中去掉only_full_group_by 我们把刚才的查询再次执行： mysql> select id,count from tt group by id; ERROR 1055 (42000): Expression #2 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'test.tt.count' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 刚才通过的查询语句被server拒绝掉了！ 所以ONLY_FULL_GROUP_BY的语义就是确定select target list中的所有列的值都是明确语义，简单的说来，在ONLY_FULL_GROUP_BY模式下，target list中的值要么是来自于聚集函数的结果，要么是来自于group by list中的表达式的值。但是由于表达式的表现形式非常丰富，对于程序来说，很难精确的确定一些表达式的输出结果是明确的，比如： mysql> select count from tt group by id+count,id; ERROR 1055 (42000): Expression #1 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'test.tt.count' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 在上面的查询语句中，其实count的值也是能被唯一确定的，但是由于程序无法分析出这种复杂的关系，所以这条查询也被拒绝掉了。 我们来看下哪些语句是在mysql的ONLY_FULL_GROUP_BY模式下是被支持的。 mysql> select id+1 from tt group by id+1; +------+ | id+1 | +------+ | 2 | | 3 | +------+ 2 rows in set (0.00 sec) 这条语句target list中的id+1和group by中的id+1是严格匹配的，所以mysql认为target list中的id+1是语义明确的，因此该语句可以通过。 但下面这条就无法通过了。 mysql> select id+1 from tt group by 1+id; ERROR 1055 (42000): Expression #1 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'test.tt.id' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 因此，如果查询语句中的target list, having condition 或者order by list里引用了的表达式不是聚集函数，但是和group by list中的表达式严格匹配，该语句也是合法的（id+1和id+1是严格匹配的，id+1和id+2在mysql认为是不严格匹配的， id+1和1+id也是不严格匹配的）。 mysql> select id,max(count) from tt group by count; ERROR 1055 (42000): Expression #1 of SELECT list is not in GROUP BY clause and contains nonaggregated column 'test.tt.id' which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 这条query被server拒绝掉了，因为target list中的id没有出现在聚集函数中，并且也没有出现在group by list中。 mysql> select id+1 as a from tt group by a order by id+1; +------+ | a | +------+ | 2 | | 3 | +------+ 2 rows in set (0.00 sec) mysql允许target list中对于非聚集函数的alias column被group by、having condition以及order by语句引用(version 5.7中允许having condition引用alias column，version 5.6不支持having condition引用alias column)，从上面两条语句可以看出，group by和order by中引用了alias column，并且其等价于基础列语义。 mysql> select id+count from tt group by id,count; +----------+ | id+count | +----------+ | 2 | | 3 | | 5 | | 6 | +----------+ 4 rows in set (0.00 sec) 从上面的语句可以看出，mysql的ONLY_FULL_GROUP_BY模式支持对basic column进行组合但是不支持对于复杂表达式进行组合，这个受限于表达式分析程度。 总结一下： MySQL对于ONLY_FULL_GROUP_BY语义的判断规则是，如果group by list中的表达式是basic column，那么target list中允许出现表达式是group by list中basic column或者alias column的组合结果，如果group by list中的表达式是复杂表达式(非basic column或者alias column)，那么要求target list中的表达式必须能够严格和group by list中的表达式进行匹配，否者这条查询会被认为不合法。 # 查看当前SQL MOD SHOW VARIABLES LIKE 'sql_mode'; # 设置临时会话级别的SQL MOD。会话结束后会自动恢复 set session sql_mode='STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION'; Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/mysql-binlog.html":{"url":"origin/mysql-binlog.html","title":"Binglog","keywords":"","body":"MySQL Binlog 一、简介 MySQL Binlog（MySQL Binary Log，MySQL的二进制日志文件）,它记录了所有的 DDL 和 DML 语句（除了数据查询语句select、show等），以事件形式记录，还包含语句所执行的消耗的时间。MySQL的二进制日志是事务安全型的，并以二进制的形式保存在磁盘中； 作用： 查看数据库的变更历史 数据库增量备份和恢复 MySQL的复制（主从数据库的复制） 二、Binglog 1、Binglog日志格式 ①STATEMENT格式：基于SQL语句的复制 每一条会修改数据的sql都会记录在binlog中。 在 MySQL 5.7.7 之前，默认的格式是 STATEMENT 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。 缺点：由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在slave得到和在master端执行时候相同 的结果。另外mysql 的复制,像一些特定函数功能，slave可与master上要保持一致会有很多相关问题。 注意：相比row能节约多少性能与日志量，这个取决于应用的SQL情况，正常同一条记录修改或者插入row格式所产生的日志量还小于Statement产生的日志量，但是考虑到如果带条件的update操作，以及整表删除，alter表等操作，ROW格式会产生大量日志，因此在考虑是否使用ROW格式日志时应该跟据应用的实际情况，其所产生的日志量会增加多少，以及带来的IO性能问题。 ②ROW格式：基于行的复制 5.1.5版本的MySQL才开始支持row level的复制,它不记录sql语句上下文相关信息，仅保存哪条记录被修改。 在 MySQL 5.7.7 及更高版本中，默认值是 ROW 优点： binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题. 缺点:所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容。 注意:新版本的MySQL中对row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变更的时候就会以statement模式来记录，如果sql语句确实就是update或者delete等修改数据的语句，那么还是会记录所有行的变更。 ③MIXED格式：混合模式复制 从5.1.8版本开始，MySQL提供了Mixed格式，实际上就是Statement与Row的结合。 在Mixed模式下，一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog，MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种。 2、Binlog日志文件 二进制日志索引文件（文件名后缀为.index）用于记录所有有效的的二进制文件 二进制日志文件（文件名后缀为.00000*）记录数据库所有的DDL和DML语句事件 binlog是一个二进制文件集合，每个binlog文件以一个4字节的魔数开头，接着是一组Events: 魔数：0xfe62696e对应的是0xfebin； Event：每个Event包含header和data两个部分；header提供了Event的创建时间，哪个服务器等信息，data部分提供的是针对该Event的具体信息，如具体数据的修改； 第一个Event用于描述binlog文件的格式版本，这个格式就是event写入binlog文件的格式； 其余的Event按照第一个Event的格式版本写入； 最后一个Event用于说明下一个binlog文件； binlog的索引文件是一个文本文件，其中内容为当前的binlog文件列表 3、Binlog事件类型 binlog 事件的结构主要有3个版本： Binlog 版本 MySQL版本 1 MySQL 3.23 - 支持 \"statement based replication events\" 2 MySQL 4.0.0 - 4.0.1 3 MySQL 4.0.2 - added the relay logs and changed the meaning of the log position 4 MySQL 5.0.0+ added the FORMAT_DESCRIPTION_EVENT and made the protocol extensible a FORMAT_DESCRIPTION_EVENT version = 4 a START_EVENT_V3 if event-size == 13 + 56: version = 1 if event-size == 19 + 56: version = 3 otherwise: invalid v4版本的binlog事件类型： Hex 事件类型 说明 0x00 UNKNOWN_EVENT 此事件从不会被触发，也不会被写入binlog中；发生在当读取binlog时，不能被识别其他任何事件，那被视为UNKNOWN_EVENT 0x01 START_EVENT_V3 每个binlog文件开始的时候写入的事件，此事件被用在MySQL3.23 – 4.1，MYSQL5.0以后已经被 FORMAT_DESCRIPTION_EVENT 取代 0x02 QUERY_EVENT 执行更新语句时会生成此事件，包括：create，insert，update，delete； 0x03 STOP_EVENT 当mysqld停止时生成此事件 0x04 ROTATE_EVENT 当mysqld切换到新的binlog文件生成此事件，切换到新的binlog文件可以通过执行flush logs命令或者binlog文件大于 max_binlog_size 参数配置的大小； 0x05 INTVAR_EVENT 当sql语句中使用了AUTO_INCREMENT的字段或者LAST_INSERT_ID()函数；此事件没有被用在binlog_format为ROW模式的情况下 0x06 LOAD_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL 3.23版本中使用 0x07 SLAVE_EVENT 未使用 0x08 CREATE_FILE_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL4.0和4.1版本中使用 0x09 APPEND_BLOCK_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL4.0版本中使用 0x0a EXEC_LOAD_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL4.0和4.1版本中使用 0x0b DELETE_FILE_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL4.0版本中使用 0x0c NEW_LOAD_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL4.0和4.1版本中使用 0x0d RAND_EVENT 执行包含RAND()函数的语句产生此事件，此事件没有被用在binlog_format为ROW模式的情况下 0x0e USER_VAR_EVENT 执行包含了用户变量的语句产生此事件，此事件没有被用在binlog_format为ROW模式的情况下 0x0f FORMAT_DESCRIPTION_EVENT 描述事件，被写在每个binlog文件的开始位置，用在MySQL5.0以后的版本中，代替了START_EVENT_V3 0x10 XID_EVENT 支持XA的存储引擎才有，本地测试的数据库存储引擎是innodb，所有上面出现了XID_EVENT；innodb事务提交产生了QUERY_EVENT的BEGIN声明，QUERY_EVENT以及COMMIT声明，如果是myIsam存储引擎也会有BEGIN和COMMIT声明，只是COMMIT类型不是XID_EVENT 0x11 BEGIN_LOAD_QUERY_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL5.0版本中使用 0x12 EXECUTE_LOAD_QUERY_EVENT 执行LOAD DATA INFILE 语句时产生此事件，在MySQL5.0版本中使用 0x13 TABLE_MAP_EVENT 用在binlog_format为ROW模式下，将表的定义映射到一个数字，在行操作事件之前记录（包括：WRITE_ROWS_EVENT，UPDATE_ROWS_EVENT，DELETE_ROWS_EVENT） PRE_GA_WRITE_ROWS_EVENT 已过期，被 WRITE_ROWS_EVENT 代替 PRE_GA_UPDATE_ROWS_EVENT 已过期，被 UPDATE_ROWS_EVENT 代替 PRE_GA_DELETE_ROWS_EVENT 已过期，被 DELETE_ROWS_EVENT 代替 0x14 WRITE_ROWS_EVENT 用在binlog_format为ROW模式下，对应 insert 操作 0x15 UPDATE_ROWS_EVENT 用在binlog_format为ROW模式下，对应 update 操作 0x16 DELETE_ROWS_EVENT 用在binlog_format为ROW模式下，对应 delete 操作 0x1a INCIDENT_EVENT 主服务器发生了不正常的事件，通知从服务器并告知可能会导致数据处于不一致的状态 0x1b HEARTBEAT_LOG_EVENT 主服务器告诉从服务器，主服务器还活着，不写入到日志文件中 4、Binlog事件结构 一个事件对象分为事件头和事件体，事件的结构如下： +=====================================+ | event | timestamp 0 : 4 | | header +----------------------------+ | | type_code 4 : 1 | | +----------------------------+ | | server_id 5 : 4 | | +----------------------------+ | | event_length 9 : 4 | | +----------------------------+ | | next_position 13 : 4 | | +----------------------------+ | | flags 17 : 2 | | +----------------------------+ | | extra_headers 19 : x-19 | +=====================================+ | event | fixed part x : y | | data +----------------------------+ | | variable part | +=====================================+ 如果事件头的长度是 x 字节，那么事件体的长度为 (event_length - x) 字节；设事件体中 fixed part 的长度为 y 字节，那么 variable part 的长度为 (event_length - (x + y)) 字节 三、配置Binlog 1、配置 在/etc/my.cnf的[mysqld]部分添加以下配置，然后重启MySQL [mysqld] log-bin=/data/mysql/logs/binlogs/mysql-bin.log expire-logs-days=14 max-binlog-size=500M server-id=1 binlog_format=ROW binlog_row_image=FULL relay_log_info_repository=TABLE 2、验证 show variables like '%binlog%' # 检查MySQL是否已经开启binlog show variables like 'log_bin' # 查看binlog文件列表及大小 show binary logs # 查看binlog内容 show binlog events # 查看当前最新一个binlog日志文件的状态信息，显示正在写入的二进制文件，及当前position show master status; #查看所有binlog日志列表 show master logs; 3、注意 当遇到以下3种情况时，MySQL会重新生成一个新的日志文件，文件序号递增： MySQL服务器停止或重启时 使用 flush logs 命令； 当 binlog 文件大小超过 max_binlog_size 变量的值时； max_binlog_size 的最小值是4096字节，最大值和默认值是 1GB (1073741824字节)。事务被写入到binlog的一个块中，所以它不会在几个二进制日志之间被拆分。因此，如果你有很大的事务，为了保证事务的完整性，不可能做切换日志的动作，只能将该事务的日志都记录到当前日志文件中，直到事务结束，你可能会看到binlog文件大于 max_binlog_size 的情况。 四、使用mysqlbinlog命令查看Binlog 因为binlog日志文件是二进制文件，没法用vi等打开，这时就需要mysql的自带的mysqlbinlog工具进行解码 mysqlbinlog [参数] binlog文件路径 参数： -?, --help Display this help and exit. --base64-output=name Determine when the output statements should be base64-encoded BINLOG statements: 'never' disables it and works only for binlogs without row-based events; 'decode-rows' decodes row events into commented pseudo-SQL statements if the --verbose option is also given; 'auto' prints base64 only when necessary (i.e., for row-based events and format description events). If no --base64-output[=name] option is given at all, the default is 'auto'. --bind-address=name IP address to bind to. --character-sets-dir=name Directory for character set files. -d, --database=name List entries for just this database (local log only). --debug-check Check memory and open file usage at exit . --debug-info Print some debug info at exit. --default-auth=name Default authentication client-side plugin to use. -D, --disable-log-bin Disable binary log. This is useful, if you enabled --to-last-log and are sending the output to the same MySQL server. This way you could avoid an endless loop. You would also like to use it when restoring after a crash to avoid duplication of the statements you already have. NOTE: you will need a SUPER privilege to use this option. -F, --force-if-open Force if binlog was not closed properly. (Defaults to on; use --skip-force-if-open to disable.) -f, --force-read Force reading unknown binlog events. -H, --hexdump Augment output with hexadecimal and ASCII event dump. -h, --host=name Get the binlog from server. -l, --local-load=name Prepare local temporary files for LOAD DATA INFILE in the specified directory. -o, --offset=# Skip the first N entries. -p, --password[=name] Password to connect to remote server. --plugin-dir=name Directory for client-side plugins. -P, --port=# Port number to use for connection or 0 for default to, in order of preference, my.cnf, $MYSQL_TCP_PORT, /etc/services, built-in default (3306). --protocol=name The protocol to use for connection (tcp, socket, pipe, memory). -R, --read-from-remote-server Read binary logs from a MySQL server. This is an alias for read-from-remote-master=BINLOG-DUMP-NON-GTIDS. --read-from-remote-master=name Read binary logs from a MySQL server through the COM_BINLOG_DUMP or COM_BINLOG_DUMP_GTID commands by setting the option to either BINLOG-DUMP-NON-GTIDS or BINLOG-DUMP-GTIDS, respectively. If --read-from-remote-master=BINLOG-DUMP-GTIDS is combined with --exclude-gtids, transactions can be filtered out on the master avoiding unnecessary network traffic. --raw Requires -R. Output raw binlog data instead of SQL statements, output is to log files. -r, --result-file=name Direct output to a given file. With --raw this is a prefix for the file names. --secure-auth Refuse client connecting to server if it uses old (pre-4.1.1) protocol. (Defaults to on; use --skip-secure-auth to disable.) --server-id=# Extract only binlog entries created by the server having the given id. --server-id-bits=# Set number of significant bits in server-id --set-charset=name Add 'SET NAMES character_set' to the output. -s, --short-form Just show regular queries: no extra info and no row-based events. This is for testing only, and should not be used in production systems. If you want to suppress base64-output, consider using --base64-output=never instead. -S, --socket=name The socket file to use for connection. --start-datetime=name Start reading the binlog at first event having a datetime equal or posterior to the argument; the argument must be a date and time in the local time zone, in any format accepted by the MySQL server for DATETIME and TIMESTAMP types, for example: 2004-12-25 11:25:56 (you should probably use quotes for your shell to set it properly). -j, --start-position=# Start reading the binlog at position N. Applies to the first binlog passed on the command line. --stop-datetime=name Stop reading the binlog at first event having a datetime equal or posterior to the argument; the argument must be a date and time in the local time zone, in any format accepted by the MySQL server for DATETIME and TIMESTAMP types, for example: 2004-12-25 11:25:56 (you should probably use quotes for your shell to set it properly). --stop-never Wait for more data from the server instead of stopping at the end of the last log. Implicitly sets --to-last-log but instead of stopping at the end of the last log it continues to wait till the server disconnects. --stop-never-slave-server-id=# The slave server_id used for --read-from-remote-server --stop-never. --stop-position=# Stop reading the binlog at position N. Applies to the last binlog passed on the command line. -t, --to-last-log Requires -R. Will not stop at the end of the requested binlog but rather continue printing until the end of the last binlog of the MySQL server. If you send the output to the same MySQL server, that may lead to an endless loop. -u, --user=name 连接远程服务器的用户名。 -v, --verbose Reconstruct pseudo-SQL statements out of row events. -v -v adds comments on column data types. -V, --version 打印mysqlbinlog的版本信息并退出 --open-files-limit=# Used to reserve file descriptors for use by this program. -c, --verify-binlog-checksum Verify checksum binlog events. --binlog-row-event-max-size=# The maximum size of a row-based binary log event in bytes. Rows will be grouped into events smaller than this size if possible. 改值必须是256的倍数。 指定基于行的binlog的大小， --skip-gtids Do not print Global Transaction Identifier information (SET GTID_NEXT=... etc). --include-gtids=name Print events whose Global Transaction Identifiers were provided. --exclude-gtids=name Print all events but those whose Global Transaction Identifiers were provided. Variables (--variable-name=value) and boolean options {FALSE|TRUE} Value (after reading options) --------------------------------- ---------------------------------------- base64-output (No default value) bind-address (No default value) character-sets-dir (No default value) database (No default value) debug-check FALSE debug-info FALSE default-auth (No default value) disable-log-bin FALSE force-if-open TRUE force-read FALSE hexdump FALSE host (No default value) local-load (No default value) offset 0 plugin-dir (No default value) port 0 read-from-remote-server FALSE read-from-remote-master (No default value) raw FALSE result-file (No default value) secure-auth TRUE server-id 0 server-id-bits 32 set-charset (No default value) short-form FALSE socket /data/mysql/data/mysqld.sock start-datetime (No default value) start-position 4 stop-datetime (No default value) stop-never FALSE stop-never-slave-server-id -1 stop-position 18446744073709551615 to-last-log FALSE user (No default value) open-files-limit 64 verify-binlog-checksum FALSE binlog-row-event-max-size 4294967040 skip-gtids FALSE include-gtids (No default value) exclude-gtids (No default value) 五、使用SQL语句查看Binlog show binlog events [IN 'log_name'] [FROM pos] [LIMIT [offset,] row_count]; -- IN 'log_name' ：指定要查询的binlog文件名(不指定就是第一个binlog文件) -- FROM pos ：指定从哪个pos起始点开始查起(不指定就是从整个文件首个pos点开始算) -- LIMIT [offset,] ：偏移量(不指定就是0) -- row_count ：查询总条数(不指定就是所有行) 参考： https://blog.csdn.net/ouyang111222/article/details/50300851 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-09-30 17:53:48 "},"origin/mysql-backup-restore.html":{"url":"origin/mysql-backup-restore.html","title":"MySQL的数据备份与恢复","keywords":"","body":"MySQL的数据库备份与恢复 一、前言 我们试着想一想, 在生产环境中什么最重要？如果我们服务器的硬件坏了可以维修或者换新, 软件问题可以修复或重新安装, 但是如果数据没了呢？这可能是最恐怖的事情了吧, 我感觉在生产环境中应该没有什么比数据跟更为重要. 那么我们该如何保证数据不丢失、或者丢失后可以快速恢复呢？只要看完这篇, 大家应该就能对MySQL中实现数据备份和恢复能有一定的了解。 二、为什么需要备份数据？ 其实在前言中也大概说明了为什么要备份数据, 但是我们还是应该具体了解一下为什么要备份数据 在生产环境中我们数据库可能会遭遇各种各样的不测从而导致数据丢失, 大概分为以下几种. 硬件故障 软件故障 自然灾害 黑客攻击 误操作 (占比最大) 所以, 为了在数据丢失之后能够恢复数据, 我们就需要定期的备份数据, 备份数据的策略要根据不同的应用场景进行定制, 大致有几个参考数值, 我们可以根据这些数值从而定制符合特定环境中的数据备份策略 能够容忍丢失多少数据 恢复数据需要多长时间 需要恢复哪一些数据 三、数据的备份类型 数据的备份类型根据其自身的特性主要分为以下几组 完全备份：指的是备份整个数据集( 即整个数据库 ) 部分备份：指的是备份部分数据集(例如: 只备份一个表) 增量备份：指的是备份自上一次备份以来(增量或完全)以来变化的数据; 特点: 节约空间、还原麻烦 差异备份：指的是备份自上一次完全备份以来变化的数据 特点: 浪费空间、还原比增量备份简单 四、MySQL备份数据的方式 在MySQl中我们备份数据一般有几种方式 热备份：指的是当数据库进行备份时, 数据库的读写操作均不是受影响 温备份：指的是当数据库进行备份时, 数据库的读操作可以执行, 但是不能执行写操作 冷备份：指的是当数据库进行备份时, 数据库不能进行读写操作, 即数据库要下线 MySQL中进行不同方式的备份还要考虑存储引擎是否支持 MyISAM InnoDB 热备 × √ 温备 √ √ 冷备 √ √ 我们在考虑完数据在备份时, 数据库的运行状态之后还需要考虑对于MySQL数据库中数据的备份方式 物理备份一般就是通过tar,cp等命令直接打包复制数据库的数据文件达到备份的效果 逻辑备份一般就是通过特定工具从数据库中导出数据并另存备份(逻辑备份会丢失数据精度) 物理备份 逻辑备份 五、备份需要考虑的问题 一般情况下, 我们需要备份的数据分为以下几种 数据 二进制日志, InnoDB事务日志 代码(存储过程、存储函数、触发器、事件调度器) 服务器配置文件 六、备份工具 这里我们列举出常用的几种备份工具 mysqldump : 逻辑备份工具, 适用于所有的存储引擎, 支持温备、完全备份、部分备份、对于InnoDB存储引擎支持热备 cp, tar 等归档复制工具: 物理备份工具, 适用于所有的存储引擎, 冷备、完全备份、部分备份 lvm2 snapshot: 几乎热备, 借助文件系统管理工具进行备份 mysqlhotcopy: 名不副实的的一个工具, 几乎冷备, 仅支持MyISAM存储引擎 xtrabackup: 一款非常强大的InnoDB/XtraDB热备工具, 支持完全备份、增量备份, 由percona提供 七、设计合适的备份策略 针对不同的场景下, 我们应该制定不同的备份策略对数据库进行备份, 一般情况下, 备份策略一般为以下三种 直接cp,tar复制数据库文件 mysqldump+复制BIN LOGS lvm2快照+复制BIN LOGS xtrabackup 以上的几种解决方案分别针对于不同的场景 如果数据量较小, 可以使用第一种方式, 直接复制数据库文件 如果数据量还行, 可以使用第二种方式, 先使用mysqldump对数据库进行完全备份, 然后定期备份BINARY LOG达到增量备份的效果 如果数据量一般, 而又不过分影响业务运行, 可以使用第三种方式, 使用lvm2的快照对数据文件进行备份, 而后定期备份BINARY LOG达到增量备份的效果 如果数据量很大, 而又不过分影响业务运行, 可以使用第四种方式, 使用xtrabackup进行完全备份后, 定期使用xtrabackup进行增量备份或差异备份 八、实战演练 1、使用cp进行备份 我们这里使用的是使用yum安装的mysql-5.1的版本, 使用的数据集为从网络上找到的一个员工数据库 查看数据库的信息 mysql> SHOW DATABASES; #查看当前的数据库, 我们的数据库为employees +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) mysql> USE employees; Database changed mysql> SHOW TABLES; #查看当前库中的表 +---------------------+ | Tables_in_employees | +---------------------+ | departments | | dept_emp | | dept_manager | | employees | | salaries | | titles | +---------------------+ 6 rows in set (0.00 sec) mysql> SELECT COUNT(*) FROM employees; #由于篇幅原因, 我们这里只看一下employees的行数为300024 +----------+ | COUNT(*) | +----------+ | 300024 | +----------+ 1 row in set (0.05 sec) 向数据库施加读锁 mysql> FLUSH TABLES WITH READ LOCK; #向所有表施加读锁 Query OK, 0 rows affected (0.00 sec) 备份数据文件 mkdir /backup #创建文件夹存放备份数据库文件 cp -a /var/lib/mysql/* /backup #保留权限的拷贝源数据文件 ls /backup #查看目录下的文件 employees ibdata1 ib_logfile0 ib_logfile1 mysql mysql.sock test 模拟数据丢失并恢复 rm -rf /var/lib/mysql/* #删除数据库的所有文件 service mysqld restart #重启MySQL, 如果是编译安装的应该不能启动, 如果rpm安装则会重新初始化数据库 mysql> SHOW DATABASES; #因为我们是rpm安装的, 连接到MySQL进行查看, 发现数据丢失了！ +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | test | +--------------------+ 3 rows in set (0.00 sec) rm -rf /var/lib/mysql/* #这一步可以不做 cp -a /backup/* /var/lib/mysql/ #将备份的数据文件拷贝回去 service mysqld restart #重启MySQL #重新连接数据并查看 mysql> SHOW DATABASES; #数据库已恢复 +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) mysql> USE employees; mysql> SELECT COUNT(*) FROM employees; #表的行数没有变化 +----------+ | COUNT(*) | +----------+ | 300024 | +----------+ 1 row in set (0.06 sec) 2、使用mysqldump+复制BINARY LOG备份 我们这里使用的是使用yum安装的mysql-5.1的版本, 使用的数据集为从网络上找到的一个员工数据库 我们通过mysqldump进行一次完全备份, 再修改表中的数据, 然后再通过binary log进行恢复 二进制日志需要在mysql配置文件中添加 log_bin=on 开启 mysqldump命令介绍 mysqldump是一个客户端的逻辑备份工具, 可以生成一个重现创建原始数据库和表的SQL语句, 可以支持所有的存储引擎, 对于InnoDB支持热备 官方文档介绍 #基本语法格式 shell> mysqldump [options] db_name [tbl_name ...] 恢复需要手动CRATE DATABASES shell> mysqldump [options] --databases db_name ... 恢复不需要手动创建数据库 shell> mysqldump [options] --all-databases 恢复不需要手动创建数据库 其他选项: -E, --events: 备份事件调度器 -R, --routines: 备份存储过程和存储函数 --triggers: 备份表的触发器; --skip-triggers --master-date[=value] 1: 记录为CHANGE MASTER TO 语句、语句不被注释 2: 记录为注释的CHANGE MASTER TO语句 基于二进制还原只能全库还原 --flush-logs: 日志滚动 锁定表完成后执行日志滚动 查看数据库的信息 mysql> SHOW DATABASES; #查看当前的数据库, 我们的数据库为employees +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) mysql> USE employees; Database changed mysql> SHOW TABLES; #查看当前库中的表 +---------------------+ | Tables_in_employees | +---------------------+ | departments | | dept_emp | | dept_manager | | employees | | salaries | | titles | +---------------------+ 6 rows in set (0.00 sec) mysql> SELECT COUNT(*) FROM employees; #由于篇幅原因, 我们这里只看一下employees的行数为300024 +----------+ | COUNT(*) | +----------+ | 300024 | +----------+ 1 row in set (0.05 sec) 使用mysqldump备份数据库 mysql -uroot -p -e 'SHOW MASTER STATUS' #查看当前二进制文件的状态, 并记录下position的数字 +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000003 | 106 | | | +------------------+----------+--------------+------------------+ mysqldump --all-databases --lock-all-tables > backup.sql #备份数据库到backup.sql文件中 mysql> CREATE DATABASE TEST1; #创建一个数据库 Query OK, 1 row affected (0.00 sec) mysql> SHOW MASTER STATUS; #记下现在的position +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000003 | 191 | | | +------------------+----------+--------------+------------------+ 1 row in set (0.00 sec) cp /var/lib/mysql/mysql-bin.000003 /root #备份二进制文件 service mysqld stop #停止MySQL rm -rf /var/lib/mysql/* #删除所有的数据文件 service mysqld start #启动MySQL, 如果是编译安装的应该不能启动(需重新初始化), 如果rpm安装则会重新初始化数据库 mysql> SHOW DATABASES; #查看数据库, 数据丢失! +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | test | +--------------------+ 3 rows in set (0.00 sec) mysql> SET sql_log_bin=OFF; #暂时先将二进制日志关闭 Query OK, 0 rows affected (0.00 sec) mysql> source backup.sql #恢复数据，所需时间根据数据库时间大小而定 mysql> SET sql_log_bin=ON; 开启二进制日志 mysql> SHOW DATABASES; #数据库恢复, 但是缺少TEST1 +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) mysqlbinlog --start-position=106 --stop-position=191 mysql-bin.000003 | mysql employees #通过二进制日志增量恢复数据 mysql> SHOW DATABASES; #现在TEST1出现了！ +--------------------+ | Database | +--------------------+ | information_schema | | TEST1 | | employees | | mysql | | test | +--------------------+ 5 rows in set (0.00 sec) 3、使用lvm2快照备份数据 做实验之前我们先回顾一下lvm2-snapshot的知识 LVM快照简单来说就是将所快照源分区一个时间点所有文件的元数据进行保存，如果源文件没有改变，那么访问快照卷的相应文件则直接指向源分区的源文件，如果源文件发生改变，则快照卷中与之对应的文件不会发生改变。快照卷主要用于辅助备份文件。 这里只简单介绍，点击查看详细介绍 部署lvm环境 添加硬盘; 这里我们直接实现SCSI硬盘的热插拔, 首先在虚拟机中添加一块硬盘, 不重启 ls /dev/sd* #只有以下几块硬盘, 但是我们不重启可以让系统识别新添加的硬盘 /dev/sda /dev/sda1 /dev/sda2 echo '- - -' > /sys/class/scsi_host/host0/scan echo '- - -' > /sys/class/scsi_host/host1/scan echo '- - -' > /sys/class/scsi_host/host2/scan ls /dev/sd* #看！sdb识别出来了 /dev/sda /dev/sda1 /dev/sda2 /dev/sdb fdisk /dev/sdb #分区,步骤省略 partx -a /dev/sdb BLKPG: Device or resource busy error adding partition 1 ##创建逻辑卷 pvcreate /dev/sdb1 Physical volume \"/dev/sdb1\" successfully created vgcreate myvg /dev/sdb1 Volume group \"myvg\" successfully created lvcreate -n mydata -L 5G myvg Logical volume \"mydata\" created. mkfs.ext4 /dev/mapper/myvg-mydata #格式化 mkdir /lvm_data mount /dev/mapper/myvg-mydata /lvm_data #挂载到/lvm_data vim /etc/my.cnf #修改mysql配置文件的datadir如下 datadir=/lvm_data service mysqld restart #重启MySQL ####重新导入employees数据库########略过#### 查看数据库的信息 mysql> SHOW DATABASES; #查看当前的数据库, 我们的数据库为employees +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) mysql> USE employees; Database changed mysql> SHOW TABLES; #查看当前库中的表 +---------------------+ | Tables_in_employees | +---------------------+ | departments | | dept_emp | | dept_manager | | employees | | salaries | | titles | +---------------------+ 6 rows in set (0.00 sec) mysql> SELECT COUNT(*) FROM employees; #由于篇幅原因, 我们这里只看一下employees的行数为300024 +----------+ | COUNT(*) | +----------+ | 300024 | +----------+ 1 row in set (0.05 sec) 创建快照卷并备份 mysql> FLUSH TABLES WITH READ LOCK; #锁定所有表 Query OK, 0 rows affected (0.00 sec) [root@node1 lvm_data]# lvcreate -L 1G -n mydata-snap -p r -s /dev/mapper/myvg-mydata #创建快照卷 Logical volume \"mydata-snap\" created. mysql> UNLOCK TABLES; #解锁所有表 Query OK, 0 rows affected (0.00 sec) [root@node1 lvm_data]# mkdir /lvm_snap #创建文件夹 [root@node1 lvm_data]# mount /dev/myvg/mydata-snap /lvm_snap/ #挂载snap mount: block device /dev/mapper/myvg-mydata--snap is write-protected, mounting read-only [root@node1 lvm_data]# cd /lvm_snap/ [root@node1 lvm_snap]# ls employees ibdata1 ib_logfile0 ib_logfile1 mysql mysql-bin.000001 mysql-bin.000002 mysql-bin.000003 mysql-bin.index test [root@node1 lvm_snap]# tar cf /tmp/mysqlback.tar * #打包文件到/tmp/mysqlback.tar umount /lvm_snap/ #卸载snap lvremove myvg mydata-snap #删除snap 恢复数据 [root@node1 lvm_snap]# rm -rf /lvm_data/* service mysqld start #启动MySQL, 如果是编译安装的应该不能启动(需重新初始化), 如果rpm安装则会重新初始化数据库 mysql> SHOW DATABASES; #查看数据库, 数据丢失! +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | test | +--------------------+ 3 rows in set (0.00 sec) cd /lvm_data/ [root@node1 lvm_data]# rm -rf * #删除所有文件 [root@node1 lvm_data]# tar xf /tmp/mysqlback.tar #解压备份数据库到此文件夹 [root@node1 lvm_data]# ls #查看当前的文件 employees ibdata1 ib_logfile0 ib_logfile1 mysql mysql-bin.000001 mysql-bin.000002 mysql-bin.000003 mysql-bin.index test mysql> SHOW DATABASES; #数据恢复了 +--------------------+ | Database | +--------------------+ | information_schema | | employees | | mysql | | test | +--------------------+ 4 rows in set (0.00 sec) ##完成 4、使用Xtrabackup备份 九、阿里云RDS MYSQL物理备份恢复 1、全量全库恢复 以恢复RDS MySQL 5.6到ubuntu 18.04中为例 ①下载物理备份文件 在阿里云控制台RSD实例的备份与恢复页面下载要恢复的物理备份文件 ②解压 解压后文件很大，注意磁盘容量 tar -izxvf .tar.gz -C /data/mysql/data ③安装Percona XtraBackup 注意Percona XtraBackup版本 MySQL 5.6及之前的版本需要安装 Percona XtraBackup 2.3，安装指导请参见官方文档Percona XtraBackup 2.3。 MySQL 5.7版本需要安装 Percona XtraBackup 2.4，安装指导请参见官方文档Percona XtraBackup 2.4。 MySQL 8.0版本需要安装 Percona XtraBackup 8.0，安装指导请参见官方文档Percona XtraBackup 8.0。 # ubuntu 安装Percona XtraBackup2.3 wget https://repo.percona.com/apt/percona-release_0.1-6.$(lsb_release -sc)_all.deb dpkg -i percona-release_0.1-6.bionic_all.deb apt-get update apt-get install percona-xtrabackup # CentOS 安装Percona XtraBackup2.3 yum install -y perl rsync perl-Data-Dumper wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.3.5/binary/redhat/7/x86_64/percona-xtrabackup-2.3.5-1.el7.x86_64.rpm yum clean all yum repolist yum localinstall percona-xtrabackup-2.3.2-1.el7.x86_64.rpm ④安装MySQL Deb安装 # 安装MySQL 5.6 wget https://downloads.mysql.com/archives/get/p/23/file/mysql-community-server_5.6.46-1debian8_amd64.deb wget https://downloads.mysql.com/archives/get/p/23/file/mysql-community-client_5.6.46-1debian8_amd64.deb wget https://downloads.mysql.com/archives/get/p/23/file/mysql-server_5.6.46-1debian8_amd64.deb wget https://downloads.mysql.com/archives/get/p/23/file/mysql-client_5.6.46-1debian8_amd64.deb dpkg -i mysql* YUM 安装 bash -c 'cat > /etc/yum.repos.d/mysql.repo ⑤恢复解压后的备份文件 ## MySQL 5.6/5.7 innobackupex --defaults-file=/data/mysql/data/backup-my.cnf --apply-log /data/mysql/data ## MySQL 8.0 xtrabackup --prepare --target-dir=/data/mysql/data xtrabackup --datadir=/var/lib/mysql --copy-back --target-dir=/home/mysql/data 若系统返回如下类似结果，则说明备份文件已成功恢复到自建数据库 修改解压后文件的权限 chown -R mysql.mysql /data/mysql/data ⑥修改配置参数 修改/data/mysql/data/backup-my.cnf中的配置参数，自建数据库不支持如下参数，需要注释掉。 #innodb_log_checksum_algorithm #innodb_fast_checksum #innodb_log_block_size #innodb_doublewrite_file #rds_encrypt_data #innodb_encrypt_algorithm #redo_log_version #master_key_id #server_uuid 修改过后的配置文件 [mysqld] innodb_checksum_algorithm=innodb #innodb_log_checksum_algorithm=innodb innodb_data_file_path=ibdata1:200M:autoextend innodb_log_files_in_group=2 innodb_log_file_size=524288000 #innodb_fast_checksum=false innodb_page_size=16384 #innodb_log_block_size=512 innodb_undo_directory=. innodb_undo_tablespaces=0 #rds_encrypt_data=false #innodb_encrypt_algorithm=aes_128_ecb skip-grant-tables ⑦启动 mysqld_safe --defaults-file=/data/mysql/data/backup-my.cnf --user=mysql --datadir=/data/mysql/data 启动的日志会放在/data/mysql/data/主机名.err文件中，如果没有“ERR”级别报错，即启动成功。 ⑧修改root密码 由于上一步中MySQL配置文件中添加skip-grant-tables,此时，是可以不用输入密码就可以登录 如果原始数据库中，root用户没有远程登录权限，可在跳过密码验证阶段，使用navicat连接上后，直接在界面更新mysql.user的root用户localhost为root用户%的权限。或者使用以下SQL进行重置 mysql -u root -p mysql > use mysql mysql > update user set PASSWORD = PASSWORD('***新密码***') where user = 'root'; mysql > flush privileges; 修改完成后，将配置文件中的skip-grant-tables删掉，禁止跳过密码登录，重新部署启动。 mysql -u root -h127.0.0.1 -p mysql > GRANT ALL PRIVILEGES ON *.* TO root@\"%\" IDENTIFIED BY '**新密码***' WITH GRANT OPTION; mysql > flush privileges; 2、单表物理.ibd文件恢复 MySQL 5.6 版本引入了可传输表空间(transportable tablespace) 的方法，可以通过导出 + 导入表空间的方式，实现物理拷贝表的功能。 .frm文件：保存了每个表的元数据，包括表结构的定义等，该文件与数据库引擎无关。 .ibd文件：InnoDB引擎开启了独立表空间(my.ini中配置innodb_file_per_table = 1)产生的存放该表的数据和索引的文件 以恢复到本地电脑MySQL 5.6中为例 ①解压要恢复的表.ibd文件 # 查看压缩文件中的内容 tar -tf 备份文件 # 解压指定库名表名的物理文件 tar -zxvf 备份文件 --occurrence 库名/表名.ibd ## --occurrence参数默认会在解压到第一次匹配的文件后不再处理后续解压。极大节省了解压时间 ②安装MySQL 5.6 MacOS # 安装 brew install mysql@5.6 # 启动 /usr/local/opt/mysql@5.6/bin/mysql.server start # 登录 mysql -uroot ③新建相同表结构 mysql> create database DB名（可随意）; mysql> use DB名 ; # 这个时候会生成 frm ， ibd 文件 ④获取源表的创建语句并创建表 # 获取源表的创建语句并创建表 mysql> create table 表名 ⑤删除表空间 Alter table 表名 discard tablespace ⑥复制 ibd 文件到MySQL 文件目录下 mv 表名.ibd /usr/local/var/mysql/库名 ⑦导入表空间 alter table 表名 import tablespace 导入表时如果报Error Code: 1808. Schema mismatch (Table has ROW_TYPE_DYNAMIC row format, .ibd file has ROW_TYPE_COMPACT row format.的错误，则在建表语句后加上ROW_FORMAT=COMPACT重新来一遍上面的操作。 ⑧验证 登录本地数据库即可看到恢复的单个表的数据 十、总结 备份方法 备份速度 恢复速度 便捷性 功能 一般用于 cp 快 快 一般、灵活性低 很弱 少量数据备份 mysqldump 慢 慢 一般、可无视存储引擎的差异 一般 中小型数据量的备份 lvm2快照 快 快 一般、支持几乎热备、速度快 一般 中小型数据量的备份 xtrabackup 较快 较快 实现innodb热备、对存储引擎有要求 强大 较大规模的备份 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-09-28 14:52:46 "},"origin/mysql-dumper.html":{"url":"origin/mysql-dumper.html","title":"MysqlDump常用操作","keywords":"","body":"MysqlDump常用操作 1、备份所有数据库 $> mysqldump -uroot -proot --all-databases > /data/mysql-backup/all.sql 2、备份db1、db2两个数据库的所有数据 $> mysqldump -uroot -proot --databases db1 db2 > /data/mysql-backup/user.sql 3、备份db1中的a1、a2表 $> mysqldump -uroot -proot --databases db1 --tables a1 a2 > /data/mysql-backup/db1.sql PS:导出指定表只能针对一个数据库进行导出，且导出的内容中和导出数据库也不一样，导出指定表的导出文本中没有创建数据库的判断语句，只有删除表-创建表-导入数据 4、筛选导出表 mysqldump -u user -proot --databases db1 db2 --ignore-table=db1.tbl --ignore-table=db2.tb2 > /data/mysql-backup/db.sql 5、只导出表结构不导出数据，--no-data $> mysqldump -uroot -proot --no-data --databases db1 >/tmp/db1.sql 6、跨服务区导出导入数据库 PS：远程数据库必须存在 # 将h1服务器中的db1数据库的所有数据导入到h2中的db2数据库中 mysqldump --host=192.168.1.8 -uroot -proot -C --databases test | mysql --host=192.168.1.9 -uroot -proot test # PS：远程数据库必须存在 7、定时备份任务 # 每天早上1点定时备份MySQL数据 0 1 * * * mysqldump -u user -proot --databases db1 db2 --ignore-table=db1.tb1 --ignore-table=db2.tbl1 > /data/mysql-backup/db-$(date +\"\\%Y\\%m\\%d\\%H\\%M\").sql 8、MysqlDump的参数说明 --all-databases , -A 导出全部数据库。 mysqldump -uroot -p --all-databases --all-tablespaces , -Y 导出全部表空间。 mysqldump -uroot -p --all-databases --all-tablespaces --no-tablespaces , -y 不导出任何表空间信息。 mysqldump -uroot -p --all-databases --no-tablespaces --add-drop-database 每个数据库创建之前添加drop数据库语句。 mysqldump -uroot -p --all-databases --add-drop-database --add-drop-table 每个数据表创建之前添加drop数据表语句。(默认为打开状态，使用--skip-add-drop-table取消选项) mysqldump -uroot -p --all-databases (默认添加drop语句) mysqldump -uroot -p --all-databases –skip-add-drop-table (取消drop语句) --add-locks 在每个表导出之前增加LOCK TABLES并且之后UNLOCK TABLE。(默认为打开状态，使用--skip-add-locks取消选项) mysqldump -uroot -p --all-databases (默认添加LOCK语句) mysqldump -uroot -p --all-databases –skip-add-locks (取消LOCK语句) --allow-keywords 允许创建是关键词的列名字。这由表名前缀于每个列名做到。 mysqldump -uroot -p --all-databases --allow-keywords --apply-slave-statements 在'CHANGE MASTER'前添加'STOP SLAVE'，并且在导出的最后添加'START SLAVE'。 mysqldump -uroot -p --all-databases --apply-slave-statements --character-sets-dir 字符集文件的目录 mysqldump -uroot -p --all-databases --character-sets-dir=/usr/local/mysql/share/mysql/charsets --comments 附加注释信息。默认为打开，可以用--skip-comments取消 mysqldump -uroot -p --all-databases (默认记录注释) mysqldump -uroot -p --all-databases --skip-comments (取消注释) --compatible 导出的数据将和其它数据库或旧版本的MySQL 相兼容。值可以为ansi、mysql323、mysql40、postgresql、oracle、mssql、db2、maxdb、no_key_options、no_tables_options、no_field_options等， 要使用几个值，用逗号将它们隔开。它并不保证能完全兼容，而是尽量兼容。 mysqldump -uroot -p --all-databases --compatible=ansi --compact 导出更少的输出信息(用于调试)。去掉注释和头尾等结构。可以使用选项：--skip-add-drop-table --skip-add-locks --skip-comments --skip-disable-keys mysqldump -uroot -p --all-databases --compact --complete-insert, -c 使用完整的insert语句(包含列名称)。这么做能提高插入效率，但是可能会受到max_allowed_packet参数的影响而导致插入失败。 mysqldump -uroot -p --all-databases --complete-insert --compress, -C 在客户端和服务器之间启用压缩传递所有信息 mysqldump -uroot -p --all-databases --compress --create-options, -a 在CREATE TABLE语句中包括所有MySQL特性选项。(默认为打开状态) mysqldump -uroot -p --all-databases --databases, -B 导出几个数据库。参数后面所有名字参量都被看作数据库名。 mysqldump -uroot -p --databases test mysql --debug 输出debug信息，用于调试。默认值为：d:t,/tmp/mysqldump.trace mysqldump -uroot -p --all-databases --debug mysqldump -uroot -p --all-databases --debug=” d:t,/tmp/debug.trace” --debug-check 检查内存和打开文件使用说明并退出。 mysqldump -uroot -p --all-databases --debug-check --debug-info 输出调试信息并退出 mysqldump -uroot -p --all-databases --debug-info --default-character-set 设置默认字符集，默认值为utf8 mysqldump -uroot -p --all-databases --default-character-set=utf8 --delayed-insert 采用延时插入方式（INSERT DELAYED）导出数据 mysqldump -uroot -p --all-databases --delayed-insert --delete-master-logs master备份后删除日志. 这个参数将自动激活--master-data。 mysqldump -uroot -p --all-databases --delete-master-logs --disable-keys 对于每个表，用/*!40000 ALTER TABLE tbl_name DISABLE KEYS */;和/*!40000 ALTER TABLE tbl_name ENABLE KEYS */;语句引用INSERT语句。这样可以更快地导入dump出来的文件，因为它是在插入所有行后创建索引的。该选项只适合MyISAM表，默认为打开状态。 mysqldump -uroot -p --all-databases --dump-slave 该选项将主的binlog位置和文件名追加到导出数据的文件中(show slave status)。设置为1时，将会以CHANGE MASTER命令输出到数据文件；设置为2时，会在change前加上注释。该选项将会打开--lock-all-tables，除非--single-transaction被指定。该选项会自动关闭--lock-tables选项。默认值为0。 mysqldump -uroot -p --all-databases --dump-slave=1 mysqldump -uroot -p --all-databases --dump-slave=2 --master-data 该选项将当前服务器的binlog的位置和文件名追加到输出文件中(show master status)。如果为1，将会输出CHANGE MASTER 命令；如果为2，输出的CHANGE MASTER命令前添加注释信息。该选项将打开--lock-all-tables 选项，除非--single-transaction也被指定（在这种情况下，全局读锁在开始导出时获得很短的时间；其他内容参考下面的--single-transaction选项）。该选项自动关闭--lock-tables选项。 mysqldump -uroot -p --host=localhost --all-databases --master-data=1; mysqldump -uroot -p --host=localhost --all-databases --master-data=2; --events, -E 导出事件。 mysqldump -uroot -p --all-databases --events --extended-insert, -e 使用具有多个VALUES列的INSERT语法。这样使导出文件更小，并加速导入时的速度。默认为打开状态，使用--skip-extended-insert取消选项。 mysqldump -uroot -p --all-databases mysqldump -uroot -p --all-databases--skip-extended-insert (取消选项) --fields-terminated-by 导出文件中忽略给定字段。与--tab选项一起使用，不能用于--databases和--all-databases选项 mysqldump -uroot -p test test --tab=”/home/mysql” --fields-terminated-by=”#” --fields-enclosed-by 输出文件中的各个字段用给定字符包裹。与--tab选项一起使用，不能用于--databases和--all-databases选项 mysqldump -uroot -p test test --tab=”/home/mysql” --fields-enclosed-by=”#” --fields-optionally-enclosed-by 输出文件中的各个字段用给定字符选择性包裹。与--tab选项一起使用，不能用于--databases和--all-databases选项 mysqldump -uroot -p test test --tab=”/home/mysql” --fields-enclosed-by=”#” --fields-optionally-enclosed-by =”#” --fields-escaped-by 输出文件中的各个字段忽略给定字符。与--tab选项一起使用，不能用于--databases和--all-databases选项 mysqldump -uroot -p mysql user --tab=”/home/mysql” --fields-escaped-by=”#” --flush-logs 开始导出之前刷新日志。 请注意：假如一次导出多个数据库(使用选项--databases或者--all-databases)，将会逐个数据库刷新日志。除使用--lock-all-tables或者--master-data外。在这种情况下，日志将会被刷新一次，相应的所以表同时被锁定。因此，如果打算同时导出和刷新日志应该使用--lock-all-tables 或者--master-data 和--flush-logs。 mysqldump -uroot -p --all-databases --flush-logs --flush-privileges 在导出mysql数据库之后，发出一条FLUSH PRIVILEGES 语句。为了正确恢复，该选项应该用于导出mysql数据库和依赖mysql数据库数据的任何时候。 mysqldump -uroot -p --all-databases --flush-privileges --force 在导出过程中忽略出现的SQL错误。 mysqldump -uroot -p --all-databases --force --help 显示帮助信息并退出。 mysqldump --help --hex-blob 使用十六进制格式导出二进制字符串字段。如果有二进制数据就必须使用该选项。影响到的字段类型有BINARY、VARBINARY、BLOB。 mysqldump -uroot -p --all-databases --hex-blob --host, -h 需要导出的主机信息 mysqldump -uroot -p --host=localhost --all-databases --ignore-table 不导出指定表。指定忽略多个表时，需要重复多次，每次一个表。每个表必须同时指定数据库和表名。例如：--ignore-table=database.table1 --ignore-table=database.table2 …… mysqldump -uroot -p --host=localhost --all-databases --ignore-table=mysql.user --include-master-host-port 在--dump-slave产生的'CHANGE MASTER TO..'语句中增加'MASTER_HOST=，MASTER_PORT=' mysqldump -uroot -p --host=localhost --all-databases --include-master-host-port --insert-ignore 在插入行时使用INSERT IGNORE语句. mysqldump -uroot -p --host=localhost --all-databases --insert-ignore --lines-terminated-by 输出文件的每行用给定字符串划分。与--tab选项一起使用，不能用于--databases和--all-databases选项。 mysqldump -uroot -p --host=localhost test test --tab=”/tmp/mysql” --lines-terminated-by=”##” --lock-all-tables, -x 提交请求锁定所有数据库中的所有表，以保证数据的一致性。这是一个全局读锁，并且自动关闭--single-transaction 和--lock-tables 选项。 mysqldump -uroot -p --host=localhost --all-databases --lock-all-tables --lock-tables, -l 开始导出前，锁定所有表。用READ LOCAL锁定表以允许MyISAM表并行插入。对于支持事务的表例如InnoDB和BDB，--single-transaction是一个更好的选择，因为它根本不需要锁定表。 请注意当导出多个数据库时，--lock-tables分别为每个数据库锁定表。因此，该选项不能保证导出文件中的表在数据库之间的逻辑一致性。不同数据库表的导出状态可以完全不同。 mysqldump -uroot -p --host=localhost --all-databases --lock-tables --log-error 附加警告和错误信息到给定文件 mysqldump -uroot -p --host=localhost --all-databases --log-error=/tmp/mysqldump_error_log.err --max_allowed_packet 服务器发送和接受的最大包长度。 mysqldump -uroot -p --host=localhost --all-databases --max_allowed_packet=10240 --net_buffer_length TCP/IP和socket连接的缓存大小。 mysqldump -uroot -p --host=localhost --all-databases --net_buffer_length=1024 --no-autocommit 使用autocommit/commit 语句包裹表。 mysqldump -uroot -p --host=localhost --all-databases --no-autocommit --no-create-db, -n 只导出数据，而不添加CREATE DATABASE 语句。 mysqldump -uroot -p --host=localhost --all-databases --no-create-db --no-create-info, -t 只导出数据，而不添加CREATE TABLE 语句。 mysqldump -uroot -p --host=localhost --all-databases --no-create-info --no-data, -d 不导出任何数据，只导出数据库表结构。 mysqldump -uroot -p --host=localhost --all-databases --no-data --no-set-names, -N 等同于--skip-set-charset mysqldump -uroot -p --host=localhost --all-databases --no-set-names --opt 等同于--add-drop-table, --add-locks, --create-options, --quick, --extended-insert, --lock-tables, --set-charset, --disable-keys 该选项默认开启, 可以用--skip-opt禁用. mysqldump -uroot -p --host=localhost --all-databases --opt --order-by-primary 如果存在主键，或者第一个唯一键，对每个表的记录进行排序。在导出MyISAM表到InnoDB表时有效，但会使得导出工作花费很长时间。 mysqldump -uroot -p --host=localhost --all-databases --order-by-primary --password, -p 连接数据库密码 --pipe(windows系统可用) 使用命名管道连接mysql mysqldump -uroot -p --host=localhost --all-databases --pipe --port, -P 连接数据库端口号 --protocol 使用的连接协议，包括：tcp, socket, pipe, memory. mysqldump -uroot -p --host=localhost --all-databases --protocol=tcp --quick, -q 不缓冲查询，直接导出到标准输出。默认为打开状态，使用--skip-quick取消该选项。 mysqldump -uroot -p --host=localhost --all-databases mysqldump -uroot -p --host=localhost --all-databases --skip-quick --quote-names,-Q 使用（`）引起表和列名。默认为打开状态，使用--skip-quote-names取消该选项。 mysqldump -uroot -p --host=localhost --all-databases mysqldump -uroot -p --host=localhost --all-databases --skip-quote-names --replace 使用REPLACE INTO 取代INSERT INTO. mysqldump -uroot -p --host=localhost --all-databases --replace --result-file, -r 直接输出到指定文件中。该选项应该用在使用回车换行对（\\\\r\\\\n）换行的系统上（例如：DOS，Windows）。该选项确保只有一行被使用。 mysqldump -uroot -p --host=localhost --all-databases --result-file=/tmp/mysqldump_result_file.txt --routines, -R 导出存储过程以及自定义函数。 mysqldump -uroot -p --host=localhost --all-databases --routines --set-charset 添加'SET NAMES default_character_set'到输出文件。默认为打开状态，使用--skip-set-charset关闭选项。 mysqldump -uroot -p --host=localhost --all-databases mysqldump -uroot -p --host=localhost --all-databases --skip-set-charset --single-transaction 该选项在导出数据之前提交一个BEGIN SQL语句，BEGIN 不会阻塞任何应用程序且能保证导出时数据库的一致性状态。它只适用于多版本存储引擎，仅InnoDB。本选项和--lock-tables 选项是互斥的，因为LOCK TABLES 会使任何挂起的事务隐含提交。要想导出大表的话，应结合使用--quick 选项。 mysqldump -uroot -p --host=localhost --all-databases --single-transaction --dump-date 将导出时间添加到输出文件中。默认为打开状态，使用--skip-dump-date关闭选项。 mysqldump -uroot -p --host=localhost --all-databases mysqldump -uroot -p --host=localhost --all-databases --skip-dump-date --skip-opt 禁用–opt选项. mysqldump -uroot -p --host=localhost --all-databases --skip-opt --socket,-S 指定连接mysql的socket文件位置，默认路径/tmp/mysql.sock mysqldump -uroot -p --host=localhost --all-databases --socket=/tmp/mysqld.sock --tab,-T 为每个表在给定路径创建tab分割的文本文件。注意：仅仅用于mysqldump和mysqld服务器运行在相同机器上。注意使用--tab不能指定--databases参数 mysqldump -uroot -p --host=localhost test test --tab=\"/home/mysql\" --tables 覆盖--databases (-B)参数，指定需要导出的表名，在后面的版本会使用table取代tables。 mysqldump -uroot -p --host=localhost --databases test --tables test --triggers 导出触发器。该选项默认启用，用--skip-triggers禁用它。 mysqldump -uroot -p --host=localhost --all-databases --triggers --tz-utc 在导出顶部设置时区TIME_ZONE='+00:00' ，以保证在不同时区导出的TIMESTAMP 数据或者数据被移动其他时区时的正确性。 mysqldump -uroot -p --host=localhost --all-databases --tz-utc --user, -u 指定连接的用户名。 --verbose, --v 输出多种平台信息。 --version, -V 输出mysqldump版本信息并退出 --where, -w 只转储给定的WHERE条件选择的记录。请注意如果条件包含命令解释符专用空格或字符，一定要将条件引用起来。 mysqldump -uroot -p --host=localhost --all-databases --where=” user=’root’” --xml, -X 导出XML格式. mysqldump -uroot -p --host=localhost --all-databases --xml --plugin_dir 客户端插件的目录，用于兼容不同的插件版本。 mysqldump -uroot -p --host=localhost --all-databases --plugin_dir=”/usr/local/lib/plugin” --default_auth 客户端插件默认使用权限。 mysqldump -uroot -p --host=localhost --all-databases --default-auth=”/usr/local/lib/plugin/” Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/xtrabackup-backup.html":{"url":"origin/xtrabackup-backup.html","title":"Xtrabackup备份","keywords":"","body":"Xtrabackup数据库备份工具 一、简介 Xtrabackup是由percona提供的mysql数据库备份工具，据官方介绍，这也是世界上惟一一款开源的能够对innodb和xtradb数据库进行热备的工具。特点： 备份过程快速、可靠； 备份过程不会打断正在执行的事务； 能够基于压缩等功能节约磁盘空间和流量； 自动实现备份检验； 还原速度快； 二、概念 1、xtrabackup与MySQL版本对应关系 Percona XtraBackup软件版本通常与Mysql版本对应 2.4.20 based on MySQL server 5.7.26 2、备份的文件 文件 内容 backup-my.cnf 只包含[mysqld]配置片段和备份有关的选项。 xtrabackup_binlog_info 记录的是当前使用的二进制日志文件。 xtrabackup_checkpoints 记录了备份的类型是全备(full-backuped)还是增备(incremental)，还有备份的起始、终止LSN号。 xtrabackup_info 记录的是备份过程中的一些信息。 xtrabackup_logfile 复制和监控后写的redo日志。该日志是备份后下一个操作\"准备\"的关键。只有通过它才能实现数据一致性。 ibdata1 用来构建innodb系统表空间的文件，这个文件包含了innodb表的元数据、undo日志、修改buffer和双写buffer 其他文件 源库的物理存储文件 3、备份使用的用户权限 RELOAD和LOCK TABLES权限，执行FLUSH TABLES WITH READ LOCA； REPLICATION CLIENT权限，获取binary log（二进制日志文件）位置； CREATE TABLEPACE权限，导入表，用户表级别的恢复； SUPER权限，在slave环境下备份用来启用和关闭slave线程。 create user 'xtrabackup'@'192.168.1.%' identified by '密码'; GRANT reload, LOCK TABLES, process, replication client ON *.* TO 'xtrabackup'@'192.168.1.%'; flush privileges; 4、备份恢复过程步骤 ①备份过程(backup) 在启动xtrabackup时记下LSN并将redo log拷贝到备份目标目录下的xtrabackup_logfile文件中。由于拷贝需要一定时间，如果在拷贝时间段内有日志写入，将导致拷贝的日志和MySQL的redo log不一致，所以xtrabackup还有一个后台进程监控着mysql的redo log，每秒监控一次，当MySQL的redo log有变化，该监控进程会立即将变化的内容写入到xtrabackup_logfile文件，这样就能保证拷贝走的redo log中记录了一切变化。但是这也是有风险的，因为redo是轮训式循环写入的，如果某一时刻有非常大量的日志写到redo log中，使得还没开始复制的日志就被新日志覆盖了，这样会日志丢失，并报错。 拷贝完初始版的redo log后，xtrabackup开始拷贝innodb表的数据文件(即表空间文件.ibd文件和ibdata1)。注意，此时不拷贝innodb的frm文件。 当innodb相关表的数据文件拷贝完成后，xtrabackup开始准备拷贝非innodb的文件。但在拷贝它们之前，要先对非innodb表进行加锁防止拷贝时有语句修改这些类型的表数据。 对于不支持backup lock的版本，只能通过flush tables with read lock来获取全局读锁，但这样也同样会锁住innodb表，杀伤力太大。所以使用xtrabackup备份Oracle的MySQL，实质上只能实现innodb表的部分时间热备、部分时间温备。 对于支持backup lock的版本，xtrabackup通过lock tables for backup获取轻量级的backup locks来替代flush tables with read lock，因为它只锁定非innodb表，所以由此实现了innodb表的真正热备。 当获取到非innodb表的锁以后，开始拷贝非innodb表的数据和.frm文件。当这些拷贝完成之后，继续拷贝其他存储引擎类型的文件。(实际上，拷贝非innodb表的数据是在获取backup locks(如果支持)后自动进行的，它们属于同一个过程) 当拷贝阶段完成后，就到了备份的收尾阶段。包括获取二进制日志中一致性位置的坐标点、结束redo log的监控和拷贝、释放锁等。 对于不支持backup lock的版本，收尾阶段的过程是这样的：获取二进制日志的一致性坐标点、结束redo log的监控和拷贝、释放锁。 对于支持backup lock的版本，收尾阶段的过程是这样的：先通过lock binlog for bakcup来获取二进制日志锁，然后结束redo log的监控和拷贝，再unlock tables释放表锁，随后获取二进制日志的一致性位置坐标点，最后unlock binlog释放二进制日志锁。 如果一切都OK，xtrabackup将以状态码0退出。 ②准备过程(prepare) 由于备份的时候拷贝走的数据文件可能是不一致的，比如监控着MySQL的redo log中在拷贝过程完成后又新的事务提交了，而拷贝走的数据是未提交状态的，那么就需要对该事务前滚；如果监控到的日志中有事务未提交，那么该事务就需要回滚。 但是如果只备份了myisam表或其他非事务表数据，因为备份阶段直接锁定了这些表，所以不会有不一致的状态。 xtrabackup有一个\"准备\"的阶段。这个阶段的实质就是对备份的innodb数据应用redo log，该回滚的回滚，该前滚的前滚，最终保证xtrabackup_logfile中记录的redo log已经全部应用到备份数据页上，并且实现了一致性。当应用结束后，会重写\"xtrabackup_logfile\"再次保证该redo log和备份的数据是对应的。 准备过程不需要连接数据库，该过程可以在任意装了xtrabackup软件的机器上进行，之所能实现是因为xtrabackup软件的内部嵌入了一个简化的innodb存储引擎，可以通过它完成日志的应用 ③恢复过程(copy back) xtrabackup的恢复过程实质是将备份的数据文件和结构定义等文件拷贝回MySQL的datadir。同样可以拷贝到任意机器上。 要求恢复之前MySQL必须是停止运行状态，且datadir是空目录。 三、安装 YUM yum install -y perl rsync perl-Data-Dumper wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.3.5/binary/redhat/7/x86_64/percona-xtrabackup-2.3.5-1.el7.x86_64.rpm yum clean all yum repolist yum localinstall percona-xtrabackup-2.3.2-1.el7.x86_64.rpm APT wget https://repo.percona.com/apt/percona-release_0.1-6.$(lsb_release -sc)_all.deb dpkg -i percona-release_0.1-6.bionic_all.deb apt-get update apt-get install percona-xtrabackup # 安装percona-xtrabackup-24 wget https://repo.percona.com/apt/percona-release_latest.$(lsb_release -sc)_all.deb dpkg -i percona-release_latest.$(lsb_release -sc)_all.deb apt update apt install -y percona-xtrabackup-24 qpress # 安装percona-xtrabackup-80 wget https://repo.percona.com/apt/percona-release_latest.$(lsb_release -sc)_all.deb dpkg -i percona-release_latest.$(lsb_release -sc)_all.deb apt update apt install -y percona-xtrabackup-80 qpress zstd 源代码 ①下载源代码 git clone https://github.com/percona/percona-xtrabackup.git ②安装编译依赖 apt-get install build-essential flex bison automake autoconf \\ libtool cmake libaio-dev mysql-client libncurses-dev zlib1g-dev \\ libgcrypt11-dev libev-dev libcurl4-gnutls-dev vim-common yum install cmake gcc gcc-c++ libaio libaio-devel automake autoconf \\ bison libtool ncurses-devel libgcrypt-devel libev-devel libcurl-devel \\ vim-common ③编译安装 cd percona-xtrabackup git checkout 2.3 cmake -DBUILD_CONFIG=xtrabackup_release -DWITH_MAN_PAGES=OFF make -j4 make install # 默认安装在/usr/local/xtrabackup路径下，可使用 make DESTDIR=安装路径 install 指定安装路径 2.3 编译参考文档：https://www.percona.com/doc/percona-xtrabackup/2.3/installation/compiling_xtrabackup.html 2.4 编译参考文档：https://docs.percona.com/percona-xtrabackup/2.4/installation/compiling_xtrabackup.html 2.4.x不再支持在MacOS X上编译，参考https://bugs.launchpad.net/percona-xtrabackup/+bug/1736119 Docker 只有2.4以上的版本 需要挂载数据库实例数据目录到容器内相同路径下，可使用只读模式进行挂载。 同时可以持久化挂载本地文件到容器的/backup路径下，将数据备份到该路径下。 docker run -it --rm --name percona-xtrabackup \\ -v ./stg-mysql56-backups:/backup \\ -v /usr/local/var/mysql57:/usr/local/var/mysql57:ro \\ percona/percona-xtrabackup:2.4.26 \\ innobackupex --backup -uxtrabackup -p**密码** -P3306 -H127.0.0.1 /backup 对于容器化的MySQL，实例数据路径需要挂载到/var/lib/mysql docker run -it --rm --name percona-xtrabackup \\ -v ./stg-mysql56-backups:/backup \\ -v /data/docker-volume-data/mysql57:/var/lib/mysql:ro \\ percona/percona-xtrabackup:2.4.26 \\ innobackupex --backup -uxtrabackup -p**密码** -P3306 -H127.0.0.1 /backup MacOS 默认安装最新的8.+版本 brew install percona-xtrabackup # 默认安装在/usr/local/Cellar/percona-xtrabackup/版本 路径下 四、命令参数 命令 功能 xtrabackup 主程序 innobackupex 在以前是一个perl脚本，会调用xtrabackup这个二进制工具，从xtrabackup 2.3开始，该工具使用C语言进行了重写，当前它是xtabackup二进制工具的一个软连接，但是实际的使用方法却不同，并且在以后的版本中会删除该工具。 xbcrypt 加密备份集 xbstream 是xtrabackup的流数据功能，通过流数据功能，可将备份内容打包并传给管道后的压缩工具进行压缩； 1、innobackupex命令 参数 含义 --compress 该选项表示压缩innodb数据文件的备份。 --compress-threads 该选项表示并行压缩worker线程的数量。 --compress-chunk-size 该选项表示每个压缩线程worker buffer的大小，单位是字节，默认是64K。 --encrypt 该选项表示通过ENCRYPTION_ALGORITHM的算法加密innodb数据文件的备份.目前支持的算法有ASE128,AES192,AES256。 --encrypt-threads 该选项表示并行加密的worker线程数量。 --encrypt-chunk-size 该选项表示每个加密线程worker buffer的大小，单位是字节，默认是64K。 --encrypt-key 该选项使用合适长度加密key，因为会记录到命令行，所以不推荐使用。 --encryption-key-file 该选项表示文件必须是一个简单二进制或者文本文件，加密key可通过以下命令行命令生成 --include 该选项表示使用正则表达式匹配表的名字[db.tb]，要求为其指定匹配要备份的表的完整名称，即databasename.tablename。 --user 该选项表示备份账号。 --password 该选项表示备份的密码。 --port 该选项表示备份数据库的端口。 --host 该选项表示备份数据库的地址。 --databases 该选项接受的参数为数据名，如果要指定多个数据库，彼此间需要以空格隔开；如 \"xtra_test dba_test\"，同时，在指定某数据库时，也可以只指定其中的某张表。如 --tables-file 该选项表示指定含有表列表的文件，格式为database.table，该选项直接传给--tables-file。 --socket 该选项表示mysql.sock所在位置，以便备份进程登录mysql。 --no-timestamp 该选项可以表示不要创建一个时间戳目录来存储备份，指定到自己想要的备份文件夹。 --ibbackup 该选项指定了使用哪个xtrabackup二进制程序。IBBACKUP-BINARY是运行percona xtrabackup的命令。这个选项适用于xtrbackup二进制不在你是搜索和工作目录，如果指定了该选项,innoabackupex自动决定用的二进制程序。 --slave-info 该选项表示对slave进行备份的时候使用，打印出master的名字和binlog pos，同样将这些信息以change master的命令写入xtrabackup_slave_info文件。可以通过基于这份备份启动一个从库。 --safe-slave-backup 该选项表示为保证一致性复制状态，这个选项停止SQL线程并且等到show status中的slave_open_temp_tables为0的时候开始备份，如果没有打开临时表，bakcup会立刻开始，否则SQL线程启动或者关闭知道没有打开的临时表。如果slave_open_temp_tables在--safe-slave-backup-timeount（默认300秒）秒之后不为0，从库sql线程会在备份完成的时候重启。 --rsync 该选项表示通过rsync工具优化本地传输。当指定这个选项，innobackupex使用rsync拷贝非Innodb文件而替换cp，当有很多DB和表的时候会快很多，不能--stream一起使用。 --kill-long-queries-timeout 该选项表示从开始执行FLUSH TABLES WITH READ LOCK到kill掉阻塞它的这些查询之间等待的秒数。默认值为0，不会kill任何查询，使用这个选项xtrabackup需要有Process和super权限。 --kill-long-query-type 该选项表示kill的类型，默认是all，可选select。 --ftwrl-wait-threshold 该选项表示检测到长查询，单位是秒，表示长查询的阈值。 --ftwrl-wait-query-type 该选项表示获得全局锁之前允许那种查询完成，默认是ALL，可选update。 --galera-info 该选项表示生成了包含创建备份时候本地节点状态的文件xtrabackup_galera_info文件，该选项只适用于备份PXC。 --stream 该选项表示流式备份的格式，backup完成之后以指定格式到STDOUT，目前只支持tar和xbstream。 --defaults-file 该选项指定了从哪个文件读取MySQL配置，必须放在命令行第一个选项的位置。 --defaults-extra-file 该选项指定了在标准defaults-file之前从哪个额外的文件读取MySQL配置，必须在命令行的第一个选项的位置。一般用于存备份用户的用户名和密码的配置文件。 ----defaults-group 该选项表示从配置文件读取的组，innobakcupex多个实例部署时使用。 --no-lock 该选项表示关闭FTWRL的表锁，只有在所有表都是Innodb表并且不关心backup的binlog pos点，如果有任何DDL语句正在执行或者非InnoDB正在更新时（包括mysql库下的表），都不应该使用这个选项，后果是导致备份数据不一致，如果考虑备份因为获得锁失败，可以考虑--safe-slave-backup立刻停止复制线程。 --tmpdir 该选项表示指定--stream的时候，指定临时文件存在哪里，在streaming和拷贝到远程server之前，事务日志首先存在临时文件里。在 使用参数stream=tar备份的时候，你的xtrabackup_logfile可能会临时放在/tmp目录下，如果你备份的时候并发写入较大的话 xtrabackup_logfile可能会很大(5G+)，很可能会撑满你的/tmp目录，可以通过参数--tmpdir指定目录来解决这个问题。 --history 该选项表示percona server 的备份历史记录在percona_schema.xtrabackup_history表。 --incremental 该选项表示创建一个增量备份，需要指定--incremental-basedir。 --incremental-basedir 该选项表示接受了一个字符串参数指定含有full backup的目录为增量备份的base目录，与--incremental同时使用。 --incremental-dir 该选项表示增量备份的目录。 --incremental-force-scan 该选项表示创建一份增量备份时，强制扫描所有增量备份中的数据页。 --incremental-lsn 该选项表示指定增量备份的LSN，与--incremental选项一起使用。 --incremental-history-name 该选项表示存储在PERCONA_SCHEMA.xtrabackup_history基于增量备份的历史记录的名字。Percona Xtrabackup搜索历史表查找最近（innodb_to_lsn）成功备份并且将to_lsn值作为增量备份启动出事lsn.与innobackupex--incremental-history-uuid互斥。如果没有检测到有效的lsn，xtrabackup会返回error。 --incremental-history-uuid 该选项表示存储在percona_schema.xtrabackup_history基于增量备份的特定历史记录的UUID。 --close-files 该选项表示关闭不再访问的文件句柄，当xtrabackup打开表空间通常并不关闭文件句柄目的是正确的处理DDL操作。如果表空间数量巨大，这是一种可以关闭不再访问的文件句柄的方法。使用该选项有风险，会有产生不一致备份的可能。 --compact 该选项表示创建一份没有辅助索引的紧凑的备份。 --throttle 该选项表示每秒IO操作的次数，只作用于bakcup阶段有效。apply-log和--copy-back不生效不要一起用。 2、xtrabackup命令 参数 含义 --apply-log-only prepare备份的时候只执行redo阶段，用于增量备份。 --backup 创建备份并且放入--target-dir目录中 --close-files 不保持文件打开状态，xtrabackup打开表空间的时候通常不会关闭文件句柄，目的是为了正确处理DDL操作。如果表空间数量非常巨大并且不适合任何限制，一旦文件不在被访问的时候这个选项可以关闭文件句柄.打开这个选项会产生不一致的备份。 --compact 创建一份没有辅助索引的紧凑备份 --compress 压缩所有输出数据，包括事务日志文件和元数据文件，通过指定的压缩算法，目前唯一支持的算法是quicklz.结果文件是qpress归档格式，每个xtrabackup创建的*.qp文件都可以通过qpress程序提取或者解压缩 --compress-chunk-size=# 压缩线程工作buffer的字节大小，默认是64K --compress-threads=# xtrabackup进行并行数据压缩时的worker线程的数量，该选项默认值是1，并行压缩（'compress-threads'）可以和并行文件拷贝('parallel')一起使用。例如:'--parallel=4 --compress --compress-threads=2'会创建4个IO线程读取数据并通过管道传送给2个压缩线程。 --create-ib-logfile 这个选项目前还没有实现，目前创建Innodb事务日志，你还是需要prepare两次。 --datadir=DIRECTORY backup的源目录，mysql实例的数据目录。从my.cnf中读取，或者命令行指定。 --defaults-extra-file=[MY.CNF] 在global files文件之后读取，必须在命令行的第一选项位置指定。 --defaults-file=[MY.CNF] 唯一从给定文件读取默认选项，必须是个真实文件，必须在命令行第一个选项位置指定。 --defaults-group=GROUP-NAME 从配置文件读取的组，innobakcupex多个实例部署时使用。 --export 为导出的表创建必要的文件 --extra-lsndir=DIRECTORY (for --bakcup):在指定目录创建一份xtrabakcup_checkpoints文件的额外的备份。 --incremental-basedir=DIRECTORY 创建一份增量备份时，这个目录是增量别分的一份包含了full bakcup的Base数据集。 --incremental-dir=DIRECTORY prepare增量备份的时候，增量备份在DIRECTORY结合full backup创建出一份新的full backup。 --incremental-force-scan 创建一份增量备份时，强制扫描所有增在备份中的数据页即使完全改变的page bitmap数据可用。 --incremetal-lsn=LSN 创建增量备份的时候指定lsn。 --innodb-log-arch-dir 指定包含归档日志的目录。只能和xtrabackup --prepare选项一起使用。 --innodb-miscellaneous 从My.cnf文件读取的一组Innodb选项。以便xtrabackup以同样的配置启动内置的Innodb。通常不需要显示指定。 --log-copy-interval=# 这个选项指定了log拷贝线程check的时间间隔（默认1秒）。 --log-stream xtrabakcup不拷贝数据文件，将事务日志内容重定向到标准输出直到--suspend-at-end文件被删除。这个选项自动开启--suspend-at-end。 --no-defaults 不从任何选项文件中读取任何默认选项,必须在命令行第一个选项。 --databases=# 指定了需要备份的数据库和表。 --database-file=# 指定包含数据库和表的文件格式为databasename1.tablename1为一个元素，一个元素一行。 --parallel=# 指定备份时拷贝多个数据文件并发的进程数，默认值为1。 --prepare xtrabackup在一份通过--backup生成的备份执行还原操作，以便准备使用。 --print-default 打印程序参数列表并退出，必须放在命令行首位。 --print-param 使xtrabackup打印参数用来将数据文件拷贝到datadir并还原它们。 --rebuild_indexes 在apply事务日志之后重建innodb辅助索引，只有和--prepare一起才生效。 --rebuild_threads=# 在紧凑备份重建辅助索引的线程数，只有和--prepare和rebuild-index一起才生效。 --stats xtrabakcup扫描指定数据文件并打印出索引统计。 --stream=name 将所有备份文件以指定格式流向标准输出，目前支持的格式有xbstream和tar。 --suspend-at-end 使xtrabackup在--target-dir目录中生成xtrabakcup_suspended文件。在拷贝数据文件之后xtrabackup不是退出而是继续拷贝日志文件并且等待知道xtrabakcup_suspended文件被删除。这项可以使xtrabackup和其他程序协同工作。 --tables=name 正则表达式匹配database.tablename。备份匹配的表。 --tables-file=name 指定文件，一个表名一行。 --target-dir=DIRECTORY 指定backup的目的地，如果目录不存在，xtrabakcup会创建。如果目录存在且为空则成功。不会覆盖已存在的文件。 --throttle=# 指定每秒操作读写对的数量。 --tmpdir=name 当使用--print-param指定的时候打印出正确的tmpdir参数。 --to-archived-lsn=LSN 指定prepare备份时apply事务日志的LSN，只能和xtarbackup --prepare选项一起用。 --user-memory = # 通过--prepare prepare备份时候分配多大内存，目的像innodb_buffer_pool_size。默认值100M如果你有足够大的内存。1-2G是推荐值，支持各种单位(1MB,1M,1GB,1G)。 --version 打印xtrabackup版本并退出。 --xbstream 支持同时压缩和流式化。需要客服传统归档tar,cpio和其他不允许动态streaming生成的文件的限制，例如动态压缩文件，xbstream超越其他传统流式/归档格式的的优点是，并发stream多个文件并且更紧凑的数据存储（所以可以和--parallel选项选项一起使用xbstream格式进行streaming）。 五、备份 1、全量备份 MySQL 5.6及之前的版本只能使用XtraBackup 2.3版本的innobackupex命令进行备份（XtraBackup 2.3版本的安装参照第三章节的源码安装步骤） -H, --host 指定主机 -u, --user 指定用户名 -p, --password 指定密码 -P, --port 指定端口 --databases 指定数据库 /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --backup \\ ./stg-mysql56-backups # 2.4.x备份命令 xtrabackup \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --backup --target-dir=/data/backups/ 备份成功后 2、加解密备份 加密 生成加密key：openssl rand -base64 24 –encrypt=加密算法 支持的加密算法：AES128, AES192, AES256 –encrypt-key=密钥 --encrypt-threads=N 加密进程个数。默认一个 encryptkey=$(openssl rand -base64 24) /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --encrypt=AES256 \\ --encrypt-key=$encryptkey \\ --encrypt-threads=3 \\ --backup \\ --compress \\ ./stg-mysql56-backups 解密 /usr/local/xtrabackup2.3/bin/innobackupex \\ --decrypt=AES256 \\ --encrypt-key=$encryptkey \\ --remove-original \\ ./stg-mysql56-backups/2022-09-28_16-44-37 3、解压缩备份 ①自带quicklz压缩算法 *.qp压缩文件是qpress压缩格式 --compress：开启压缩 --compress-threads=N ： 使用N个压缩线程 --compress-chunk-size：该选项表示每个压缩线程worker buffer的大小，单位是字节，默认是64K。 /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --compress \\ --compress-threads=4 \\ --backup \\ ./stg-mysql56-backups 解压备份文件 解压需要安装qpress命令。 # https://github.com/mhorbul/homebrew-qpress brew tap mhorbul/qpress brew install qpress yum install epel-release yum install -y qpress --decompress 解压参数 --remove-original 解压完成后并删除压缩文件 --parallel 多线程解压 /usr/local/xtrabackup2.3/bin/innobackupex \\ --decompress \\ --remove-original \\ --parallel 4 \\ ./stg-mysql56-backups/2022-09-28_16-44-37 # 手动删除原压缩文件 find ./stg-mysql56-backups/2022-09-28_16-44-37 -name \"*.qp\" | xargs rm -f ②流式tar --stream=tar 表示流式备份的格式，backup完成之后以指定格式到STDOUT，目前只支持tar和xbstream。 /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --stream=tar /tmp |gzip -> stg-mysql56-backups-20220928.tar.gz 将备份发送到另一个主机并解压 /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --stream=tar ./ | ssh user@destination \\ \"cat - > /data/mysql-backups/stg-mysql56-backups-20220928.tar\" 解压备份文件 tar -zxvf stg-mysql56-backups-20220928.tar.gz ③流式xstream 压缩备份 /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --compress \\ --stream=xbstream /tmp > ./stg-mysql56-backups-20220928.xb # 使用容器化的xtrabackup 2.4备份MySQL 5.7 mkdir test && cd test ; \\ docker run -it --rm --name percona-xtrabackup \\ -v MySQL57的数据目录:MySQL配置中datadir参数对应的数据路径 \\ -v ${PWD}:/backup \\ percona/percona-xtrabackup:2.4.26 \\ /bin/bash -c 'innobackupex \\ -H host.docker.internal \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --compress \\ --stream=xbstream /tmp > /backup/stg-mysql57-backups-20220928.xb' 将备份发送到另一个主机并解压 /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --compress \\ --stream=xbstream /tmp | ssh user@destination \\ \"/usr/local/xtrabackup2.3/bin/xbstream -x -C /data/mysql-backups/stg-mysql56-backups-20220928\" 解压备份文件 cat ./stg-mysql56-backups-20220928.xb | /usr/local/xtrabackup2.3/bin/xbstream -x -C ./test # 使用容器化的xtrabackup 2.4解压备份文件 mkdir ./restore-data ; \\ docker run -it --rm --name percona-xtrabackup \\ -v ${PWD}:/backup \\ percona/percona-xtrabackup:2.4.26 \\ /bin/bash -c 'cat /backup/stg-mysql57-backups-20220928.xb | xbstream -x -v -C /backup/restore-data' 解压备份文件夹中的压缩数据文件*.qb # 对于MySQL 5.6/5.7 innobackupex --decompress --remove-original /home/mysql/data # 对于MySQL 8.0 xtrabackup --decompress --remove-original --target-dir=/home/mysql/data # 使用容器化的xtrabackup 2.4解压备份文件夹中的压缩数据文件*.qb docker run -it --rm --name percona-xtrabackup \\ -v ${PWD}:/backup \\ percona/percona-xtrabackup:2.4.26 \\ /bin/bash -c 'innobackupex --decompress --remove-original /backup/restore-data' ④压缩方式对比 压缩方式 耗时 大小 未压缩 14s 1.2G qr压缩 16s 618M 流式tar压缩 66s 432M 流式xstream压缩 6s 596M 4、准备备份文件 --apply-log 回滚未提交的事务及同步已经提交的事务至数据文件使数据文件处于一致性状态。 --redo-only 不回滚未提交事务 /usr/local/xtrabackup2.3/bin/innobackupex \\ --apply-log \\ --redo-only ./stg-mysql56-backups/2022-09-28_16-44-37 5、增量备份 --incremental：增量备份目录 --incremental-basedir：全量备份目录 /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --compress \\ --compress-threads=4 \\ --incremental ./stg-mysql56-backups \\ --incremental-basedir=./stg-mysql56-backups/2022-09-28_16-44-37 6、合并增量备份 将增量备份数据合并到全备数据目录当中 /usr/local/xtrabackup2.3/bin/innobackupex \\ --apply-log \\ --redo-only ./stg-mysql56-backups/2022-09-28_16-44-37 \\ --incremental-dir=./stg-mysql56-backups/2022-09-28_17-51-02 7、选择性备份 选择性备份有三种方式 --include：可以指定正则来匹配要备份的表，多个正则规则使用\"|\"分割。 --tables-file / --databases-file ：从文件中读取要备份的库或表 一行一个库名或者全限定表名 最后一个行必须是空行 文件路径必须是绝对路径。因为命令执行的 --databases：指定要备份的数据库或表，指定备份的表时要使用完整对象引用格式，多个元素使用空格分割。 使用前两种部分备份方式，只能备份innodb表，不会备份任何myisam，即使指定了也不会备份。而且要备份的表必须有独立的表空间文件，也就是说必须开启了innodb_file_per_table，更精确的说，要备份的表是在开启了innodb_file_per_table选项之后才创建的。 第三种备份方式可以备份myisam表。 ①备份指定的库或表 # 指定备份某个database所有的表 backup_database=\"sysbench sgp\" /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --databases=${backup_database} \\ --backup \\ ./stg-mysql56-backups # 从文件中读取要备份的库 echo -e 'sysbench\\nsgp' > backup-database-file /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --databases-file=./stg-mysql56-backups/backup-database-file \\ --backup \\ ./stg-mysql56-backups # ========================================================================================== # 备份某个database指定的表 backup_tables=\"sysbench.sbtest1 sgp.user\" /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --databases=${backup_tables} \\ --backup \\ ./stg-mysql56-backups # 从文件中读取要备份的表 echo -e 'sysbench.sbtest1\\nsgp.user' > backup-tables-file /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --tables-file=./stg-mysql56-backups/backup-tables-file \\ --backup \\ ./stg-mysql56-backups ②正则过滤表备份 backup_tables=\"sgp.m|sysbench.*|^mat.*[.]matomo_a.*\" # 正则表达匹配的是sgp库中m开头的所有表，sysbench库的所有表，mat开头的所有库中的matomo_a开头的所有表 /usr/local/xtrabackup2.3/bin/innobackupex \\ -H 127.0.0.1 \\ -P 3306 \\ -u xtrabackup \\ -p **密码** \\ --include=$backup_tables \\ --backup \\ ./stg-mysql56-backups 六、恢复 1、恢复全量备份到空的MySQL实例 2、使用mysqld_safe启动备份文件 ①安装对应版本的MySQL Deb安装 # 安装MySQL 5.6 wget https://downloads.mysql.com/archives/get/p/23/file/mysql-community-server_5.6.46-1debian8_amd64.deb wget https://downloads.mysql.com/archives/get/p/23/file/mysql-community-client_5.6.46-1debian8_amd64.deb wget https://downloads.mysql.com/archives/get/p/23/file/mysql-server_5.6.46-1debian8_amd64.deb wget https://downloads.mysql.com/archives/get/p/23/file/mysql-client_5.6.46-1debian8_amd64.deb dpkg -i mysql* YUM 安装 bash -c 'cat > /etc/yum.repos.d/mysql.repo ②修改配置文件参数 修改备份文件夹下backup-my.cnf中的配置参数，自建数据库不支持如下参数，需要将backup-my.cnf中出现的以下参数注释掉。 #innodb_log_checksum_algorithm #innodb_fast_checksum #innodb_log_block_size #innodb_doublewrite_file #innodb_encrypt_algorithm #redo_log_version #master_key_id #server_uuid 再添加一个参数，使启动的实例不需要密码即可登录。 skip-grant-tables ③启动 mysqld_safe \\ --user=mysql \\ --defaults-file=./stg-mysql56-backups/2022-09-28_16-44-37/backup-my.cnf \\ --datadir=./stg-mysql56-backups/2022-09-28_16-44-37 启动的日志会放在./stg-mysql56-backups/2022-09-28_16-44-37/主机名.err文件中，如果没有“ERR”级别报错，即启动成功。 七、操作示例 1、备份恢复MySQL MacOS Brew安装的MySQL5.7。数据目录特殊设置在了/usr/local/var/mysql57 xstream备份、压缩 encryptkey=$(openssl rand -base64 24) mysql_password=\"MySQL备份用户密码\" docker run -it --rm --name percona-xtrabackup \\ -v /usr/local/var/mysql57:/usr/local/var/mysql57/ \\ -v ${PWD}:/backup \\ percona/percona-xtrabackup:2.4.26 \\ /bin/bash -c 'innobackupex \\ -H host.docker.internal \\ -P 3307 \\ -u xtrabackup \\ -p '${mysql_password}' \\ --compress \\ --encrypt=AES256 \\ --encrypt-key='${encryptkey}' \\ --encrypt-threads=5 \\ --stream=xbstream /tmp > /backup/test.xb' 解压备份文件 backupfile=test.xb mkdir ./restore-data ; docker run -it --rm --name percona-xtrabackup \\ -v ${PWD}:/backup \\ percona/percona-xtrabackup:2.4.26 \\ /bin/bash -c 'cat /backup/'${backupfile}' | xbstream -x -v -C /backup/restore-data' 解压备份文件中的压缩文件 encryptkey=加密密码 docker run -it --rm --name percona-xtrabackup \\ -v ${PWD}:/backup \\ percona/percona-xtrabackup:2.4.26 \\ /bin/bash -c 'innobackupex \\ --decompress \\ --remove-original \\ --decrypt=AES256 \\ --encrypt-key='${encryptkey}' \\ /backup/restore-data' 准备解压好的备份文件 docker run -it --rm --name percona-xtrabackup \\ -v ${PWD}:/backup \\ percona/percona-xtrabackup:2.4.26 \\ /bin/bash -c 'innobackupex --defaults-file=/backup/restore-data/backup-my.cnf --apply-log /backup/restore-data' 修改配置文件 sed -i '' \\ -e '/innodb_log_checksum_algorithm/ s/^/# /' \\ -e '/innodb_fast_checksum/ s/^/# /' \\ -e '/innodb_log_block_size/ s/^/# /' \\ -e '/innodb_encrypt_algorithm/ s/^/# /' \\ -e '/innodb_doublewrite_file/ s/^/# /' \\ -e '/rds_encrypt_data/ s/^/# /' \\ -e '/redo_log_version/ s/^/# /' \\ -e '/master_key_id/ s/^/# /' \\ -e '/server_uuid/ s/^/# /' \\ ${PWD}/restore-data/backup-my.cnf echo \"port=3309\\nskip-grant-tables\\nlower_case_table_names=1\" >> ${PWD}/restore-data/backup-my.cnf 启动 /usr/local/Cellar/mysql@5.7/5.7.39/bin/mysqld --defaults-file=${PWD}/restore-data/backup-my.cnf --user=mysql --datadir=${PWD}/restore-data 参考 https://www.fordba.com/mysql-innobackupex-usage-explain.html https://www.cnblogs.com/wxzhe/p/10033983.html https://www.jianshu.com/p/42d4764037ef https://www.cnblogs.com/f-ck-need-u/p/9018716.html https://blog.csdn.net/qq_43164571/article/details/113255886 https://zhuanlan.zhihu.com/p/419385309 https://www.cnblogs.com/zhoujinyi/p/5893333.html https://forums.percona.com/t/mysql-backup-of-remote-server-in-my-local-machine-using-percona-xtrabackup/6415/16 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-12-08 14:05:03 "},"origin/mysql-procedure-grammar.html":{"url":"origin/mysql-procedure-grammar.html","title":"存储过程语法","keywords":"","body":"MySQL存储过程 一、存储过程简介 SQL语句需要先编译然后执行，而存储过程（Stored Procedure）是一组为了完成特定功能的SQL语句集，经编译后存储在数据库中，用户通过指定存储过程的名字并给定参数（如果该存储过程带有参数）来调用执行它。 存储过程是数据库的一个重要的功能，MySQL 5.0以前并不支持存储过程，这使得MySQL在应用上大打折扣。好在MySQL 5.0开始支持存储过程，这样即可以大大提高数据库的处理速度，同时也可以提高数据库编程的灵活性。 存储过程是可编程的函数，在数据库中创建并保存，可以由SQL语句和控制结构组成。当想要在不同的应用程序或平台上执行相同的函数，或者封装特定功能时，存储过程是非常有用的。数据库中的存储过程可以看做是对编程中面向对象方法的模拟，它允许控制数据的访问方式。 存储过程的优点： 增强SQL语言的功能和灵活性：存储过程可以用控制语句编写，有很强的灵活性，可以完成复杂的判断和较复杂的运算。 标准组件式编程：存储过程被创建后，可以在程序中被多次调用，而不必重新编写该存储过程的SQL语句。而且数据库专业人员可以随时对存储过程进行修改，对应用程序源代码毫无影响。 较快的执行速度：如果某一操作包含大量的Transaction-SQL代码或分别被多次执行，那么存储过程要比批处理的执行速度快很多。因为存储过程是预编译的。在首次运行一个存储过程时查询，优化器对其进行分析优化，并且给出最终被存储在系统表中的执行计划。而批处理的Transaction-SQL语句在每次运行时都要进行编译和优化，速度相对要慢一些。 减少网络流量：针对同一个数据库对象的操作（如查询、修改），如果这一操作所涉及的Transaction-SQL语句被组织进存储过程，那么当在客户计算机上调用该存储过程时，网络中传送的只是该调用语句，从而大大减少网络流量并降低了网络负载。 作为一种安全机制来充分利用：通过对执行某一存储过程的权限进行限制，能够实现对相应的数据的访问权限的限制，避免了非授权用户对数据的访问，保证了数据的安全。 存储过程的定义信息保存在数据字典表information_schema.routines中 mysql中存储过程和函数的语法非常接近所以就放在一起，主要区别就是函数必须有返回值（return），并且函数的参数只有IN类型而存储过程有IN、OUT、INOUT这三种类型。 二、存储过程语法规则 CREATE [DEFINER = { user | CURRENT_USER }] PROCEDURE 过程名([[IN|OUT|INOUT] 参数名 数据类型[,[IN|OUT|INOUT] 参数名 数据类型…]]) [特性 ...] 过程体:(有效的SQL语句,记得末尾加;) DEFINER：用于指明存储过程是由哪个用户定义的，默认存储过程的定义者是存储过程，跟存储过程的使用权限无关 （如果在创建存储过程时指定为root@%，将有可能导致root@localhost在使用存储过程时出现权限问题。-- root@localhost是默认值） 形参： IN 输入参数:表示该参数的值必须在调用存储过程时指定，在存储过程中修改该参数的值不能被返回，为默认值 OUT 输出参数:该值可在存储过程内部被改变，并可返回 INOUT 输入输出参数:调用时指定，并且可被改变和返回 特性: COMMENT 'string' | LANGUAGE SQL | [NOT] DETERMINISTIC | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } -- COMMENT：给存储过程添加注释信息 -- LANGUAGE：用来说明语句部分是SQL语句，未来可能会支持其它类型的语句。 -- SQL SECURITY { DEFINER | INVOKER }指明谁有权限来执行： DEFINER表示只有定义者自己才能够执行；INVOKER表示调用者可以执行。 -- [NOT] DETERMINISTIC： --如果程序或线程总是对同样的输入参数产生同样的结果，则被认为它是“确定的”，否则就是“非确定”的。如果既没有给定DETERMINISTIC也没有给定NOT DETERMINISTIC，默认的就是NOT DETERMINISTIC（非确定的） -- NO SQL：表示子程序不包含SQL语句。 -- READS SQL DATA：表示子程序包含读数据的语句，但不包含写数据的语句。 -- MODIFIES SQL DATA：表示子程序包含写数据的语句。 存储过程体包含了在过程调用时必须执行的语句，例如：dml、ddl语句，if-then-else和while-do语句、声明变量的declare语句等,使用DELIMITER $$ 命令将语句的结束符号从分号 ; 临时改为两个$$，使得过程体中使用的分号被直接传递到服务器，而不会被客户端（如mysql）解释 -- 简单过程体 begin dml、ddl语句，if-then-else和while-do语句、声明变量的declare语句等; end$$ -- 加标签的过程体（） [begin_label:] BEGIN 　 　dml、ddl语句，if-then-else和while-do语句、声明变量的declare语句等; END [end_label] --嵌套的过程体 （ ） BEGIN 　 　BEGIN 　 　　　BEGIN 　　　 　　　statements; 　　 　　END 　 　END END 变量作用域 内部的变量在其作用域范围内享有更高的优先权，当执行到end。变量时，内部变量消失，此时已经在其作用域外，变量不再可见了，应为在存储 过程外再也不能找到这个申明的变量，但是你可以通过out参数或者将其值指派 给会话变量来保存其值。 mysql > DELIMITER // mysql > CREATE PROCEDURE proc3() -> begin -> declare x1 varchar(5) default 'outer'; -> begin -> declare x1 varchar(5) default 'inner'; -> select x1; -> end; -> select x1; -> end; -> // mysql > DELIMITER ; 条件语句 if-then -else语句 mysql > DELIMITER // mysql > CREATE PROCEDURE proc2(IN parameter int) -> begin -> declare var int; -> set var=parameter+1; -> if var=0 then -> insert into t values(17); -> end if; -> if parameter=0 then -> update t set s1=s1+1; -> else -> update t set s1=s1+2; -> end if; -> end; -> // mysql > DELIMITER ; case语句： mysql > DELIMITER // mysql > CREATE PROCEDURE proc3 (in parameter int) -> begin -> declare var int; -> set var=parameter+1; -> case var -> when 0 then -> insert into t values(17); -> when 1 then -> insert into t values(18); -> else -> insert into t values(19); -> end case; -> end; -> // mysql > DELIMITER ; case when var=0 then insert into t values(30); when var>0 then when var循环语句 while ···· end while mysql > DELIMITER // mysql > CREATE PROCEDURE proc4() -> begin -> declare var int; -> set var=0; -> while var insert into t values(var); -> set var=var+1; -> end while; -> end; -> // mysql > DELIMITER ; while条件 do --循环体 endwhile repeat···· end repeat 它在执行操作后检查结果，而while则是执行前进行检查。 mysql > DELIMITER // mysql > CREATE PROCEDURE proc5 () -> begin -> declare v int; -> set v=0; -> repeat -> insert into t values(v); -> set v=v+1; -> until v>=5 -> end repeat; -> end; -> // mysql > DELIMITER ; repeat --循环体 until循环条件 endrepeat; loop ·····endloop loop循环不需要初始条件，这点和while 循环相似，同时和repeat循环一样不需要结束条件, leave语句的意义是离开循环。 mysql > DELIMITER // mysql > CREATE PROCEDURE proc6 () -> begin -> declare v int; -> set v=0; -> LOOP_LABLE:loop -> insert into t values(v); -> set v=v+1; -> if v >=5 then -> leave LOOP_LABLE; -> end if; -> end loop; -> end; -> // mysql > DELIMITER ; LABLES 标号 标号可以用在begin repeat while 或者loop 语句前，语句标号只能在合法的语句前面使用。可以跳出循环，使运行指令达到复合语句的最后一步。 ITERATE 迭代 通过引用复合语句的标号,来从新开始复合语句 mysql > DELIMITER // mysql > CREATE PROCEDURE proc10 () -> begin -> declare v int; -> set v=0; -> LOOP_LABLE:loop -> if v=3 then -> set v=v+1; -> ITERATE LOOP_LABLE; -> end if; -> insert into t values(v); -> set v=v+1; -> if v>=5 then -> leave LOOP_LABLE; -> end if; -> end loop; -> end; -> // mysql > DELIMITER ; 三、存储过程的基本函数 字符串类 CHARSET(str) //返回字串字符集 CONCAT (string2 [,... ]) //连接字串 INSTR (string ,substring ) //返回substring首次在string中出现的位置,不存在返回0 LCASE (string2 ) //转换成小写 LEFT (string2 ,length ) //从string2中的左边起取length个字符 LENGTH (string ) //string长度 LOAD_FILE (file_name ) //从文件读取内容 LOCATE (substring , string [,start_position ] ) 同INSTR,但可指定开始位置 LPAD (string2 ,length ,pad ) //重复用pad加在string开头,直到字串长度为length LTRIM (string2 ) //去除前端空格 REPEAT (string2 ,count ) //重复count次 REPLACE (str ,search_str ,replace_str ) //在str中用replace_str替换search_str RPAD (string2 ,length ,pad) //在str后用pad补充,直到长度为length RTRIM (string2 ) //去除后端空格 STRCMP (string1 ,string2 ) //逐字符比较两字串大小, SUBSTRING (str , position [,length ]) //从str的position开始,取length个字符, 注：mysql中处理字符串时，默认第一个字符下标为1，即参数position必须大于等于1 1. mysql> select substring('abcd',0,2); 2. +-----------------------+ 3. | substring('abcd',0,2) | 4. +-----------------------+ 5. | | 6. +-----------------------+ 7. 1 row in set (0.00 sec) 8. 9. mysql> select substring('abcd',1,2); 10.+-----------------------+ 11.| substring('abcd',1,2) | 12.+-----------------------+ 13.| ab | 14.+-----------------------+ 15.1 row in set (0.02 sec) TRIM([[BOTH|LEADING|TRAILING][padding] FROM]string2) //去除指定位置的指定字符 UCASE (string2 ) //转换成大写 RIGHT(string2,length) //取string2最后length个字符 SPACE(count) //生成count个空格 数学类 ABS (number2 ) //绝对值 BIN (decimal_number ) //十进制转二进制 CEILING (number2 ) //向上取整 CONV(number2,from_base,to_base) //进制转换 FLOOR (number2 ) //向下取整 FORMAT (number,decimal_places ) //保留小数位数 HEX (DecimalNumber ) //转十六进制 注：HEX()中可传入字符串，则返回其ASC-11码，如HEX('DEF')返回4142143 也可以传入十进制整数，返回其十六进制编码，如HEX(25)返回19 LEAST (number , number2 [,..]) //求最小值 MOD (numerator ,denominator ) //求余 POWER (number ,power ) //求指数 RAND([seed]) //随机数 ROUND (number [,decimals ]) //四舍五入,decimals为小数位数] 注：返回类型并非均为整数，如： (1)默认变为整形值 1. mysql> select round(1.23); 2. +-------------+ 3. | round(1.23) | 4. +-------------+ 5. | 1 | 6. +-------------+ 7. 1 row in set (0.00 sec) 8. 9. mysql> select round(1.56); 10.+-------------+ 11.| round(1.56) | 12.+-------------+ 13.| 2 | 14.+-------------+ 15.1 row in set (0.00 sec) (2)可以设定小数位数，返回浮点型数据 1. mysql> select round(1.567,2); 2. +----------------+ 3. | round(1.567,2) | 4. +----------------+ 5. | 1.57 | 6. +----------------+ 7. 1 row in set (0.00 sec) SIGN (number2 ) // 日期时间类 ADDTIME (date2 ,time_interval )//将time_interval加到date2 CONVERT_TZ (datetime2 ,fromTZ ,toTZ ) //转换时区 CURRENT_DATE ( ) //当前日期 CURRENT_TIME ( ) //当前时间 CURRENT_TIMESTAMP ( ) //当前时间戳 DATE (datetime ) //返回datetime的日期部分 DATE_ADD (date2 , INTERVAL d_value d_type ) //在date2中加上日期或时间 DATE_FORMAT (datetime ,FormatCodes ) //使用formatcodes格式显示datetime DATE_SUB (date2 , INTERVAL d_value d_type ) //在date2上减去一个时间 DATEDIFF (date1 ,date2 ) //两个日期差 DAY (date ) //返回日期的天 DAYNAME (date ) //英文星期 DAYOFWEEK (date ) //星期(1-7) ,1为星期天 DAYOFYEAR (date ) //一年中的第几天 EXTRACT (interval_name FROM date ) //从date中提取日期的指定部分 MAKEDATE (year ,day ) //给出年及年中的第几天,生成日期串 MAKETIME (hour ,minute ,second ) //生成时间串 MONTHNAME (date ) //英文月份名 NOW ( ) //当前时间 SEC_TO_TIME (seconds ) //秒数转成时间 STR_TO_DATE (string ,format ) //字串转成时间,以format格式显示 TIMEDIFF (datetime1 ,datetime2 ) //两个时间差 TIME_TO_SEC (time ) //时间转秒数] WEEK (date_time [,start_of_week ]) //第几周 YEAR (datetime ) //年份 DAYOFMONTH(datetime) //月的第几天 HOUR(datetime) //小时 LAST_DAY(date) //date的月的最后日期 MICROSECOND(datetime) //微秒 MONTH(datetime) //月 MINUTE(datetime) //分返回符号,正负或0 SQRT(number2) //开平方 MySql分页存储过程 MySql测试版本：5.0.41-community-nt DROP PROCEDURE IF EXISTS pr_pager; CREATE PROCEDURE pr_pager( IN p_table_name VARCHAR(1024), IN p_fields VARCHAR(1024), IN p_page_size INT, IN p_page_now INT, IN p_order_string VARCHAR(128), IN p_where_string VARCHAR(1024), OUT p_out_rows INT ) NOT DETERMINISTIC SQL SECURITY DEFINER COMMENT '分页存储过程' BEGIN DECLARE m_begin_row INT DEFAULT 0; DECLARE m_limit_string CHAR(64); SET m_begin_row = (p_page_now - 1) * p_page_size; SET m_limit_string = CONCAT(' LIMIT ', m_begin_row, ', ', p_page_size); SET @COUNT_STRING = CONCAT('SELECT COUNT(*) INTO @ROWS_TOTAL FROM ', p_table_name, ' ', p_where_string); SET @MAIN_STRING = CONCAT('SELECT ', p_fields, ' FROM ', p_table_name, ' ', p_where_string, ' ', p_order_string,m_limit_string); PREPARE count_stmt FROM @COUNT_STRING; EXECUTE count_stmt; DEALLOCATE PREPARE count_stmt; SET p_out_rows = @ROWS_TOTAL; PREPARE main_stmt FROM @MAIN_STRING; EXECUTE main_stmt; DEALLOCATE PREPARE main_stmt; END; 调用 mysql> call pr_pager(\"t\",\"var\",3,3,\"\",\"\",@result); mysql> call pr_pager(\"t\",\"var\",3,2,\"\",\"\",@result); 四、存储过程操作 1、查看存储过程 #查询存储过程 SELECT name FROM mysql.proc WHERE db='数据库名' and `type` = 'PROCEDURE' \\G; SELECT routine_name FROM information_schema.routines WHERE routine_schema='数据库名'; SHOW PROCEDURE STATUS WHERE db='数据库名'; #查看存储过程详细信息 SHOW CREATE PROCEDURE 数据库.存储过程名; 2、调用存储过程 call 数据库.存储过程名; 3、删除存储过程 DROP PROCEDURE [IF EXISTS] db_name.sp_name; #如果存储过程或存储函数不存在时，仍然进行删除，可以使用IF EXISTS子句，它可以防止发生错误，产生一个用SHOW WARNINGS查看的警告。 4、修改存储过程 -- 只能改变存储过程的特征，不能修改过程的参数以及过程体。如果想做这样的修改，必须先使用DROP PROCEDURE 删除过程，然后使用and CREATE PROCEDURE重建过程。 ALTER {PROCEDURE | FUNCTION} sp_name [特征 ...] Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/mysql-procedure.html":{"url":"origin/mysql-procedure.html","title":"常用存储过程","keywords":"","body":"常用存储过程 1、修改Database中所有表的所有字段的编码格式（mysql） delimiter $$ CREATE PROCEDURE Test () begin DECLARE cnt VARCHAR(100); -- 声明变量用来记录查询出的表名 DECLARE i int; -- 循环条件，同时可以用来标记表第几张表 set i = 0; -- 循环开始 while i 参考：https://blog.csdn.net/LUNG108/article/details/78285054 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/mysql-transaction.html":{"url":"origin/mysql-transaction.html","title":"事物隔离级别","keywords":"","body":"MySQL的事务隔离级别 一、首先什么是事务？ 事务是应用程序中一系列严密的操作，所有操作必须成功完成，否则在每个操作中所作的所有更改都会被撤消。也就是事务具有原子性，一个事务中的一系列的操作要么全部成功，要么一个都不做。 事务的结束有两种，当事务中的所以步骤全部成功执行时，事务提交。如果其中一个步骤失败，将发生回滚操作，撤消撤消之前到事务开始时的所以操作。 二、事务的 ACID 事务具有四个特征：原子性（ Atomicity ）、一致性（ Consistency ）、隔离性（ Isolation ）和持续性（ Durability ）。这四个特性简称为 ACID 特性。 1 、原子性。事务是数据库的逻辑工作单位，事务中包含的各操作要么都做，要么都不做 2 、一致性。事 务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。因此当数据库只包含成功事务提交的结果时，就说数据库处于一致性状态。如果数据库系统 运行中发生故障，有些事务尚未完成就被迫中断，这些未完成事务对数据库所做的修改有一部分已写入物理数据库，这时数据库就处于一种不正确的状态，或者说是 不一致的状态。 3 、隔离性。一个事务的执行不能其它事务干扰。即一个事务内部的操作及使用的数据对其它并发事务是隔离的，并发执行的各个事务之间不能互相干扰。 4 、持续性。也称永久性，指一个事务一旦提交，它对数据库中的数据的改变就应该是永久性的。接下来的其它操作或故障不应该对其执行结果有任何影响。 三、Mysql的四种隔离级别 SQL标准定义了4类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。 Read Uncommitted（读取未提交内容） 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。 Read Committed（读取提交内容） 这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。 Repeatable Read（可重读） 这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。 Serializable（可串行化） 这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。 这四种隔离级别采取不同的锁类型来实现，若读取的是同一个数据的话，就容易发生问题。例如： 脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。 不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。 幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。 ​ 四、MySQL设置隔离级别 系统默认事务级别为：repeatable-read。 查看事务隔离级别： SELECT @@tx_isolation 1、 服务器启动时设置级别。 在MySQL配置文件/etc/my.cnf中设置，然后重启MySQL. [mysqld] transaction-isolation = [SERIALIZABLE |READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ ] 2、使用SET TRANSACTION ISOLATION LEVEL语句为正在运行的服务器设置。 # 设置全局级别默认事务隔离级别，适用于从设置时起所有新建立的客户机连接。现有连接不受影响。 SET GLOBAL TRANSACTION ISOLATION LEVEL [SERIALIZABLE |READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ ] ; #设置会话级别默认事务隔离级别，如果没有显式指定，则事务隔离级别将按会话进行设置，应用于当前session内之后的所有事务。 SET SESSION TRANSACTION ISOLATION LEVEL [SERIALIZABLE |READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ ]; #适用于当前session内的下一个还未开始的事务 SET TRANSACTION ISOLATION LEVEL [SERIALIZABLE |READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ ]; Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/mysql-temporary.html":{"url":"origin/mysql-temporary.html","title":"临时表","keywords":"","body":"MySQL临时表 一、临时表的种类 全局临时表 这种临时表从数据库实例启动后开始生效，在数据库实例销毁后失效。在MySQL里面这种临时表对应的是内存表，即memory引擎。 会话级别临时表 这种临时表在用户登录系统成功后生效，在用户退出时失效。在MySQL里的临时表指的就是以create temporary table这样的关键词创建的表。 事务级别临时表 这种临时表在事务开始时生效，事务提交或者回滚后失效。 在MySQL里面没有这种临时表，必须利用会话级别的临时表间接实现。 检索级别临时表 这种临时表在SQL语句执行之间产生，执行完毕后失效。 在MySQL里面这种临时表不是很固定，跟随MySQL默认存储引擎来变化。 比如默认存储引擎是MyISAM，临时表的引擎就是MyISAM，并且文件生成形式以及数据运作形式和MyISAM一样，只是数据保存在内存里；如果默认引擎是INNODB，那么临时表的引擎就是INNODB，此时它的所有信息都保存在共享表空间ibdata里面。 二、MySQL 5.7的临时表空间优化 MySQL 5.7 把临时表的数据以及回滚信息（仅限于未压缩表）从共享表空间里面剥离出来，形成自己单独的表空间，参数为innodb_temp_data_file_path。 在MySQL 5.7 中把临时表的相关检索信息保存在系统信息表中：information_schema.innodb_temp_table_info. 而MySQL 5.7之前的版本想要查看临时表的系统信息是没有太好的办法。 注意： 虽然INNODB临时表有自己的表空间，但是目前还不能自己定义临时表空间文件的保存路径，只能是继承innodb_data_home_dir。此时如果想要拿其他的磁盘，比如内存盘来充当临时表空间的保存地址，只能用老办法，做软链。 三、临时表使用建议 设置 innodb_temp_data_file_path 选项，设定文件最大上限，超过上限时，需要生成临时表的SQL无法被执行（一般这种SQL效率也比较低，可借此机会进行优化）。 检查 INFORMATION_SCHEMA.INNODB_TEMP_TABLE_INFO，找到最大的临时表对应的线程，kill之即可释放，但 ibtmp1 文件则不能释放（除非重启）。 择机重启实例，释放ibtmp1文件，和ibdata1不同，ibtmp1重启时会被重新初始化而 ibdata1 则不可以。 定期检查运行时长超过N秒（比如N=300）的SQL，考虑干掉，避免垃圾SQL长时间运行影响业务。 四、临时表的创建 mysql> create temporary table temp1(sid int,sname varchar(10)); Query OK, 0 rows affected (0.00 sec) mysql> insert into temp1 values(1,'aaa'); Query OK, 1 row affected (0.00 sec) mysql> select * from temp1; +------+-------+ | sid | sname | +------+-------+ | 1 | aaa | +------+-------+ 1 row in set (0.00 sec) mysql> show tables; +-----------------+ | Tables_in_test1 | +-----------------+ | app01 | | app02 | | app03 | +-----------------+ 3 rows in set (0.00 sec) 另起一个会话： mysql> use test1; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> show tables; +-----------------+ | Tables_in_test1 | +-----------------+ | app01 | | app02 | | app03 | +-----------------+ 3 rows in set (0.00 sec) mysql> select * from temp1; ERROR 1146 (42S02): Table 'test1.temp1' doesn't exist 退出本次会话： mysql> use test1; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> select * from temp1; ERROR 1146 (42S02): Table 'test1.temp1' doesn't exist Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/mysql-problem-solution.html":{"url":"origin/mysql-problem-solution.html","title":"MySQL常见问题解决","keywords":"","body":"MySQL常见问题解决方案 1、容器化MySQL启动失败 上下文 部署在K8s中的单MySQL实例，数据文件路径使用NFS存储后端挂载的PVC持久化的。再一次重建MySQL Pod后，创建失败。报错如下： 2021-12-17T11:57:36.429443Z 0 [ERROR] InnoDB: Only one log file found. 2021-12-17T11:57:36.429472Z 0 [ERROR] InnoDB: Plugin initialization aborted with error not found 2021-12-17T11:57:37.029965Z 0 [ERROR] Plugin 'InnoDB' init function returned error. 2021-12-17T11:57:37.029998Z 0 [ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE failed. 2021-12-17T11:57:37.030005Z 0 [ERROR] Failed to initialize builtin plugins. 2021-12-17T11:57:37.030008Z 0 [ERROR] Aborting 2021-12-17T11:57:37.030012Z 0 [Note] Binlog end 2021-12-17T11:57:37.030062Z 0 [Note] Shutting down plugin 'CSV' 2021-12-17T11:57:37.031982Z 0 [Note] mysqld: Shutdown complete 检查数据路径下存在ib_logfile0 、ib_logfile1文件 原因： ib_logfile0 、ib_logfile1文件是MySQL InnoDB引擎的事务日志。MySQL崩溃重启时，会进行事务重做；在系统正常时，每次checkpoint时间点，会将之前写入事务应用到数据文件中。 解决方案： 备份所有ib_logfile*文件到其他目录后重启。 参考： https://dba.stackexchange.com/questions/41542/issue-after-moving-the-ib-logfile1-and-ib-logfile0-files https://www.codeleading.com/article/33775310436/ https://www.cnblogs.com/qianyuliang/p/9916372.html https://dba.stackexchange.com/questions/87692/flush-clean-mysql-ib-logfile0-ib-logfile1 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/redis-basic.html":{"url":"origin/redis-basic.html","title":"基础概念","keywords":"","body":"Redis基础概念 一、简介 Redis 是一个开源（BSD许可）的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。 它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。 Redis是一个使用ANSI C编写的开源、包含多种数据结构、支持网络、基于内存、可选持久性的键值对存储数据库，其具备如下特性： 基于内存运行，性能高效 支持分布式，理论上可以无限扩展 key-value存储系统 开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API 相比于其他数据库类型，Redis具备的特点是： C/S通讯模型 单进程单线程模型 丰富的数据类型 操作具有原子性 持久化 高并发读写 支持lua脚本 官网：https://redis.io/ 中文官网：http://www.redis.cn/ 教程：https://www.runoob.com/redis/redis-intro.html 二、基础概念 1、数据结构 Redis支持五种数据类型： string（字符串） 二进制安全的字符串，意味着它不仅能够存储字符串、还能存储图片、视频等多种类型, 最大长度支持512M。操作命令如下： GET/MGET SET/SETEX/MSET/MSETNX INCR/DECR GETSET DEL hash（哈希） 该类型是由field和关联的value组成的map。其中，field和value都是字符串类型的。Hash的操作命令如下： HGET/HMGET/HGETALL HSET/HMSET/HSETNX HEXISTS/HLEN HKEYS/HDEL HVALS list（列表） 该类型是一个插入顺序排序的字符串元素集合, 基于双链表实现。List的操作命令如下： LPUSH/LPUSHX/LPOP/RPUSH/RPUSHX/RPOP/LINSERT/LSET LINDEX/LRANGE LLEN/LTRIM set（集合） Set类型是一种无顺序集合, 它和List类型最大的区别是：集合中的元素没有顺序, 且元素是唯一的。Set类型的底层是通过哈希表实现的，其操作命令为： SADD/SPOP/SMOVE/SCARD SINTER/SDIFF/SDIFFSTORE/SUNION zset(sorted set：有序集合) ZSet是一种有序集合类型，每个元素都会关联一个double类型的分数权值，通过这个权值来为集合中的成员进行从小到大的排序。与Set类型一样，其底层也是通过哈希表实现的。 ZSet命令： ZADD/ZPOP/ZMOVE/ZCARD/ZCOUNT ZINTER/ZDIFF/ZDIFFSTORE/ZUNION 2、DB 在 Redis 下，默认有16个数据库，数据库是由一个整数索引标识（就是说数据库名是 0-15），而不是由一个数据库名称。默认情况下，一个客户端连接到数据库 0。 三、Redis 高可用 在 Redis 中，实现 高可用 的技术主要包括 持久化、复制、哨兵 和 集群： 持久化：持久化是 最简单的 高可用方法。它的主要作用是 数据备份，即将数据存储在 硬盘，保证数据不会因进程退出而丢失。 复制：复制是高可用 Redis 的基础，哨兵 和 集群 都是在 复制基础 上实现高可用的。复制主要实现了数据的多机备份以及对于读操作的负载均衡和简单的故障恢复。缺陷是故障恢复无法自动化、写操作无法负载均衡、存储能力受到单机的限制。 哨兵：在复制的基础上，哨兵实现了 自动化 的 故障恢复。缺陷是 写操作 无法 负载均衡，存储能力 受到 单机 的限制。 集群：通过集群，解决 写操作 无法 负载均衡 以及 存储能力 受到 单机限制 的问题，实现了较为 完善 的 高可用方案。 1、主从模式 Redis 主从复制 可将 主节点 数据同步给 从节点，从节点此时有两个作用： 一旦 主节点宕机，从节点 作为 主节点 的 备份 可以随时顶上来。 扩展 主节点 的 读能力，分担主节点读压力。 主从复制 同时存在以下几个问题： 一旦 主节点宕机，从节点 晋升成 主节点，同时需要修改 应用方 的 主节点地址，还需要命令所有 从节点 去 复制 新的主节点，整个过程需要 人工干预。 主节点 的 写能力 受到 单机的限制。 主节点 的 存储能力 受到 单机的限制。 原生复制 的弊端在早期的版本中也会比较突出，比如：Redis 复制中断 后，从节点 会发起 psync。此时如果 同步不成功，则会进行 全量同步，主库 执行 全量备份 的同时，可能会造成毫秒或秒级的 卡顿。 2、主从哨兵模式Sentinel Sentinel 的主要功能包括 主节点存活检测、主从运行情况检测、自动故障转移 （failover）、主从切换。Redis 的 Sentinel 最小配置是 一主一从。 Redis 的 Sentinel 系统可以用来管理多个 Redis 服务器，该系统可以执行以下四个任务： 监控：Sentinel 会不断的检查 主服务器 和 从服务器 是否正常运行。 通知：当被监控的某个 Redis 服务器出现问题，Sentinel 通过 API 脚本 向 管理员 或者其他的 应用程序 发送通知。 自动故障转移：当 主节点 不能正常工作时，Sentinel 会开始一次 自动的 故障转移操作，它会将与 失效主节点 是 主从关系 的其中一个 从节点 升级为新的 主节点，并且将其他的 从节点 指向 新的主节点。 配置提供者：在 Redis Sentinel 模式下，客户端应用 在初始化时连接的是 Sentinel 节点集合，从中获取 主节点 的信息。 Redis Sentinel的工作原理 每个 Sentinel 以 每秒钟 一次的频率，向它所知的 主服务器、从服务器 以及其他 Sentinel 实例 发送一个 PING 命令。 如果一个 实例（instance）距离 最后一次 有效回复 PING 命令的时间超过 down-after-milliseconds 所指定的值，那么这个实例会被 Sentinel 标记为 主观下线。 如果一个 主服务器 被标记为 主观下线，那么正在 监视 这个 主服务器 的所有 Sentinel 节点，要以 每秒一次 的频率确认 主服务器 的确进入了 主观下线 状态。 如果一个 主服务器 被标记为 主观下线，并且有 足够数量 的 Sentinel（至少要达到 配置文件 指定的数量）在指定的 时间范围 内同意这一判断，那么这个 主服务器 被标记为 客观下线。 当一个 主服务器 被 Sentinel 标记为 客观下线 时，Sentinel 向 下线主服务器 的所有 从服务器 发送 INFO 命令的频率，会从 10 秒一次改为 每秒一次。 Sentinel 和其他 Sentinel 协商 主节点 的状态，如果 主节点 处于 SDOWN 状态，则投票自动选出新的 主节点。将剩余的 从节点 指向 新的主节点 进行 数据复制。 当没有足够数量的 Sentinel 同意 主服务器 下线时， 主服务器 的 客观下线状态 就会被移除。当 主服务器 重新向 Sentinel 的 PING 命令返回 有效回复 时，主服务器 的 主观下线状态 就会被移除。 注意：一个有效的 PING 回复可以是：+PONG、-LOADING 或者 -MASTERDOWN。如果 服务器 返回除以上三种回复之外的其他回复，又或者在 指定时间 内没有回复 PING 命令， 那么 Sentinel 认为服务器返回的回复 无效（non-valid） 哨兵模式下客户端的连接过程 1、遍历哨兵集合获取到一个可用的哨兵节点。因为哨兵节点之间是共享数据的，任意节点都可以获取到主节点的信息。 2、通过 sentinel get-master-addr-by-name master-nameAPI 来获取对应主节点的信息。 3、验证获取到的主节点是不是真正的主节点，防止故障转移期间主节点的变化。 4、保持和哨兵节点集合的联系，时刻获取关于主节点的相关信息。 3、集群Cluster 参考： https://juejin.cn/post/6844903663362637832 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/reids-common-opreations.html":{"url":"origin/reids-common-opreations.html","title":"常用操作","keywords":"","body":"Redis常用操作 一、全局命令 0、配置操作 ①添加设置 config set parameter value ②获取设置 config get parameter ③重置INFO命令统计的一些计算器 config resetstat # 被重置的数据如下: # Keyspace hits # Keyspace misses # Number of commands processed # Number of connections received # Number of expired keys 1、DB库操作 ①切换库 select db # 16个DB库，0-15.默认0库 2、客户端操作 ①查询客户端连接信息 client list ②剔除杀掉指定客户端 client kill ip:port ③返回当前连接名 client getname # 返回当前连接由CLIENT SETNAME设置的名字。如果没有用CLIENT SETNAME设置名字，将返回一个空的回复。 ④返回当前连接的ID client id # 每个ID符合如下约束： # 1.永不重复。当调用命令CLIENT ID返回相同的值时，调用者可以确认原连接未被断开，只是被重用 ，因此仍可以认为是同一连接 # 2.ID值单调递增。若某一连接的ID值比其他连接的ID值大，可以确认该连接是较新创建的 3、全局数据操作 ①清除数据 # 清除DB中的所有数据 flushdb # 清除所有库中的数据 flushall 4、监控统计操作 ①实时显示接受到的命令 monitor ②Redis服务器与统计信息 info [section] # section: # 1.server: Redis服务器的一般信息 # 2.clients: 客户端的连接部分 # 3.memory: 内存消耗相关信息 # 4.persistence: RDB和AOF相关信息 # 5.stats: 一般统计 # 6.replication: 主/从复制信息 # 7.cpu: 统计CPU的消耗 # 8.commandstats: Redis命令统计 # 9.cluster: Redis集群信息 # 10.keyspace: 数据库的相关统计 # all: 返回所有信息 # default: 值返回默认设置的信息 # 如果没有使用任何参数时，默认为default。 info memory指标 指标 含义 used_memory 由 Redis 分配器分配的内存总量，包含了redis进程内部的开销和数据占用的内存，以字节（byte）为单位，即当前redis使用内存大小。 used_memory_human 已更直观的单位展示分配的内存总量。 used_memory_rss 向操作系统申请的内存大小，与 top 、 ps等命令的输出一致，即redis使用的物理内存大小。 used_memory_rss_human 已更直观的单位展示向操作系统申请的内存大小。 used_memory_peak redis的内存消耗峰值(以字节为单位)，即历史使用记录中redis使用内存峰值。 used_memory_peak_human 以更直观的格式返回redis的内存消耗峰值 used_memory_peak_perc 使用内存达到峰值内存的百分比，used_memory/ used_memory_peak) 100%，即当前redis使用内存/历史使用记录中redis使用内存峰值100% used_memory_overhead Redis为了维护数据集的内部机制所需的内存开销，包括所有客户端输出缓冲区、查询缓冲区、AOF重写缓冲区和主从复制的backlog。 used_memory_startup Redis服务器启动时消耗的内存 used_memory_dataset 数据实际占用的内存大小，即used_memory-used_memory_overhead used_memory_dataset_perc 数据占用的内存大小的百分比，100%*(used_memory_dataset/(used_memory-used_memory_startup)) total_system_memory 整个系统内存 total_system_memory_human 以更直观的格式显示整个系统内存 used_memory_lua Lua脚本存储占用的内存 used_memory_lua_human 以更直观的格式显示Lua脚本存储占用的内存 maxmemory Redis实例的最大内存配置 maxmemory_human 以更直观的格式显示Redis实例的最大内存配置 maxmemory_policy 当达到maxmemory时的淘汰策略 mem_fragmentation_ratio 碎片率，used_memory_rss/ used_memory。ratio指数>1表明有内存碎片，越大表明越多， mem_allocator 内存分配器 active_defrag_running 表示没有活动的defrag任务正在运行，1表示有活动的defrag任务正在运行（defrag:表示内存碎片整理）详解 lazyfree_pending_objects 0表示不存在延迟释放的挂起对象 ③查询键值在内存中占用的字节数 memory usage key # 返回的结果是key的值以及为管理该key分配的内存总字节数。对于嵌套数据类型，可以使用选项SAMPLES，其中COUNT表示抽样的元素个数，默认值为5。当需要抽样所有元素时，使用SAMPLES 0 # 如上，实际数据为空，但是存储时仍然耗费了一些内存，这些内存用于Redis 服务器维护内部数据结构。随着key和value的增大，内存使用量和key 大小基本成 线性关系。 ④获取最后一次同步磁盘的时间 lastsave # 执行成功时返回UNIX时间戳 5、慢日志操作 Redis慢查询日志是一个记录超过指定执行时间的查询的系统。 这里的执行时间不包括IO操作，比如与客户端通信，发送回复等等，而只是实际执行命令所需的时间（这是唯一在命令执行过程中线程被阻塞且不能同时处理其他请求的阶段）。 慢查询日志在内存中堆积，因此不会写入一个包含慢速命令执行信息的文件。 这使得慢查询日志非常快，你可以开启所有命令的日志记录（设置slowlog-log-slower-than参数值为零），但性能较低。 ①获取慢日志 slowlog get [条数] 每一个条目由四个字段组成： 每个慢查询条目的唯一的递增标识符。 处理记录命令的unix时间戳。 命令执行所需的总时间，以微秒为单位。 组成该命令的参数的数组。 条目的唯一ID可以用于避免慢查询条目被多次处理（例如，你也许有一个脚本使用每个新的慢查询日志条目给你发送报警邮件）。 条目ID在Redis服务器运行期间绝不会被重置，仅在Redis服务重启才重置它。 ②设置慢查询日志 config set _slowlog-log-slower-than_ milliseconds # Redis命令的执行时间超过多少微秒将会被记录。 请注意， # 1.负数将会关闭慢查询日志 # 2.0将强制记录每一个命令 config set _slowlog-max-len_ 慢日志条目最大条数 # 设置慢查询日志的长度。 最小值是0。 当一个新命令被记录，且慢查询日志已经达到其最大长度时，将从记录命令的队列中移除删除最旧的命令以腾出空间。 ③获取慢查询日志的最新条目ID slowlog len ④重置慢查询日志 slowlog reset # 删除后，信息将永远丢失 二、Key操作 查找key 查找所有符合给定模式pattern（正则表达式）的 key key # 支持的正则表达模式： # h?llo 匹配 hello, hallo 和 hxllo # h*llo 匹配 hllo 和 heeeello # h[ae]llo 匹配 hello 和 hallo, 但是不匹配 hillo # h[^e]llo 匹配 hallo, hbllo, … 但是不匹配 hello # h[a-b]llo 匹配 hallo 和 hbllo 查询key的值 get key # 不存在则返回nil 批量获取key的值 mget key [key .....] # mget 1 2 3 查询当前DB的key总数 dbsize 检查key是否存在 exist key # 存在返回1，不存在返回0 查询key的过期时间 ttl key # 查询键在多少秒后过期 （>0 剩余过期时间；-1 没有设置过期时间；-2 键不存在） pttl key # 查询键在多少毫秒后过期 查询key的数据类型 type key （如果键不存在，则返回none） 删除Key del key 统计Key scan cursor match 正则表达式 三、String类型键的操作命令 1、DDL ①设置键值 set key value [ex] [px] [nx|xx] # ex 秒级过期时间 # px 毫秒级过期时间 ②设置键值并返回原值 getset key value ③批量设置键值 mset key value [key value] # mset 1 a 2 b 3 c ④设置键的过期时间 expire key senconds/milliseconds # 设置key在second秒/milliseconds毫秒后过期 expire key timestamp/milliseconds-timestamp # 设置key在秒级时间戳/毫秒时间戳戳后过期 ⑤重命名键 rename key newkey ⑥追加内容到String类型键的末尾 append key value 2、DQL ①查询string类型键的长度 strlen key ②查询String类型键指定长度的值 getrange key start end # start和end指从0开始的开始与结束偏移量 四、Hash操作命令 1、DDL ①创建hash字段 hset key field value ②批量创建hash字段 hmset key field value [field value ...] ③删除hash中一个或多个field hdel key field [key field ....] 2、DQL ①获取字段的值 hset get key field ②批量获取字段的值 hmget key field [field ...] ③获取所有字段的数量 hlen key ④获取所有的字段 hkeys key ⑤获取指定字段的长度 hstrlen key field ⑥获取所有的字段值 hvals key ⑦判断是否存在field hexists key field ⑧读取所有的field与值 hgetall key 五、List操作命令 1、DDL ①将一个或多个的值插入列表的头部 lpush key value1 [value2 ...] # 如果 key 不存在，那么在进行 push 操作前会创建一个空列表。 # 元素是从最左端的到最右端的、一个接一个被插入到 list 的头部。 # redis版本>= 2.4才可以接受多个 value 参数。 ② 将一个或多个的值插入列表的尾部 rpush key value1 [value2 ...] # 如果 key 不存在，那么会创建一个空的列表然后再进行 push 操作 # 元素是从左到右一个接一个从列表尾部插入 # 返回值是在 push 操作后的列表长度。 rpushx key value1 [value2 ...] # 只有当 key 已经存在并且存着一个 list 的时候，在这个 key 下面的 list 的尾部部插入 value。 ③插入已有列表某个元素前面 linsert key before value new_value ④弹出列表第一个元素并返回元素的值 lpop key # 返回第一个元素的值，或者当 key 不存在时返回 nil。 ⑤弹出列表最后一个元素并返回元素的值 rpop key # 当 key 不存在的时候返回 nil。 ⑥弹出列表中的最后一个元素，并将其追加到另外一个列表的头部 rpoplpush list1 list2 # 如果 list2 不存在，那么会返回 nil 值，并且不会执行任何操作。 如果 list1 和 list2 是同样的，那么这个操作等同于移除列表最后一个元素并且把该元素放在列表头部， 所以这个命令也可以当作是一个旋转列表的命令 ⑦从列表中删除指定个数的重复元素 lrem key count value # 从存于 key 的列表里移除前 count 次出现的值为 value 的元素。 这个 count 参数通过下面几种方式影响这个操作： # count > 0: 从头往尾移除值为 value 的元素。 # count ⑧设置指定索引位置元素的值 lset key index value # 当index超出范围时会返回一个error。 2、DQL ①根据索引获取一个元素 lindex key 0 # 0 是表示第一个元素， 1 表示第二个元素，并以此类推。 负数索引用于指定从列表尾部开始索引的元素。在这种方法下，-1 表示最后一个元素，-2 表示倒数第二个元素，并以此往前推。 ②查询指定范围内的元素 lrange key start end # start 和 end 偏移量都是基于0的下标 # 当下标超过list范围的时候不会产生error。 如果start比list的尾部下标大的时候，会返回一个空列表。 如果stop比list的实际尾部大的时候，Redis会当它是最后一个元素的下标。 ③获取队列的长度 llen key # 如果 key 不存在，那么就被看作是空list，并且返回长度为 0。 当存储在 key 里的值不是一个list的话，会返回error。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-18 11:49:41 "},"origin/reids-install-deploy.html":{"url":"origin/reids-install-deploy.html","title":"安装部署","keywords":"","body":"Redis的安装部署及客户端 一、Redis单节点的安装部署 1、Docker DockerHub镜像地址：https://hub.docker.com/_/redis/ docker run -d -p 6379:6379 --name redis redis:6.0.9-alpine reids镜像相关信息： 环境变量配置：官方镜像不支持 持久化目录： /data 额外配置文件挂载：-v /myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf 镜像定制： FROM redis:6.0.9-alpine COPY redis.conf /usr/local/etc/redis/redis.conf CMD [ \"redis-server\", \"/usr/local/etc/redis/redis.conf\" ] 2、Kubernetes helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update 3、包管理器安装 Brew brew install redis brew services start redis apt apt install redis-server yum yum install epel-release yum install redis 4、源码安装 redis要求gcc版本高于5.3，CentOS7.4默认版本4.8.5，所以先升级gcc redis要求tcl版本高于8.5，yum install -y tcl yum -y install centos-release-scl yum -y install devtoolset-9-gcc devtoolset-9-gcc-c++ devtoolset-9-binutils scl enable devtoolset-9 bash echo \"source /opt/rh/devtoolset-9/enable\" >> /etc/profile gcc -v source /etc/profile version=6.0.9 wget https://download.redis.io/releases/redis-$version.tar.gz tar xzf redis-$version.tar.gz cd redis-$version make make install # 二进制文件存放在src目录下,例如 source /etc/profile nohup redis-server --protected-mode no >> /var/log/redis-server.log 2>&1 & 二、Redis配置 1、禁用重命名高危命令 [SECURITY] rename-command FLUSHALL \"\" rename-command FLUSHDB \"\" rename-command KEYS \"XXXXX\" rename-command CONFIG \"XXXXX\" # 重启Redis即可 三、Redis Cluster安装部署 四、Redis客户端 1、CLI 2、Application Another Redis Desktop Manager Github：https://github.com/qishibo/AnotherRedisDesktopManager Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-23 17:58:03 "},"origin/redis-persistence.html":{"url":"origin/redis-persistence.html","title":"redis的持久化策略","keywords":"","body":"Redis的持久化策略 一、简介 Redis 是一种内存数据库，将数据保存在内存中，读写效率要比传统的将数据保存在磁盘上的数据库要快很多。但是一旦进程退出，Redis 的数据就会丢失。 为了解决这个问题，Redis 提供了 RDB 和 AOF 两种持久化方案，将内存中的数据保存到磁盘中，避免数据丢失。Redis的所有数据都是保存在内存中，然后不定期的通过异步方式保存到磁盘上(这称为“半持久化模式”)；也可以把每一次数据变化都写入到一个append only file(aof)里面(这称为“全持久化模式”)。 Redis 持久化拥有以下三种方式： RDB(快照方式, Redis DataBase)：将某一个时刻的内存数据，以二进制的方式写入磁盘； AOF(文件追加方式, Append Only File)：记录所有的操作命令，并以文本的形式追加到文件中； 混合持久化方式：Redis 4.0 之后新增的方式，混合持久化是结合了 RDB 和 AOF 的优点，在写入的时候，先把当前的数据以 RDB 的形式写入文件的开头，再将后续的操作命令以 AOF 的格式存入文件，这样既能保证 Redis 重启时的速度，又能减低数据丢失的风险。 二、RDB持久化策略 RDB（Redis DataBase）是将某一个时刻的内存快照（Snapshot），以二进制的方式写入磁盘的过程。 1、触发RDB持久化 RDB 的持久化触发方式有两类：一类是手动触发，另一类是自动触发。 手动触发 手动触发持久化的操作有两个： save 和 bgsave ，主要区别体现在：是否阻塞 Redis 主线程的执行。 save 命令 在客户端中执行 save 命令，就会触发 Redis 的持久化，但同时也是使 Redis 处于阻塞状态，直到 RDB 持久化完成，才会响应其他客户端发来的命令，所以在生产环境一定要慎用。 bgsave 命令（background save，后台保存） 它和 save 命令最大的区别就是 bgsave 会 fork() 一个子进程来执行持久化，整个过程中只有在 fork() 子进程时有短暂的阻塞，当子进程被创建之后，Redis 的主进程就可以响应其他客户端的请求了。 注意：在 Linux 系统中，调用 fork() 时，会创建出一个新进程，称为子进程，子进程会拷贝父进程的 page table。如果进程占用的内存越大，进程的 page table 也会越大，那么 fork 也会占用更多的时间。如果 Redis 占用的内存很大，那么在 fork 子进程时，则会出现明显的停顿现象。 自动触发 save m n save m n 是指在 m 秒内，如果有 n 个键发生改变，则自动触发持久化。 参数 m 和 n 可以在 Redis 的配置文件中找到，例如，save 60 1 则表明在 60 秒内，至少有一个键发生改变，就会触发 RDB 持久化。 自动触发持久化，本质是 Redis 通过判断，如果满足设置的触发条件，自动执行一次 bgsave 命令。 注意：当设置多个 save m n 命令时，满足任意一个条件都会触发持久化。 例如，我们设置了以下两个 save m n 命令： save 60 10 save 600 1 当 60s 内如果有 10 次 Redis 键值发生改变，就会触发持久化；如果 60s 内 Redis 的键值改变次数少于 10 次，那么 Redis 就会判断 600s 内，Redis 的键值是否至少被修改了一次，如果满足则会触发持久化。 flushall flushall 命令用于清空 Redis 数据库，在生产环境下一定慎用，当 Redis 执行了 flushall 命令之后，则会触发自动持久化，把 RDB 文件清空。 主从同步触发 在 Redis 主从复制中，当从节点执行全量复制操作时，主节点会执行 bgsave 命令，并将 RDB 文件发送给从节点，该过程会自动触发 Redis 持久化。 2、配置 # 触发RDB持久化的条件参数 # 当设置多个 save m n 命令时，满足任意一个条件都会触发持久化。 save 60 10 save 600 1 # 当 60s 内如果有 10 次 Redis 键值发生改变，就会触发持久化；如果 60s 内 Redis 的键值改变次数少于 10 次，那么 Redis 就会判断 600s 内，Redis 的键值是否至少被修改了一次，如果满足则会触发持久化。 # bgsave 失败之后，是否停止持久化数据到磁盘，yes 表示停止持久化，no 表示忽略错误继续写文件。 stop-writes-on-bgsave-error yes # 表示是否开起RDB文件压缩，Redis会采用LZF算法进行压缩。如果不想消耗CPU性能来进行文件压缩的话，可以设置为关闭此功能 rdbcompression yes # 表示是否开启RDB文件检查.写入文件和读取文件时是否开启 RDB 文件检查，检查是否有无损坏，如果在启动是检查发现损坏，则停止启动。 rdbchecksum yes # RDB 文件名 dbfilename dump.rdb # RDB 文件目录 dir ./ 3、RDB 文件恢复 当 Redis 服务器启动时，如果Redis数据目录存在 RDB 文件 dump.rdb，Redis 就会自动加载 RDB 文件恢复持久化数据。 如果数据目录没有 dump.rdb 文件，请先将 dump.rdb 文件移动到 Redis 的根目录。 验证 RDB 文件是否被加载 Redis 在启动时有日志信息，会显示是否加载了 RDB 文件 4、RDB持久化优缺点 优点 RDB 的内容为二进制的数据，占用内存更小，更紧凑，更适合做为备份文件； RDB 对灾难恢复非常有用，它是一个紧凑的文件，可以更快的传输到远程服务器进行 Redis 服务恢复； RDB 可以更大程度的提高 Redis 的运行速度，因为每次持久化时 Redis 主进程都会 fork() 一个子进程，进行数据持久化到磁盘，Redis 主进程并不会执行磁盘 I/O 等操作； 与 AOF 格式的文件相比，RDB 文件可以更快的重启。 缺点 因为 RDB 只能保存某个时间间隔的数据，如果中途 Redis 服务被意外终止了，则会丢失一段时间内的 Redis 数据； RDB 需要经常 fork() 才能使用子进程将其持久化在磁盘上。如果数据集很大，fork() 可能很耗时，并且如果数据集很大且 CPU 性能不佳，则可能导致 Redis 停止为客户端服务几毫秒甚至一秒钟。 三、AOF持久化策略 1、简介 AOF（Append Only File）顾名思义可以把 Redis 每个键值对操作都记录到文件（appendonly.aof）中。 使用 RDB 持久化有一个风险，它可能会造成最新数据丢失的风险。因为 RDB 的持久化有一定的时间间隔，在这个时间段内如果 Redis 服务意外终止的话，就会造成最新的数据全部丢失。 可能会操作 Redis 服务意外终止的条件： 安装 Redis 的机器停止运行，蓝屏或者系统崩溃； 安装 Redis 的机器出现电源故障，例如突然断电； 使用 kill -9 Redis_PID 等。 2、AOF 重写流程 AOF 文件重写是生成一个全新的文件，并把当前数据的最少操作命令保存到新文件上，当把所有的数据都保存至新文件之后，Redis 会交换两个文件，并把最新的持久化操作命令追加到新文件上。 3、触发AOF持久化 AOF 持久化开启之后，只要满足一定条件，就会触发 AOF 持久化。触发AOF 持久化条件分为两种：自动触发和手动触发。 自动触发 满足配置文件中 AOF 设置的策略：如果满足Redis配置文件redis.conf中appendfsync设置的条件 always：每条 Redis 操作命令都会写入磁盘，最多丢失一条数据； everysec：每秒钟写入一次磁盘，最多丢失一秒的数据； no：不设置写入磁盘的规则，根据当前操作系统来决定何时写入磁盘，Linux 默认 30s 写入一次数据至磁盘。 满足 AOF 重写触发条件 AOF 是通过记录 Redis 的执行命令来持久化（保存）数据的，所以随着时间的流逝 AOF 文件会越来越多，这样不仅增加了服务器的存储压力，也会造成 Redis 重启速度变慢，为了解决这个问题 Redis 提供了 AOF 重写的功能。而触发 AOF 文件重写，要满足两个条件，这两个条件也是配置在 Redis 配置文件中的，它们分别： auto-aof-rewrite-min-size：允许 AOF 重写的最小文件容量，默认是 64mb 。 auto-aof-rewrite-percentage：AOF 文件重写的大小比例，默认值是 100，表示 100%，也就是只有当前 AOF 文件，比最后一次（上次）的 AOF 文件大一倍时，才会启动 AOF 文件重写。 手动触发 REWRITEAOF：进行 AOF 重写，但是会阻塞主进程，服务器将无法处理客户端发来的命令请求，通常不会直接使用该命令。 BGREWRITEAOF：fork 子进程来进行 AOF 重写，阻塞只会发生在 fork 子进程的时候，之后主进程可以正常处理请求。 4、配置 # 是否开启 AOF，yes 为开启，默认是关闭 appendonly yes # AOF 默认文件名 appendfilename \"appendonly.aof\" # AOF 持久化策略配置 # appendfsync always appendfsync everysec # appendfsync no # AOF 文件重写的大小比例，默认值是 100，表示 100%，也就是只有当前 AOF 文件，比最后一次的 AOF 文件大一倍时，才会启动 AOF 文件重写。 auto-aof-rewrite-percentage 100 # 允许 AOF 重写的最小文件容量 auto-aof-rewrite-min-size 64mb # 是否开启启动时加载 AOF 文件效验，默认值是 yes，表示尽可能的加载 AOF 文件，忽略错误部分信息，并启动 Redis 服务。 # 如果值为 no，则表示，停止启动 Redis，用户必须手动修复 AOF 文件才能正常启动 Redis 服务。 aof-load-truncated yes 5、AOF 后台重写存在的问题 AOF 后台重写使用子进程进行从写，解决了主进程阻塞的问题，但是仍然存在另一个问题：子进程在进行 AOF 重写期间，服务器主进程还需要继续处理命令请求，新的命令可能会对现有的数据库状态进行修改，从而使得当前的数据库状态和重写后的 AOF 文件保存的数据库状态不一致。为了解决上述问题，Redis 引入了 AOF 重写缓冲区（aof_rewrite_buf_blocks），这个缓冲区在服务器创建子进程之后开始使用，当 Redis 服务器执行完一个写命令之后，它会同时将这个写命令追加到 AOF 缓冲区和 AOF 重写缓冲区。这样一来可以保证： 现有 AOF 文件的处理工作会如常进行。这样即使在重写的中途发生停机，现有的 AOF 文件也还是安全的。 从创建子进程开始，也就是 AOF 重写开始，服务器执行的所有写命令会被记录到 AOF 重写缓冲区里面。 当子进程完成 AOF 重写工作后，父进程会在 serverCron 中检测到子进程已经重写结束，则会执行以下工作： 将 AOF 重写缓冲区中的所有内容写入到新 AOF 文件中，这时新 AOF 文件所保存的数据库状态将和服务器当前的数据库状态一致。 对新的 AOF 文件进行改名，原子的覆盖现有的 AOF 文件，完成新旧两个 AOF 文件的替换。 AOF 重写缓冲区内容过多怎么办？将 AOF 重写缓冲区的内容追加到新 AOF 文件的工作是由主进程完成的，所以这一过程会导致主进程无法处理请求，如果内容过多，可能会使得阻塞时间过长，显然是无法接受的。Redis 中已经针对这种情况进行了优化，尽量让 AOF 重写缓冲区的内容更少，以减少主进程阻塞的时间： 在进行 AOF 后台重写时，Redis 会创建一组用于父子进程间通信的管道，同时会新增一个文件事件，该文件事件会将写入 AOF 重写缓冲区的内容通过该管道发送到子进程。 在重写结束后，子进程会通过该管道尽量从父进程读取更多的数据，每次等待可读取事件1ms，如果一直能读取到数据，则这个过程最多执行1000次，也就是1秒。如果连续20次没有读取到数据，则结束这个过程。 6、AOF文件恢复 正常数据恢复 正常情况下，只要开启了 AOF 持久化，并且提供了正常的 appendonly.aof 文件，在 Redis 启动时就会自定加载 AOF 文件并启动。 简单异常数据恢复 在 AOF 写入文件时如果服务器崩溃，或者是 AOF 存储已满的情况下，AOF 的最后一条命令可能被截断，这就是异常的 AOF 文件。 在 AOF 文件异常的情况下，如果为修改 Redis 的配置文件，也就是使用 aof-load-truncated 等于 yes 的配置，Redis 在启动时会忽略最后一条命令，并启动 Redis。 复杂异常数据恢复 AOF 文件可能出现更糟糕的情况，当 AOF 文件不仅被截断，而且中间的命令也被破坏，这个时候再启动 Redis 会提示错误信息并中止运行，错误信息如下： * Reading the remaining AOF tail... # Bad file format reading the append only file: make a backup of your AOF file, then use ./redis-check-aof --fix 出现此类问题的解决方案： 首先使用 AOF 修复工具，检测出现的问题，在命令行中输入 redis-check-aof 命令，它会跳转到出现问题的命令行，这个时候可以尝试手动修复此文件； 如果无法手动修复，我们可以使用 redis-check-aof --fix 自动修复 AOF 异常文件，不过执行此命令，可能会导致异常部分至文件末尾的数据全部被丢弃。 7、AOF持久化优缺点 优点 AOF 持久化保存的数据更加完整，AOF 提供了三种保存策略：每次操作保存、每秒钟保存一次、跟随系统的持久化策略保存，其中每秒保存一次，从数据的安全性和性能两方面考虑是一个不错的选择，也是 AOF 默认的策略，即使发生了意外情况，最多只会丢失 1s 钟的数据； AOF 采用的是命令追加的写入方式，所以不会出现文件损坏的问题，即使由于某些意外原因，导致了最后操作的持久化数据写入了一半，也可以通过 redis-check-aof 工具轻松的修复； AOF 持久化文件，非常容易理解和解析，它是把所有 Redis 键值操作命令，以文件的方式存入了磁盘。即使不小心使用 flushall 命令删除了所有键值信息，只要使用 AOF 文件，删除最后的 flushall 命令，重启 Redis 即可恢复之前误删的数据。 缺点 对于相同的数据集来说，AOF 文件要大于 RDB 文件； 在 Redis 负载比较高的情况下，RDB 比 AOF 性能更好； RDB 使用快照的形式来持久化整个 Redis 数据，而 AOF 只是将每次执行的命令追加到 AOF 文件中，因此从理论上说，RDB 比 AOF 更健壮 四、持久化文件加载规则 如果只开启了 AOF 持久化，Redis 启动时只会加载 AOF 文件（appendonly.aof），进行数据恢复； 如果只开启了 RDB 持久化，Redis 启动时只会加载 RDB 文件（dump.rdb），进行数据恢复； 如果同时开启了 RDB 和 AOF 持久化，Redis 启动时只会加载 AOF 文件（appendonly.aof），进行数据恢复。 在 AOF 开启的情况下，即使 AOF 文件不存在，只有 RDB 文件，也不会加载 RDB 文件。 五、混合持久化策略 1、简介 RDB 和 AOF 持久化各有利弊，RDB 可能会导致一定时间内的数据丢失，而 AOF 由于文件较大则会影响 Redis 的启动速度，为了能同时使用 RDB 和 AOF 各种的优点，Redis 4.0 之后新增了混合持久化的方式。 混合持久化本质是通过 AOF 后台重写（bgrewriteaof 命令）完成的，不同的是当开启混合持久化时，fork 出的子进程先将当前全量数据以 RDB 方式写入新的 AOF 文件，然后再将 AOF 重写缓冲区（aof_rewrite_buf_blocks）的增量命令以 AOF 方式写入到文件，写入完成后通知主进程将新的含有 RDB 格式和 AOF 格式的 AOF 文件替换旧的的 AOF 文件。 2、混合持久化的加载流程 ① 判断是否开启 AOF 持久化，开启继续执行后续流程，未开启执行加载 RDB 文件的流程； ② 判断 appendonly.aof 文件是否存在，文件存在则执行后续流程； ③ 判断 AOF 文件开头是 RDB 的格式, 先加载 RDB 内容再加载剩余的 AOF 内容； ④ 判断 AOF 文件开头不是 RDB 的格式，直接以 AOF 格式加载整个文件。 3、配置 在Redis 配置文件中开起 aof-use-rdb-preamble yes 通过命令行开启 config set aof-use-rdb-preamble yes 4、混合持久化优缺点 优点 混合持久化结合了 RDB 和 AOF 持久化的优点，开头为 RDB 的格式，使得 Redis 可以更快的启动，同时结合 AOF 的优点，有减低了大量数据丢失的风险。 缺点 AOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性变得很差； 兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了。 参考 https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/Redis%20%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%98/03%20Redis%20%E6%8C%81%E4%B9%85%E5%8C%96%E2%80%94%E2%80%94RDB.md https://zhuanlan.zhihu.com/p/340082703 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-12 14:19:08 "},"origin/redis-backup-restore.html":{"url":"origin/redis-backup-restore.html","title":"数据迁移备份恢复","keywords":"","body":"Redis数据的备份恢复迁移 一、Redis-dump Redis-dump Github：https://github.com/delano/redis-dump 1、安装 MacOS brew install ruby gem sources --add https://mirrors.aliyun.com/rubygems/ gem sources --remove https://rubygems.org/ gem sources --list gem install redis-dump -V CentOS yum inatll -y ruby gem sources --add https://mirrors.aliyun.com/rubygems/ gem sources --remove https://rubygems.org/ gem sources --list # gem安装redis需要ruby版本高于2.3.0，CentOS7默认安装的ruby版本为2.0.0，所以先升级Ruby gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB curl -sSL https://get.rvm.io | bash -s stable source /etc/profile.d/rvm.sh rvm -v rvm list known rvm install 2.5 ruby -V gem install redis-dump -V source /etc/profile redis-dump -V 2、redis-dump导出数据到JSON Usage: redis-dump [global options] COMMAND [command options] -u, --uri=S Redis URI (e.g. redis://hostname[:port]) -d, --database=S Redis database (e.g. -d 15) -a, --password=S Redis password (e.g. -a 'my@pass/word') -s, --sleep=S Sleep for S seconds after dumping (for debugging) -c, --count=S Chunk size (default: 10000) -f, --filter=S Filter selected keys (passed directly to redis' KEYS command) -b, --base64 Encode key values as base64 (useful for binary values) -O, --without_optimizations Disable run time optimizations -V, --version Display version -D, --debug --nosafe 示例 redis-dump -u redis://127.0.0.1:6379 -d 0 -c 50000 > redis-backup-$(date \"+%Y%m%d\").json 3、redis-load导入JSON数据文件到Redis redis-load [global options] COMMAND [command options] -u, --uri=S Redis URI (e.g. redis://hostname[:port]) -d, --database=S Redis database (e.g. -d 15) -a, --password=S Redis password (e.g. -a 'my@pass/word') -s, --sleep=S Sleep for S seconds after dumping (for debugging) -b, --base64 Decode key values from base64 (used with redis-dump -b) -n, --no_check_utf8 -V, --version Display version -D, --debug --nosafe 示例 cat redis-backup.json| redis-load -u redis://127.0.0.1:6379 -d 0 注意 相同的Key，值会被覆盖 二、使用redis-cli命令的pipe模式导入 redis在2.6版本推出了一个新的功能pipe mode，即将支持Redis协议的文本文件直接通过pipe导入到服务端。 需要导入的数据格式是redis-cli里可执行的命令。如果想把redis-dump导出的JSON格式数据转换成可导入的数据，参照第三章 seq -f \"SET %g test\" 2 100000 > data.txt cat data.txt | redis-cli -n 1 --pipe 参考： https://redis.io/docs/reference/patterns/bulk-loading/ 三、redis-dump导出数据转换为redis命令格式数据 cat redis-backup-$(date \"+%Y%m%d\").json |jq -r \"\\\"set \\(.key) '\\(.value)' EX \\(.ttl)\\\"\" Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-03-29 18:07:38 "},"origin/keepalived.html":{"url":"origin/keepalived.html","title":"Keepalived","keywords":"","body":"keepalived 一、简介 虚拟路由冗余协议 (Virtual Router Redundancy Protocol，简称VRRP 二、安装配置 安装 包管理器安装 yum/apt install -y keepalived # 配置文件：/etc/keepalived/keepalived.conf # 程序文件：/usr/sbin/keepalived # Unit File：keepalived.service # Unit File的环境配置文件：/etc/sysconfig/keepalived 源码安装 version=2.1.5 curl -x https://www.keepalived.org/software/keepalived-$version.tar.gz | tar -xC /opt cd /opt/keepalived-$version ./configure make make install keepalived --version 配置 /etc/keepalived/keepalived.conf global_defs { router_id Curiouser # 声明虚拟路标识符，一般会写当前主机名称 vrrp_skip_check_adv_addr # 所有报文都检查比较消耗性能，此配置为如果收到的报文和上一个报文是同一个路由器发出的则跳过检查报文中的源地址。 vrrp_iptables #禁用防火墙策略,keepalived默认启动时会自动生成iptables策略，因此我们启用此项就可以不生成iptables的策略。 #严格遵守VRRP协议,即不允许以下三种状况:1.没有VIP地址;2.单播邻居;3.在VRRP版本2中有IPv6地址 #vrrp_strict #由于我下面配置基于单播地址发送消息,因此我这里手动禁用了严格模式，直接注释即可。 vrrp_garp_interval 0 # ARP报文发送延迟时间,0表示不延迟。 vrrp_gna_interval 0 # 消息发送延迟,0表示不延迟。 } vrrp_script check_nginx { script \"/etc/keepalived/keepalived-untils.sh check_ngix\" interval 10 weight -20 } #使用vrrp_instance指令定义一个VIP实例名称,这里定义VIP实例的名称为\"VIP1\",生产环境建议该名称和业务相结合。 vrrp_instance VIP1 { state BACKUP # 指定当前实例默认角色,当前节点在此虚拟路由器上的初始状态，状态为MASTER或者BACKUP #定义工作模式为非抢占模式(即当master重启,VIP会飘移到其它节点,重启完成后并不会将vip抢过来),需要两个keepalived的state的值均为\"BACKUP\",让它们根据个节点的优先级选择对应的master nopreempt interface eth1 # 指定通过哪个本地网卡发送vrrp广播 virtual_router_id 27 # 定义当前虚拟路由器惟一标识,该id的范围是0-255,注意，用一组keepalived的id编号必须要一致 priority 150 # 当前物理节点在此虚拟路由器中的优先级；范围1-254 advert_int 2 # 定义vrrp广播的时间间隔，默认1s unicast_src_ip 192.168.1.11 # 指定单播地址的源地址,需要禁用严格模式\"vrrp_strict\" unicast_peer { 172.30.1.103 # 指定单播地址的对端地址 } authentication { auth_type PASS # 定义认证机制,密码仅支持8位 auth_pass 12345678 } #指定虚拟IP地址,可以指定多个。 virtual_ipaddress { 192.168.1.11 dev eth1 label eth1:0 192.168.1.12 dev eth1 label eth1:1 } track_script { check_nginx } #定义通知脚本,当前节点成为主节点时触发的脚本 notify_master \"/etc/keepalived/notify.sh master\" #定义通知脚本,当前节点转为备节点时触发的脚本 notify_backup \"/etc/keepalived/notify.sh backup\" #定义通知脚本,当前节点转为“失败”状态时触发的脚本 notify_fault \"/etc/keepalived/notify.sh fault\" } 脚本 keepalived-untils.sh #!/bin/sh case $1 in check_ngix ) nginxpid=$(ps -C nginx --no-header|wc -l) #1.判断Nginx是否存活,如果不存活则尝试启动Nginx if [ $nginxpid -eq 0 ];then systemctl start nginx sleep 3 #2.等待3秒后再次获取一次Nginx状态 nginxpid=$(ps -C nginx --no-header|wc -l) #3.再次进行判断, 如Nginx还不存活则停止Keepalived,让地址进行漂移,并退出脚本 if [ $nginxpid -eq 0 ];then systemctl stop keepalived fi fi ;; notify_master ) ;; notify_slave ) ;; notify_fault ) ;; * ) ;; esac Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/lvs.html":{"url":"origin/lvs.html","title":"LVS","keywords":"","body":"LVS 一、简介 简介 工作原理 1、基于NAT的LVS模式负载均衡 NAT（Network Address Translation）即网络地址转换，原理是是通过修改数据报头，让内网IP地址可以访问外网，以及外部用用户可以访问位于内网的IP主机。LVS负载调度器可以使用两块网卡配置不同的IP地址，eth0设置为私有IP与内部网络通过交换设备相互连接，eth1设备为外网IP与外部网络联通。 ​ 第一步，用户访问到负载均衡的外网地址，该LVS外网IP称为Virtual IP Address（VIP），用户通过访问VIP，从而间接连接到后端的真实服务器（Real Server），并且这一切用户都是无感知的，用户以为自己访问的就是真实服务器，但用户不知道被访问的VIP仅仅是一个调度器，这个调度器后端还有服务器。 第二步，用户将请求发送至eth1，此时LVS将根据预设的算法选择后端的一台真实服务器，将数据请求包转发给真实服务器，并且在转发之前LVS会修改数据包中的目标地址以及目标端口，目标地址与目标端口将被修改为选出的真实服务器IP地址以及相应的端口。 第三步，真实的服务器将响应数据包返回给LVS调度器，调度器在得到响应的数据包后会将源地址和源端口修改为VIP及调度器相应的端口，修改完成后，由调度器将响应数据包发送回终端用户，另外，由于LVS调度器有一个连接Hash表，该表中会记录连接请求及转发信息，当同一个连接的下一个数据包发送给调度器时，从该Hash表中可以直接找到之前的连接记录，并根据记录信息选出相同的真实服务器及端口信息。 2、基于TUN的LVS负载均衡 ​ 在LVS（NAT）模式的集群环境中，由于所有的数据请求及响应的数据包都需要经过LVS调度器转发，如果后端服务器的数量大于10台，则调度器就会成为整个集群环境的瓶颈。我们知道，数据请求包往往远小于响应数据包的大小。因为响应数据包中包含有客户需要的具体数据，所以LVS（TUN）的思路就是将请求与响应数据分离，让调度器仅处理数据请求，而让真实服务器响应数据包直接返回给客户端。其中，IP隧道（IP tunning）是一种数据包封装技术，它可以将原始数据包封装并添加新的包头（内容包括新的源地址及端口、目标地址及端口），从而实现将一个目标为调度器的VIP地址的数据包封装，通过隧道转发给后端的真实服务器（Real Server），通过将客户端发往调度器的原始数据包封装，并在其基础上添加新的数据包头（修改目标地址为调度器选择出来的真实服务器的IP地址及对应端口），LVS（TUN）模式要求真实服务器可以直接与外部网络连接，真实服务器在收到请求数据包后直接给客户端主机响应数据。 3、基于DR的LVS负载均衡 在LVS（TUN）模式下，由于需要在LVS调度器与真实服务器之间创建隧道连接，这同样会增加服务器的负担。与LVS（TUN）类似，DR模式也叫直接路由模式，该模式中LVS依然仅承担数据的入站请求以及根据算法选出合理的真实服务器，最终由后端真实服务器负责将响应数据包发送返回给客户端。与隧道模式不同的是，直接路由模式（DR模式）要求调度器与后端服务器必须在同一个局域网内，VIP地址需要在调度器与后端所有的服务器间共享，因为最终的真实服务器给客户端回应数据包时需要设置源IP为VIP地址，目标IP为客户端IP，这样客户端访问的是调度器的VIP地址，回应的源地址也依然是该VIP地址（真实服务器上的VIP），客户端是感觉不到后端服务器存在的。由于多台计算机都设置了同样一个VIP地址，所以在直接路由模式中要求调度器的VIP地址是对外可见的，客户端需要将请求数据包发送到调度器主机，而所有的真实服务器的VIP地址必须配置在Non-ARP的网络设备上，也就是该网络设备并不会向外广播自己的MAC及对应的IP地址，真实服务器的VIP对外界是不可见的，但真实服务器却可以接受目标地址VIP的网络请求，并在回应数据包时将源地址设置为该VIP地址。调度器根据算法在选出真实服务器后，在不修改数据报文的情况下，将数据帧的MAC地址修改为选出的真实服务器的MAC地址，通过交换机将该数据帧发给真实服务器。整个过程中，真实服务器的VIP不需要对外界可见。 IPVS调度算法 针对不同的网络服务需求和服务器配置，IPVS调度器实现了如下八种负载调度算法: 轮叫(Round Robin) 加权轮叫(Weighted Round Robin) 最少链接(Least Connections) 加权最少链接(Weighted Least Connections) 基于局部性的最少链接(Locality-Based Least Connections) 带复制的基于局部性最少链接(Locality-Based Least Connections with Replication) 目标地址散列(Destination Hashing) 源地址散列(Source Hashing) 二、 三、操作 1、安装 yum install ipvsadm 2、前端服务管理 # 添加前端服务 ipvsadm -A -t 192.16.1.16:80 -s rr # 修改前端服务的调度算法 ipvsadm -E -t 192.16.1.16:80 -s wlc # 删除所有前端服务 ipvsadm -C 3、后端服务器管理 # 添加后端服务器 ipvsadm -a -t 192.16.1.16:80 -r 192.16.1.11 -g -w 1 ipvsadm -a -t 192.16.1.16:80 -r 192.16.1.15 -g -w 1 # 删除集群服务中的一个真实服务器 ipvsadm -d -t 192.16.1.16:80 -r 192.16.0.11 # 修改集群服务中的一个真实服务器的权重值 ipvsadm -e -t 192.16.1.16:80 -r 192.16.0.11:80 -m -w 2 4、查看 ipvsadm -Lcn # 显示速率信息 ipvsadm -L --rate # 对虚拟服务器和真是服务器排序输出 ipvsadm -L --sort # 显示统计信息 ipvsadm -L --stats # 显示tcp tcpfin udp 的timeout值 ipvsadm -L --timeout 5、规则管理 # 保存规则到文件 ipvsadm -Sn > /etc/sysconfig/ipvsadm # 加载规则文件 ipvsadm -R 6、SystemD服务管理规则 /usr/lib/systemd/system/ipvsadm.service [Unit] Description=Initialise the Linux Virtual Server After=syslog.target network.target [Service] Type=oneshot ExecStart=/bin/bash -c \"exec /sbin/ipvsadm-restore /etc/sysconfig/ipvsadm\" #停止ipvsadm服务时(执行systemctl stop ipvsadm时)，自动将规则写入/etc/sysconfig/ipvsadm中 ExecStop=/sbin/ipvsadm -C RemainAfterExit=yes [Install] WantedBy=multi-user.target. systemctl daemon-reload && systemctl start ipvsadm 7、ipvs管理 # 清空转发请求计数器 ipvsadm -Z 参考 https://blog.csdn.net/weixin_34406061/article/details/92974711 https://blog.csdn.net/yangyijun1990/article/details/109145347 https://blog.csdn.net/weixin_49172531/article/details/116144786 https://cloud.tencent.com/developer/article/2054205 https://juejin.cn/post/7169552758167568392 https://blog.csdn.net/lupengfei1009/article/details/86514445 https://blog.csdn.net/weixin_34406061/article/details/92974711 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-11-30 14:57:46 "},"origin/正反向代理服务的区别.html":{"url":"origin/正反向代理服务的区别.html","title":"代理服务器","keywords":"","body":"正反向代理服务的区别 一、代理的作用 首先要明确代理服务器的作用: 代理：可以直白地理解为：代理服务器是一种代替谁去访问什么的服务器。代理客户端浏览器去访问客户端浏览器访问不了的服务，代理应用服务器负载均衡地对外提供服务。可以根据代替谁来划分为\"正向代理\"和\"反向代理\" 缓存加速：缓存那些不经常变动的资源，加速访问。 鉴权过滤记录：允许那些认证过的客户端去访问指定的资源或服务，还可以记录下访问记录。 二、正向代理 正向代理（forward proxy）是指代替内部网络的客户端，去访问Internet或其他网络上的服务，并将访问的结果返还给客户端，同时将结果缓存下来，加速访问。 正向代理还可以按客户端是否感知分为透明代理与传统代理。 常见的正向代理服务软件有: Squid Varnish Nginx 三、反向代理 反向代理（Reverse Proxy）方式是指以代理服务器来接受Internet上的连接请求，然后将请求转发给内部网络上的服务器；并将从服务器上得到的结果返回给Internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。 常见的反向代理服务软件有: Nginx Apache HAProxy 未完代待整理更新 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/transparent_proxy.html":{"url":"origin/transparent_proxy.html","title":"透明代理的应用","keywords":"","body":"透明代理 { \"routing\": {...}, \"inbounds\": [ { ... }, { \"port\": 12345, //开放的端口号 \"protocol\": \"dokodemo-door\", \"settings\": { \"network\": \"tcp,udp\", \"followRedirect\": true // 这里要为 true 才能接受来自 iptables 的流量 }, \"sniffing\": { \"enabled\": true, \"destOverride\": [\"http\", \"tls\"] } } ], \"outbounds\": [ { ... \"streamSettings\": { ... \"sockopt\": { \"mark\": 255 //这里是 SO_MARK，用于 iptables 识别，每个 outbound 都要配置；255可以改成其他数值，但要与下面的 iptables 规则对应；如果有多个 outbound，最好将所有 outbound 的 SO_MARK 都设置成一样的数值 } } } ... ] } 设定 TCP 透明代理的 iptables 规则，命令如下(#代表注释)： # 新建一个名为 V2RAY 的链 iptables -t nat -N V2RAY # 直连 192.168.0.0/16 iptables -t nat -A V2RAY -d 192.168.0.0/16 -j RETURN # 直连 SO_MARK 为 0xff 的流量(0xff 是 16 进制数，数值上等同与上面配置的 255)，此规则目的是避免代理本机(网关)流量出现回环问题 iptables -t nat -A V2RAY -p tcp -j RETURN -m mark --mark 0xff # 其余流量转发到 12345 端口（即 V2Ray） iptables -t nat -A V2RAY -p tcp -j REDIRECT --to-ports 12345 # 对局域网其他设备进行透明代理 iptables -t nat -A PREROUTING -p tcp -j V2RAY # 对本机进行透明代理 iptables -t nat -A OUTPUT -p tcp -j V2RAY # 查看链表规则 iptables -L V2RAY -t nat 然后设定 UDP 流量透明代理的 iptables 规则，命令如下 ip rule add fwmark 1 table 100 ip route add local 0.0.0.0/0 dev lo table 100 iptables -t mangle -N V2RAY_MASK iptables -t mangle -A V2RAY_MASK -d 192.168.0.0/16 -j RETURN iptables -t mangle -A V2RAY_MASK -p udp -j TPROXY --on-port 12345 --tproxy-mark 1 iptables -t mangle -A PREROUTING -p udp -j V2RAY_MASK 将 iptables(REDIRECT/TPROXY) 流量转换为 socks5(tcp/udp) 流量 https://github.com/zfl9/ipt2socks?tab=readme-ov-file https://github.com/zfl9/ss-tproxy https://gist.github.com/zfl9/d52482118f38ce2c16195583dffc44d2 https://blog.indigo.codes/2021/11/20/iptables-tproxy-and-home/ https://guide.v2fly.org/app/transparent_proxy.html#%E4%BC%98%E7%82%B9 https://guide.v2fly.org/app/tproxy.html#%E8%AE%BE%E7%BD%AE%E7%BD%91%E5%85%B3 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-04-19 11:41:36 "},"origin/常见正向代理服务软件之间的区别.html":{"url":"origin/常见正向代理服务软件之间的区别.html","title":"正向代理","keywords":"","body":"常见正向代理服务软件的对比区别(挖坑) https://www.zhihu.com/search?type=content&q=%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E6%A6%82%E5%BF%B5 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/squid-简介安装.html":{"url":"origin/squid-简介安装.html","title":"简介安装日志","keywords":"","body":"正向代理服务Squid 一、简介 Squid是一个高性能的正向代理缓存服务器，支持FTP、gopher、HTTPS、HTTP等协议，主要提供了缓存加速、应用层过滤的功能。和一般的代理缓存软件不同，Squid用一个单独的、非模块化的、I/O驱动的进程来处理所有的客户端请求。 squid代理服务器的工作机制： 代理服务器（Proxy Server）是个人网络和Internet服务商之间的中间代理机构，负责转发合法的网络信息，对转发进行控制和登记。其最基本的功能就是连接，此外还包括安全性、缓存，内容过滤，访问控制管理等功能。当客户机通过代理请求Web页面时，执行的代理服务器会先检查自己的缓存，当缓存中有客户机需要访问的页面，则直接将缓存服务器中的页面内容反馈给客户机；如果缓存中没有客户机需要访问的页面，则由代理服务器想Internet发送访问请求，当获得返回的Web页面以后，将页面数据保存到缓存中并发送给客户机。 由于客户机的web访问请求实际上代理服务器来代替完成的，所以隐藏了用户的真实IP地址，从而起到一定的保护作用。 Squid可以基于访问控制列表（ACL）和访问权限列表（ARL）执行内容过滤与权限管理功能，还可以基于多种条件禁止用户访问存在威胁或不适宜的网站资源。 根据实现的方式不同，正向代理模式可以分为： 传统代理：也就是普通的代理服务，需要我们客户端在浏览器、聊天工具等一些程序中设置代理服务器的地址和端口，然后才能使用代理来访问网络，这种方式相比较而言比较麻烦，因为客户机还需手动指定代理服务器，所以一般用于Internet环境。 透明代理：与传统代理实现的功能是一样的，区别在于客户机不需要手动指定代理服务器的地址和端口，而是通过默认路由、防火墙策略将web访问重定向，实际上仍然交给代理服务器来处理，重定向的过程完全是由squid服务器进行的，所以对于客户机来说，甚至不知道自己使用了squid代理服务，因此呢，我们称之为透明模式。透明代理多用于局域网环境，如在Linux网关中启用透明代理后，局域网主机无须进行额外设置就能享受更好的上网速度。 二、安装 YUM yum install squid -y; \\ systemctl enable squid; \\ systemctl start squid 三、传统代理服务配置 配置文件/etc/squid/squid.conf acl localnet src 10.0.0.0/8 # RFC1918 possible internal network acl localnet src 172.16.0.0/12 # RFC1918 possible internal network acl localnet src 192.168.0.0/16 # RFC1918 possible internal network acl localnet src fc00::/7 # RFC 4193 local private network range acl localnet src fe80::/10 # RFC 4291 link-local (directly plugged) machines acl SSL_ports port 443 acl Safe_ports port 80 # http acl Safe_ports port 21 # ftp acl Safe_ports port 443 # https acl Safe_ports port 70 # gopher acl Safe_ports port 210 # wais acl Safe_ports port 1025-65535 # unregistered ports acl Safe_ports port 280 # http-mgmt acl Safe_ports port 488 # gss-http acl Safe_ports port 591 # filemaker acl Safe_ports port 777 # multiling http acl CONNECT method CONNECT http_access allow Safe_ports http_access deny CONNECT !SSL_ports http_access allow localhost manager http_access deny manager http_access allow localnet http_access allow localhost http_access deny all http_port 3128 # Squid代理服务监听端口 cache_dir ufs /data/squid 100 16 256 coredump_dir /data/squid refresh_pattern ^ftp: 1440 20% 10080 refresh_pattern ^gopher: 1440 0% 1440 refresh_pattern -i (/cgi-bin/|\\?) 0 0% 0 refresh_pattern . 0 20% 4320 四、常用命令 1、启动等命令 squid reload #不重启服务，生效配置 squid –z #初始化缓存空间 初始化你在 squid.conf 里配置的 cache 目录,只需要第一次的时候执行就可以了 squid -k parse #验证squid.conf的语法和配置 squid -N -d1 #在前台启动squid，并输出启动过程 squid -s #后台运行squid。 squid -k shutdown #停止 squid squid -k reconfigure #载入新的配置文件 squid -k rotate #轮循日志 2、squid命中率分析 # 获取squid运行状态信息 squidclient -p 3128 mgr:info squidclient -p 3128 mgr:5min # 可以看到详细的性能情况,其中PORT是你的proxy的端口，5min可以是60min #获取squid内存使用情况 squidclient -p 3128 mgr:mem #获取squid已经缓存的列表 squidclient -p 3128 mgr:objects use it carefully,it may crash #获取squid的磁盘使用情况 squidclient -p 3128 mgr:diskd #强制更新某个url squidclient -p 3128 -m PURGE http://www.xxx.com/xxx.php #更多的请查看 squidclient -h 或者 squidclient -p 3128 mgr: #查命中率： squidclient -h(具体侦听IP) -p80(具体侦听端口) mgr:info 3、定期清除swap.state内无效数据 当squid应用运行了一段时间以后，cache_dir对应的swap.state文件就会变得越来越大，里面的无效接口数据越来越多，这可能影响squid的响应时间，因此需要使用rotate命令来使squid清理swap.state里面的无效数据，减少swap.state的大小 squid -k rotate -f /path/to/squid/conf_file #添加定时清理任务 vi /etc/crontab 0 0 * * * root /usr/local/sbin/squid -k rotate -f /usr/local/etc/squid/squid1.conf 4、统计客户端个数 netstat -lanp|grep 3128|grep \"ESTABLISHED\"|awk '{print $5}'|awk -F':' '{print $1}'|sort -u|wc -l 5、统计客户端的连接总数 netstat -lanp|grep 3128|grep \"ESTABLISHED\"|wc -l 6、显示传输数据大于指定大小的访问 tailf /var/log/squid/access.log | awk '{if($5>1000)print}'|awk '{print $3 \" \" $5 \" \" $7}' 五、日志默认输出格式 squid日志配置项是在/etc/squid/squid.conf中配置的，默认日志输出文件路径/var/log/squid/access.log 默认的日志输出格式 #1:时间戳 2:响应时间 3:客户端IP 4:结果/状态码 5:传输大小 6:请求方式 7:客户端请求的URL 8:客户端身份 9:对端编码/对端主机 10:内容类型 1531077064.951 81 10.248.2.67 TCP_MISS/200 6277 GET http://bbs.talkop.com/forum.php? - HIER_DIRECT/180.76.184.69 text/xml 时间戳（%tl %ts）: 请求完成时间，以 Unix 时间来记录的（UTC 1970-01-01 00:00:00 开始的时间）它是毫秒级的。squid使用这种格式而不是人工可读的时间格式，是为了简化某些日志处理程序的工作 响应时间（%6tr）: 对HTTP响应来说，该域表明squid花了多少时间来处理请求。在squid接收到HTTP请求时开始计时，在响应完全送出后计时终止。响应时间是毫秒级的。尽管时间值是毫秒级的，但是精度可能是10毫秒。在squid负载繁重时，计时变得没那么精确 客户端地址（%>a）: 该域包含客户端的IP地址，或者是主机名 结果/状态码（%Ss/%03Hs）: 该域包含2个 token，以斜杠分隔。第一个token叫结果码，它把协议和响应结果（例如TCPHIT或UDP_DENIED）进行归类。这些是squid专有的编码，以TCP开头的编码指HTTP请求，以UDP_开头的编码指ICP查询。第2个token是HTTP响应状态码（例如200,304,404等）。状态码通常来自原始服务器。在某些情形下，squid可能有义务自己选择状态码 传输size（%: 该域指明传给客户端的字节数。严格的讲，它是squid告诉TCP/IP协议栈去发送给客户端的字节数。这就是说，它不包括TCP/IP头部的overhead。也请注意，传输size正常来说大于响应的Content-Length。传输size包括了HTTP响应头部，然而Content- Length不包括 请求方式（%rm）: 该域包含请求方式 URI（%ru）: 该域包含来自客户端请求的URI。大多数记录下来的URI实际是URL（例如，它们有主机名）。在记日志时，squid删掉了在第一个问号(?)之后的所有URI字符，除非禁用了strip_query_terms指令 客户端身份: 无 对端编码/对端主机: 对端信息包含了2个token，以斜杠分隔。它仅仅与cache 不命中的请求有关。第一个token指示如何选择下一跳，第二个token是下一跳的地址。当squid发送一个请求到邻居cache时，对端主机地址是邻居的主机名。假如请求是直接送到原始服务器的，则squid会写成原始服务器的IP地址或主机名–假如禁用了log_ip_on_direct。NONE/-这个值指明squid不转发该请求到任何其他服务器 内容类型（%mt）: 原始access.log的默认的最后一个域，是HTTP响应的内容类型。 squid从响应的Content-Type头部获取内容类型值。假如该头部丢失了，squid使用一个横杠(-)代替 假如激活了 log_mime_hdrs 指令，squid在每行追加2个附加的域： HTTP请求头部: Squid 编码HTTP请求头部，并且在一对方括号之间打印它们。方括号是必须的，因为squid不编码空格字符。编码方案稍许奇怪。回车（ASCII 13）和换行（ASCII 10）分别打印成\\r和\\n。其他不可打印的字符以RFC 1738风格来编码，例如Tab（ASCII 9）变成了%09。 HTTP响应头部: Squid编码HTTP响应头部，并且在一对方括号之间打印它们。注意这些是发往客户端的头部，可能不同于从原始服务器接收到的头部。 参考链接 http://www.squid-cache.org/ https://blog.51cto.com/10693404/2149207 https://blog.51cto.com/14154700/2406060 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-07-05 13:29:02 "},"origin/squid-acl访问权限控制.html":{"url":"origin/squid-acl访问权限控制.html","title":"ACL访问权限","keywords":"","body":"一、ACL概念 Squid提供了强大的代理控制机制，通过合理设置ACL（Access Control List，访问控制列表）并进行限制，可以针对源地址、目标地址、访问的URL路径、访问的时间等各种条件进行过滤。 ACL访问控制的步骤： 使用acl配置项定义需要控制的条件 通过http_access配置项对已定义的列表做“允许”或“拒绝”访问的控制 二、ACL用法概述 1、定义ACL访问列表 定义格式： acl aclname acltype string1… #acl 列表名称 列表类型 列表内容 ... acl aclname acltype \"File_Path\"… #acl 列表名称 列表类型 \"文件路径\" ... #当使用文件时，该文件的格式为每行包含一个条目。 常用的ACL列表类型: src：指明源地址 acl aclname src ip-address/netmask ... 客户ip地址 acl aclname src addr1-addr2/netmask ... 地址范围 dst：指明目标地址，即客户请求的服务器的IP地址。语法为： acl aclname dst ip-address/netmask ... srcdomain：指明客户所属的域，Squid将根据客户IP反向查询DNS。语法为： acl aclname srcdomain foo.com ... dstdomain：指明请求服务器所属的域，由客户请求的URL决定。语法为： acl aclname dstdomain foo.com ... 此处需要注意的是：如果用户使用服务器IP而非完整的域名时，Squid将进行反向的DNS解析来确定其完整域名，如果失败，就记录为“none”。 time：指明访问时间。语法如下： acl aclname time [day-abbrevs] [h1:m1-h2:m2][hh:mm-hh:mm] 日期的缩写指代关系如下： S：指代Sunday M：指代Monday T：指代Tuesday W：指代Wednesday H：指代Thursday F：指代Friday A：指代Saturday 另外，h1：m1必须小于h2：m2，表达式为[hh：mm-hh：mm]。 port：指定访问端 acl aclname port 80 70 21 ... acl aclname port 0-1024 ... 指定一个端口范围 method：指定请求方法。比如： acl aclname method GET POST ... url_regex：URL规则表达式匹配，语法为： acl aclname url_regex[-i] pattern urlpath_regex：URL-path规则表达式匹配，略去协议和主机名。其语法为： acl aclname urlpath_regex[-i] pattern Notes： acltype可以是任一个在ACL中定义的名称。 任何两个ACL元素不能用相同的名字。 每个ACL由列表值组成。当进行匹配检测的时候，多个值由逻辑或运算连接；换句话说，任一ACL元素的值被匹配，则这个ACL元素即被匹配。 并不是所有的ACL元素都能使用访问列表中的全部类型。 不同的ACL元素写在不同行中，Squid将这些元素组合在一个列表中。 2、http_access访问控制列表使用访问控制 根据访问控制列表允许或禁止某一类用户访问。如果某个访问没有相符合的项目，则默认为应用最后一条项目的“非”。比如最后一条为允许，则默认就是禁止。通常应该把最后的条目设为“deny all”或“allow all”来避免安全性隐患。使用该访问控制列表要注意如下问题： 这些规则按照它们的排列顺序进行匹配检测，一旦检测到匹配的规则，匹配检测就立即结束。 访问列表可以由多条规则组成。 如果没有任何规则与访问请求匹配，默认动作将与列表中最后一条规则对应。 一个访问条目中的所有元素将用逻辑与运算连接（如下所示）： http_access Action声明1 AND 声明2 AND 多个http_access声明间用或运算连接，但每个访问条目的元素间用与运算连接。 列表中的规则总是遵循由上而下的顺序。 三、ACL示例 允许网段10.0.0.124/24以及192.168.10.15/24内的所有客户机访问代理服务器，并且允许在文件/etc/squid/guest列出的客户机访问代理服务器，除此之外的客户机将拒绝访问本地代理服务器： acl clients src 10.0.0.124/24 192.168.10.15/24 acl guests src “/etc/squid/guest” acl all src 0.0.0.0/0.0.0.0 http_access allow clients http_access allow guests http_access deny all 其中，文件“/etc/squid/guest”中的内容为： 172.168.10.3/24 210.113.24.8/16 10.0.1.24/25 允许域名为job.net、gdfq.edu.cn的两个域访问本地代理服务器，其他的域都将拒绝访问本地代理服务器： acl permitted_domain src job.net gdfq.edu.cn acl all src 0.0.0.0/0.0.0.0 http_access allow permitted_domain http_access deny all 使用正则表达式，拒绝客户机通过代理服务器访问包含有诸如“sexy”等关键字的网站： acl deny_url url_regex -i sexy http_access deny deny_url 拒绝客户机通过代理服务器访问文件中指定IP或者域名的网站，其中文件/etc/squid/ deny_ip中存放有拒绝访问的IP地址，文件/etc/squid/deny_dns中存放有拒绝访问的域名： acl deny_ip dst “etc/squid/deny_ip” acl deny_dns dst “etc/squid/deny_dns” http_access deny deny_ip http_access deny deny_dns 允许和拒绝指定的用户访问指定的网站，其中，允许客户1访问网站http://www.sina.com.cn，而拒绝客户2访问网站http://www.163.com： acl client1 src 192.168.0.118 acl client1_url url_regex ^http://www.sina.com.cn acl client2 src 192.168.0.119 acl client2_url url_regex ^http://www.163.com http_access allow client1 client1_url http_access deny client2 client2_url 允许所有的用户在规定的时间内（周一至周四的8：30到20：30）访问代理服务器，只允许特定的用户（系统管理员，其网段为：192.168.10.0/24）在周五下午访问代理服务器，其他的在周五下午一点至六点一律拒绝访问代理服务器： acl allclient src 0.0.0.0/0.0.0.0 acl administrator 192.168.10.0/24 acl common_time time MTWH 8:30-20:30 acl manage_time time F 13:00-18:00 http_access allow allclient common_time http_access allow administrator manage_time http_access deny manage_time Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/nginx-install-setup.html":{"url":"origin/nginx-install-setup.html","title":"Nginx安装配置","keywords":"","body":"Nginx安装配置及基础 一、简介 官网：http://nginx.org/ nginx是一款自由的、开源的、高性能的HTTP服务器和反向代理服务器；同时也是一个IMAP、POP3、SMTP代理服务器；nginx可以作为一个HTTP服务器进行网站的发布处理，另外nginx可以作为反向代理进行负载均衡的实现。 反向代理 负载均衡 负载均衡调度算法 weight轮询（默认）：接收到的请求按照顺序逐一分配到不同的后端服务器，即使在使用过程中，某一台后端服务器宕机，nginx会自动将该服务器剔除出队列，请求受理情况不会受到任何影响。 这种方式下，可以给不同的后端服务器设置一个权重值（weight），用于调整不同的服务器上请求的分配率；权重数据越大，被分配到请求的几率越大；该权重值，主要是针对实际工作环境中不同的后端服务器硬件配置进行调整的。 ip_hash：每个请求按照发起客户端的ip的hash结果进行匹配，这样的算法下一个固定ip地址的客户端总会访问到同一个后端服务器，这也在一定程度上解决了集群部署环境下session共享的问题。 fair：智能调整调度算法，动态的根据后端服务器的请求处理到响应的时间进行均衡分配，响应时间短处理效率高的服务器分配到请求的概率高，响应时间长处理效率低的服务器分配到的请求少；结合了前两者的优点的一种调度算法。但是需要注意的是nginx默认不支持fair算法，如果要使用这种调度算法，请安装upstream_fair模块 url_hash：按照访问的url的hash结果分配请求，每个请求的url会指向后端固定的某个服务器，可以在nginx作为静态服务器的情况下提高缓存效率。同样要注意nginx默认不支持这种调度算法，要使用的话需要安装nginx的hash软件包 二、进制安装 1、DEB(apt) release_name=$(lsb_release -c | awk '{print $2}') echo -e \"deb https://nginx.org/packages/ubuntu/ $release_name nginx\" >> /etc/apt/sources.list.d/nginx.list echo -e \"deb-src https://nginx.org/packages/ubuntu/ $release_name nginx\" >> /etc/apt/sources.list.d/nginx.list curl -o /tmp/nginx_signing.key https://nginx.org/keys/nginx_signing.key gpg --dry-run --quiet --import --import-options import-show /tmp/nginx_signing.key mv /tmp/nginx_signing.key /etc/apt/trusted.gpg.d/nginx_signing.asc apt update apt install nginx nginx -V 2、RPM(yum) 以RPM方式安装的配置文件在/etc/nginx/目录下 二进制安装自带的模块 二进制安装(例如YUM)的nginx不支持动态的安装和新加载模块的，新增模块需要重新编译安装了nginx #To set up the yum repository for RHEL/CentOS, create the file named /etc/yum.repos.d/nginx.repo with the following contents: [nginx] name=nginx repo baseurl=http://nginx.org/packages/OS/OSRELEASE/$basearch/ gpgcheck=0 enabled=1 #Replace “OS” with “rhel” or “centos”, depending on the distribution used, and “OSRELEASE” with “6” or “7”, for 6.x or 7.x versions, respectively. $ bash -c 'cat > /etc/yum.repos.d/nginx.repo 三、源码编译 1、安装编译工具 yum install -y gcc gc++ perl gcc-c++ 2、安装编译必备库 PCRE ：required by NGINX Core and Rewrite modules and provides support for regular expressions pcre是一个正则库，nginx使用正则进行重写要用到，必须安装 pcre库有两个版本：pcre、pcre2(新版的库)。推荐下载pcre，pcre2是编译是通不过的。 编译pcre就必须用到c++编译器，使用pcre2就使用gcc编译器。 # yum安装 $ rpm -qa pcre pcre-devel $ yum install pcre pcre-devel # 源码编译安装 $ version=8.43 && \\ wget ftp://ftp.pcre.org/pub/pcre/pcre-$version.tar.gz && \\ tar -zxf pcre-$version.tar.gz&& \\ cd pcre-$version && \\ ./configure && \\ make && \\ make install zlib： required by NGINX Gzip module for headers compression: # yum安装 $ rpm -qa zlib zlib-devel $ yum install zlib zlib-devel # 源码编译安装 $ version=1.2.11 && \\ wget http://zlib.net/zlib-$version.tar.gz && \\ tar -zxf zlib-$version.tar.gz && \\ cd zlib-$version && \\ ./configure && \\ make && \\ make install OpenSSL：required by NGINX SSL modules to support the HTTPS protocol OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及 SSL 协议，并提供丰富的应用程序供测试或其它目的使用。nginx 不仅支持 http 协议，还支持 https（即在ssl协议上传输http），所以需要在 Centos 安装 OpenSSL 库 # yum安装 $ rpm -qa openssl openssl-devel $ yum install openssl openssl-devel # 源码编译安装 $ version=3.0.5 && \\ wget http://www.openssl.org/source/openssl-$version.tar.gz && \\ tar -zxf openssl-$version.tar.gz && \\ cd openssl-$version && \\ ./config && \\ make && \\ make install # 如果是在MacOS下源码编译，配置时手动指定OS平台 ./Configure darwin64-x86_64-cc && \\ make && \\ sudo make install 3、下载解压源码包 version=1.18.0 && \\ mkdir nginx-source && \\ cd nginx-source && \\ curl -s -# https://www.openssl.org/source/openssl-1.1.1.tar.gz | tar zxvf - -C ./ && \\ curl -s -# https://nginx.org/download/nginx-$version.tar.gz | tar zxvf - -C ./ && \\ cd nginx-$version 4、查看Nginx默认开起的模块 cat ./auto/options | grep YES 5、编译常用模块 模块 详解 模块编译参数 依赖 auth模块 基于用户名密码的访问控制 --with-http_auth_request_module 访问限制模块 连接频率限制 默认集成 请求频率模块 请求频率限制 默认集成 状态监控模块 nginx状态监控 access模块 基于ip的访问控制 gzip模块 gunzip模块 6、配置编译参数 创建nginx用户----->创建相关目录------>配置编译参数 编译参数文档：http://nginx.org/en/docs/configure.html $ ./configure --help #查看编译配置参数 --with开头的，默认是禁用的(没启动的，想使用的话需要在编译的时候加上) --without开头的，默认是启用的(不想启用此模块时，可以在编译的时候加上这个参数) # --help print this message # --prefix=PATH 指定安装目录 # --sbin-path=PATH 指定二进制执行程序文件存放位置。 # --modules-path=PATH 指定第三方模块的存放路径 # --conf-path=PATH 指定配置文件nginx.conf存放位置 # --error-log-path=PATH 指定错误日志存放位置 # --pid-path=PATH 指定nginx.pid文件存放位置 # --lock-path=PATH 指定nginx.lock文件存放位置 # --user=USER 指定程序运行时的非特权用户 # --group=GROUP 指定程序运行时的非特权用户组 # --build=NAME set build name # --builddir=DIR 指定编译目录 # --with-select_module enable select module # --without-select_module disable select module # --with-poll_module enable poll module # --without-poll_module disable poll module # --with-threads enable thread pool support # --with-file-aio enable file AIO support # --with-http_ssl_module enable ngx_http_ssl_module # --with-http_v2_module enable ngx_http_v2_module # --with-http_realip_module enable ngx_http_realip_module # --with-http_addition_module enable ngx_http_addition_module # --with-http_xslt_module enable ngx_http_xslt_module # --with-http_xslt_module=dynamic enable dynamic ngx_http_xslt_module # --with-http_image_filter_module enable ngx_http_image_filter_module # --with-http_image_filter_module=dynamic enable dynamic ngx_http_image_filter_module # --with-http_geoip_module enable ngx_http_geoip_module # --with-http_geoip_module=dynamic enable dynamic ngx_http_geoip_module # --with-http_sub_module enable ngx_http_sub_module # --with-http_dav_module enable ngx_http_dav_module # --with-http_flv_module enable ngx_http_flv_module # --with-http_mp4_module enable ngx_http_mp4_module # --with-http_gunzip_module enable ngx_http_gunzip_module # --with-http_gzip_static_module enable ngx_http_gzip_static_module # --with-http_auth_request_module enable ngx_http_auth_request_module # --with-http_random_index_module enable ngx_http_random_index_module # --with-http_secure_link_module enable ngx_http_secure_link_module # --with-http_degradation_module enable ngx_http_degradation_module # --with-http_slice_module enable ngx_http_slice_module # --with-http_stub_status_module enable ngx_http_stub_status_module # --without-http_charset_module disable ngx_http_charset_module # --without-http_gzip_module disable ngx_http_gzip_module # --without-http_ssi_module disable ngx_http_ssi_module # --without-http_userid_module disable ngx_http_userid_module # --without-http_access_module disable ngx_http_access_module # --without-http_auth_basic_module disable ngx_http_auth_basic_module # --without-http_autoindex_module disable ngx_http_autoindex_module # --without-http_geo_module disable ngx_http_geo_module # --without-http_map_module disable ngx_http_map_module # --without-http_split_clients_module disable ngx_http_split_clients_module # --without-http_referer_module disable ngx_http_referer_module # --without-http_rewrite_module disable ngx_http_rewrite_module # --without-http_proxy_module disable ngx_http_proxy_module # --without-http_fastcgi_module disable ngx_http_fastcgi_module # --without-http_uwsgi_module disable ngx_http_uwsgi_module # --without-http_scgi_module disable ngx_http_scgi_module # --without-http_memcached_module disable ngx_http_memcached_module # --without-http_limit_conn_module disable ngx_http_limit_conn_module # --without-http_limit_req_module disable ngx_http_limit_req_module # --without-http_empty_gif_module disable ngx_http_empty_gif_module # --without-http_browser_module disable ngx_http_browser_module # --without-http_upstream_hash_module disable ngx_http_upstream_hash_module # --without-http_upstream_ip_hash_module disable ngx_http_upstream_ip_hash_module # --without-http_upstream_least_conn_module disable ngx_http_upstream_least_conn_module # --without-http_upstream_keepalive_module disable ngx_http_upstream_keepalive_module # --without-http_upstream_zone_module disable ngx_http_upstream_zone_module # --with-http_perl_module enable ngx_http_perl_module # --with-http_perl_module=dynamic enable dynamic ngx_http_perl_module # --with-perl_modules_path=PATH set Perl modules path # --with-perl=PATH set perl binary pathname # --http-log-path=PATH set http access log pathname # --http-client-body-temp-path=PATH set path to store http client request body temporary files # --http-proxy-temp-path=PATH set path to store http proxy temporary files # --http-fastcgi-temp-path=PATH set path to store http fastcgi temporary files # --http-uwsgi-temp-path=PATH set path to store http uwsgi temporary files # --http-scgi-temp-path=PATH set path to store http scgi temporary files # --without-http disable HTTP server # --without-http-cache disable HTTP cache # --with-mail enable POP3/IMAP4/SMTP proxy module # --with-mail=dynamic enable dynamic POP3/IMAP4/SMTP proxy module # --with-mail_ssl_module enable ngx_mail_ssl_module # --without-mail_pop3_module disable ngx_mail_pop3_module # --without-mail_imap_module disable ngx_mail_imap_module # --without-mail_smtp_module disable ngx_mail_smtp_module # --with-stream enable TCP/UDP proxy module # --with-stream=dynamic enable dynamic TCP/UDP proxy module # --with-stream_ssl_module enable ngx_stream_ssl_module # --with-stream_realip_module enable ngx_stream_realip_module # --with-stream_geoip_module enable ngx_stream_geoip_module # --with-stream_geoip_module=dynamic enable dynamic ngx_stream_geoip_module # --with-stream_ssl_preread_module enable ngx_stream_ssl_preread_module # --without-stream_limit_conn_module disable ngx_stream_limit_conn_module # --without-stream_access_module disable ngx_stream_access_module # --without-stream_geo_module disable ngx_stream_geo_module # --without-stream_map_module disable ngx_stream_map_module # --without-stream_split_clients_module disable ngx_stream_split_clients_module # --without-stream_return_module disable ngx_stream_return_module # --without-stream_upstream_hash_module disable ngx_stream_upstream_hash_module # --without-stream_upstream_least_conn_module disable ngx_stream_upstream_least_conn_module # --without-stream_upstream_zone_module disable ngx_stream_upstream_zone_module # --with-google_perftools_module enable ngx_google_perftools_module # --with-cpp_test_module enable ngx_cpp_test_module # --add-module=PATH enable external module # --add-dynamic-module=PATH enable dynamic external module # --with-compat dynamic modules compatibility # --with-cc=PATH set C compiler pathname # --with-cpp=PATH set C preprocessor pathname # --with-cc-opt=OPTIONS set additional C compiler options # --with-ld-opt=OPTIONS set additional linker options # --with-cpu-opt=CPU build for the specified CPU, valid values:pentium, pentiumpro, pentium3, pentium4,athlon, opteron, sparc32, sparc64, ppc64 # --without-pcre disable PCRE library usage # --with-pcre force PCRE library usage # --with-pcre=DIR 设置pcre源码目录路径 # --with-pcre-opt=OPTIONS set additional build options for PCRE # --with-pcre-jit build PCRE with JIT compilation support # --with-zlib=DIR set path to zlib library sources # --with-zlib-opt=OPTIONS set additional build options for zlib # --with-zlib-asm=CPU use zlib assembler sources optimized for the specified CPU, valid values:pentium, pentiumpro # --with-libatomic force libatomic_ops library usage # --with-libatomic=DIR set path to libatomic_ops library sources # --with-openssl=DIR set path to OpenSSL library sources # --with-openssl-opt=OPTIONS set additional build options for OpenSSL # --with-debug enable debug logging $ groupadd nginx && \\ useradd nginx -s /sbin/nologin -M -g nginx && \\ mkdir -p /opt/nginx-1.17.6/logs 7、编译安装 # make命令将源代码编译为二进制文件 $ make # 根据配置阶段指定的路径和功能将软件以特定的方式安装到指定位置 $ make install 8、设置环境变量 ln -s /opt/nginx-1.17.6/nginx /usr/bin/nginx 9、启动 手动控制Nginx的生命周期 $ nginx -t #启动测试 $ nginx #启动 托管给Systemd $ bash -c 'cat > /usr/lib/systemd/system/nginx.service 10、验证 # 查看监听的端口 $ lsof -i :80 $ netstat -lanp |grep 80 # 使用命令行工具访问页面 $ curl 127.0.0.1 $ wget 127.0.0.1 # 查看进程 $ ps -ef | grep nginx # root 2564 1 0 23:21 ? 00:00:00 nginx: master process /opt/nginx-1.17.6/nginx # nginx 2565 2564 0 23:21 ? 00:00:00 nginx: worker process 四、Nginx目录结构 编译安装的目录结构 #由于编译时指定了相关路径 $ tree /opt/nginx-1.17.6 /opt/nginx-1.17.6 ├── 3party_module ├── client_body_temp ├── fastcgi.conf ├── fastcgi.conf.default ├── fastcgi_params ├── fastcgi_params.default ├── fastcgi_temp ├── html # 站点目录 │ ├── 50x.html # 错误页 │ └── index.html # 首页 ├── koi-utf ├── koi-win ├── logs # 日志目录 │ ├── access.log # nginx访问日志 │ └── error.log # Nginx的错误日志 ├── mime.types # 媒体类型 ├── mime.types.default ├── nginx # Nginx的二进制启动命令脚本 ├── nginx.conf # Nginx的主要配置文件 ├── nginx.conf.default ├── nginx.pid # Nginx所有的进程号文件 ├── nginx-rtmp-module ├── proxy_temp # 临时目录 ├── scgi_params ├── scgi_params.default ├── scgi_temp ├── uwsgi_params ├── uwsgi_params.default ├── uwsgi_temp └── win-utf 五、命令行参数 $ nginx -s signal # stop — fast shutdown # quit — graceful shutdown # reload — 重新加载配置文件 # reopen — reopening the log files Nginx重新加载配置文件的过程：主进程接受到加载信号后： 1、首先会校验配置的语法，然后生效新的配置， 2、如果成功，则主进程会启动新的工作进程，同时发送终止信号给旧的工作进程。 3、否则主进程回退配置，继续工作。 在第二步，旧的工作进程收到终止信号后，会停止接收新的连接请求，知道所有现有的请求处理完，然后退出。 $ nginx -t # 检查配置文件语法是否错误，并尝试启动 $ nginx -q # suppress non-error messages during configuration testing. $ nginx -T # same as -t, but additionally dump configuration files to standard output (1.9.2). $ nginx #启动Nginx $ nginx -v #查看nginx的版本 $ nginx -V #查看nginx的版本，编译器版本，编译时的参数等 $ nginx -p prefix # set nginx path prefix, i.e. a directory that will keep server files (default value is /usr/local/nginx). $ nginx -c file # 指定配置文件（不使用默认路径下的配置文件） $ nginx -? | -h # print help for command-line parameters. $ nginx -g directives # set global configuration directives, for example, #nginx -g \"pid /var/run/nginx.pid; worker_processes `sysctl -n hw.ncpu`;\" 六、配置文件结构 全局配置：用来设置影响Nginx服务器整体运行的配置，作用于全局。（从文件开始到events块的内容） 作用：通常包括服务器的用户组，允许生成的worker process、Nginx进程PID的存放路径、日志的存放路径和类型以及配置文件引入等 事件配置：涉及的指令主要影响Nginx服务器和用户的网络连接。 作用：常用到的设置包括是否开启多worker process下的网络连接进行序列化，是否允许同时接收多个网络连接，选择何种时间驱动模型处理连接请求，每个worker process可以同时支持的最大连接数等 模块配置 HTTP模块 HTTP模块的全局配置 虚拟主机的配置 # Nginx全局配置 user nobody; # 指定Nginx的worker进程运行用户以及用户组，默认由nobody nobody运行 worker_processes 1; # 指定Nginx要开启的进程数，默认为1 error_log logs/error.log # 全局错误日志文件路径。日志级别：debug/info/notice/warn/error/crit pid logs/nginx.pid; # 指定进程PID文件的路径 # 事件配置，设定nginx的工作模式及连接数上限 events { accept_mutex on; # 设置网路连接序列化，防止惊群现象发生，默认为on multi_accept on; # 设置一个进程是否同时接受多个网络连接，默认为off use epoll; # 指定nginx的工作模式。支持的工作模式有select ,poll,kqueue,epoll,rtsig,/dev/poll # epoll是多路复用IO(I/O Multiplexing)中的一种方式， # select和poll都是标准的工作模式，kqueue和epoll是高效的工作模式， # 对于linux系统，epoll是首选。 worker_connections 1024; # 设置nginx每个进程最大的连接数，默认是1024，所以nginx最大的连接数 # max_client=worker_processes * worker_connections。 # 进程最大连接数受到系统最大打开文件数的限制，需要设置ulimit。 } # HTTP模块配置 http { include mime.types; # 配置处理前端请求的MIME类型 default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log logs/access.log main; sendfile on; # 开启高效文件传输模式（zero copy 方式），避免内核缓冲区数据和用户缓冲区数据之间的拷贝。sendfile 参数和I/O有关，当上传文件时，内核首先缓冲数据，然后将数据发送到应用应用程序缓冲区。 应用程序反过来将数据发送到目的地。Sendfile方法是一种改进的数据传输方法，其中数据在操作系统内核空间内的文件描述符之间复制，而不将数据传输到应用程序缓冲区。由于没有了用户态和内核态之间的切换，也没有内核缓冲区和用户缓冲区之间的拷贝，大大提升了传输性能。 tcp_nopush on; keepalive_timeout 65; # 设置客户端连接超时时间 gzip on; # 设置是否开启gzip模块 # 虚拟主机 server { listen 80; # 虚拟主机的服务端口 server_name localhost; server_tokens off; # 隐藏响应头中的有关操作系统和Nginx服务器版本号的信息，保障安全性 charset koi8-r; access_log logs/host.access.log main; location / { root html; index index.html index.htm; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } #server { # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / { # root html; # index index.html index.htm; # } #} } 七、nginx内置变量 变量 含义 $args / $query_string 请求中的参数，两个指令等同，表示HTTP请求的查询字符串列表 $binary_remote_addr 二进制格式的客户端地址，四个字节。 $body_bytes_sent 发送给客户端的消息体字节数 $content_length HTTP请求信息里的\"Content-Length\" $content_type 请求信息里的\"Content-Type\" $document_root 针对当前请求的根路径设置值 $document_uri 与$uri相同 $host 请求信息中的\"Host\"，如果请求中没有Host行，则等于设置的服务器名; $http_cookie cookie 信息，HTTP请求中cookie列表信息 $http_referer 来源地址 $http_user_agent 客户端代理信息，HTTP请求中“userAgent”信息 $http_via 最后一个访问服务器的Ip地址 $http_x_forwarded_for 相当于网络访问路径。 $limit_rate 对连接速率的限制 $remote_addr 客户端地址 $remote_port 客户端端口号 $remote_user 客户端用户名，认证用 $request 用户请求信息 $request_body 用户请求主体 $request_body_file 发往后端的本地文件名称 $request_filename 当前请求的文件路径名 $request_method 请求的方法，比如\"GET\"、\"POST\"等 $request_uri 请求的URI，带参数 $server_addr 服务器地址，如果没有用listen指明服务器地址，使用这个变量将发起一次系统调用以取得地址(造成资源浪费) $server_name 请求到达的服务器名 $server_port 请求到达的服务器端口号 $server_protocol 请求的协议版本，\"HTTP/1.0\"或\"HTTP/1.1\" $uri 请求的URI，可能和最初的值有不同，比如经过重定向之类的 八、location匹配规则 nginx的location指令是配置的核心，用于匹配client请求uri的path部分，然后对不同的请求提供不同的静态内容，或者通过反向代理重定向到内部的server。 对于client的request， nginx会进行预处理，nginx首先对采用 ’%XX’(uri采用%+十六进制格式用于在浏览器和插件中显示非标准的字母和字符) 格式文本编码的uri进行解码。然后处理path中的相对路径符号’.’和‘..’，然后对于含有两个及以上的’/’压缩成一个’/’。 这样处理完后会得到一个干净的path，然后会用这个path会在server的location指令的参数进行匹配。 location匹配的过程 首先nginx会把request的uri在location正常字符串参数中匹配出符合的最长字符串，并保存这个结果； 如果最长前缀匹配结果前面有 ”^~”修饰符，那么停止继续搜索； 如果最长匹配结果前有”=”修饰符，也会停止继续搜索； 接下来，去匹配参数为正则表达式的所有location，根据location的配置顺序，在匹配到第一个正则表达式时，即停止搜索其他的正则表达式； 如果有多个location指令块匹配到，nginx的选择策略是the longest prefix最长前缀匹配原则。 location 大致可以分为三类： 精准匹配：location = / {} 一般匹配：location / {} 正则匹配：location ~ / {} location 语法： location 参数修饰符/变量/无参数修饰符 uri { 指令 } location [ = | ~ | ~* | ^~ ] uri { .. } ——————>'=' location @name { .. } location 常用的匹配规则 | 字符 | 涵义 | | ---- | ------------------------------------------------------------ | | = | 进行普通字符精确匹配，也就是完全匹配 | | ^~ | 表示普通字符匹配。使用前缀匹配。如果匹配成功，则不再匹配其它 location | | ~ | 区分大小写的匹配 | | ~ | 不区分大小写的匹配 | | !~ | 区分大小写的匹配取非 | | !~ | 不区分大小写的匹配取非 | 官网的匹配示例： location = / { [ configuration A ] } location / { [ configuration B ] } location /documents/ { [ configuration C ] } location ^~ /images/ { [ configuration D ] } location ~* \\.(gif|jpg|jpeg)$ { [ configuration E ] } 当请求”/”时，匹配到A 当请求”/index.html”，会匹配到B； 当请求”/documens/document.html”，会匹配到C； 请求”/images/1.gif”，会匹配到D；匹配流程：首先会匹配到D，由于D的location的参数含有修饰符”^~”，当匹配到D后，不会再搜索参数为正则表达式的location； 当请求”/documents/1.jpg”，会匹配到E；匹配流程：首先会匹配到C，此时会保存C的匹配结果，然后继续搜索参数为正则的location，结果发现E匹配上了，那么会丢弃之前匹配到的C 九、Nginx添加模块并不停服升级 不管Nginx是用YUM二进制还是源码编译方式安装的，后续如果有新需求是现有Nginx模块无法满足，需要添加新模块才能完成的情况时，都是要对Nginx进行重新编译安装，然后不停服，不能影响现有的业务地平滑升级 （该操作有风险，需在开发环境测试通过再在生产环境进行操作） ① 查看现有的nginx编译参数 nginx -V # 或者 /opt/nginx1.17.6/nginx -V ② 备份旧版本的nginx可执行文件 期间nginx不会停止服务 mv /opt/nginx1.17.6/nginx /opt/nginx1.17.6/nginx.bak ③ 安装编译必备组件 ④ 下载相同版本的nginx源码包 ⑤ 下载第三方模块 ⑥ 配置编译参数 要加上原有的编译参数 ⑦ 编译新的Nginx 只make, 不要make install，不然会覆盖原来已安装的nginx ⑧ 替换Nginx文件 ⑨ 修改新配置文件， 并检查配置文件语法** ⑩ 新配置的平滑升级 $ kill -USR2 旧Nginx主进程号或进程文件路径 # 此时旧的Nginx主进程将会把自己的进程文件改名为.oldbin，然后执行新版Nginx。新旧Nginx会同时运行，共同处理请求。 这时要逐步停止旧版 Nginx $ kill -WINCH 旧Nginx主进程号 # 慢慢旧Nginx进程就都会随着任务执行完毕而退出，新的Nginx进程会逐渐取代旧进程。 十、常见问题 启动Nginx时报“nginx: [emerg] getpwnam(\"nginx\") failed” 原因：nginx用户没有创建成功 浏览器，curl、wget等访问不了nginx页面 ​ 原因：可能是没有关闭SELinux和防火墙 ，检查一下 访问资源403的问题排查 通过yum安装的nginx一切正常，但是访问时报403，于是查看nginx日志，路径为/var/log/nginx/error.log。打开日志发现报错Permission denied，详细报错如下： open() \"/data/www/1.txt\" failed (13: Permission denied), client: 192.168.1.194, server: www.web1.com, request: \"GET /1.txt HTTP/1.1\", host: \"www.web1.com\" 原因： 由于启动用户和nginx工作用户不一致所致 查看nginx的启动用户，发现是nobody，而为是用root启动的 ps aux | grep \"nginx: worker process\" | awk'{print $1}' 解决方案：将nginx.config中的user改为和启动用户一致 配置文件中指定的文件 例如配置文件中index index.html index.htm这行中的指定的文件。 server { listen 80; server_name localhost; index index.php index.html; root /data/www/; } 如果在/data/www/下面没有index.php,index.html的时候，直接访问文件会报403 forbidden 解决方案：创建一下相应的文件 权限问题，如果nginx启动用户没有web目录的操作权限，也会出现403错误。 解决办法：修改web目录的读写权限，或者是把nginx的启动用户改成目录的所属用户，重启Nginx即可解决 SELinux设置为开启状态（enabled）的原因。 getenforce # Enforcing 为开启状态 解决方案： # 临时关闭Selinux setenforce 0 参考 https://juejin.cn/post/7166943522531049486 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-09 14:29:13 "},"origin/nginx-config.html":{"url":"origin/nginx-config.html","title":"Nginx常用配置及功能","keywords":"","body":"Nginx非常规配置 一、设置访问日志JSON格式化 1、配置 # ....全局配置省略..... ; http { # ....HTTP模块其他配置省略..... ; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; # 设置日志格式 log_format json_log [escape=default|json|none] '{ \"@timestamp\": \"$time_iso8601\", ' '\"app\": \"$app\", ' '\"remote_addr\": \"$remote_addr\", ' '\"referer\": \"$http_referer\", ' '\"request\": \"$request\", ' '\"status\": $status, ' '\"bytes\": $body_bytes_sent, ' '\"agent\": \"$http_user_agent\", ' '\"x_forwarded\": \"$http_x_forwarded_for\", ' '\"up_addr\": \"$upstream_addr\",' '\"up_host\": \"$upstream_http_host\",' '\"up_resp_time\": $upstream_response_time,' '\"request_time\": $request_time,' '\"server_name\": \"$server_name\"' ' }'; server { # .....虚拟主机其他配置省略..... set $app test ; # 设置变量app的值为“test” access_log logs/host.access.log json_log; # 设置访问日志按照“json_log”的格式进行输出 } } 2、escape参数 [escape=default|json|none] default：会把“或\\转换为\\xXX json：会把“转换为\\“，会把\\转换为\\\\“ 参考：http://nginx.org/en/docs/http/ngx_http_log_module.html#log_format 3、日志格式可用变量 变量 含义 $remote_addr 记录客户端IP地址 $remote_user 记录客户端用户名 $time_local 记录通用的本地时间 $time_iso8601 记录ISO8601标准格式下的本地时间 $request 记录请求的方法以及请求的http协议 $status 记录请求状态码(用于定位错误信息) $body_bytes_sent 发送给客户端的资源字节数，不包括响应头的大小 $bytes_sent 发送给客户端的总字节数 $msec 日志写入时间。单位为秒，精度是毫秒。 $http_referer 记录从哪个页面链接访问过来的 $http_user_agent 记录客户端浏览器相关信息 $http_x_forwarded_for 记录客户端IP地址 $request_length 请求的长度（包括请求行，请求头和请求正文）。 $request_time 请求花费的时间，单位为秒，精度毫秒 4、输出的JSON格式日志 { \"@timestamp\": \"2020-03-09T17:54:49+08:00\", \"app\": \"test\", \"remote_addr\": \"127.0.0.1\", \"referer\": \"-\", \"request\": \"GET /Empty-2C4G80G.ovf HTTP/1.1\", \"status\": 200, \"bytes\": 7532, \"agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36\", \"x_forwarded\": \"33.100.12.1, 100.7.88.12\", \"up_addr\": \"-\", \"up_host\": \"-\", \"up_resp_time\": 0.096, \"request_time\": 0.095, \"server_name\": \"\" } 二、按天保留日志文件 server{ ... if ($time_iso8601 ~ '(\\d{4}-\\d{2}-\\d{2})') { set $tttt $1; } access_log logs/nginx-access-$tttt.log main; ... } 三、配置HTTP基础认证 Nginx 使用 ngx_http_auth_basic_module 模块支持 HTTP基本身份验证 功能 。 1、安装httpd-tools yum install -y httpd-tools apt install -y apache2-utils 2、创建授权用户和密码 htpasswd -c -d /etc/nginx/basic-auth-pass-file test_user htpasswd 其他操作参考：htpasswd操作 3、配置nginx server { # ...省略 auth_basic \"登录认证\"; auth_basic_user_file basic-auth-pass-file; # ...省略 # 或者只设置某些URL进行登录认证 # location /api { # auth_basic \"登录认证\"; # auth_basic_user_file basic-auth-pass-file; #} } 4、使用 # 浏览器中使用 直接在浏览器中输入地址, 会弹出用户密码输入框, 输入即可访问 # wget wget --http-user=test_user --http-passwd=*** http://**** # curl curl -u test_user:**** -O http://**** 四、自定义日志格式引用自定义Header 自定日志格式时如果想获取请求中的自定义Header，nginx引用方式为$http_headername，如果header命名中包含-，需要转换成_。例如下面示例中引用开发自定义的x-zz-app-info http { log_format json_log '{ \"@timestamp\": \"$time_iso8601\", ' '\"app\": \"$app\", ' '\"remote_addr\": \"$remote_addr\", ' '\"referer\": \"$http_referer\", ' '\"request\": \"$request\", ' '\"status\": $status, ' '\"bytes\": $body_bytes_sent, ' '\"agent\": \"$http_user_agent\", ' '\"x_forwarded\": \"$http_x_forwarded_for\", ' '\"up_addr\": \"$upstream_addr\",' '\"up_host\": \"$upstream_http_host\",' '\"up_resp_time\": $upstream_response_time,' '\"request_time\": $request_time,' '\"server_name\": \"$server_name\",' '\"x-zz-app-info\": \"$http_x_zz_app_info\"' ' }'; server { listen 80; server_name localhost; set $app test; access_log /var/log/nginx/host.access.log json_log; location / { root /usr/share/nginx/html; index index.html index.htm; } } 五、禁用iframe嵌入使用 在Nginx中配置添加X-Frame-Options 响应头 在http配置里设置X-Frame-Options pid /var/run/nginx.pid; http{ include /etc/nginx/mime.types; default_type application/octet-stream; .... add_header X-Frame-Options SAMEORIGIN; .... } 在server配置里设置X-Frame-Options server{ listen 443; .... add_header X-Frame-Options SAMEORIGIN; ... } X-Frame-Options 响应头： X-Frame-Options HTTP 响应头是用来给浏览器指示允许一个页面可否在 , 或者 中展现的标记。网站可以使用此功能，来确保自己网站的内容没有被嵌到别人的网站中去，也从而避免了点击劫持 (clickjacking) 的攻击。 X-Frame-Options 有三个值: DENY：表示该页面不允许在 frame 中展示，即便是在相同域名的页面中嵌套也不允许。 SAMEORIGIN：表示该页面可以在相同域名页面的 frame 中展示。 ALLOW-FROM uri：表示该页面可以在指定来源的 frame 中展示。 参考：https://cloud.tencent.com/developer/article/1182220 六、镜像流量 1、简介 nginx 1.13.4及后续版本内置了ngx_http_mirror_module模块，提供流量镜像复制的功能。 支持流量放大，做法为：配置多份相同镜像。 相比tcp-copy的优势：无需录制流量，实时可用；配置相当简单。 源站请求，直接原路返回；正常配置下，mirror请求不影响源站请求及响应，源站将流量复制到mirror站后，两者不再有任何交集。 使用ngx_http_mirror_module模块实现流量拷贝。例如将生产环境的流量拷贝复制到预上线环境或测试环境 可以验证功能是否正常，以及服务的性能； 用真实有效的流量请求去验证，又不用造数据，不影响线上正常访问； 相比于灰度发布，镜像流量不会影响真实流量； 可以用来排查线上问题； 重构，假如服务做了重构，这也是一种测试方式 2、镜像请求复制配置 server { listen 80; error_log logs/nginx-error.log; access_log logs/nginx-access.log main; location / { root /var/www/html; index index.html index.htm; autoindex on; # 开起复制请求体 mirror_request_body on; # 指定镜像uri为/mirror mirror /mirror; } location /mirror { # internal标志该location只为内部的重定向服务， 外部的调用请求则返回404 internal; # 指定是否镜像请求body部分，此选项与proxy_request_buffering、fastcgi_request_buffering、scgi_request_buffering和 uwsgi_request_buffering冲突，一旦开启mirror_request_body为on，则请求自动缓存; proxy_pass_request_body on; proxy_set_header Host $host; # 设置镜像流量的头部 proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 原始uri不会镜像，可以通过$request_uri变量取得原始请求的uri proxy_set_header X-Original-URI $request_uri; proxy_pass http://127.0.0.1:8080$request_uri; } } server { listen 8080; error_log logs/nginx-error-mirror.log; access_log logs/nginx-access-mirror.log main; location / { root /var/www/html; index index.html index.htm; autoindex on; } } 3、镜像请求放大复制配置 server{ ... mirror /mirror; # 多加一份mirror，流量放大一倍 mirror /mirror; ... } 参考： http://nginx.org/en/docs/http/ngx_http_mirror_module.html https://cloud.tencent.com/developer/article/1751894 https://segmentfault.com/a/1190000039870391 https://www.it610.com/article/1306183699749965824.htm https://blog.csdn.net/zfw_666666/article/details/124660479 七、限流 Nginx 提供两种限流方式，一是控制请求速率，二是控制并发连接数。 1、限制连接频率 ①简介 nginx的ngx_http_limit_conn_module 模块提供了限制连接数的能力。主要是利用limit_conn_zone和limit_conn两个指令来限制单个IP的请求数。并非所有的连接都被计数，只有在服务器处理了请求并且已经读取了整个请求头时，连接才被计数。 ②指令配置 limit_conn_zone：limit_conn_zone 【key】 【zone=name:size】 key：定义限流对象，key可以为文本、变量或者二者的组合来指定要根据什么对象进行标识来达到限流。 通常key为$binary_remote_addr，$binary_remote_addr是$remote_addr（客户端IP）的二进制格式，固定占用4个字节。即限定每个客户端IP的请求处理速率。binary_ 开头的目的是压缩内存占用量 $ remote_addr变量的大小可以从7到15个字节不等。存储的状态在32位平台上占用32或64字节的内存，在64位平台上总是占用64字节。对于IPv4地址，$ binary_remote_addr变量的大小始终为4个字节，对于IPv6地址则为16个字节。存储状态在32位平台上始终占用32或64个字节，在64位平台上占用64个字节。一个兆字节的区域可以保持大约32000个32字节的状态或大约16000个64字节的状态。如果区域存储耗尽，服务器会将错误返回给所有其他请求。 zone：指定zone的名称和大小，用来存储访问的频次信息。 name:10m表示一个大小为10M，名字为name的内存区域 1M能存储16000 IP地址的访问信息，10M可以存储16W IP地址访问信息 limit_conn：limit_conn 【zone】 【number】 zone：指定limit_conn_zone定义的zone名称，与上面limit_conn_zone 里的name对应。 number：指定最多持有的连接个数 limit_conn_status：limit_conn_status 【code】 code： 指定超过limit的请求的HTTP返回状态码，默认值为503 limit_conn_log_level指令格式： limit_req_log_level info | notice | warn | error; http{ ... # 表示限制根据用户的IP地址来显示，设置存储地址为的内存大小10M limit_conn_zone $binary_remote_addr zone=perip:10m ; limit_conn_zone $server_name zone=perserver:10m ; ... } server{ ... location / { # 表示同一个IP地址的客户端最多最多持有20个连接。 limit_conn perip 20; # 表示虚拟主机(server) 能同时处理100个并发连接总数。 limit_conn perserver 100; } ... } 2、限制请求频率 ①简介 nginx的limit_req_module模块可以控制速率限流，在一定的时间内只处理特定数量的请求。 nginx的limit_req_module模块针对每个key限定请求处理的速率（rate），比较典型的是，按照客户端IP，限定此IP下请求的处理速率。 nginx是采用漏桶算法实现控制速率限流。漏桶(Leaky Bucket)算法思路很简单,水(请求)先进入到漏桶里,漏桶以一定的速度出水(接口有响应速率),当水流入速度过大会直接溢出(访问频率超过接口响应速率),然后就拒绝请求,可以看出漏桶算法能强行限制数据的传输速率. ②指令配置 limit_req_zone：limit_req_zone【key】 【zone=name:size】【rate=rate】 key：定义限流对象，key可以为文本、变量或者二者的组合来指定要根据什么对象进行标识来达到限流。 通常key为$binary_remote_addr，$binary_remote_addr是$remote_addr（客户端IP）的二进制格式，固定占用4个字节。即限定每个客户端IP的请求处理速率。binary_ 开头的目的是压缩内存占用量。 $ remote_addr变量的大小可以从7到15个字节不等。存储的状态在32位平台上占用32或64字节的内存，在64位平台上总是占用64字节。对于IPv4地址，$ binary_remote_addr变量的大小始终为4个字节，对于IPv6地址则为16个字节。存储状态在32位平台上始终占用32或64个字节，在64位平台上占用64个字节。一个兆字节的区域可以保持大约32000个32字节的状态或大约16000个64字节的状态。如果区域存储耗尽，服务器会将错误返回给所有其他请求。 zone：指定zone的名称和大小，用来存储访问的频次信息。 name:10m表示一个大小为10M，名字为name的内存区域 1M能存储16000 IP地址的访问信息，10M可以存储16W IP地址访问信息 rate：用于设置最大访问速率，表示允许相同标识的客户端的访问频次，单位是每秒几次，r/s。 rate=2r/s 表示每秒最多处理2个请求。 Nginx 实际上以毫秒为粒度来跟踪请求信息，因此 2r/s 实际上是限制：每100毫秒处理两个请求。这意味着，自上一个请求处理完后，若后续100毫秒内又有请求到达，将拒绝处理该请求。 limit_req： limit_req【zone=name】【burst=number】【nodelay】 zone=name： 指定limit_req_zone定义的zone名称，与上面limit_req_zone 里的name对应。 burst=number：指定burst的容量大小，默认为0。 burst译为突发、爆发，表示在超过设定的处理速率后能额外处理的请求数。即允许指定个数的请求进行等待处理，对于后续过量的请求，将会被拒绝并返回503错误。如果burst=20 ，若同时有21个请求到达，Nginx 会处理第一个请求，剩余20个请求将放入队列，然后每隔100ms从队列中获取一个请求进行处理。若请求数大于21，将拒绝处理多余的请求，直接返回503. nodelay：如果设置，会在瞬时提供处理(burst + rate)个请求的能力，请求超过（burst + rate）的时候就会直接返回503，永远不存在请求需要等待的情况。如果没有设置，则所有请求会依次等待排队。 nodelay参数允许请求在排队的时候就立即被处理，也就是说只要请求能够进入burst队列，就会立即被后台worker处理，请注意，这意味着burst设置了nodelay时，系统瞬间的QPS可能会超过rate设置的阈值。nodelay参数要跟burst一起使用才有作用。 limit_req_log_level： limit_req_log_level info | notice | warn | error; limit_req_status：limit_req_status code; http{ ... limit_req_zone $binary_remote_addr zone=myReqLimit:10m rate=5r/s; ... } server{ ... location / { limit_req zone=myReqLimit; # 如果有时正常流量突然增大，超出的请求将被拒绝，无法处理突发流量，可以结合 burst 参数使用来解决该问题 limit_req zone=myReqLimit burst=4 nodelay; # burst=4 nodelay 表示平均每秒允许不超过2个请求，突发不超过4个请求，并且处理突发4个请求的时候，没有延迟，等到完成之后，按照正常的速率处理。 } ... } 参考 https://nginx.org/en/docs/http/ngx_http_limit_req_module.html https://nginx.org/en/docs/http/ngx_http_limit_conn_module.html https://www.cnblogs.com/biglittleant/p/8979915.html https://www.cnblogs.com/daijiting/p/14909190.html https://blog.csdn.net/lt326030434/article/details/122989403 https://segmentfault.com/a/1190000022530919 八、gzip压缩功能 Nginx支持对指定类型的文件进行压缩然后再传输给客户端，而且压缩还可以设置压缩比例，压缩后的文件大小将比源文件显著变小，这样有助于降低出口带宽的利用率 Nginx对文件的压缩功能是依赖于模块 ngx_http_gzip_module,默认是内置模块。 配置适用上下文：http，server，location # 启用或禁用gzip压缩，默认关闭。 gzip on | off; # 压缩比由低到高从1到9，默认为1 gzip_comp_level level; # 禁用IE6 gzip功能 gzip_disable \"MSIE [1-6]\\.\"; # gzip压缩的最小文件，小于设置值的文件将不会压缩 gzip_min_length 1k; # 启用压缩功能时，协议的最小版本，默认HTTP/1.1 gzip_http_version 1.0 | 1.1; # 指定Nginx服务需要向服务器申请的缓存空间的个数和大小,平台不同,默认:32 4k或者16 8k; gzip_buffers number size; # 指明仅对哪些类型的资源执行压缩操作;默认为gzip_types text/html，不用显示指定，否则出错。当gzip开启时，只对指定的“MIME”类型的响应使用gzip，但无论如何“text/html;”类型一定会压缩。 gzip_types mime-type ...; # 默认值为“off”；如果nginx开启了“gzip”、“gunzip”、“gzip_static”，且在当前请求中应用，那么是否在响应的中添加\"Vary:Accept-Encoding\" header；这个header可以帮助我们（以及浏览器）判定请求是否使用gzip。 gzip_vary on | off; # 预压缩，即直接从磁盘找到对应文件的gz后缀的式的压缩文件返回给用户，无需消耗服务器CPU # 注意: 来自于ngx_http_gzip_static_module模块 gzip_static on | off; 九、大文件传输配置 配置项 含义 client_max_body_size 设置请求体允许的最大体积，默认1m。若超过所设定的大小，返回413错误。 client_header_timeout 等待客户端发送一个请求头的超时时间，若超过所设定的大小，返回408错误。 client_body_timeout 设置读取请求体的超时时间，若超过所设定的大小，返回413错误。 proxy_connect_timeout http请求无法立即被容器(tomcat, netty等)处理，被放在nginx的待处理池中等待被处理。此参数为等待的最长时间，默认为60秒，官方推荐最长不要超过75秒。 proxy_read_timeout http请求被容器(tomcat, netty等)处理后，nginx会等待处理结果，也就是容器返回的response。即为服务器响应时间，Nginx等待的最长时间。默认60秒。 proxy_send_timeout 设置后端向Nginx返回响应时的超时时间，默认60秒。 参考： https://blog.csdn.net/weixin_38306434/article/details/86491905 https://zhuanlan.zhihu.com/p/82548284 十、设置IP黑白名单 1、简介 Nginx的ngx_http_access_module模块可以根据IP地址进行放行或限制。默认加载此模块。核心指令有allow/deny allow/deny指令格式：allow|deny address|CIDR|all; address: deny 1.2.3.4; # 屏蔽单个ip访问 allow 1.2.3.4; # 允许单个ip访问 CIDR: deny 1.0.0.0/8; # 屏蔽整个段即从1.0.0.1到1.255.255.254的IP段 deny 1.2.0.0/16; # 屏蔽IP段即从1.2.0.1到1.2.255.254的IP段 deny 1.2.3.0/24; # 屏蔽即从1.2.3.1到1.2.3.254的IP段 all： deny all; # 屏蔽所有ip访问 allow all; # 允许所有ip访问 access/deny配置规则生效顺序：依次检查规则，直到找到第一个匹配项。类似于iptables规则 access/deny指令配置范围 在http范围的黑白单中，访问该服务器的所有服务都要被黑名单过滤。 在server范围黑白名单中，只有访问该http服务器的当前server服务时，才会被黑名单过滤。 在location范围黑白名单中，针对当前转发才会被黑名单过滤。 2、配置 ①白名单模式 server{ location / { allow 123.13.123.12; allow 23.53.32.1/100; deny all; } } ②黑名单模式 server{ location / { # 黑名单设置，deny后面接限制的IP，为什么不加allow all? 因为这个默认是开启的 deny 123.13.123.12; } } ③读取黑白IP地址配置文件 IP地址配置文件如果有变更，需要reload重新加载配置 黑白IP地址配置文件中的规则与指令配置的规则匹配顺序是先匹配配置文件中 server{ location / { # 默认位置路径为当前server指令配置文件路径下 include whitelist.txt ; deny all ; } } # 文件/etc/nginx/conf.d/whitelist.txt内容如下： # allow 192.168.1.123; # allow 192.168.2.123; server{ location / { # 默认位置路径为当前server指令配置文件路径下 include blockip.txt ; # 默认是allow all; } } # 文件/etc/nginx/conf.d/blockip.txt内容如下： # deny 192.168.1.123; # deny 192.168.2.123; 参考 https://nginx.org/en/docs/http/ngx_http_access_module.html https://www.jb51.net/article/255998.htm 十一、屏蔽私密文件 location ~ (.git|.gitattributes|.gitignore|.svn) { deny all; } 十二、跳转WWW server { listen 80; # 配置正常的带www的域名 server_name www.test.curiouser.com; root /var/www/html/test; location / { try_files $uri $uri/ /index.html =404; } } server { # 这个要放到下面， # 将不带www的 test.curiouser.com 永久性重定向到 https://www.test.curiouser.com server_name test.curiouser.com; rewrite ^(.*) https://www.test.curiouser.com$1 permanent; } 十三、Nginx与PHP FastCGI Nginx本身不能处理PHP，它只是个web服务器，当接收到请求后，如果是php请求，则发给php解释器处理，并把结果返回给客户端 Nginx与php-fpm之间的请求处理流程： 请求发送到nginx服务 nginx收到请求，通过fast-cgi协议，将请求数据发送到php-fpm进程管理器 php-fpm进程管理器将任务发给子进程（fork的子worker进程） worker进程中的php解释器执行php文件来处理请求 php解释器处理好之后再通过fast-cgi协议将处理结果发给Nginx服务器 nginx将结果返回到客户端 Nginx与php-fpm之间有2种通信方式： TCP socket：通过 ip:port的方式进行通信 可以将nginx和php-fpm分布在不同的服务器上 tcp socket：需要重新打包、拆包、计算校验、应答等 缺点：需要有多余的tcp开销 优点：高并发下，保证通信的正确性和完整性 Unix socket：通过 php启动生成的socket文件进行通信 nginx和php-fpm就只能部署在一台机器上 优点：效率比tcp高，只是将数据进行进程间的拷贝，减少不必要的tcp开销 缺点：高并发时不稳定，产生大量长时缓存，数据可能出错不返回异常 server { location / { try_files $uri /index.php?$args; } location ~ \\.php$ { fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass 127.0.0.1:9000; # fastcgi进程监听的IP地址和端口 fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param PATH_INFO $fastcgi_path_info; } } 十四、Rewrite重写 Nginx通过ngx_http_rewrite_module 模块支持URL重写、支持if条件判断，但不支持else rewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向 rewrite只能放在server{},location{},if{}中，并且默认只能对域名后边的除去传递的参数外的字符串起作用， rewrite 执行顺序 执行 server 块里面的 rewrite 指令 执行 location 匹配 执行选定的 location 中的 rewrite 指令 语法格式： rewrite [flag] regex ：表示正则匹配规则 replacement ：表示跳转后的内容 flag ：表示 rewrite 支持的 flag 标记 last ：本条规则匹配完成后，继续向下匹配新的location URI规则，一般用在 server 和 if 中 break ：本条规则匹配完成即终止，不再匹配后面的任何规则，一般使用在 location 中 redirect：返回302临时重定向，浏览器地址会显示跳转后的URL地址 permanent：返回301永久重定向，浏览器地址栏会显示跳转后的URL地址。 rewrite 示例 基于域名跳转 server { listen 80; server_name www.test.curiouser.com; #域名修改 charset utf-8; access_log /var/log/nginx/www_test_curiouser_com.log; #日志修改 location / { #添加域名重定向 if ($host = 'www.test.curiouser.com'){ #$host为rewrite全局变量，代表请求主机头字段或主机名 rewrite ^/(.*)$ http://www.test1.curiouser.com/$1 permanent; #$1为正则匹配的内容，即域名后边的字符串 } root html; index index.html index.htm; } } 基于客户端 IP 访问跳转 server { listen 80; server_name www.curiouser.com; #域名修 charset utf-8; access_log log/nginx/www-curiouser-com-access.log; #日志修改 # 设置是否合法的IP标记 set $rewrite true; #设置变量$rewrite，变量值为boole值true # 判断是否为合法IP if ($remote_addr = \"192.168.1.8\"){ #当客户端IP为192.168.1.8时，将变量值设为false，不进行重写 set $rewrite false; } # 除了合法IP，其它都是非法IP，进行重写跳转维护页面 if ($rewrite = true){ #当变量值为true时，进行重写 rewrite (.+) /re.html; #将域名后边的路径重写成/re.html后转发，例如www.curiouser.com/re.html } location = /re.html { root /var/www/html; #网页返回/var/www/html/re.html的内容 } location / { root html; index index.html index.htm; } } 参考： https://zhuanlan.zhihu.com/p/359801091 https://blog.51cto.com/u_14842009/5146017 十五、代理 WebSocket 官方文档：https://nginx.org/en/docs/http/websocket.html map $http_upgrade $connection_upgrade { # 如果$http_upgrade 不为 '' (空)， 则$connection_upgrade 为 upgrade 。 default upgrade; # 如果$http_upgrade 为 '' (空)， 则 $connection_upgrade 为 close。 '' close; } server { listen 80; server_name example.com; location /socket.io { # 将请求转发到websocket服务上 proxy_pass http://192.168.1.1:8084; # 使用 HTTP 1.1 协议进行代理。 proxy_http_version 1.1; # 设置 Upgrade 请求头，其值与客户端请求中的 Upgrade 头相同。 proxy_set_header Upgrade $http_upgrade; # 设置 Connection 请求头为 \"upgrade\"，表明这是一个 WebSocket 连接。 proxy_set_header Connection $connection_upgrade; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_read_timeout 3600s; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } 参考：https://www.cnblogs.com/binghe001/p/14752404.html 十六、支持 Brotli压缩算法 目前仅在NGINX Plus版本中支持。参考：https://docs.nginx.com/nginx/admin-guide/dynamic-modules/brotli/ 手动编译支持：https://github.com/google/ngx_brotli 参考： https://www.cnblogs.com/-wenli/p/13594882.html https://cloud.tencent.com/developer/article/1501009 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-04 17:22:00 "},"origin/nginx-log-kafka.html":{"url":"origin/nginx-log-kafka.html","title":"Nginx日志写入kafka","keywords":"","body":"Nginx日志写入Kafka 一、简介 方案 Nginx module：编译集成第三方kafka相关模块，可直接将日志发送到kafka Nginx + 第三方应用：结合第三方应用将Nginx日志发送到kafka tail | kafkacat ———> kafka rsyslog ———> kafka Nginx stdout | k8s 第三方operator ———> kafka 二、Nginx module -> kafka openssl_version=1.1.1 && \\ nginx_version=1.18.0 && \\ mkdir compile-dir && cd compile-dir && \\ curl -s -# https://www.openssl.org/source/openssl-$openssl_version.tar.gz | tar zxf - -C ./ && \\ curl -s -# https://nginx.org/download/nginx-$nginx_version.tar.gz | tar zxf - -C ./ && \\ cd nginx-$nginx_version && \\ ./configure \\ --prefix=/opt/nginx-1.18.0 \\ --user=nginx \\ --group=nginx \\ --modules-path=/opt/nginx-1.18.0/modules \\ --sbin-path=/opt/nginx-1.18.0/sbin/nginx \\ --error-log-path=/opt/nginx-1.18.0/logs/error.log \\ --http-log-path=/opt/nginx-1.18.0/logs/access.log \\ --conf-path=/opt/nginx-1.18.0/conf/nginx.conf \\ --pid-path=/opt/nginx-1.18.0/nginx.pid \\ --lock-path=/opt/nginx-1.18.0/nginx.lock \\ --with-pcre \\ --with-openssl=../openssl-1.1.1 \\ --with-http_stub_status_module \\ --with-http_realip_module \\ --with-http_addition_module \\ --with-http_sub_module \\ --with-http_dav_module \\ --with-http_flv_module \\ --with-http_mp4_module \\ --with-http_gunzip_module \\ --with-http_gzip_static_module \\ --with-http_random_index_module \\ --with-http_secure_link_module \\ --with-http_stub_status_module \\ --with-http_auth_request_module \\ --with-http_xslt_module=dynamic \\ --with-http_image_filter_module=dynamic \\ --with-http_geoip_module=dynamic \\ --with-http_image_filter_module \\ --with-http_v2_module \\ --with-http_slice_module \\ --with-threads \\ --with-stream \\ --with-stream_ssl_module \\ --with-stream_ssl_preread_module \\ --with-stream_realip_module \\ --with-stream_geoip_module=dynamic \\ --with-mail \\ --with-mail_ssl_module \\ --with-compat && \\ make && \\ make install 参考： https://github.com/brg-liuwei/ngx_kafka_module https://github.com/kaltura/nginx-kafka-log-module 三、tail | kafkacat -> kafka 参考： https://github.com/edenhill/kafkacat 四、nginx -> rsyslog -> kafka 1、Prerequisite Nginx >= v1.7.1 （之后才支持 syslog 的方式处理日志） Rsyslog >= v8.7.0 （数据需要通过 Rsyslog 的 omkafka 模块写入到 Kafka，omkafka 在 Rsyslog 的 v8.7.0+ 版本才支持） rsyslog的omkafka模块rsyslog-kafka已安装 2、rsyslog配置 vi /etc/rsyslog.dc/rsyslog_nginx_kafka_cluster.conf module(load=\"imudp\") input(type=\"imudp\" port=\"514\") # nginx access log ==> rsyslog server(local) ==> kafka module(load=\"omkafka\") template(name=\"nginx-rsyslog-kafka\" type=\"string\" string=\"%msg%\") if $inputname == \"imudp\" then { if ($programname == \"nginx-rsyslog\") then action(type=\"omkafka\" template=\"nginx-rsyslog-kafka\" broker=[\"localhost:9092\"] topic=\"test\" partitions.auto=\"on\" confParam=[ \"socket.keepalive.enable=true\" ] ) } :rawmsg, contains, \"nginx-rsyslog\" ~ 启动rsyslog rsyslogd 3、Nginx配置 http { log_format json_log '{ \"@timestamp\": \"$time_iso8601\", ' '\"app\": \"$app\", ' '\"remote_addr\": \"$remote_addr\", ' '\"referer\": \"$http_referer\", ' '\"request\": \"$request\", ' '\"status\": $status, ' '\"bytes\": $body_bytes_sent, ' '\"agent\": \"$http_user_agent\", ' '\"x_forwarded\": \"$http_x_forwarded_for\", ' '\"up_addr\": \"$upstream_addr\",' '\"up_host\": \"$upstream_http_host\",' '\"up_resp_time\": \"$upstream_response_time\",' '\"request_time\": \"$request_time\",' '\"server_name\": \"$server_name\",' '\"x-zz-app-info\": \"$http_x_zz_app_info\"' ' }'; server { listen 80; server_name localhost; # set $app test; # access_log /var/log/nginx/host.access.log json_log; access_log syslog:server=localhost,facility=local7,tag=nginx-rsyslog,severity=info main; location / { root /usr/share/nginx/html; index index.html index.htm; } } 4、测试 nginx的日志 ———> rsyslog —————> Kafka wrk -t10 -c500 -d30s --latency http://127.0.0.1:80 30.1秒发送了38861个请求，QPS大约1291，Transfer/sec: 1.05MB nginx的日志 ————> 本地文件 wrk -t10 -c500 -d30s --latency http://127.0.0.1:80 30.07秒发送了115156个请求，QPS大约3829，Transfer/sec: 3.10MB nginx的日志通过网络发送到rsyslog再到Kafka，比落盘形成文件，QPS小了三倍。同时压测完成到所有日志进kafka有15秒延迟 因素： 网络消耗 磁盘性能可能导致kafka写入慢 参考： http://zhang-jc.github.io/2019/03/15/%E4%BD%BF%E7%94%A8-Rsyslog-%E5%B0%86-Nginx-Access-Log-%E5%86%99%E5%85%A5-Kafka/ 五、Nginx stdout -> k8s 第三方operator-> kafka 参考： https://banzaicloud.com/docs/one-eye/logging-operator/quickstarts/kafka-nginx/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/gitbook-简介安装配置.html":{"url":"origin/gitbook-简介安装配置.html","title":"GitBook","keywords":"","body":"GitBook简介安装配置 一、GitBook简介 gitbook 是一个基于node.js的命令行工具 gitbook 支持markdown/asciiDoc语法格式构建书籍 gitbook 支持输出静态网页（可定制和可扩展）和电子书（PDF，ePub或Mobi）等多种格式，其中默认输出静态网页格式 gitbook 不仅支持本地构建书籍,还可以托管在gitbook 官网上，或者Github上 二、GitBook安装 1、安装NodeJs环境 NodeJs官网下载链接:https://nodejs.org/en/download/ Linux 以安装NodeJs 10.16.3为例 wget https://nodejs.org/dist/v10.16.3/node-v10.16.3-linux-x64.tar.xz && \\ tar -xvf node-v10.16.3-linux-x64.tar.xz -C /opt/ && \\ rm -rf node-v10.16.3-linux-x64.tar.xz && \\ ln -s /opt/node-v10.16.3-linux-x64 /opt/nodejs && \\ sed -i '$a export NODEJS_HOME=/opt/nodejs\\nexport PATH=$PATH:$NODEJS_HOME/bin' /etc/profile && \\ source /etc/profile && \\ yum install gcc-c++ make -y && \\ npm config set registry https://registry.npm.taobao.org && \\ npm config set sass_binary_site https://npm.taobao.org/mirrors/node-sass/ && \\ node -v && \\ npm version Windows 直接在官网下载MSI格式的安装包进行安装 2、安装Gitbook CLI命令行工具 gitbook-cli 是 gitbook 的一个命令行工具, 通过它可以在电脑上安装和管理多个版本的gitbook. npm install gitbook-cli -g # 不可以直接使用npm安装gitbook 三、GitBook版本的管理 gitbook-cli 和 gitbook 是两个软件，gitbook-cli 会将下载的 gitbook 的不同版本放到 ~/.gitbook中, 可以通过设置GITBOOK_DIR环境变量来指定另外的文件夹 GitBook可以在本地安装多个版本并在执行命令的时候指定某个版本，如果指定的版本还没安装就会自动下载安装，下载后的GitBook会被放到~/.gitbook目录下。 $ gitbook --help Usage: gitbook [options] [command] Options: -v, --gitbook [version] specify GitBook version to use -d, --debug enable verbose error -V, --version Display running versions of gitbook and gitbook-cli -h, --help output usage information Commands: ls List versions installed locally current Display currently activated version ls-remote List remote versions available for install fetch [version] Download and install a alias [folder] [version] Set an alias named pointing to uninstall [version] Uninstall a version update [tag] Update to the latest version of GitBook help List commands for GitBook * run a command with a specific gitbook version # 查看当前GitBook CLI版本 gitbook -V # 列出本地安装版本 gitbook ls # 列出当前使用版本 gitbook current # 列出远程可用版本 gitbook ls-remote # 安装指定版本(如果安装比较慢的话，将npm镜像源切到国内的CNPM镜像源。可使用NRM管理NPM的镜像源) gitbook fetch [version] # 卸载指定版本 gitbook uninstall [version] # 更新指定版本 gitbook update [tag] 使用gitbook安装gitbook-cli时出现gitbook-cli/node_modules/npm/node_modules/graceful-fs/polyfills.js:287 cb.apply is not a function报错时，是因为最新NodeJS的graceful-fs模块与gitbook-cli不兼容导致的，只需将graceful-fs的版本降到4.2.0即可。 cd /usr/local/lib/node_modules/gitbook-cli/node_modules/npm/node_modules/ npm install graceful-fs@4.2.0 --save # 或者 直接替换/usr/local/lib/node_modules/gitbook-cli/node_modules/npm/node_modules/graceful-fs/polyfills.js的内容为https://raw.githubusercontent.com/isaacs/node-graceful-fs/168bdb8f0bb3174e8499d4bc5878deead4172c39/polyfills.js里的 参考：https://stackoverflow.com/questions/64211386/gitbook-cli-install-error-typeerror-cb-apply-is-not-a-function-inside-graceful 四、GitBook CLI命令 1、gitbook 可用命令 $ gitbook help build [book] [output] 构建书籍 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) --format Format to build to (Default is website; Values are website, json, ebook) --[no-]timing Print timing debug information (Default is false) serve [book] [output] serve the book as a website for testing --port 指定监听端口(默认端口4000) --lrport Port for livereload server to listen on (Default is 35729) --[no-]watch Enable file watcher and live reloading (Default is true) --[no-]live Enable live reloading (Default is true) --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) --format Format to build to (Default is website; Values are website, json, ebook) install [book] 安装所有插件资源 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) parse [book] parse and print debug information about a book --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) init [book] 初始化创建书籍文件结构 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) pdf [book] [output] 构建书籍为ebook文件 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) epub [book] [output] 构建书籍为ebook文件 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) mobi [book] [output] 构建书籍为ebook文件 --log 指定日志输出级别(值为debug, info默认, warn, error, disabled) 2、gitbook init初始化创建书籍文件结构 gitbook init # 在当前路径下自动生成README.md 和 SUMMARY.md。也可以先手动创建SUMMARY.md，再执行gitbook init，如果SUMMARY.md中配置的文件夹和文件不存在，就会自动创建文件夹和文件，已经存在的文件夹和文件不会被覆盖。 gitbook init ./directory # 可将书籍初始化到指定目录 3、gitbook build构建gitbook书籍静态HTML资源 gitbook build [book] [output] # 会在书籍的文件夹中生成一个 _book 的文件夹, 里面有生成的静态HTML资源。可将 _book 文件夹下的文件拷贝到nginx、httpd等web服务器内 gitbook build --gitbook=2.0.1 # 指定Gitbook版本 4、gitbook serve启动本地预览书籍服务 gitbook serve [book] [output] 浏览器中打开： http://localhost:4000 预览GitBook书籍 5、输出书籍文件 Prerequisite： ebook-convert：GitBook在生成PDF的过程中使用到calibre的转换功能，没有安装Calibre或安装了Calibre没有配置环境变量都会导致转换PDF失败。Calibre下载地址：https://calibre-ebook.com/download 在 Typora 中安装 Pandoc 进行导出 # 输出书籍为PDF格式文件 gitbook pdf [book] [output] # 输出书籍为epub格式文件 gitbook epub [book] [output] # 输出书籍为mobi格式文件 gitbook mobi [book] [output] 6、gitbook install安装插件样式资源 gitbook install [book] #会在当前路径下生成node_modules文件夹，里面为插件的样式资源 7、gitbook parse 解析电子书 gitbook parse [book] 五、GitBook的文件结构 文件/文件夹 描述 是否必须 README.md 书籍的简介 必须 SUMMARY.md 书籍的目录结构 可选 book.json GitBook的插件样式配置文件 可选 GLOSSARY.md 词汇、术语列表 可选 _book文件夹 GitBook输出的静态HTML文件 node_modules文件夹 插件的样式资源 六、SUMMARY.md编写规则 SUMMARY.md 的格式是一个链接列表。链接的标题将作为章节的标题，链接的目标是该章节文件的路径 向父章节添加嵌套列表将创建子章节 每章都有一个专用页面（part#/README.md），并分为子章节。 目录中的章节可以使用锚点指向文件的特定部分。 目录可以分为以标题或水平线 ---- 分隔的部分 Parts 只是章节组，没有专用页面，但根据主题，它将在导航中显示。 七、book.json编写规则 常规设置 变量 描述 root 包含所有图书文件的根文件夹的路径，除了 book.json structure 指定 Readme，Summary，Glossary 和 Languages 的名称（而不是使用默认名称，如README.md）。这些文件必须在项目的根目录下（或 root 属性指定的根目录）structure.readme：Readme 文件名（默认值是 README.md ）structure.summary：Summary 文件名（默认值是 SUMMARY.md ）structure.glossary：Glossary 文件名（默认值是 GLOSSARY.md ）structure.languages：Languages 文件名（默认值是 LANGS.md ） title 您的书名，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预填的。 description 您的书籍的描述，默认值是从 README 中提取出来的。在 GitBook.com 上，这个字段是预填的。 author 作者名。在GitBook.com上，这个字段是预填的。 isbn 国际标准书号 ISBN language 本书的语言类型 —— ISO code 。默认值是 en direction 文本阅读顺序。可以是 rtl （从右向左）或 ltr （从左向右），默认值依赖于 language 的值。 gitbook 应该使用的GitBook版本。使用 SemVer 规范，并接受类似于 “> = 3.0.0” 的条件。 links 在左侧导航栏添加链接信息 plugins 要加载的插件列表 pluginsConfig 插件的配置 Gitbook 默认带有 5 个插件： highlight：语法高亮插件 search：搜索插件 sharing：分享插件 font-settings：字体设置插件 livereload：热加载插件 Note：去除插件\"plugins\": [ \"-search\" ] 插件配置示例 { \"author\": \"Curiouser \", \"title\": \"Devops Roadmap\", \"plugins\": [ \"-search\", \"-lunr\", \"-sharing\", \"-highlight\", \"search-pro\", \"splitter\", \"github\", \"popup\", \"sectionx\", \"expandable-chapters\", \"sharing-plus\", \"code\", \"auto-scroll-table\", \"theme-fexa\", \"tbfed-pagefooter\", \"back-to-top-button\", \"emphasize\", \"edit-link\", \"prism\", \"donate\", \"theme-comscore\", \"github-buttons\", \"github-issue-feedback\" ], \"pluginsConfig\": { \"theme-default\": { \"showLevel\": true }, \"github-issue-feedback\": { \"repo\": \"RationalMonster/rationalmonster.github.io\" }, \"github\": { \"url\": \"https://github.com/RationalMonster\" }, \"github-buttons\": { \"buttons\": [{ \"user\": \"RationalMonster\", \"repo\": \"rationalmonster.github.io\", \"type\": \"star\", \"size\": \"small\", \"count\": \"true\" }] }, \"sharing\": { \"weibo\": true, \"qq\": \"true\", \"google\": true, \"all\": [ \"facebook\", \"twitter\" ] }, \"code\": { \"copybuttons\": \"true\" }, \"theme-fexa\":{ \"search-placeholder\":\"搜索文章\", \"logo\": \"assets/logo.png\" }, \"tbfed-pagefooter\": { \"copyright\":\"Copyright Curiouser\", \"modify_label\": \"该文件最后修改时间：\", \"modify_format\": \"YYYY-MM-DD HH:mm:ss\" }, \"edit-link\": { \"base\": \"https://github.com/RationalMonster/rationalmonster.github.io/blob/master\", \"label\": \"ORIGIN In Github\" }, \"prism\": { \"css\": [ \"prismjs/themes/prism-tomorrow.css\" ], \"lang\": { \"flow\": \"typescript\" }, \"ignore\": [ \"mermaid\", \"eval-js\" ] }, \"donate\": { \"wechat\": \"../assets/wechat-donate.jpg\", \"title\": \"\", \"button\": \"赏\", \"wechatText\": \"微信打赏\" } } } 八、GLOSSARY.md 编写规则 GLOSSARY.md 的格式是 h2 标题的列表，以及描述段落 九、忽略文件和文件夹 GitBook将读取 .gitignore，.bookignore 和 .ignore 文件，来过滤不需要进行git版本控制的文件和文件夹。这些文件中的格式遵循 .gitignore 的规则： # This is a comment # Ignore the file test.md test.md # Ignore everything in the directory \"bin\" bin/* ### gitbook ### _node !docs _book node_modules ### IDEA ### .idea/ ### VS Code ### .vscode/ ### OS ### .DS_Store 十、封面 封面用于所有电子书格式。您可以自己提供一个，也可以使用 autocover plugin 生成一个。 要提供封面，请将 cover.jpg 文件放在书本的根目录下。添加一个 cover_small.jpg 将指定一个较小版本的封面。封面应为 JPEG 文件。 好的封面应该遵守以下准则： cover.jpg 的尺寸为 1800x2360 像素，cover_small.jpg 为 200x262 没有边界 清晰可见的书名 任何重要的文字应该在小版本中可见 十一、多语言支持 gitbook 支持构建用多种语言书写的书籍。每种语言应该是一个子目录，遵循正常的gitbook格式，然后需要在根目录下放置一个名为 LANGS.md 的文件，存放下列内容： # Languages * [English](en/) * [French](fr/) * [Español](es/) 注意： 当一个语言的书(如：en)有 book.json 时，它的配置将扩展主要配置。 唯一的一个例外是插件，插件是全局设置的，并且不能指定语言特定的插件 插件的配置必须写在根目录下的 book.json 文件中。然后其他语言的配置可以分别写在各自语言目录下的 book.json 文件中。 LANGS.md 文件中各个语言出现的顺序，就是书籍首页出现的顺利。因此，写在第一位的语言，就自然成为书籍首页打开时的默认语言。 当一个语言的书(如：en)有 book.json 时，它的配置将扩展主要配置。 唯一的一个例外是插件，插件是全局设置的，并且不能指定语言特定的插件。 十二、托管到 GitHub Pages 知道如何编写gitbook了，那怎么放到网上让更多人看到呢。如果自己在各大云厂商那儿买个云主机自己搭建一个运行环境，一年又得多处几百大洋的开销。本着能“白嫖”就不掏腰包的精神，尝试使用其他途径免费部署自己的Gitbook。 Github 有个功能 GitHub Pages 。它允许用户在 GitHub 仓库托管你的个人、组织或项目的静态页面（自动识别 html、css、javascript）。只要仓库指定分支中的内容符合一个静态站点要求，就可以在如下地址中进行访问：https://Github用户名.github.com/仓库名 在Github中设置GitHub Page Source时可指定分支。 gh-pages branch master branch master branch /docs folder 不同的方式，无非是决定gitbook构建后的静态页面HTML文件存放在哪儿的问题。 方式一(推荐)：在master分支中存放gitbook原始Markdown文件、配置文件等，在gh-pages分支中存放gitbook构建后的静态页面HTML文件。 方式二：在master分支中即存放gitbook原始Markdown文件，也存放gitbook构建后的静态页面HTML文件。只不过是在/docs文件下 1、在GitHub中建立仓库并将本地代码推送至master分支 GitHub只建立空仓库是无法创建分支的。所以需要将本地代码推送到远程仓库master才可以 git init git remote add origin git@github.com:**/gitbbook-devops-roadmap.git git add . git commit -m \"init commit\" git push origin master 2、(方式二)在本地建立空白gh-pages分支并同步到GitHub 仓库中建立一个名为 gh-pages 的分支。 git checkout --orphan gh-pages # 该命令会创建gh-pages分支，并且该分支下有master分支下的所有文件 git rm -rf * # 删除master分支带过来的文件(“git rm -rf *\"命令并不能删除隐藏文件,可使用““git rm -rf .\"命令) rm '.gitignore' # 如果master中.gitignore文件,可删除 # 如果没有任何文件提交的话，分支是看不到的，可以创建一个新文件后再次提交则新创建的gh-pages分支就会显示出来。 echo \"# MacOS\\n*.DS_Store\" > .gitignore git add . git commit -m \"init commit\" git push origin gh-pages 当本地gh-pages分支同步到远程GitHub仓库分支后，在GitHub分支进行验证 3、在本地构建静态页面HTML文件 在Master分支下使用“gitbook build”命令生产静态页面HTML文件 gitbook install # 该命令后会下载构建gitbook所需的插件及资源到当前目录“node_modules”下 gitbook build . [静态页面HTML文件输出文件夹] # 该命令默认会将静态页面HTML文件输出到当前目录\"_book\"下。如果指定，则输出到指定目录。 4、(方式二)使用gh-pages插件将本地静态页面HTML文件推送到远处仓库gh-pages分支 本地安装gh-pages插件 npm install gh-pages -g gh-pages命令详解 Usage: gh-pages [options] Options: -V, --version output the version number -d, --dist Base directory for all source files -s, --src Pattern used to select which files to publish (default: \"**/*\") -b, --branch Name of the branch you are pushing to (default: \"gh-pages\") -e, --dest Target directory within the destination branch (relative to the root) (default: \".\") -a, --add Only add, and never remove existing files -x, --silent Do not output the repository url -m, --message commit message (default: \"Updates\") -g, --tag add tag to commit --git Path to git executable (default: \"git\") -t, --dotfiles Include dotfiles -r, --repo URL of the repository you are pushing to -p, --depth depth for clone (default: 1) -o, --remote The name of the remote (default: \"origin\") -u, --user The name and email of the user (defaults to the git config). Format is \"Your Name \". -v, --remove Remove files that match the given pattern (ignored if used together with --add). (default: \".\") -n, --no-push Commit only (with no push) -f, --no-history Push force new commit without parent history -h, --help output usage information 推送静态页面HTML文件 gh-pages -d _book 5、GitHub配置gh-pages服务 6、(可选)自定义域名访问GitHub Pages托管的Gitbook 此时你可以通过https:/Github用户名.github.io/仓库名访问你的Gitbook啦。但是如果你在阿里云上购买的有自己的域名，可直接将域名指定到gitbook page域名，使用自己的域名访问。 在GitHub中设置仓库的CNAME 直接在GitHub中进行设置 (推荐)在master分支代码根路径下创建CNAME文件，文件中写入你自定义的域名(每个CNAME文件能且只能指定一个域名)。例如：gitbook.curiouser.top。 Github读取你的CNAME文件之后，Github服务器会设置gitbook.curiouser.top为你的主域名，然后将rationalmonster.github.ioo重定向到gitbook.curiouser.top 在阿里云上添加域名解析记录 等待3～10分钟，通过自定义域名访问自己的gitbook。 问题 gitbook serve时，偶尔不规律性地出现编译错误,而且每次出现的错误文件还可能不一样，实在是头疼得很，每次修改要编译多次才能成功 修改 C:\\Users\\当前用户名\\.gitbook\\versions\\当前使用的gitbook版本\\lib\\output\\website\\copyPluginAssets.js文件中的112行，将confirm: true改为confirm: false 参考链接 https://www.jianshu.com/p/f38d8ff999cb?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-03-20 14:54:26 "},"origin/telegram-Bot机器.html":{"url":"origin/telegram-Bot机器.html","title":"Telegram机器人","keywords":"","body":"Telegram Bot机器人 一、简介 Telegram Bot是运行在Telegram内部的第三方应用程序，相当于Telegram的一个特殊账户。 用户可以向Telegram Bot发送消息，命令和内联请求等方式与Telegram Bot人进行交互，而Telegram Bot开发者可以通过Telegram Bot API，用https请求方式来控制机器人 二、创建 客户端搜索\"Botfather\" 查看帮助 发送\"/newboot\"来创建Bot机器人,根据提示一步一步进行.(当设置用户名时) TOKEN 一定要保护好！以后接口访问都要用到！ 三、API Telegram有两种api，一种是bot api，一种是telegram api。bot api是基于http访问，telegram api是基于mtproto访问，访问需要加密，相对要复杂一些。后者也可以实现发送消息等功能 可使用PostMan或者Curl等工具发送HTTPS请求调用Bot的API。 当时用Curl命令时可使用\"-x\"参数设置代理。例如“curl -x 127.0.0.1:3128 -sk https://www.google.com” 四、Bot API Bot API文档链接 1. Bot API相关信息 Bot API支持GET和POST方法的HTTPS请求，URL格式为: \"https://api.telegram.org/bot[你的bot机器人Token]/方法名\" 例如：https://api.telegram.org/bot123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11/getMe 支持一下几种传参方式： URL query string application/x-www-form-urlencoded application/json (except for uploading files) multipart/form-data (use to upload files) Bot机器人将返回JSON格式的对象，里面会包含返回状态信息 注意: API方法大小写敏感 请求格式必须是UTF-8编码 2. 示例 使用PostMan给Bot机器人发送消息 使用Curl命令给Bot机器人发送消息 curl -x 梯子IP地址 -sk \\ -X POST \\ https://api.telegram.org/bot90****93:AAF***RfFma8/sendMessage \\ -d 'chat_id=623***17' \\ -d 'parse_mode=Markdown' \\ -d 'text=*Jenkins '$BUILD_NUMBER' *' 3. 支持的消息格式 MarkDown风格 *bold text* _italic text_ [inline URL](http://www.example.com/) [inline mention of a user](tg://user?id=123456789) `inline fixed-width code` ​```block_language pre-formatted fixed-width code block ​ ``` HTML风格 *bold text* _italic text_ [inline URL](http://www.example.com/) [inline mention of a user](tg://user?id=123456789) `inline fixed-width code` ​```block_language pre-formatted fixed-width code block ​ ``` 标签不能嵌套 所有不属于标签或HTML实体的' '和' & '符号必须替换为相应的HTML实体 (\"\"对应\"\\>\"、\"\\&\"对应\"\\&\") 支持所有数字类型的HTML实体 该API目前仅支持以下命名的HTML实体:' '、' & '和' \" ' 4. 支持的方法 Bot API方法 描述 getMe sendMessage 发送文本信息,支持Markdown、HTML格式化的文本信息 forwardMessage sendPhoto 发送图片 sendAudio 发送音频，最大50 MB sendDocument 发送文档，最大50 MB sendVideo 发送视频，最大50 MB sendAnimation 发送动图，最大50 MB(支持无声音的GIF或H.264/MPEG-4 AVC格式动图) sendVoice 发送录音，最大50 MB sendVideoNote sendMediaGroup sendLocation 发送定位 editMessageLiveLocation stopMessageLiveLocation sendVenue sendContact 发送名片 sendPoll 发送投票 sendChatAction getUserProfilePhotos getFile kickChatMember unbanChatMember restrictChatMember promoteChatMember setChatPermissions exportChatInviteLink setChatPhoto deleteChatPhoto setChatTitle 设置聊天室标题 setChatDescription 设置聊天室描述 pinChatMessage unpinChatMessage leaveChat 离开聊天室 getChat 查找聊天室 getChatAdministrators 获取聊天室管理员 getChatMembersCount 获取聊天室成员个数 getChatMember 获取聊天室成员 setChatStickerSet deleteChatStickerSet answerCallbackQuery Inline mode methods Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/openvpn-server.html":{"url":"origin/openvpn-server.html","title":"OpenVPN Server","keywords":"","body":"OpenVPN 一、简介 OpenVPN 是一个基于 OpenSSL 库的应用层 VPN 实现。和传统 VPN 相比，它的优点是简单易用。 [1] OpenVPN允许参与建立VPN的单点使用共享金钥，电子证书，或者用户名/密码来进行身份验证。它大量使用了OpenSSL加密库中的SSLv3/TLSv1 协议函式库。OpenVPN能在Solaris、Linux、OpenBSD、FreeBSD、NetBSD、Mac OS X与Windows 2000/XP/Vista上运行，并包含了许多安全性的功能。它并不是一个基于Web的VPN软件，也不与IPsec及其他VPN软件包兼容。 OpenVPN2.0后引入了用户名/口令组合的身份验证方式，它可以省略客户端证书，但是仍有一份服务器证书需要被用作加密。 OpenVPN所有的通信都基于一个单一的IP端口， 默认且推荐使用UDP协议通讯，同时TCP也被支持。OpenVPN连接能通过大多数的代理服务器，并且能够在NAT的环境中很好地工作。服务端具有向客 户端“推送”某些网络配置信息的功能，这些信息包括：IP地址、路由设置等。 OpenVPN提供了两种虚拟网络接口：通用Tun/Tap驱动，通过它们， 可以建立三层IP隧道，或者虚拟二层以太网，后者可以传送任何类型的二层以太网络数据。传送的数据可通过LZO算法压缩。在选择协议时候，需要注意2个加密隧道之间的网络状况， 如有高延迟或者丢包较多的情况下，请选择TCP协议作为底层协议，UDP协议由于存在无连接和重传机制，导致要隧道上层的协议进行重传，效率非常低下。 二、安装 1、在Synology上安装部署OpenVPN 2、使用脚本在Linux服务器搭建 GitHub有个脚本项目专门安装OpenVPN server，地址：https://github.com/Nyr/openvpn-install ，但是功能过少。为了便于管理openvpn,基于此脚本进行了功能优化，github地址：https://github.com/RationalMonster/install-manage-openvpn ，以下为优化的功能点： 汉化 增加选择客户端分配IP地址池网段的功能 增加用户名密码验证脚本 增加配置SMTP发送邮件的功能 增加创建用户后将用户名密码及配置文件等信息通过SMTP邮件服务发送到用户邮箱 去除不必要的脚本代码 首次运行该脚本是安装openvpn服务，再次运行可执行其他服务。 注意 所有的iptables规则配置都是由systemD服务openvpn-iptables.service(/etc/systemd/system/openvpn-iptables.service)进行配置的。 [Unit] Before=network.target [Service] Type=oneshot ExecStart=/sbin/iptables -t nat -A POSTROUTING -s 10.8.6.0/24 ! -d 10.8.6.0/24 -j SNAT --to 192.168.1.2 ExecStart=/sbin/iptables -I INPUT -p udp -d 192.168.1.2 --dport 30668 -j ACCEPT ExecStart=/sbin/iptables -I INPUT -p tcp -s 10.8.6.0/24 -d 192.168.1.2 --dport 9092 -j ACCEPT ExecStart=/sbin/iptables -I FORWARD -p tcp -s 10.8.6.0/24 -d 192.168.1.3 ! --destination-port 6443 -j DROP ExecStart=/sbin/iptables -A INPUT -s 10.8.6.0/24 -d 192.168.1.2 -j DROP ExecStart=/sbin/iptables -I INPUT -p tcp -s 10.8.6.166/32 -d 192.168.1.2 --dport 22 -j ACCEPT ExecStop=/sbin/iptables -D FORWARD -p tcp -s 10.8.6.0/24 -d 192.168.1.3 ! --destination-port 6443 -j DROP ExecStop=/sbin/iptables -t nat -D POSTROUTING -s 10.8.6.0/24 ! -d 10.8.6.0/24 -j SNAT --to 192.168.1.2 ExecStop=/sbin/iptables -D INPUT -p udp -d 192.168.1.2 --dport 30668 -j ACCEPT ExecStop=/sbin/iptables -D INPUT -p tcp -s 10.8.6.0/24 -d 192.168.1.2 --dport 9092 -j ACCEPT ExecStop=/sbin/iptables -D INPUT -p tcp -s 10.8.6.166/32 -d 192.168.1.2 --dport 22 -j ACCEPT ExecStop=/sbin/iptables -D INPUT -s 10.8.6.0/24 -d 192.168.1.2 -j DROP RemainAfterExit=yes [Install] WantedBy=multi-user.target 三、Openvpn Access Server OpenVPN 的商业收费版本 OpenVPN Access Server，其免费的 license 可以支持2个 VPN 用户的同时在线， 1、Ubuntu/Debian ①APT apt update && apt -y install ca-certificates wget net-tools gnupg wget -qO - https://as-repository.openvpn.net/as-repo-public.gpg | apt-key add - echo \"deb http://as-repository.openvpn.net/as/debian bionic main\">/etc/apt/sources.list.d/openvpn-as-repo.list apt update && apt -y install openvpn-as ②Deb包（推荐） 使用APT安装时、服务器可能需要能翻墙。deb包手动下载地址： https://openvpn.net/downloads/openvpn-as-latest-ubuntu18.amd_64.deb https://openvpn.net/downloads/openvpn-as-bundled-clients-latest.deb apt update apt install -y liblzo2-2 bridge-utils net-tools python-pyrad python-serial libsasl2-2 iproute2 sqlite3 libsqlite3-0 iptables liblz4-1 python-pkg-resources python-mysqldb libmariadbclient18 libssl1.1 dpkg -i openvpn-as-bundled-clients-11.deb openvpn-as_2.8.5-f4ad562b-Ubuntu18_amd64.deb 2、CentOS/Redhat yum -y install https://as-repository.openvpn.net/as-repo-centos7.rpm # 或者 yum -y install https://as-repository.openvpn.net/as-repo-centos8.rpm yum -y install openvpn-as 3、使用OVA模版在ESXI上部署 官方文档：https://openvpn.net/vpn-server-resources/deploying-the-access-server-appliance-on-vmware-esxi/ 最新ESXI OVA部署模板下载地址：https://openvpn.net/downloads/openvpn-as-latest-vmware.ova 支持在ESXI 5.0+ 上部署。直接使用vSphere Client客户端导入OVA文件创建虚拟机，步骤省略。 使用OVA部署OpenVPN Server的License只支持两个用户同时在线 默认使用sqlite存储数据，支持将数据转换存储到MySQL中 支持对接LDAP认证 虚拟机基本信息 1vCPU 1GB内存 512MB交换内存 OS版本：Ubuntu 18.04.3 Server LTS x64 默认SSH用户密码：root / openvpnas 软件根路径：/usr/local/openvpn_as 日志目录：/usr/local/openvpn_as/init.log 重新配置命令：/usr/local/openvpn_as/bin/ovpn-init 已安装VM Tools，未安装curl 安装后注意 修改时区为CST。默认时区为US(Pacific - Los Angeles) timedatectl set-timezone \"Asia/Shanghai\" # 设置时区 timedatectl status # 查看当前的时区状态 date -R # 查看时区 设置openvpn用户密码（默认没有设置） passwd openvpn (可选)设置静态IP地址（默认DHCP） nano /etc/netplan/01-netcfg.yaml # 配置模板 network: version: 2 renderer: networkd ethernets: eth0: dhcp4: no # ip设置为192.168.79.2 addresses: [192.168.70.2/24] gateway4: 192.168.70.254 nameservers: addresses: [192.168.70.254] netplan apply Web UI访问地址 普通用户访问地址：https://openvpnas-ip:943 管理员访问地址 ：https://openvpnas-ip:943/admin （默认用户openvpn，密码初始没有，需设置） 四、OpenVPN服务端配置 push \"route 192.168.1.0 255.255.255.0\" push \"route 10.8.0.0 255.255.255.0\" push \"dhcp-option DNS 192.168.1.7\" dev tun management 127.0.0.1 1195 server 10.8.0.0 255.255.255.0 client-config-dir ccd dh keys/ca..pm ca keys/ca.crt cert keys/server.crt key keys/server.key max-clients 5 comp-lzo persist-tun persist-key verb 3 #1~4:正常使用范围 5:每个数据包读写时将R和W字符输出到控制台,大写用于TCP/UDP数据包,小写用于 TUN/TAP 数据包。6~11:显示数据包中源/目标地址的转换。 #log-append /var/log/openvpn.log keepalive 10 60 reneg-sec 0 plugin /var/packages/VPNCenter/target/lib/radiusplugin.so /var/packages/VPNCenter/target/etc/openvpn/radiusplugin.cnf client-cert-not-required username-as-common-name duplicate-cn status /tmp/ovpn_status_2_result 30 status-version 2 proto tcp6-server port 19382 cipher AES-256-CBC auth RSA-SHA256 1、可执行脚本的阶段与参数 OpenVPN 可以在 OpenVPN 进程生命周期的各个阶段执行外部脚本。 --up：在 TCP/UDP 套接字绑定和 TUN/TAP 打开后执行。 --tls-verify：有一个不受信任的远程对等点时执行 --ipchange：连接认证或远程IP地址改变后执行。 --client-connect：端身份验证后立即在 --mode 服务器模式下执行。 --route-up：连接身份验证后执行，可以立即执行，也可以在 --route-delay 选项定义的几秒后执行。 --route-pre-down：删除路由之前执行。 --client-disconnect：客户端实例关闭时以 --mode 服务器模式执行。 --down：TCP/UDP 和 TUN/TAP 关闭后执行。 --learn-address：当 IPv4 地址/路由或 MAC 地址添加到 OpenVPN 的内部路由表时，都会在 --mode 服务器模式下执行。 --auth-user-pass-verify cmd method：客户端仍然不受信任时，在新客户端连接上以 --mode 服务器模式执行。 cmd cmd 由脚本（或可执行程序）的路径组成，后面可以选择跟随参数。路径和参数可以是单引号或双引号和/或使用反斜杠转义，并且应该用一个或多个空格分隔。 该脚本应检查用户名和密码，如果要接受客户端的身份验证请求，则返回成功退出代码 (0)，如果要拒绝客户端，则返回失败代码 (1)。 该指令旨在启用插件式接口来扩展 OpenVPN 的身份验证功能。 为了防止客户端传递恶意形成的用户名或密码字符串，用户名字符串必须仅包含以下字符：字母数字、下划线 ('')、破折号 ('-')、点 ('.') 或 at ( '@'）。密码字符串可以包含除 CR 或 LF 之外的任何可打印字符。用户名或密码字符串中的任何非法字符都将转换为下划线 ('')。 method: via-env：该方法会将用户名、密码等信息以环境变量的形式传递给调用的脚本 via-file：该方法会将用户名、密码等信息写入临时文件的前两行。临时文件名将作为参数传递给脚本，脚本返回后 OpenVPN 将自动删除该文件。临时文件的位置由 --tmp-dir 选项控制，如果未指定，则默认为当前目录。为了安全起见，请考虑将 --tmp-dir 设置为易失性存储介质，例如 /dev/shm（如果可用），以防止用户名/密码文件接触硬盘驱动器。 2、脚本中可以使用变量 bytes_received： bytes_sent： client_connect_config_file： 3、可支持内联文件内容的参数 对于客户端或服务端配置文件中相关参数指定的文件，可以直接将文件内容直接写在配置文件。以下是支持的参数。格式为：文件内容 --ca, --cert, --dh, --extra-certs, --key, --pkcs12, --secret, --crl-verify, --http-proxy-user-pass, --tls-auth, --auth-gen-token-secret, --tls-crypt, --tls-crypt-v2 4、状态信息格式 --status-version n可以指定在管理端口和status日志文件中显示用户信息数据的格式，默认以逗号分割。n可以是1(默认)、2、3。1 只会显示前四个。2显示下表中的所有字段，但多了以CLIENT_LIST开头的字段。3显示的字段与 2一样，只不过是以tab制表符分隔。 CLIENT_LIST Common Name Real Address Virtual Address Virtual IPv6 Address Bytes Received Bytes Sent Connected Since Connected Since (time_t) Username Client ID Peer ID Data Channel Cipher 2、3 级别特有的开头标识 用户名 客户端 IP\\:port 虚拟ipv4地址 虚拟ipv6地址 收到的字节数 发送的字节数 连接时间 连接时间戳 用户名 客户端ID 客户端Peer ID 加密数据通道的密码算法 CLIENT_LIST test 192.168.1.1:51434 10.1.1.2 none 4562 5665465 2024-04-11 13:28:35 1712813315 test 5 0 AES-256-GCM 5、客户端推送额外信息 当客户端配置文件中添加push-peer-info参数时，可向服务器推送以下表格中的信息。例如可根据IV_HWADDR 变量的有无判断客户端配置push-peer-info有没有，进而判断客户端配置有没有被修改。IV_HWADDR变量可以用以表示同一个出口公网下的唯一标识，进而进行其他操作 信息变量 信息内容 IV_HWADDR=\\ 这是客户端的唯一且持久的 ID。字符串值可以是任何最多 64 个字节的可读 ASCII 字符串。 OpenVPN 2.x 和其他一些实现使用客户端接口的 MAC 地址来访问默认网关。如果该字符串是由客户端生成的，则它应该在独立会话中保持一致并保留 IV_SSL=\\ 客户端使用的OpenSSL 版本 IV_PLAT_VER=x.y 操作系统的版本，例如6.1 适用于 Windows 7。 UV_\\=\\ 6、开起管理socket套接字 socket套接字常常用于进程间的通信。有Unix域套接字、TCP套接字、UDP套接字、链路层套接字等等。最常用的是TCP套接字。 OpenVPN 管理接口允许通过 TCP 或 Unix 域套接字从外部程序对 OpenVPN 进行管理控制，可在 OpenVPN 作为客户端或服务器运行时使用。参数management支持以下两种方式： Unix域套接字（推荐） Unix域套接字是比网络套接字轻量且高效的多，因为它不涉及网络通信，不需要监听连接，不需要绑定地址，不需要关心协议类型。 仅限于本地主机不同进程间的通信 创建Unix域套接字后返回两个文件描述符，这两个文件描述符均对套接字可读、可写，从而实现全双工的双向通信。同样的，为了避免使用单个文件描述符同时读、写造成的数据错乱，Unix域套接字也有两个内核空间的buffer。一个buffer用于写，称为recv buffer。一个buffer用于读，称为send buffer，它们统称为socket buffer。 通信过程 用户空间-进程 1 内核空间-写缓存Recv Buffer 用户空间-进程 2 用户空间-进程 1 内核空间-读缓存Send Buffer 用户空间-进程 2 management socket-name unix # 使用--management-client-user和--management-client-group参数对Unix域套接字限制访问 management socket-name unix pw-file # 例如：management /dev/openvpn unix management-psw-file Unix域套接字连接工具socat：socat - UNIX-CONNECT:/dev/openvpn TCP Socket TCP套接字是基于TCP/IP网络协议的套接字 TCP套接字可通过网络用于不同主机之间的进程进行双向通信 通信过程 用户空间-进程1 -----> 内核空间-写缓存Recv Buffer -----> 网卡 ------> 网络 ----> 网卡----> 内核空间-读缓存Send Buffer -----> 用户空间-进程2 用户空间-进程1 management IP port # 使用pw-file对管理socket加密码验证。因为任何可以连接到此 TCP IP端口的用户都将能够管理和控制OpenVPN进程。 # pw-file（如果指定）是密码文件，其中密码必须位于第一行。它可以使用关键字 stdin 代替文件名，将在 OpenVPN 启动时提示用户输入密码。 # 建议将 IP 设置为 127.0.0.1 (localhost)，以限制本地客户端对管理服务器的访问 management IP port pw-file # 例如: management 127.0.0.1 25354 management-psw-file TCP连接工具Telnet：telnet 127.0.0.1 25354 Socket套接字中的管理命令 auth-retry t : Auth failure retry mode (none,interact,nointeract). bytecount n : Show bytes in/out, update every n secs (0=off). echo [on|off] [N|all] : Like log, but only show messages in echo buffer. exit|quit : 退出当前会话 forget-passwords : Forget passwords entered so far. help : 打印帮助信息 hold [on|off|release] : Set/show hold flag to on/off state, or release current hold and start tunnel. kill cn : 杀掉通用名为cn的客户端 kill IP:port : 杀掉来自指定ip和端口的客户端。 load-stats : 显示全局状态信息 log [on|off] [N|all] : 打开/关闭时实的日志显示 + 显示最后N条或者'所有' 历史日志. mute [n] : Set log mute level to n, or show level if n is absent. needok type action : Enter confirmation for NEED-OK request of 'type',where action = 'ok' or 'cancel'. needstr type action : Enter confirmation for NEED-STR request of 'type',where action is reply string. net : (Windows only) Show network info and routing table. password type p : Enter password p for a queried OpenVPN password. remote type [host port] : Override remote directive, type=ACCEPT|MOD|SKIP. proxy type [host port flags] : Enter dynamic proxy server info. pid : 显示openvpn的进程号 pkcs11-id-count : Get number of available PKCS#11 identities. pkcs11-id-get index : Get PKCS#11 identity at index. client-auth CID KID : Authenticate client-id/key-id CID/KID (MULTILINE) client-auth-nt CID KID : Authenticate client-id/key-id CID/KID client-deny CID KID R [CR] : Deny auth client-id/key-id CID/KID with log reason text R and optional client reason text CR client-kill CID [M] : Kill client instance CID with message M (def=RESTART) env-filter [level] : Set env-var filter level client-pf CID : Define packet filter for client CID (MULTILINE) rsa-sig : Enter an RSA signature in response to >RSA_SIGN challenge Enter signature base64 on subsequent lines followed by END certificate : Enter a client certificate in response to >NEED-CERT challenge Enter certificate base64 on subsequent lines followed by END signal s : 发送信号给openvpn进程, s = SIGHUP|SIGTERM|SIGUSR1|SIGUSR2. SIGUSR1 – 有条件的重启，非root用户重启OpenVPN进程 SIGHUP – 重启 SIGUSR2 – 输出连接状态到log文件或者系统log SIGTERM, SIGINT – 退出 state [on|off] [N|all] : 跟log一样,但是静态显示。 status [n] : 显示现在进程的状态信息。格式：#n. test n : Produce n lines of output for testing/debugging. username type u : Enter username u for a queried OpenVPN username. verb [n] : Set log verbosity level to n, or show if n is absent. version : 显示openvpn版本信息 https://openvpn.net/community-resources/reference-manual-for-openvpn-2-5/management-interface/ 五、生成服务端证书 openvpn服务端参数 作用 备注 ca ca.crt 指定证书颁发机构（CA）的证书文件。这个文件包含CA的公钥，用于验证客户端证书的有效性。 cert server.crt 指定OpenVPN服务器的证书文件。这个证书用于验证服务器的身份。 key server.key 指定OpenVPN服务器的私钥文件。这个私钥用于对通信进行加密和解密 dh dh.pem 指定Diffie-Hellman参数文件。这个文件包含Diffie-Hellman密钥交换的参数，用于生成加密密钥。 tls-crypt tc.key 指定TLS加密密钥文件。这个文件用于对控制通道进行加密，增强安全性。 crl-verify crl.pem 指定证书吊销列表（CRL）文件。这个文件包含已被CA吊销的证书列表，用于在连接建立时验证证书的有效性。 该文件为吊销证书的名单，配合index.txt识别客户端是否可用。若未在配置文件中配置该行，则即使使用指令注销客户端后仍可以正常连接。 查看crl.pem中哪些证书被吊销 openssl crl -in crl.pem -text -noout easyrsa revoke ${Serial Number} Index.txt文件格式 V 331126083443Z D14FCBE7DBAF76752D36DABE081D2B9F unknown /CN=jian V 331208070531Z FC6AD1BDE3E8DABF3BC49B4A26C68658 unknown /CN=ya V 331210055718Z BC3E6327A9FBCDBBE9EB4C1032DFF15A unknown /CN=zha V 331216013445Z 7106760A35D7752E65AFC222E7B35060 unknown /CN=xue V 331223031425Z 4F13B8A7C11EE6397B59A6F493A92A0D unknown /CN=fan V 331226093347Z 68903DC7E48730D71E07941E5B736655 unknown /CN=luw R 331231023015Z 240124065851Z 60E9E3C0C86064E8614D211B037136A1 unknown /CN=chen V 331231060110Z 246B65CD91F3FF17BCCE337668A423C5 unknown /CN=maqi V 340109020252Z 09AABA5CAAD15A795ECD7A7BEBA0B58F unknown /CN=taow V 340112095201Z 90497B799E239AB60CD7E71AD8F65ACD unknown /CN=che V 340127035810Z EF6E5E38C62155CCE03FB1D2D4BAA350 unknown /CN=houl 第一列字段表示：证书的状态 V：有效（Valid） R：吊销（Revoked） E：过期（Expired） S：挂起（Suspended） C：已撤销（Ceased） 第二列字段表示：证书的序列号，通常为十六进制格式。 第三列字段表示：证书的吊销日期 查询Index.txt记录的用户名 tail -n +2 index.txt | grep \"^V\" | cut -d '=' -f 2 /opt/easyrsa3/server/pki/index.txt文件 可理解为openvpn 客户端的数据库，所有生成的openvpn 客户端包括可用的以及注销的都会在这里看到记录。文件中通过第一列标志识别是否为注销状态，V为可用，R为注销。 六、客户端连接配置 不管是在Synology还是ESXI上安装的OpenVPN Server，都提供下载配置文件的连接。下载好配置文件后，可直接使用各个平台下的客户端直接导入打开 官方提供了各种平台下的客户端程序并提供了对应的文档说明 各客户端官方文档：https://openvpn.net/vpn-server-resources/connecting/ MacOS客户端tunnelblick MacOS上有好多客户端可以连接OpenVPN，功能大同小异。同时官方也有自己的macOS客户端OpenVPN Connect Client。但是推荐tunnelblick（官方也推荐），可同时连接多个OpenVPN Server 官方客户端文档：https://openvpn.net/vpn-server-resources/connecting-to-access-server-with-macos/ OpenVPN Connect Client for MacOS下载地址：https://openvpn.net/downloads/openvpn-connect-v3-macos.dmg Tunnelblick下载地址：https://github.com/Tunnelblick/Tunnelblick/releases Windows客户端OpenVPN GUI OpenVPN官网提供Windows平台客户端OpenVPN GUI。 只需将配置文件放在C:\\Users\\当前用户\\OpenVPN\\config文件下即可。(~\\OpenVPN\\config需手动创建) 官方客户端一次只能连一个服务端，如果有连接多个服务端的话，需要来回切换。 官方文档：https://openvpn.net/vpn-server-resources/connecting-to-access-server-with-windows/ 下载地址：https://openvpn.net/community-downloads/ Android 安卓手机平台官方虽说也提供客户端，但是只能在Google Play Store商店中下载，同时还一次只能连一个服务端。所以我们只好使用第三方客户端ics-openvpn GitHub地址：https://github.com/schwabe/ics-openvpn APK下载地址：http://plai.de/android/ IOS 对于Apple IOS手机客户端，官方APP名为OpenVPN Connect。而且一次只能连一个服务端。同时国内App Store还下不到。你说气不气。其他第三方客户端大多收费。幸好手机不是Iphone。这个就不管了！ Linux OpenVPN协议不是Linux内置的协议。因此，需要一个客户端程序，该程序可以处理捕获OpenVPN隧道发送的流量，并将其加密并将其传递给OpenVPN服务器。当然，反之亦然，解密返回的流量。因此，需要一个客户端程序。在大多数Linux发行版中该软件包简称为 openvpn(OpenVPN 服务端的程序包为 openvpnas或 openvpn-as)。 # CentOS yum install -y openvpn # Ubuntu apt-get install -y openvpn openvpn支持同时连接多个OpenVPN服务器，并且还带有一个服务组件，该组件可以自动和静默地启动在/etc/openvpn中找到的任何自动登录配置文件。可以将该服务组件设置为使用Linux发行版中提供的工具在启动时自动启动。在Ubuntu和Debian上，当您安装 openvpn软件包时，它会自动配置为在引导时启动。将client.ovpn配置文件放在 /etc/openvpn/中并重命名该文件。它必须以.conf结尾 作为文件扩展名。确保重新启动后可以运行服务守护程序，然后再重新启动系统即可。自动登录类型配置文件将自动被提取，并且连接将自动启动。您可以通过检查例如ifconfig命令的输出来验证这一点 ，然后您将在列表中看到 tun0网络适配器。 手动指定配置文件： openvpn --config client.ovpn --auth-user-pass --daemon 命令行客户端缺少的一项主要功能是能够自动实现VPN服务器推送的DNS服务器，但是需要您安装DNS管理程序，例如resolvconf或openresolv，并且它可能与操作系统中的现有网络管理软件冲突，也可能不冲突。但在Ubuntu和Debian上，openvpn软件包随附了 /etc/openvpn/update-resolv-conf 脚本，该脚本处理这些操作系统的DNS实现。只需要在客户端配置文件中设置连接建立断开时执行它。 # 编辑客户端配置文件 vi client.ovpn script-security 2 # 设置执行额外的脚本 up /etc/openvpn/update-resolv-conf # 设置在连接建立时要执行的脚本路径 down /etc/openvpn/update-resolv-conf # 设置在连接断开时要执行的脚本路径 六、openvpn功能设置 1、分配指定IP地址给客户端用户 ①OpenVPN服务端配置文件添加 client-config-dir ccd ②新建ccd目录及客户端文件 新建ccd目录，在ccd目录下新建以用户名命名的文件。并且通过ifconfig-push分配地址，注意这里需要分配两个地址，一个是客户端本地地址，另一个是服务器的ip端点。 mkdir ccd echo ”ifconfig-push 10.8.0.9 10.8.0.10\" >> ccd/vpn_test_user 每个端点的IP地址对的最后8位字节必须取自下面的集合 [1, 2] [5, 6] [9, 10] [13, 14] [17, 18] [21, 22] [25, 26] [29, 30] [33, 34] [37, 38] [41, 42] [45, 46] [49, 50] [53, 54] [57, 58] [61, 62] [65, 66] [69, 70] [73, 74] [77, 78] [81, 82] [85, 86] [89, 90] [93, 94] [97, 98] [101,102] [105,106] [109,110] [113,114] [117,118] [121,122] [125,126] [129,130] [133,134] [137,138] [141,142] [145,146] [149,150] [153,154] [157,158] [161,162] [165,166] [169,170] [173,174] [177,178] [181,182] [185,186] [189,190] [193,194] [197,198] [201,202] [205,206] [209,210] [213,214] [217,218] [221,222] [225,226] [229,230] [233,234] [237,238] [241,242] [245,246] [249,250] [253,254] 客户端连接验证地址分配 utun2: flags=8051 mtu 1500 inet 10.8.0.9 --> 10.8.0.10/32 utun2 2、设置用户名密码加证书的方式登录认证 ①增加openvpn服务端配置 在/etc/openvpn/server/server.conf中追加一下内容 # ....省略 auth-user-pass-verify /etc/openvpn/server/checkpsw.sh via-env username-as-common-name script-security 3 client-config-dir ccd ②创建用户名密码验证脚本 /etc/openvpn/server/checkpsw.sh #!/bin/sh PASSFILE=\"/etc/openvpn/server/psw-file\" LOG_FILE=\"/etc/openvpn/server/logs/openvpn-all-$(date \"+%Y-%m-%d\").log\" TIME_STAMP=`date \"+%Y-%m-%d %T\"` swap_seconds () { SEC=$1 [ \"$SEC\" -le 60 ] && echo \"$SEC秒\" [ \"$SEC\" -gt 60 ] && [ \"$SEC\" -le 3600 ] && echo \"$(( SEC / 60 ))分钟$(( SEC % 60 ))秒\" [ \"$SEC\" -gt 3600 ] && echo \"$(( SEC / 3600 ))小时$(( (SEC % 3600) / 60 ))分钟$(( (SEC % 3600) % 60 ))秒\" } if [ $script_type = 'user-pass-verify' ] ; then if [ ! -r \"${PASSFILE}\" ]; then echo \"${TIME_STAMP}: Could not open password file \"${PASSFILE}\" for reading.\" >> ${LOG_FILE} exit 1 fi CORRECT_PASSWORD=`cat ${PASSFILE} | grep -w \"${username}\" | awk '{print $3}' ` userchinesename=`cat ${PASSFILE} | grep -w \"${username}\" | awk '{print $1}' ` if [ \"${CORRECT_PASSWORD}\" = \"\" ]; then echo \"${TIME_STAMP}: User does not exist: username=\"${username}\", password=\"${password}\".\" >> ${LOG_FILE} exit 1 fi if [ \"${password}\" = \"${CORRECT_PASSWORD}\" ]; then echo \"${TIME_STAMP}: ${username}:${userchinesename} 成功通过了密码验证. \" >> ${LOG_FILE} exit 0 fi echo \"${TIME_STAMP}: Incorrect password: username=\"${username}\", password=\"${password}\".\" >> ${LOG_FILE} exit 1 fi case \"$IV_PLAT\" in os ) device_type=ios ;; win ) device_type=Windows ;; linux ) device_type=Linux ;; solaris ) device_type=Solaris ;; openbsd ) device_type=OpenBSD ;; mac ) device_type=Mac ;; netbsd ) device_type=NetBSD ;; freebsd ) device_type=FreeBSD ;; * ) device_type=None ;; esac if [ $script_type = 'client-connect' ] ; then echo \"${TIME_STAMP}: $common_name 连接了OpenVPN. 设备: $device_type IP端口: $trusted_ip:$trusted_port 端对端IP: $ifconfig_pool_remote_ip $ifconfig_local\" >> ${LOG_FILE} fi if [ $script_type = 'client-disconnect' ]; then duration_time=`swap_seconds $time_duration` echo \"${TIME_STAMP}: $common_name 断开了OpenVPN. 设备: $device_type IP端口: $trusted_ip:$trusted_port 端对端IP: $ifconfig_pool_remote_ip $ifconfig_local 持续时间: $duration_time \" >> ${LOG_FILE} fi ③创建用户密码文件 新增/etc/openvpn/server/psw-file # 一行一个账号 用户中文 用户名 密码 $ chmod 400 /etc/openvpn/server/psw-file $ chown nobody.nobody /etc/openvpn/server/psw-file ④(可选)客户端openvpn配置文件追加配置 auth-user-pass 3、使用iptables限制用户的访问 在客户端连接到openvpn服务端后，针对哪些客户端用户可以访问哪些网段的服务，一般是使用openvpn服务端所在服务器的iptables进行控制。有以下两种重要的常见场景都是使用iptables进行实现的： ①场景一：作为局域网的网络入口跳板机，SNAT转发流量到其他内网服务器 例如在一些公有云的服务器，由于公网IP太贵，不可能给每一台服务都分配，同时也不安全。只要给安装openvpn的服务器分配一个公网IP地址，然后就可以使用iptables的SNAT功能进行网络流量转发，就能访问openvpn所在内网其他服务器上的服务 第一步：VPN服务器设置iptables进行SNAT流量转发 iptables -t nat -A POSTROUTING -s 10.6.8.0/24 -d 192.168.1.0/24 -j SNAT --to 192.168.1.2 # 上述配置通俗地解释为: # 所有分配了IP地址为10.8.6.0/24的客户端用户要想访问192.168.1.0/24的地址，都将其访问网络包中的源地址转换为192.168.1.2，这样用户的访问流量将以192.168.1.2的名义发出。进而能让192.168.1.2能访问的网段主机，客户端也能访问 第二步：配置vpn服务端给客户端推送路由 在/etc/openvpn/server/server.conf中添加 push \"route 192.168.1.0 255.255.255.0\" 这样客户端就会在连接到vpn服务端后自动在本机路由表中添加一条路由 $ ip a # ...省略... utun3: flags=8051 mtu 1500 inet 10.6.8.5 --> 10.6.8.6/32 utun3 $ ip route show # ...省略... 192.168.1.0/24 via 10.6.8.6 dev utun3 ②场景二：细分指定用户只能访问特定的服务 默认配置下，所有客户端都可以访问服务端配置中的指定网络段。但是在实际使用场景中，需要限制指定客户端访问指定网络，限制其访问某些服务。例如：只允许开发人员访问开发网络段中的服务器，测试人员只能访问测试网络段的服务器资源等等。 iptables -t nat -A POSTROUTING -s 10.6.8.0/24 ! -d 10.6.8.0/24 -j SNAT --to 192.168.1.2 iptables -I INPUT -s 10.6.8.166/32 -d 192.168.1.0/24 -j ACCEPT iptables -I INPUT -s 10.6.8.0/24 -d 192.168.1.5-192.168.1.6 ! --dport 22 -j ACCEPT iptables -I FORWARD -p tcp -s 10.6.8.0/24 -d 192.168.1.7 ! --destination-port 6443 -j DROP 4、iptables规则的维护 ①查看规则 以number的方式查看规则，一条一条的出来，然后我们根据号码来删除哪一条规则 iptables -L FORWARD --line-numbers iptables -L INPUT --line-numbers # 查看POSTROUTING链nat表中的规则 iptables -L POSTROUTING -t nat ②删除指定的规则 iptables -D FORWARD 1 #删除指定链指定表中的规则 iptables -D POSTROUTING -t nat -s 10.8.6.0/24 ! -d 10.8.6.0/24 -j SNAT --to 192.168.1.2 ③删除所有规则 iptables -F 5、客户端连接状态钉钉通知 openvpn服务端配置文件/etc/openvpn/server/server.conf中追加以下内容，然后重启openvpn服务。 -----省略------- auth-user-pass-verify openvpn-utils.sh via-env -----省略------- client-connect openvpn-utils.sh client-disconnect openvpn-utils.sh 此时客户连接或断开了openvpn服务端都会执行对应的脚本，可在脚本中通过curl命令发送信息到对应webhook的钉钉机器人。由于已经有一个用于验证用户名密码的脚本(/etc/openvpn/server/server.conf中的auth-user-pass-verify openvpn-utils.sh via-env)，可在其中通过判断调用脚本时的openvpn指令类型来添加功能。示例： 在/etc/openvpn/server/openvpn-utils.sh添加用于发送钉钉通知的逻辑。至于脚本中要使用到的变量，当客户端连接或断开进而触发执行脚本时会将此时客户端所处的信息放至进程作用域的环境变量中，可以直接在脚本中引用。具体哪些变量可用，参考：https://openvpn.net/community-resources/reference-manual-for-openvpn-2-4/ (由于篇幅较长，网页搜索“environmental variables”)。 注意有些环境变量只有在对应指令调用时才有。 #!/bin/sh PASSFILE=\"/etc/openvpn/server/psw-file\" LOG_FILE=\"/etc/openvpn/server/openvpn-authorized.log\" TIME_STAMP=`date \"+%Y-%m-%d %T\"` Ding_Webhook_Token= Ding_Webhook=\"https://oapi.dingtalk.com/robot/send?access_token=\"$Ding_Webhook_Token swap_seconds () { SEC=$1 [ \"$SEC\" -le 60 ] && echo \"$SEC秒\" [ \"$SEC\" -gt 60 ] && [ \"$SEC\" -le 3600 ] && echo \"$(( SEC / 60 ))分钟$(( SEC % 60 ))秒\" [ \"$SEC\" -gt 3600 ] && echo \"$(( SEC / 3600 ))小时$(( (SEC % 3600) / 60 ))分钟$(( (SEC % 3600) % 60 ))秒\" } if [ $script_type = 'user-pass-verify' ] ; then if [ ! -r \"${PASSFILE}\" ]; then echo \"${TIME_STAMP}: Could not open password file \"${PASSFILE}\" for reading.\" >> ${LOG_FILE} exit 1 fi CORRECT_PASSWORD=`awk '!/^;/&&!/^#/&&$1==\"'${username}'\"{print $2;exit}' ${PASSFILE}` if [ \"${CORRECT_PASSWORD}\" = \"\" ]; then echo \"${TIME_STAMP}: User does not exist: username=\"${username}\", password=\"${password}\".\" >> ${LOG_FILE} exit 1 fi if [ \"${password}\" = \"${CORRECT_PASSWORD}\" ]; then echo \"${TIME_STAMP}: Successful authentication: username=\"${username}\".\" >> ${LOG_FILE} exit 0 fi echo \"${TIME_STAMP}: Incorrect password: username=\"${username}\", password=\"${password}\".\" >> ${LOG_FILE} exit 1 fi if [ $script_type = 'client-connect' ] ; then curl -s \"$Ding_Webhook\" \\ -H 'Content-Type: application/json' \\ -d ' { \"msgtype\": \"markdown\", \"markdown\": { \"title\": \"'$common_name'连接到了OpenVPN\", \"text\": \"## '$common_name'连接到了OpenVPN\\n> #### **连接时间**: '\"$TIME_STAMP\"'\\n> #### **IP + 端口**: '$trusted_ip':'$trusted_port'\\n> #### **端对端IP**: '$ifconfig_pool_remote_ip' '$ifconfig_local'\" }, \"at\": { \"isAtAll\": true } }' fi if [ $script_type = 'client-disconnect' ]; then duration_time=`swap_seconds $time_duration` curl -s \"$Ding_Webhook\" \\ -H 'Content-Type: application/json' \\ -d ' { \"msgtype\": \"markdown\", \"markdown\": { \"title\": \"'$common_name'断开了OpenVPN\", \"text\": \"## '$common_name'断开了OpenVPN\\n> #### **断开时间**: '\"$TIME_STAMP\"'\\n> #### **IP + 端口**: '$trusted_ip':'$trusted_port'\\n> #### **端对端IP**: '$ifconfig_pool_remote_ip' '$ifconfig_local'\\n> #### **持续时间**: '$duration_time'\" }, \"at\": { \"isAtAll\": true } }' fi 6、客户端间进行互联 如果需要客户端进行互联，openvpn服务端配置文件/etc/openvpn/server/server.conf中追加以下内容，然后重启openvpn服务。 client-to-client 此时，openvpn服务端等同路由器在维护路由表。客户端之间就可以通过各自分配的虚拟IP进行通信了。比如有两个客户端, 各自使用不同的网络途径连接到服务端，各自分配VIP地址分别为10.8.0.6和10.8.0.10。此时两者就可以通过虚拟IP进行通信了 注意 在客户端可能会在TUN设备上看到两个地址, 其中一个是客户端面的VIP, 另外一个是网关. 比如下例中 utun3: flags=8051 mtu 1500 inet 10.8.0.6 --> 10.8.0.5/32 utun3 10.8.0.6是VIP, 而10.8.0.5是网关. 在互联时, 需要使用VIP而不是网关地址. 七、问题总结 1、Windows客户端OpenVPN GUI问题总结 ①Windows下使用Tun虚拟网络设备 当OpenVPN Server服务端已配置使用TUN进行连接时，在Windows下OpenVPN GUI客户端安装时安装的设备是Win-tun。所以客户端配置文件需要修改 dev tun windows-driver wintun ②手动添加路由 在OpenVPN客户端配置文件中添加以下配置 route 172.18.1.55 255.255.255.255 gateway ③CMD下OpenVPN GUI的虚拟网络设备管理 # 查看网络虚拟设备 openvpn --show-adapters # 查看网络虚拟设备的详细信息 tapinstall.exe hwids # 卸载网络虚拟设备 tapinstall.exe remove # 安装网络虚拟设备 tapinstall.exe install # 更新网络虚拟设备 tapinstall.exe update # 指的是 OpenVPN 2.2+ 的 tap0901 驱动程序标识符，但在较旧/较新的 OpenVPN 版本中可能不同。 # 通常是 OemWin2k.inf（旧的抽头驱动程序）或 OemVista.inf（新的抽头驱动程序）。您需要指定此文件的完整路径，例如 C:\\Program Files\\TAP-Windows\\driver\\OemVista.inf。 参考：https://community.openvpn.net/openvpn/wiki/ManagingWindowsTAPDrivers 2、客户端连接之后无法添加路由 表象：客户端连接之后，没有分配到IP地址。日志报错无法添加路由。内网中的服务无法访问 排查： 排查OpenVPN服务端配置文件 server 192.168.10.0 255.255.255.0 排查OpenVPN服务端日志/etc/openvpn/openvpn.log显示 ULTI: no free --ifconfig-pool addresses are available MULTI: no dynamic or static remote --ifconfig address is available for sumy /192.168.10.88:6458 原因： 客户端达到上限，服务器端IP地址的IP被分配完了。 每个用户会占用4个ip位，导致一个网段只能有60多个用户同时连接 解决： 将网段扩大，改成server 192.168.10.0 255.255.0.0 修改将iptables中的规则 参考： https://forums.openvpn.net/viewtopic.php?t=18869 https://blog.51cto.com/wenni/2477426 3、OpenVPN 2.3版本不支持的参数 表象：客户端连接时无法通过TLS握手建立连接 原因：block-outside-dns和 tls-crypt 参数只在OpenVPN 2.4.x 解决：升级客户端版本 参考：https://unix.stackexchange.com/questions/548980/openvpn-missing-parameter-tls-crypt-config-problem 附录 1. OpenSSL的证书管理文件index.txt格式 OpenSSL的index.txt文件是一个包含证书信息的文本文件，通常用于证书管理。每一行表示一个证书条目，包含以下字段： 状态（Status）：表示证书的状态，通常有以下几种值： V：有效（Valid） R：吊销（Revoked） E：过期（Expired） S：挂起（Suspended） C：已撤销（Ceased） 序列号（Serial Number）：证书的序列号，通常为十六进制格式。 发布日期（Revocation Date）：证书的吊销日期，格式为YYMMDDHHMMSSZ。 撤销原因（Revocation Reason）：证书的吊销原因，可以是以下几种之一： keyCompromise：密钥被泄露或者威胁到了密钥的安全性 CACompromise：证书颁发机构的私钥被泄露或者威胁到了私钥的安全性 affiliationChanged：证书的持有者隶属关系发生了变化 superseded：证书被新证书替代 cessationOfOperation：证书的持有者停止了运营 certificateHold：证书被暂时挂起 removeFromCRL：从撤销列表中移除 撤销日期（Last Update Date）：最后更新的日期，格式为YYMMDDHHMMSSZ。 证书主题（Certificate Subject）：证书的主题。 证书颁发者（Certificate Issuer）：颁发证书的机构。 文件名（Filename）：证书文件的文件名。 2. easyrsa3的证书管理文件index.txt格式 状态（Status）：表示证书的状态，通常有以下几种值： V：有效（Valid） R：吊销（Revoked） E：过期（Expired） U：未知（Unknown） 生效日期（Effective Date）：证书的生效日期，格式为YYYY-MM-DD HH:MM:SS。 过期日期（Expiration Date）：证书的过期日期，格式为YYYY-MM-DD HH:MM:SS。 撤销日期（Revocation Date）：证书的吊销日期，格式为YYYY-MM-DD HH:MM:SS。 序列号（Serial Number）：证书的序列号，通常为十六进制格式。 撤销原因（Revocation Reason）：证书的吊销原因，与OpenSSL的格式相同。 证书主题（Certificate Subject）：证书的主题。 证书颁发者（Certificate Issuer）：颁发证书的机构。 文件名（Filename）：证书文件的文件名。 参考 http://www.fblinux.com/?p=1181 http://www.linuxfly.org/post/86/ https://www.aikaiyuan.com/11839.html https://www.hotbak.net/key/32f744eec5330289d21a981ecf2d595a_29.html https://lesca.me/archives/iptables-examples.html https://blog.51cto.com/90xpy/5745957 https://blog.csdn.net/weixin_43690636/article/details/125635215 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-05-07 17:07:08 "},"origin/idrac.html":{"url":"origin/idrac.html","title":"iDRAC","keywords":"","body":"Dell服务器远程管理卡iDRAC 一、简介 iDRAC又称为Integrated Dell Remote Access Controller，也就是集成戴尔远程控制卡，这是戴尔服务器的独有功能 iDRAC卡相当于是附加在服务器上的一台独立运行的小型pc，通过与服务器主板上的管理芯片BMC进行通信，监控与管理服务器的硬件状态信息。它拥有自己的系统和IP地址，与服务器上的OS无关，是服务器管理员进行远程访问和管理的便利工具。 iDRAC分为 Express和Enterprise 两种版本，iDRAC Express 默认是用服务器1号网口来连接， iDRAC Enterprise默认使用专用网口连接。 二、Web页面 在Dell 的板载iDRAC控制 默认用户名密码：root / calvin （登录后最好修改密码） 三、命令行CLI show [] [] [] [== ] set [] [] = cd [] [] create [] [=] [=] delete [] exit [] reset [] [] start [] [] stop [] [] version [] help [] [] load -source [] [] dump -destination [] [] Racadm help [subcommand] -- display usage summary for a subcommand arp -- display the networking ARP table clearasrscreen -- clear the last ASR (crash) screen closessn -- close a session clrraclog -- clear the RAC log clrsel -- clear the System Event Log (SEL) config -- modify RAC configuration properties coredump -- display the last RAC coredump coredumpdelete -- delete the last RAC coredump fwupdate -- update the RAC firmware getconfig -- display RAC configuration properties getled -- Get the state of the LED on a module. getniccfg -- display current network settings getraclog -- display the RAC log getractime -- display the current RAC time getsel -- display records from the System Event Log (SEL) getssninfo -- display session information getsvctag -- display service tag information getsysinfo -- display general RAC and system information gettracelog -- display the RAC diagnostic trace log getversion -- Display the current version details getuscversion -- display the current USC version details ifconfig -- display network interface information netstat -- display routing table and network statistics ping -- send ICMP echo packets on the network ping6 -- send ICMP echo packets on the network racdump -- display RAC diagnostic information racreset -- perform a RAC reset operation racresetcfg -- restore the RAC configuration to factory defaults remoteimage -- make a remote ISO image available to the server serveraction -- perform system power management operations setniccfg -- modify network configuration properties setled -- Set the state of the LED on a module. sshpkauth -- manage SSH PK authentication keys on the RAC sslcertview -- view SSL certificate information sslcsrgen -- generate a certificate CSR from the RAC sslEncryptionStrength -- Display or modify the SSL Encryption strength. sslresetcfg -- resets the web certificate to default and restarts the web server. testemail -- test RAC e-mail notifications testkmsconnectivity -- test KMSConnectivity testtrap -- test RAC SNMP trap notifications usercertview -- view user certificate information 1、服务管理 racadm getconfig -g cfgracTuning # （cfgRacTuneWebserverEnable 卡的WEB 服务未启动，0：表示未启动；1： 表示启动） racadm config -g cfgRacTuning -o cfgRacTuneWebServerEnable 1 服务开启成功 2、会话管理 # 获取当前活动的会话 racadm getssninfo # 关闭所有会话。 racadm closessn -a # 关闭特定用户的所有会话 racadm closessn -u # 或者在Web的\"iDRAC设置\" ---> \"会话管理\"中进行管理 四、客户端racadm Docker客户端： https://hub.docker.com/r/prabhakarpujeri/racadm-docker/tags?page=1&ordering=last_updated https://github.com/prabhakarpujeri/racadm-docker/blob/master/Dockerfile 参考 https://thornelabs.net/posts/dell-idrac-racadm-commands-and-scripts.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/vsphere-exsi.html":{"url":"origin/vsphere-exsi.html","title":"vSphere ESXI","keywords":"","body":"vSphere 一、ESXI简介 VMware vSphere是VMware的服务器虚拟化软件套件，vSphere 中的核心组件为 VMware ESXi（取代原ESX），ESXi是一款可以独立安装和运行在祼机上的系统，因此与他我们以往见过的VMwareWorkstation 软件不同的是它不再依存于宿主操作系统之上。它是裸机（bare-metal）虚拟化管理程序（hypervisor），用于创建和管理虚拟机（VM）。ESXi直接安装在服务器硬件上，不依赖底层操作系统，使其性能和效率更高。 二、硬件兼容性查询 对于老硬件、不知名硬件的主机。在安装 ESXI 前，可以在以下网址查询硬件是否支持，能支持到什么版本。 查询条件可根据硬件设备类型，硬件厂商等 1、硬件兼容性 CPU兼容性：https://www.vmware.com/resources/compatibility/search.php?deviceCategory=cpu IO设备：https://www.vmware.com/resources/compatibility/search.php?deviceCategory=io GPU直通：https://www.vmware.com/resources/compatibility/search.php?deviceCategory=vmdirect ...... 2、已知不兼容硬件 主板自带的螃蟹卡网卡 老nvme版本协议的 SSD硬盘 老协议NVME驱动在6.7 U2被砍掉了，7.0以上版本砍掉了大量老协议规范及驱动 ESXI支持的NVME协议最低是1.3 参考： https://williamlam.com/2019/05/quick-tip-crucial-nvme-ssd-not-recognized-by-esxi-6-7.html https://www.pcjsb.com/archives/esxi6.7-7.0he-8.0wu-fa-shi-bie-lao-xie-yi-nvmegu-tai-ying-pan-de-jie-jue-fang-fa 三、ESXI安装 1、刻录ESXI ISO文件到U盘 2、开起主板的虚拟化配置 3、从U盘启动 4、进入安装程序 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-27 11:07:21 "},"origin/vsphere-esxi.html":{"url":"origin/vsphere-esxi.html","title":"ESXI 管理常用命令","keywords":"","body":"一、ESXI 管理常用命令 1、esxcli 获取基础信息 vmware -v # 看你的esx版本 VMware ESXi 5.0.0 build-469512 esxcfg-info -a # 显示所有ESX相关信息 esxcfg-info -w # 显示esx上硬件信息 service mgmt-vmware restart # 重新启动vmware服务 esxcfg-vmknic -l # 查看宿主机IP地址 esxcli hardware cpu list # cpu信息 Brand，Core Speed， esxcli hardware cpu global get # cpu信息 （CPU Cores） esxcli hardware memory get # 内存信息 内存 Physical Memory esxcli hardware platform get # 硬件型号，供应商等信息,主机型号,Product Name 供应商,Vendor Name esxcli hardware clock get # 当前时间 esxcli system version get # 查看ESXi主机版本号和build号 esxcli system maintenanceMode set --enable yes # 将ESXi主机进入到维护模式 esxcli system maintenanceMode set --enable no # 将ESXi主机退出维护模式 esxcli system settings advanced list -d # 列出ESXi主机上被改动过的高级设定选项 esxcli system settings kernel list -d # 列出ESXi主机上被变动过的kernel设定部分 esxcli system snmp get | hash | set | test # 列出、测试和更改SNMP设定 esxcli vm process list # 利用esxcli列出ESXi服务器上VMs的World I(运行状态的) esxcli vm process kill -t soft -w WorldI # 利用esxcli命令杀掉VM vim-cmd hostsvc/hostsummary # 查看宿主机摘要信息 vim-cmd vmsvc/get.datastores # 查看宿主存储空间信息 vim-cmd vmsvc/getallvms # 列出所有虚拟机 vim-cmd vmsvc/power.getstate VMI # 查看指定VMI虚拟状态 vim-cmd vmsvc/power.shutdown VMI # 关闭虚拟机 vim-cmd vmsvc/power.off VMI # 如果虚拟机没有关闭，使用poweroff命令 vim-cmd vmsvc/get.config VMI # 查看虚拟机配置信息 esxcli software vib install -d /vmfs/volumes/datastore/patches/xxx.zip # 为ESXi主机安装更新补丁和驱动 esxcli network nic list # 列出当前ESXi主机上所有NICs的状态 esxcli network vm list # 列出虚拟机的网路信息 esxcli storage nmp device list # 理出当前NMP管理下的设备satp和psp信息 esxcli storage core device vaai status get # 列出注册到PS设备的VI状态 esxcli storage nmp satp set --default-psp VMW_PSP_RR --satp xxxx # 利用esxcli命令将缺省psp改成Round Robin 维护模式管理 esxcli system maintenanceMode {cmd} [cmd options] Available Commands: get 获取系统维护状态 set Enable or disable the maintenance mode of the system. -e|--enable 开启维护模式 (必须) -t|--timeout= 多少秒后进入维护模式 (默认0秒) -m|--vsanmode= 在主机进入维护模式(默认ensureObjectAccessibility)之前，VSAN服务必须执行 的操作。允许的值是: ensureObjectAccessibility: 在进入维护模式之前，从磁盘中提取数据以确保虚拟SAN集群中的对象可访问性。 evacuateAllData:在进入维护模式之前，从磁盘中撤离所有数据。 noAction:在进入维护模式之前，不要将虚拟SAN数据移出磁盘。 关机重启管理（必须进入维护模式） esxcli system shutdown {cmd} [cmd options] Available Commands: poweroff 断开电源 -d|--delay= 多少秒后关机，范围在10-4294967295 -r|--reason= 执行该操作的原因 reboot 重启系统 -d|--delay= 多少秒后关机，范围在10-4294967295 -r|--reason= 执行该操作的原因 系统时间管理 esxcli system time set [cmd options] Cmd options: -d|--day= Day -H|--hour= Hour -m|--min= Minute -M|--month= Month -s|--sec= Second -y|--year= Year 查看vswitch接口信息 esxcli network vswitch standard list vSwitch0 Name: vSwitch0 Class: etherswitch Num Ports: 4352 Used Ports: 10 Configured Ports: 128 MTU: 1500 CDP Status: listen Beacon Enabled: false Beacon Interval: 1 Beacon Threshold: 3 Beacon Required By: Uplinks: vmnic0 Portgroups: VM Network, synology-iscsi, Management Network 查看物理网络接口 esxcli network nic list Name PCI Device Driver Admin Status Link Status Speed Duplex MAC Address MTU Description ------ ------------ ------ ------------ ----------- ----- ------ ----------------- ---- --------------------------------------------------------- vmnic0 0000:01:00.0 bnx2 Up Up 100 Full 84:8f:69:e3:e3:98 1500 QLogic Corporation QLogic NetXtreme II BCM5709 1000Base-T vmnic1 0000:01:00.1 bnx2 Up Down 0 Half 84:8f:69:e3:e3:9a 1500 QLogic Corporation QLogic NetXtreme II BCM5709 1000Base-T vmnic2 0000:02:00.0 bnx2 Up Down 0 Half 84:8f:69:e3:e3:9c 1500 QLogic Corporation QLogic NetXtreme II BCM5709 1000Base-T vmnic3 0000:02:00.1 bnx2 Up Down 0 Half 84:8f:69:e3:e3:9e 1500 QLogic Corporation QLogic NetXtreme II BCM5709 1000Base-T 当前运行虚拟机列表 esxcli vm process list k8s118-node1 World ID: 35805 Process ID: 0 VMX Cartel ID: 35804 UUID: 56 4d 7f 25 bf 12 08 6c-21 36 96 29 58 61 80 bc Display Name: k8s118-node1 Config File: /vmfs/volumes/5ad72ff6-920c8d20-3ee7-848f69e3e398/k8s118-node1/k8s118-node1.vmx k8s118-node2 World ID: 35830 Process ID: 0 VMX Cartel ID: 35829 UUID: 56 4d c7 a7 05 ba e8 27-4a cf e3 3c 22 77 e2 27 Display Name: k8s118-node2 Config File: /vmfs/volumes/5ad72ff6-920c8d20-3ee7-848f69e3e398/k8s118-node2/k8s118-node2.vmx 创建datastore 创建NFS类型的Datastore # ESXI安装Synology NFS VAAI 参考附录2。ESXI安装完Synology NFS VAAI后再创建NFS类型Datastore后，会显示Datastore已支持硬件加速 esxcfg-nas -a synology-nfs-datastore -o 192.168.1.7 -s /volume2/ESXI # 删除Datastore esxcfg-nas -d synology-nfs-datastore esxcli storage vmfs extent list 查看卷信息 esxcli storage filesystem list Mount Point Volume Name UUID Mounted Type Size Free ------------------------------------------------- ----------- ----------------------------------- ------- ------ ------------ ------------ /vmfs/volumes/5ad72ff6-920c8d20-3ee7-848f69e3e398 datastore1 5ad72ff6-920c8d20-3ee7-848f69e3e398 true VMFS-5 890131972096 156750577664 /vmfs/volumes/5ad93bed-3b0a5ee0-8d62-848f69e3e398 5ad93bed-3b0a5ee0-8d62-848f69e3e398 true vfat 4293591040 4257939456 /vmfs/volumes/5a613a7b-adc12ea1-59ec-8b00e5327863 5a613a7b-adc12ea1-59ec-8b00e5327863 true vfat 261853184 91848704 /vmfs/volumes/d7f0b67d-93f4d99c-13e2-fc95a98c5631 d7f0b67d-93f4d99c-13e2-fc95a98c5631 true vfat 261853184 91987968 /vmfs/volumes/5ad72ff5-a0c06292-f641-848f69e3e398 5ad72ff5-a0c06292-f641-848f69e3e398 true vfat 299712512 88342528 2、vim-cmd命令 列出所有虚拟机清单 vim-cmd vmsvc/getallvms 查看指定虚拟机设备信息 其中包括网卡型号、MC地址等信息。 vim-cmd vmsvc/device.getdevices 101 查看指定虚拟机配置 vim-cmd vmsvc/get.config 101 查看指定虚拟机摘要信息 vim-cmd vmsvc/get.summary 101 3、其他命令 查看虚拟网卡接口 esxcfg-vmknic -l 二、挂载本地磁盘上VMFS文件系统分区 esxcfg-volume -l |grep \"VMFS UUID/label\" # 会显示当前磁盘分区UUID esxcli storage filesystem list # esxcfg-volume -M UUID # 会将磁盘分区挂载到/vmfs/volumes/UUID下 # -M 重启后依旧会挂载。-m 重启后不会再挂载 应用实例： ​ 四块SAAS硬盘做的raid5。其中一块出现坏块，导致其上的VMWare系统奔溃。将硬盘位置打乱换了以后在开机期间Crtl+ R进入raid工具界面，显示raid正在重建。等重建完成后，还是无法进入VMWare。随后找到一块临时SATA即可的SSD硬盘插入光驱位，然后下载6.7的ESXI刻录到U盘中，将VMWare安装到SSD中。开机进入新的VMWare后，可以看到旧硬盘，分区依旧在，说明数据也在。此时需要将旧硬盘上的VMFS文件系统分区挂载到新的VMWare即可显示VM的数据存储。之后就可以使用各种工具备份导出VM啦，推荐使用群晖上的ABB。 三、ESXI网络抓包工具 https://www.virten.net/2015/10/esxi-network-troubleshooting-with-tcpdump-uw-and-pktcap-uw/ 1、tcpdump-uw 详细文档：https://kb.vmware.com/s/article/1031186 esxcfg-vmknic -l # 或者 esxcli network ip interface list tcpdump-uw -i vmk0 2、pktcap-uw 详细文档：https://kb.vmware.com/s/article/2051814?lang=zh_CN net-stats -l pktcap-uw --vmk vmk0 四、网络防火墙 默认 exsi 主机的 httpclient 规则没有开起。故 esxi主机无法向外部网络发送 HTTP 请求。可登录 esxi开放限制。 1、查看防火墙规则 # 查看所有的防火墙规则集 esxcli network firewall ruleset list # 查看所有的防火墙规则 esxcli network firewall ruleset rule list # 查看防火墙特定规则集的规则 esxcli network firewall ruleset rule list -r 规则集 2、 启用/禁用的防火墙规则集 esxcli network firewall ruleset set -e true -r 规则集 esxcli network firewall ruleset set -e false -r 规则集 3、更新防火墙规则 建议在 UI界面进行更新（增删改） 附录 1、ESXI VAAI 在虚拟化环境中，从资源角度来看，传统上的存储操作非常昂贵。与主机相比，存储设备可以更高效地执行克隆和快照等功能。VMware vSphere存储API阵列集成（VAAI），也称为硬件加速或硬件卸载API，是一组API，用于启用VMware vSphere ESXi主机与存储设备之间的通信。这些API定义了一组“存储原语”，它们使ESXi主机能够将某些存储操作卸载到阵列上，从而减少了ESXi主机上的资源开销，并可以显着提高存储密集型操作（如存储克隆，清零等）的性能。VAAI的目标是帮助存储供应商提供硬件帮助，以加快在存储硬件中更有效地完成的VMware I / O操作。 参考：https://www.vmware.com/techpapers/2012/vmware-vsphere-storage-apis-array-integration-10337.html 2、ESXI安装Synology NFS VAAI 下载地址：https://www.synology.cn/en-global/support/download/DS110+#utilities 安装参看：https://global.download.synology.com/download/www-res/dsm/Tools/NFSVAAIPlugin/README scp synonfs-vaai-plugin.vib root@192.168.1.103:/tmp esxcli software vib install -v /tmp/synonfs-vaai-plugin.vib --no-sig-check # 或 esxcli software vib install -d /tmp/synonfs-vaai-plugin.zip --no-sig-check 重启ESXI # 查看ESXI插件中是否已安装Synology NFS VAAI esxcli software vib list | more # 删除ESXI插件 esxcli software vib remove -n {PLUGIN_NAME} 参考：https://www.jonathanmedd.net/category/nfs Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-27 11:07:04 "},"origin/vSphere-vCenter.html":{"url":"origin/vSphere-vCenter.html","title":"vCenter","keywords":"","body":"vSphere vCenter Server 一、简介 vSphere Client虽然可以管理ESXI，但只能实现一些如创建虚拟机的简单的功能，而vsphere的高级功能都无法实现。更重要的是多台ESXI无法进行统一管理，为了解决这个问题，vsphere开发了一个非常重要的组件vCenter Server vCenter Server是安装在 Window 或 Linux 服务器里，用于管理一个或者多个ESXi服务器的工具。（可以安装在 ESXi 服务器的虚拟机里） vCenter Server Appliance：简称VCSA，是预配置的 Linux 虚拟机，针对在 Linux 上运行vCenter Server 及关联服务进行了优化，从6.0开始其实体形态是个.iso文件，需要在windows桌面上打开，通过配置过程将其安装到ESXi主机上。 二、安装部署 vCenter ISO镜像下载地址：https://my.vmware.com/cn/web/vmware/details?downloadGroup=VC60U3H&productId=491 官方文档：https://docs.vmware.com/cn/VMware-vSphere/6.7/com.vmware.vcenter.install.doc/GUID-11468F6F-0D8C-41B1-82C9-29284630A4FF.html 安装部署参考 https://blog.51cto.com/3701740/2326475 https://blog.csdn.net/weixin_44907813/article/details/99185102 https://blog.51cto.com/14227204/2418905 三、其他 1、找回vCenter 默认用户密码 在以管理员权限打开的CMD中切换至C:\\Program Files\\VMware\\vCenter Server\\vmdird目录下进入后执行vdcadmintool.exe。 提示让你输入 UPN：就是输入你创建时的登录用户名，一般默认为administrator@vSphere.local 输入后回车会给你一个 New password Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/esxi-synology-iscsi.html":{"url":"origin/esxi-synology-iscsi.html","title":"ESXI使用Synology的ISCSI存储","keywords":"","body":"ESXI使用Synology ISCSI存储 一、Synology NAS创建ISCSI存储 参考：iSCSI-简介配置使用 二、vSphere配置ISCSI适配器 参考：https://blog.csdn.net/minxihou/article/details/77233453 三、创建基于ISCSI的存储datastore 参考：https://blog.csdn.net/minxihou/article/details/77233453 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/vsphere-ovf.html":{"url":"origin/vsphere-ovf.html","title":"OVF模板详解","keywords":"","body":"Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/vmware-ovf-tool.html":{"url":"origin/vmware-ovf-tool.html","title":"VMWare OVF Tools","keywords":"","body":"OVF 管理工具VMWare OVF Tool 一、简介 VMWare OVF Tool是 一个可以在VMWare系列产品上导入导出虚拟机为OVF/OVA格式文件的命令行工具。 ESXI强大的客户端vSpere Client由于在MacOS上没有对应的版本。VMWare Fusion在MacOS上有客户端能对远程ESXI进行基本简单的管理操作！但是只能部署客户端本地的OVF/OVA文件，不能部署存储在远程Web服务器上的OVF模板文件到远程ESXI服务器上。在MacOS上，只能通过组合VMware Fusion+VMware OVF Tool+govc来实现Windows下vSphere Client的大部分功能。 官方文档： https://www.vmware.com/support/developer/ovf/ovf410/ovftool-410_userguide.pdf https://vmware.github.io/vic-product/assets/files/html/1.5/vic_vsphere_admin/deploy_vic_appliance_ovftool.html 二、安装配置 下载地址（需注册登录）：https://my.vmware.com/zh/group/vmware/details?downloadGroup=OVFTOOL430&productId=742 MacOS dmg格式安装后的目录为：/Applications/VMware OVF Tool。命令没有系统环境变量中，通过设置软连接实现：ln -s /Applications/VMware\\ OVF\\ Tool/ovftool /usr/local/bin 配置文件 通用型的配置项可以写在配置文件中 注释行以#开头 配置项是ovftool的命令行参数，一行一个配置项 配置文件读取顺序为先全局配置文件~/.ovftool，后读取本地配置文件.ovftool 例如：~/.ovftool # 指定vSphere存储池位置 datastore=datastore1 # 指定磁盘类型 diskMode=thin # 跳过vSphere连接的SSL认证 noSSLVerify # 部署完成后开机 powerOn # 接受所有用户的licenses acceptAllEulas # 指定日志输出级别 X:logLevel=error # 指定日志输出文件 X:logFile=/tmp/ovftool.log # 将日志输出控制台 X:logToConsole # 跳过OVF原数据校验 skipManifestCheck # 输出机器可读日志 machineOutput # 在OVF中注入ENV配置 X:injectOvfEnv # ESXI局域网适配器 network='VM Network=VM Network' 查看所有配置文件的配置项 ovftool --help config # Contents of global configuration options (/Users/curiouser/.ovftool): # datastore=datastore1 # diskMode=thin # noSSLVerify # powerOn # ..... # Currently no local configuration options in .ovftool 三、命令详解 命令格式 ovftool [参数项] 源目标 目的目标 源目标 源目标可以是以下资源 OVF/OVA文件(a local file path, or an HTTP, HTTPS, or FTP URL) 本地虚拟机文件 (.vmx格式) vCenter, ESXi或VMware Server上的vAPP/locatoridentifying a virtual machine A vCloud Director locator identifying a virtual machine or a vApp in vCloud Director. A local file path to a vApprun workspace entity. 目的目标 目的目标可以是以下资源 A local file path for VMX, OVF, OVA, or vApprun workspace. A vSphere locator identifying a cluster, host, or a vSphere location. A vCloud Director locator identifying a virtual machine or a vApp in vCloud Director. vSphere目的目标格式 vi://:@:/ 参数项 --acceptAllEulas : Accept all end-user licenses agreements without being prompted. --allowAllExtraConfig : Whether we allow all the ExtraConfig options. These options are a security risk as they control low-level and potential unsafe options on the VM. --allowExtraConfig : Whether we allow ExtraConfig options. These options are a security risk as they control low-level and potential unsafe options on the VM. --annotation : Add annotation to vi, vmx, vapprun, vCloud, OVF, and OVA source locators --authdPortSource : Use this to override default vmware authd port (902) when using a host as source. --authdPortTarget : Use this to override default vmware authd port (902) when using a host as target. --chunkSize : Specifies the chunk size to use for files in a generated OVF package. The default is not to chunk. The chunk size without unit is assumed to be in megabytes. Accepted units are b, kb, mb, gb; e.g., 2gb or 100kb. --compress : Compress the disks in an OVF package. Value must be between 1 and 9. 1 is the fastest, but gives the worst compression, whereas 9 is the slowest, but gives the best compression. --computerName : Sets the computer name in the guest for a VM using the syntax --computerName:=. Only applies to vCloud targets version 5.5 or newer. --coresPerSocket : Specifies the distribution of the total number of CPUs over a number of virtual sockets using the syntax --coresPerSocket:=. Only applies to vCloud targets version 5.5 or newer. -ds/--datastore : Target datastore name for a VI locator. --decodeBase64 : Decode option values with Base64. --defaultStorageProfile : The storage profile for all VMs in the OVF package. The value should be an SPBM profile ID. Only applies to VI targets version 5.5 or newer. --defaultStorageRawProfile : The storage profile for all VMs in the OVF package. The value should be raw SPBM profile. The value will overwrite that in --defaultStorageProfile. Only applies to VI targets version 5.5 or newer. --deploymentOption : Selects what deployment option to use (if the source OVF package supports multiple options.) --disableVerification : Skip validation of signature and certificate. -dm/--diskMode : Select target disk format. Supported formats are: monolithicSparse, monolithicFlat, twoGbMaxExtentSparse, twoGbMaxExtentFlat, seSparse (VI target), : eagerZeroedThick (VI target), thin (VI target), thick (VI target), sparse, and flat --diskSize : Sets the size of a VM disk in megabytes using the syntax --diskSize:,=. Only applies to vCloud targets version 5.5 or newer. --eula : EULA to be inserted in the first virtual system or virtual system collection in the OVF. If the EULA is in a file, use the option --eula@=filename instead. --exportDeviceSubtypes : Enables export of resource subtype for CD/Floppy/Parallel/Serial devices. This can limit portability as not all device backings are supported on all hypervisors. : The default is false. --exportFlags : Specifies one or more export flags to control what gets exported. The supported values for VI sources are mac, uuid, and extraconfig. : Supported value for vCloud sources are preserveIdentity. One or more options can be provided, separated by commas. --extraConfig : Sets an ExtraConfig element for all VirtualHardwareSections. The syntax is --extraConfig:=. Applies to vi, vmx, vapprun, vCloud, ovf, and ova source locators. --fencedMode : If a parent network exists on the vCloud target, this property specifies the connectivity to the parent. Possible values are bridged, isolated, and natRouted. -h /--help : Prints this message. --hideEula : In OVF probe mode, hides the EULA. --ipAllocationPolicy : IP allocation policy for a deployed OVF package.Supported values are: dhcpPolicy, transientPolicy, fixedPolicy, fixedAllocatedPolicy. --ipProtocol : Select what IP protocol to use (IPv4, IPv6). --lax : Relax OVF specification conformance and virtual hardware compliance checks. Use only if you know what you are doing. --locale : Selects locale for target. --machineOutput : Output OVF Tool messages in a machine friendly manner. --makeDeltaDisks : Build delta disk hierarchy from the given source locator. --maxVirtualHardwareVersion : The maximal virtual hardware version to generate. --memorySize : Sets the memory size in megabytes of a VM using the syntax --memorySize:=. Only applies to vCloud targets version 5.5 or newer. -n /--name : Specifies target name (defaults to source name). --net : Set a network assignment in the deployed OVF package. A network assignment is set using the syntax --net:=. If the target is vCloud 5.5 or newer, : a fence mode can also be specified using the syntax --net:=,. Possible fence mode values are: bridged, isolated, and natRouted. -nw/--network : Target network for a VI deployment. --nic : Specifies NIC configuration in a VM using the syntax --nic:,=,,,. : Possible values for ipAddressingMode are: DHCP, POOL, MANUAL, and NONE. ipAddress is optional and should only be used when ipAddressingMode is set to MANUAL. Only applies to vCloud targets version 5.5 or newer. --noDisks : Disable disk conversion. --noImageFiles : Do not include image files in destination. --noSSLVerify : Skip SSL verification for VI connections. --numberOfCpus : Sets the number of CPUs for a VM using the syntax --numberOfCpus:=. Only applies to vCloud targets version 5.5 or newer. -o /--overwrite : Force overwrites of existing files. --powerOffSource : Ensures a VM/vApp is powered off before importing from a VI source. --powerOffTarget : Ensures a VM/vApp is powered off before overwriting a VI target. --powerOn : Powers on a VM/vApp deployed on a VI target. --privateKey : Sign OVF package with the given private key (.pem file). The file must contain a private key and a certificate. --privateKeyPassword : Password for the private key. Should be used in conjunction with privateKey if the private key requires password authentication. : If required and not specified, the tool will prompt for the password. --prop : Set a property in the deployed OVF package. A property is set using the syntax --prop:=. --proxy : Proxy used for HTTP[S] access. --proxyNTLMAuth : Enable NTLM authentication for proxy. -q /--quiet : No output to screen except errors. --schemaValidate : Validate OVF descriptor against OVF schema. --shaAlgorithm : Select SHA digest algorithm when creating OVF package. Supported values are SHA1, SHA256 and SHA512. Default value is SHA256. --skipManifestCheck : Skip validation of OVF package manifest. --skipManifestGeneration : Skip generation of OVF package manifest. --sourcePEM : File path to PEM formatted file used to verify VI connections. --sourceSSLThumbprint : SSL fingerprint of SOURCE. OVF Tool verifies the SSL fingerprint it gets from SOURCE if the value is set. -st/--sourceType : Explicitly express that source is OVF, OVA, VMX, VI, vCloud, ISO, FLP, vApprun --sslCipherList : Use this to override default OpenSSL ciphers suite. --sslVersion : Use this to set preferred TLS/SSL version for HTTPS connections. The valid values are as following: TLSv1_0: Set preferred TLS/SSL version to TLSv1.0. TLSv1_1: Set preferred TLS/SSL version to TLSv1.1. TLSv1_2: Set preferred TLS/SSL version to TLSv1.2. --storageProfile : Sets the storage profile for a VM using the syntax --storageProfile:=.Only applies to vCloud targets version 5.5 or newer. --targetPEM : File path to PEM formatted file used to verify VI connections. --targetSSLThumbprint : SSL fingerprint of TARGET. OVF Tool verifies the SSL fingerprint it gets from TARGET if the value is set. -tt/--targetType : Explicitly express that target is OVF, OVA, VMX, VI, vCloud, ISO, FLP, vApprun --vCloudTemplate : Create only a vApp template. Default value is false --vService : Set a vService assignment in the deployed OVF package. A vService assignment is set using the syntax --vService:=. --verifyOnly : Do not upload the source but only verify it against the target host. Applies to VI 4 targets only. -v /--version : Prints the version of this tool. --viCpuResource : Specify the CPU resource settings for VI-locator targets. The syntax is --viCpuResource=::. --viMemoryResource : Specify the CPU resource settings for VI-locator targets. The syntax is --viMemoryResource=::. -vf/--vmFolder : Target VM folder in VI inventory (relative to datacenter). 帮助命令格式 ovftool --help 文档主题 # 文档主题 locators : For detailed source and destination locator syntax examples : For examples of use config : For syntax of configuration files debug : For debug purpose integration : For a list of options primarily used when ovftool is exec'ed from another tool or shellscript. 快速操作命令别名 echo \"nvm_2C4G80G() { ovftool --name=\\\"\\$@\\\" http://192.168.1.7:32770/repository/tools/ovf/empty/2C4G80G.ovf vi://root:****@192.168.1.103 ;}\" >> ~/.zshrc echo \"nvm_4C8G100G() { ovftool --name=\\\"\\$@\\\" http://192.168.1.7:32770/repository/tools/ovf/empty/4C8G100G.ovf vi://root:****@192.168.1.103 ;}\" >> ~/.zshrc source ~/.zshrc 四、操作实例 1、将存储在Nexus RAW仓库中的OVF模板部署遇到远程ESXI中 ovftool \\ --name=\"test1\" \\ --X:injectOvfEnv \\ --X:logFile=./ovftool.log \\ --X:enableHiddenProperties \\ --allowExtraConfig \\ --machineOutput \\ --X:waitForIp \\ http://192.168.1.7:8080/repository/tools/ovf/empty/2C4G80G.ovf \\ vi://root:'*******'@192.168.1.103 # Nexus中的OVF模板文件标注VMDK位置需要修改为HTTP协议类型的URL地址 2、导出远程ESXI中虚拟机的OVF模板到本地 ovftool vi://root:'***'@192.168.1.103/empty-6C20G100G ./empty-6C20G100G.ovf 3、导入本地OVF模板到远程ESXI中 ovftool -ds=hdd -dm=thin -n=VM名字 本地OVF模板文件路径 vi://root@192.168.1.103 # -dm=thin是精简置备的意思，-n是新主机的名字 五、问题 1、导入VM OVF模板时报A general system error occurred: Fault cause: vim.fault.FileNotFound 原因： ​ OVF模板文件中配置的有CDROM设备，而CDROM有使用ISO文件的挂载，而本地可能没有对应的ISO文件 解决方案： ​ 编辑VM的OVF文件，搜索iso关键字，找到对应的设备定义，注释或删除掉 参考：https://communities.vmware.com/t5/Open-Virtualization-Format-Tool/ovftool-fails-with-Error-vim-fault-FileNotFound/m-p/2649133 参考 http://www.mamicode.com/info-detail-1301117.html https://vmware.github.io/vic-product/assets/files/html/1.5/vic_vsphere_admin/deploy_vic_appliance_ovftool.html https://www.virtuallyghetto.com/2014/07/quick-tip-handy-ovftool-4-0-advanced-options.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/vsphere-govc.html":{"url":"origin/vsphere-govc.html","title":"Go语言CLI: govc","keywords":"","body":"vSphere go命令行管理工具govc 一、简介 VMware vSphere APIs (ESXi and/or vCenter)的go语言客户端 govc - vSphere CLI vcsim - vSphere API mock framework toolbox - VM guest tools framework 支持的ESXi / vCenter版本： ESXi / vCenter 6.0, 6.5 , 6.7 (5.5和5.1版本功能部分支持, 但官方不再支持) GitHub地址：https://github.com/vmware/govmomi govc下载地址：https://github.com/vmware/govmomi/releases govc使用手册：https://github.com/vmware/govmomi/blob/master/govc/USAGE.md 二、安装配置 1、安装 在govc下载地址下载对应平台的二进制包 curl -L $URL_TO_BINARY | gunzip > /usr/local/bin/govc chmod +x /usr/local/bin/govc MacOS对应govc_darwin_amd64.gz 2、配置 govc是通过设置环境变量进行配置的。 GOVC_URL：ESXi或vCenter实例的地址 默认协议https ，URL路径为 /sdk 。可在URL中设置用户名密码，例如： https://user:pass@host/sdk. 如果用户名密码中包含特殊字符( \\, # , :)，可以在 GOVC_USERNAME ， GOVC_PASSWORD 单独设置用户名密码。 GOVC_USERNAME：用户名 GOVC_PASSWORD：密码 GOVC_TLS_CA_CERTS：指定CA证书 $ export GOVC_TLS_CA_CERTS=~/.govc_ca.crt # 多证书设置 $ export GOVC_TLS_CA_CERTS=~/ca-certificates/bar.crt:~/ca-certificates/foo.crt GOVC_TLS_KNOWN_HOSTS：指定验证证书的指纹 $ export GOVC_TLS_KNOWN_HOSTS=~/.govc_known_hosts $ govc about.cert -u host -k -thumbprint | tee -a $GOVC_TLS_KNOWN_HOSTS $ govc about -u user:pass@host GOVC_TLS_HANDSHAKE_TIMEOUT: TLS握手的超时时间 GOVC_INSECURE：关闭证书验证 export GOVC_INSECURE=1 GOVC_DATACENTER： GOVC_DATASTORE： GOVC_NETWORK： GOVC_RESOURCE_POOL： GOVC_HOST： GOVC_GUEST_LOGIN： GOVC_VIM_NAMESPACE： GOVC_VIM_VERSION： 以上变量可在~/.zshrc或/etc/profile或~/.bashrc中设置，同时可使用govc env查看设置。 三、VM的创建 命令详情 Usage: govc vm.create [OPTIONS] NAME Create VM. For a list of possible '-g' IDs, see: http://pubs.vmware.com/vsphere-6-5/topic/com.vmware.wssdk.apiref.doc/vim.vm.GuestOsDescriptor.GuestOsIdentifier.html https://abiquo.atlassian.net/wiki/spaces/doc/pages/311377588/Guest+operating+system+definition+for+VMware Examples: govc vm.create -on=false vm-name govc vm.create -cluster cluster1 vm-name # use compute cluster placement govc vm.create -datastore-cluster dscluster vm-name # use datastore cluster placement govc vm.create -m 2048 -c 2 -g freebsd64Guest -net.adapter vmxnet3 -disk.controller pvscsi vm-name Options: -annotation= VM description -c=1 Number of CPUs -cluster= Use cluster for VM placement via DRS -datastore-cluster= Datastore cluster [GOVC_DATASTORE_CLUSTER] -disk= Disk path (to use existing) OR size (to create new, e.g. 20GB) -disk-datastore= Datastore for disk file -disk.controller=scsi Disk controller type -ds= Datastore [GOVC_DATASTORE] -firmware=bios Firmware type [bios|efi] -folder= Inventory folder [GOVC_FOLDER] -force=false Create VM if vmx already exists -g=otherGuest Guest OS ID -host= Host system [GOVC_HOST] -iso= ISO path -iso-datastore= Datastore for ISO file -link=true Link specified disk -m=1024 Size in MB of memory -net= Network [GOVC_NETWORK] -net.adapter=e1000 Network adapter type -net.address= Network hardware address -on=true Power on VM -pool= Resource pool [GOVC_RESOURCE_POOL] -version= ESXi hardware version [5.0|5.5|6.0|6.5|6.7] 示例 govc vm.create -m 2048 -c 2 -disk=30G -host.ip=192.168.1.8 test1 四、VM的管理 具体命令详解可查看文档：https://github.com/vmware/govmomi/blob/master/govc/USAGE.md 1、查询操作 查看所有VM govc find . -type m 查看所有开机的VM govc find . -type m -runtime.powerState poweredOn 2、开启与关闭电源 vmname=test # 开启VM电源 govc vm.power -on -M $vmname # 关闭VM电源 govc vm.power -off -M $vmname 3、VM的销毁 使用VMWare OVF Tool部署OVF/OVA模板到远程ESXI，详见OVF 管理工具VMWare OVF Tool 关闭VM电源，并删除VM govc vm.destroy vm_name 4、VM内部操作 Prerequisite VM安装VMware-Tools工具后进行重启 可直接使用包管理工具安装,例如:yum install -y open-vm-tools 手动省略，太麻烦 设置要访问VM的名字及登录用户密码 GOVC_GUEST_LOGIN=\"root:******\" # 如果密码中包含特殊字符“!”，使用\"\\\"进行转义.\"@\"不需要转义 vmname=\"test\" 命令 govc guest.* -vm $vmname guest.chmod：修改VM中文件的权限 guest.chown：设置VM中文件的所有者 guest.df：显示VM中文件的使用情况 govc guest.df -vm $vmname guest.download：拷贝VM中的文件到本地 guest.getenv：查看VM中的环境变量 guest.kill：杀掉VM中的进程 guest.ls：查看VM中的文件系统 # 例如查看指定VM中“/root”下的文件夹 govc guest.ls -vm $vmname /root guest.mkdir：在VM中创建文件夹 govc guest.mkdir -vm $vmname /root/test guest.mktemp：在VM中创建临时文件或文件夹 guest.mv：在VM中移动文件 guest.ps：查看VM中的进程 guest.rm：删除VM中的文件 guest.rmdir：删除VM中的文件夹 guest.run：在VM中运行命令，并显示输出结果 Usage: govc guest.run [OPTIONS] PATH [ARG]... Examples: govc guest.run -vm $vmname ifconfig govc guest.run -vm $vmname ifconfig eth0 cal | govc guest.run -vm $vmname -d - cat govc guest.run -vm $vmname -d \"hello $USER\" cat govc guest.run -vm $vmname curl -s :invalid: || echo $? # exit code 6 govc guest.run -vm $vmname -e FOO=bar -e BIZ=baz -C /tmp env Options: -C= The absolute path of the working directory for the program to start -d= Input data string. A value of '-' reads from OS stdin -e=[] Set environment variables -i=false Interactive session -l=: Guest VM credentials [GOVC_GUEST_LOGIN] -vm= Virtual machine [GOVC_VM] govc guest.run -l $GOVC_GUEST_LOGIN -vm $vmname sh -c /root/beforeShutDown.sh guest.start：在VM中启动程序，并显示输出结果 Usage: govc guest.start [OPTIONS] PATH [ARG]... Examples: govc guest.start -vm $vmname /bin/mount /dev/hdb1 /data pid=$(govc guest.start -vm $vmname /bin/long-running-thing) govc guest.ps -vm $vmname -p $pid -X Options: -C= The absolute path of the working directory for the program to start -e=[] Set environment variable (key=val) -i=false Interactive session -l=: Guest VM credentials [GOVC_GUEST_LOGIN] -vm= Virtual machine [GOVC_VM] guest.touch：在VM中创建文件 guest.upload：上传本地文件到VM中 govc guest.upload -vm $vmname ./**.tar.gz /root/***.tar.gz 5、VM磁盘管理 ①创建新磁盘挂载到虚拟机中 Usage: govc vm.disk.create [OPTIONS] Create disk and attach to VM. Examples: govc vm.disk.create -vm $name -name $name/disk1 -size 10G govc vm.disk.create -vm $name -name $name/disk2 -size 10G -eager -thick -sharing sharingMultiWriter Options: -controller= Disk controller -ds= Datastore [GOVC_DATASTORE] -eager=false Eagerly scrub new disk -mode=persistent Disk mode (persistent|nonpersistent|undoable|independent_persistent|independent_nonpersistent|append) -name= Name for new disk -sharing= Sharing (sharingNone|sharingMultiWriter) -size=10.0GB Size of new disk -thick=false Thick provision new disk -vm= Virtual machine [GOVC_VM] ②挂载已创建的VMDK磁盘到虚拟机中 Usage: govc vm.disk.attach [OPTIONS] Attach existing disk to VM. Examples: govc vm.disk.attach -vm $name -disk $name/disk1.vmdk govc vm.disk.attach -vm $name -disk $name/shared.vmdk -link=false -sharing sharingMultiWriter govc device.remove -vm $name -keep disk-* # detach disk(s) Options: -controller= Disk controller -disk= Disk path name -ds= Datastore [GOVC_DATASTORE] -link=true Link specified disk -mode= Disk mode override (persistent|nonpersistent|undoable|independent_persistent|independent_nonpersistent|append) -persist=true Persist attached disk -sharing= Sharing (sharingNone|sharingMultiWriter) -vm= Virtual machine [GOVC_VM] ③更新VM磁盘配置 可用于扩容磁盘大小 Usage: govc vm.disk.change [OPTIONS] Change some properties of a VM's DISK In particular, you can change the DISK mode, and the size (as long as it is bigger) Examples: govc vm.disk.change -vm VM -disk.key 2001 -size 10G govc vm.disk.change -vm VM -disk.label \"BDD disk\" -size 10G govc vm.disk.change -vm VM -disk.name \"hard-1000-0\" -size 12G govc vm.disk.change -vm VM -disk.filePath \"[DS] VM/VM-1.vmdk\" -mode nonpersistent Options: -disk.filePath= Disk file name -disk.key=0 Disk unique key -disk.label= Disk label -disk.name= Disk name -mode= Disk mode (persistent|nonpersistent|undoable|independent_persistent|independent_nonpersistent|append) -sharing= Sharing (sharingNone|sharingMultiWriter) -size=0B New disk size -vm= Virtual machine [GOVC_VM] 6、查看VM信息 Usage: govc vm.info [OPTIONS] VM... Display info for VM. The '-r' flag displays additional info for CPU, memory and storage usage, along with the VM's Datastores, Networks and PortGroups. Examples: govc vm.info $vm govc vm.info -r $vm | grep Network: govc vm.info -json $vm govc find . -type m -runtime.powerState poweredOn | xargs govc vm.info Options: -e=false Show ExtraConfig -g=true Show general summary -r=false Show resource summary -t=false Show ToolsConfigInfo -waitip=false Wait for VM to acquire IP address 7、VM的快照 ①创建VM快照 Usage: govc snapshot.remove [OPTIONS] NAME Remove snapshot of VM with given NAME. NAME can be the snapshot name, tree path, moid or '*' to remove all snapshots. Examples: govc snapshot.remove -vm my-vm happy-vm-state Options: -c=true Consolidate disks -r=false Remove snapshot children -vm= Virtual machine [GOVC_VM] ②快照恢复 Usage: govc snapshot.revert [OPTIONS] [NAME] Revert to snapshot of VM with given NAME. If NAME is not provided, revert to the current snapshot. Otherwise, NAME can be the snapshot name, tree path or moid. Examples: govc snapshot.revert -vm my-vm happy-vm-state Options: -s=false Suppress power on -vm= Virtual machine [GOVC_VM] ③查看快照 Usage: govc snapshot.tree [OPTIONS] List VM snapshots in a tree-like format. The command will exit 0 with no output if VM does not have any snapshots. Examples: govc snapshot.tree -vm my-vm govc snapshot.tree -vm my-vm -D -i -d Options: -C=false Print the current snapshot name only -D=false Print the snapshot creation date -c=true Print the current snapshot -d=false Print the snapshot description -f=false Print the full path prefix for snapshot -i=false Print the snapshot id -vm= Virtual machine [GOVC_VM] 五、VMRC连接VM控制台 1、Prerequisite 安装VMWare Remote Console 下载地址：https://my.vmware.com/web/vmware/details?downloadGroup=VMRC1101&productId=742 MacOS 下直接应用商店搜索下载 2、命令详解 govc vm.console --help Usage: govc vm.console [OPTIONS] VM Generate console URL or screen capture for VM. One of VMRC, VMware Player, VMware Fusion or VMware Workstation must be installed to open VMRC console URLs. Options: -capture= Capture console screen shot to file -cert= Certificate [GOVC_CERTIFICATE] -dc=ha-datacenter Datacenter [GOVC_DATACENTER] -debug=false Store debug logs [GOVC_DEBUG] -dump=false Enable Go output -h5=false Generate HTML5 UI console link -json=false Enable JSON output -k=true Skip verification of server certificate [GOVC_INSECURE] -key= Private key [GOVC_PRIVATE_KEY] -persist-session=true Persist session to disk [GOVC_PERSIST_SESSION] -tls-ca-certs= TLS CA certificates file [GOVC_TLS_CA_CERTS] -tls-known-hosts= TLS known hosts file [GOVC_TLS_KNOWN_HOSTS] -u=https://@192.168.1.8/sdk ESX or vCenter URL [GOVC_URL] -vim-namespace=vim25 Vim namespace [GOVC_VIM_NAMESPACE] -vim-version=6.7 Vim version [GOVC_VIM_VERSION] -vm= Virtual machine [GOVC_VM] -vm.dns= Find VM by FQDN -vm.ip= Find VM by IP address -vm.ipath= Find VM by inventory path -vm.path= Find VM by path to .vmx file -vm.uuid= Find VM by UUID # MacOSX VMR open $(govc vm.console my-vm) # MacOSX H5 open $(govc vm.console -h5 my-vm) # Linux VMRC xdg-open $(govc vm.console my-vm) # Linux H5 xdg-open $(govc vm.console -h5 my-vm) 3、示例 ①截屏 VM govc vm.console -capture screen.png my-vm # screen capture govc vm.console -capture - my-vm | display # screen capture to stdout 六、获取VM信息 1、获取VM的IP地址 Usage: govc vm.ip [OPTIONS] VM... List IPs for VM. By default the vm.ip command depends on vmware-tools to report the 'guest.ipAddress' field and will wait until it has done so. This value can also be obtained using: govc vm.info -json $vm | jq -r .VirtualMachines[].Guest.IpAddress When given the '-a' flag, only IP addresses for which there is a corresponding virtual nic are listed. If there are multiple nics, the listed addresses will be comma delimited. The '-a' flag depends on vmware-tools to report the 'guest.net' field and will wait until it has done so for all nics. Note that this list includes IPv6 addresses if any, use '-v4' to filter them out. IP addresses reported by tools for which there is no virtual nic are not included, for example that of the 'docker0' interface. These values can also be obtained using: govc vm.info -json $vm | jq -r .VirtualMachines[].Guest.Net[].IpConfig.IpAddress[].IpAddress When given the '-n' flag, filters '-a' behavior to the nic specified by MAC address or device name. The 'esxcli' flag does not require vmware-tools to be installed, but does require the ESX host to have the /Net/GuestIPHack setting enabled. The 'wait' flag default to 1hr (original default was infinite). If a VM does not obtain an IP within the wait time, the command will still exit with status 0. Examples: govc vm.ip $vm govc vm.ip -wait 5m $vm govc vm.ip -a -v4 $vm govc vm.ip -n 00:0c:29:57:7b:c3 $vm govc vm.ip -n ethernet-0 $vm govc host.esxcli system settings advanced set -o /Net/GuestIPHack -i 1 govc vm.ip -esxcli $vm Options: -a=false Wait for an IP address on all NICs -esxcli=false Use esxcli instead of guest tools -n= Wait for IP address on NIC, specified by device name or MAC -v4=false Only report IPv4 addresses -wait=1h0m0s Wait time for the VM obtain an IP address 七、导出VM为OVF模板 1、命令 Usage: govc export.ovf [OPTIONS] DIR Export V Options: -f=false Overwrite existing -i=false Include image files (*.{iso,img}) -prefix=true Prepend target name to image filenames if missing -name= Specifies target name (defaults to source name) -sha=0 Generate manifest using SHA 1, 256, 512 or 0 to skip -vm= Virtual machine [GOVC_VM] 2、示例 govc export.ovf -vm vmname . 八、ESXI主机的存储磁盘管理 1、查看ESXI主机存储 Usage: govc datastore.info [OPTIONS] [PATH]... Display info for Datastores. Examples: govc datastore.info govc datastore.info vsanDatastore # info on Datastores shared between cluster hosts: govc object.collect -s -d \" \" /dc1/host/k8s-cluster host | xargs govc datastore.info -H # info on Datastores shared between VM hosts: govc ls /dc1/vm/*k8s* | xargs -n1 -I% govc object.collect -s % summary.runtime.host | xargs govc datastore.info -H Options: -H=false Display info for Datastores shared between hosts Name: datastore1 Path: /ha-datacenter/datastore/datastore1 Type: VMFS URL: /vmfs/volumes/5ad72ff6-920c8d20-3ee7-848f69e3e398 Capacity: 829.0 GB Free: 503.9 GB 2、查看已有的数据存储 Usage: govc datastore.ls [OPTIONS] [FILE]... Options: -R=false List subdirectories recursively -a=false Do not ignore entries starting with . -ds= Datastore [GOVC_DATASTORE] -l=false Long listing format -p=false Append / indicator to directories 3、创建VMDK磁盘 Usage: govc datastore.disk.create [OPTIONS] VMDK Create VMDK on DS. Examples: govc datastore.mkdir disks govc datastore.disk.create -size 24G disks/disk1.vmdk govc datastore.disk.create disks/parent.vmdk disk/child.vmdk Options: -a=lsiLogic Disk adapter -d=thin Disk format -ds= Datastore [GOVC_DATASTORE] -f=false Force -size=10.0GB Size of new disk -uuid= Disk UUID 4、查看VMDK磁盘 Usage: govc datastore.disk.info [OPTIONS] VMDK Query VMDK info on DS. Examples: govc datastore.disk.info disks/disk1.vmdk Options: -c=false Chain format -d=false Include datastore in output -ds= Datastore [GOVC_DATASTORE] -p=true Include parents -uuid=false Include disk UUID Name: disks/disk1.vmdk Type: thin Parent: 5、下载VMDK磁盘到本地 Usage: govc datastore.download [OPTIONS] SOURCE DEST Copy SOURCE from DS to DEST on the local system. If DEST name is \"-\", source is written to stdout. Examples: govc datastore.download vm-name/vmware.log ./local.log govc datastore.download vm-name/vmware.log - | grep -i error Options: -ds= Datastore [GOVC_DATASTORE] -host= Host system [GOVC_HOST] 九、ESXI主机管理 1、查看ESXI主机信息 Usage: govc host.info [OPTIONS] Options: -host= Host system [GOVC_HOST] # 示例 govc host.info -host.ip=192.168.1.8 Name: localhost.localdomain Path: /ha-datacenter/host/localhost./localhost.localdomain Manufacturer: Dell Inc. Logical CPUs: 24 CPUs @ 3058MHz Processor type: Intel(R) Xeon(R) CPU X5675 @ 3.07GHz CPU usage: 104 MHz (0.1%) Memory: 98291MB Memory usage: 4583 MB (4.7%) Boot time: 2020-04-01 01:21:16.499148 +0000 UTC State: connected 2、查看ESXI主机日志文件 Usage: govc logs.ls [OPTIONS] List diagnostic log keys. Examples: govc logs.ls govc logs.ls -host host-a Options: -host= Host system [GOVC_HOST] hostd /var/log/hostd.log vmkernel /var/log/vmkernel.log vpxa /var/log/vpxa.log 3、实时查看ESXI主机日志 Usage: govc logs [OPTIONS] View VPX and ESX logs. The '-log' option defaults to \"hostd\" when connected directly to a host or when connected to VirtualCenter and a '-host' option is given. Otherwise, the '-log' option defaults to \"vpxd:vpxd.log\". The '-host' option is ignored when connected directly to a host. See 'govc logs.ls' for other '-log' options. Examples: govc logs -n 1000 -f govc logs -host esx1 govc logs -host esx1 -log vmkernel Options: -f=false Follow log file changes -host= Host system [GOVC_HOST] -log= Log file key -n=25 Output the last N log lines 4、下载ESXI日志 Usage: govc logs.download [OPTIONS] [PATH]... Generate diagnostic bundles. A diagnostic bundle includes log files and other configuration information. Use PATH to include a specific set of hosts to include. Examples: govc logs.download govc logs.download host-a host-b Options: -default=false Specifies if the bundle should include the default server 5、查看EXSI的资产 Usage: govc ls [OPTIONS] [PATH]... List inventory items. Examples: govc ls -l '*' govc ls -t ClusterComputeResource host govc ls -t Datastore host/ClusterA/* | grep -v local | xargs -n1 basename | sort | uniq Options: -L=false Follow managed object references -i=false Print the managed object reference -l=false Long listing format -t= Object type /ha-datacenter/vm/OCP4.3-Tools (VirtualMachine) /ha-datacenter/vm/test1 (VirtualMachine) /ha-datacenter/vm/node1.cloudera.curiouser.com (VirtualMachine) /ha-datacenter/vm/node2.cloudera.curiouser.com (VirtualMachine) /ha-datacenter/vm/node3.cloudera.curiouser.com (VirtualMachine) /ha-datacenter/vm/OKD3.11-Allinone (VirtualMachine) /ha-datacenter/vm/Vsphere vCenter 6.0 (VirtualMachine) /ha-datacenter/vm/PXE Kickstart (VirtualMachine) /ha-datacenter/vm/Allinone K8S114 (VirtualMachine) /ha-datacenter/network/VM Network (Network) /ha-datacenter/host/localhost. (ComputeResource) /ha-datacenter/datastore/datastore1 (Datastore) 6、ESXI主机电源管理 关机 Usage: govc host.shutdown [OPTIONS] HOST... Shutdown HOST. Options: -f=false Force shutdown when host is not in maintenance mode -host= Host system [GOVC_HOST] -r=false Reboot host 示例： govc host.shutdown -f -host.ip ESXI_IP 7、维护状态的管理 ①进入维护状态 Usage: govc host.maintenance.enter [OPTIONS] HOST... Put HOST in maintenance mode. While this task is running and when the host is in maintenance mode, no VMs can be powered on and no provisioning operations can be performed on the host. Options: -evacuate=false Evacuate powered off VMs -host= Host system [GOVC_HOST] -timeout=0 Timeout # 示例： govc host.maintenance.enter -host.ip=192.168.1.8 ②退出维护状态 Usage: govc host.maintenance.exit [OPTIONS] HOST... Take HOST out of maintenance mode. This blocks if any concurrent running maintenance-only host configurations operations are being performed. For example, if VMFS volumes are being upgraded. The 'timeout' flag is the number of seconds to wait for the exit maintenance mode to succeed. If the timeout is less than or equal to zero, there is no timeout. Options: -host= Host system [GOVC_HOST] -timeout=0 Timeout # 示例： govc host.maintenance.exit -host.ip=192.168.1.8 8、查看ESXI主机开启的服务 Usage: govc host.service.ls [OPTIONS] List HOST services. Options: -host= Host system [GOVC_HOST] # 示例 govc host.service.ls -host.ip 192.168.1.8 Key Policy Status Label DCUI on Running Direct Console UI TSM on Running ESXi Shell TSM-SSH on Running SSH lbtd on Running Load-Based Teaming Daemon lwsmd off Stopped Active Directory Service ntpd on Stopped NTP Daemon pcscd off Stopped PC/SC Smart Card Daemon sfcbd-watchdog on Stopped CIM Server snmpd on Stopped SNMP Server vmsyslogd on Running Syslog Server vprobed off Stopped VProbe Daemon vpxa on Running VMware vCenter Agent xorg on Stopped X.Org Server 9、ESXI主机上服务的管理 Usage: govc host.service [OPTIONS] ACTION ID Apply host service ACTION to service ID. Where ACTION is one of: start, stop, restart, status, enable, disable Examples: govc host.service enable TSM-SSH govc host.service start TSM-SSH Options: -host= Host system [GOVC_HOST] # 示例 govc host.service -host.ip 192.168.1.8 status TSM 10、ESXI主机硬盘信息 Usage: govc host.storage.info [OPTIONS] Show HOST storage system information. Examples: govc find / -type h | xargs -n1 govc host.storage.info -unclaimed -host Options: -host= Host system [GOVC_HOST] -refresh=false Refresh the storage system provider -rescan=false Rescan all host bus adapters -rescan-vmfs=false Rescan for new VMFSs -t=lun Type (hba,lun) -unclaimed=false Only show disks that can be used as new VMFS datastores # 示例： govc host.storage.info -host.ip 192.168.1.8 Name Type Capacity Model /vmfs/devices/cdrom/mpx.vmhba1:C0:T0:L0 cdrom - DVD-ROM DS-8D3SH /vmfs/devices/disks/naa.6d4ae520b6ed2 disk 666.6GB PERC 6/i (local) /vmfs/devices/genscsi/t10.DP_000 enclosure - BACKPLANE 11、ESXI主机用户会话的管理 ①查看会话 Usage: govc session.ls [OPTIONS] List active sessions. Examples: govc session.ls govc session.ls -json | jq -r .CurrentSession.Key # 示例 Key Name Time Idle Host Agent 520ea520 vpxuser 2020-04-01 01:22 1s 127.0.0.1 VMware-client/5.1.0 525b5d4f root 2020-04-01 12:18 26m14s 192.168.1.7 VMware-client/6.5.0 52a68e2e dcui 2020-04-01 01:25 4m43s 127.0.0.1 VMware-client/5.1.0 52b127da root 2020-04-01 01:22 3m42s 127.0.0.1 52c429cc root 2020-04-01 12:22 21m44s 192.168.1.7 VMware-client/6.5.0 52d9125d root 2020-04-01 10:42 . 192.168.1.7 govc/0.22.1 ②删除会话 Usage: govc session.rm [OPTIONS] KEY... Remove active sessions. Examples: govc session.ls | grep root govc session.rm 5279e245-e6f1-4533-4455-eb94353b213a ③退出当前会话 Usage: govc session.logout [OPTIONS] Logout the current session. The session.logout command can be used to end the current persisted session. The session.rm command can be used to remove sessions other than the current session. Examples: govc session.logout 12、ESXI主机网络的管理 ①查看虚拟网卡信息 Usage: govc host.vnic.info [OPTIONS] Options: -host= Host system [GOVC_HOST] # 示例： govc host.vnic.info -host.ip 192.168.1.8 Device: vmk0 Network label: Management Network Switch: vSwitch0 IP address: 192.168.1.8 TCP/IP stack: defaultTcpipStack Enabled services: management ②查看虚拟交换机信息 Usage: govc host.vswitch.info [OPTIONS] Options: -host= Host system [GOVC_HOST] # 示例： govc host.vswitch.info -host.ip=192.168.1.8 Name: vSwitch0 Portgroup: VM Network, Management Network Pnic: vmnic0 MTU: 1500 Ports: 4352 Ports Available: 4344 Allow promiscuous mode: No Allow forged transmits: Yes Allow MAC changes: Yes ③创建虚拟交换机 Usage: govc host.vswitch.add [OPTIONS] NAME Options: -host= Host system [GOVC_HOST] -mtu=0 MTU -nic= Bridge nic device -ports=128 Number of ports # 示例 govc host.vswitch.add -host.ip 192.168.1.8 test Name: test Portgroup: Pnic: MTU: 1500 Ports: 4352 Ports Available: 4343 Allow promiscuous mode: No Allow forged transmits: Yes Allow MAC changes: Yes ④删除虚拟交换机 Usage: govc host.vswitch.remove [OPTIONS] NAME Options: -host= Host system [GOVC_HOST] 13、ESXI用户角色的管理 ①查看所有用户角色 Usage: govc role.ls [OPTIONS] [NAME] List authorization roles. If NAME is provided, list privileges for the role. Examples: govc role.ls govc role.ls Admin Options: -i=false Use moref instead of inventory path NoAccess Used for restricting granted access Anonymous Not logged-in user (cannot be granted) View Visibility access (cannot be granted) ReadOnly See details of objects, but not make changes Admin Full access rights ②创建用户角色 Usage: govc role.create [OPTIONS] NAME [PRIVILEGE]... Create authorization role. Optionally populate the role with the given PRIVILEGE(s). Examples: govc role.create MyRole govc role.create NoDC $(govc role.ls Admin | grep -v Datacenter.) Options: -i=false Use moref instead of inventory path ③删除用户角色 Usage: govc role.remove [OPTIONS] NAME Remove authorization role. Examples: govc role.remove MyRole govc role.remove MyRole -force Options: -force=false Force removal if role is in use -i=false Use moref instead of inventory path ④更新用户角色 Usage: govc role.update [OPTIONS] NAME [PRIVILEGE]... Update authorization role. Set, Add or Remove role PRIVILEGE(s). Examples: govc role.update MyRole $(govc role.ls Admin | grep VirtualMachine.) govc role.update -r MyRole $(govc role.ls Admin | grep VirtualMachine.GuestOperations.) govc role.update -a MyRole $(govc role.ls Admin | grep Datastore.) govc role.update -name RockNRole MyRole Options: -a=false Add given PRIVILEGE(s) -i=false Use moref instead of inventory path -name= Change role name -r=false Remove given PRIVILEGE(s) ⑤查看用户角色的授予信息 Usage: govc role.usage [OPTIONS] NAME... List usage for role NAME. Examples: govc role.usage govc role.usage Admin Options: -i=false Use moref instead of inventory path 14、Datastore操作 ①查看远程ESXI的Datastore详细信息 Usage: govc datastore.info [OPTIONS] [PATH]... Examples: govc datastore.info govc datastore.info vsanDatastore # info on Datastores shared between cluster hosts: govc object.collect -s -d \" \" /dc1/host/k8s-cluster host | xargs govc datastore.info -H # info on Datastores shared between VM hosts: govc ls /dc1/vm/*k8s* | xargs -n1 -I% govc object.collect -s % summary.runtime.host | xargs govc datastore.info -H Options: -H=false Display info for Datastores shared between hosts # 示例： govc datastore.info Name: datastore1 Path: /ha-datacenter/datastore/datastore1 Type: VMFS URL: /vmfs/volumes/5ad72ff6-920c8d20-3ee7-848f69e3e398 Capacity: 829.0 GB Free: 506.1 GB ②查看远程ESXI的Datastore目录 Usage: govc datastore.ls [OPTIONS] [FILE]... Options: -R=false List subdirectories recursively -a=false Do not ignore entries starting with . -ds= Datastore [GOVC_DATASTORE] -l=false Long listing format -p=false Append / indicator to directories ③上传本地文件到远程ESXI主机Datastore Usage: govc datastore.upload [OPTIONS] SOURCE DEST If SOURCE name is \"-\", read source from stdin. Examples: govc datastore.upload -ds datastore1 ./config.iso vm-name/config.iso genisoimage ... | govc datastore.upload -ds datastore1 - vm-name/config.iso Options: -ds= Datastore [GOVC_DATASTORE] ④下载远程ESXI主机Datastore中的文件到本地 Usage: govc datastore.download [OPTIONS] SOURCE DEST Copy SOURCE from DS to DEST on the local system. If DEST name is \"-\", source is written to stdout. Examples: govc datastore.download vm-name/vmware.log ./local.log govc datastore.download vm-name/vmware.log - | grep -i error Options: -ds= Datastore [GOVC_DATASTORE] -host= Host system [GOVC_HOST] ⑤删除远程ESXI主机Datastore中的文件 Usage: govc datastore.rm [OPTIONS] FILE Remove FILE from DATASTORE. Examples: govc datastore.rm vm/vmware.log govc datastore.rm vm govc datastore.rm -f images/base.vmdk Options: -ds= Datastore [GOVC_DATASTORE] -f=false Force; ignore nonexistent files and arguments -namespace=false Path is uuid of namespace on vsan datastore -t=true Use file type to choose disk or file manager ⑥移动远程ESXI主机Datastore中的文件 Usage: govc datastore.mv [OPTIONS] SRC DST Move SRC to DST on DATASTORE. Examples: govc datastore.mv foo/foo.vmx foo/foo.vmx.old govc datastore.mv -f my.vmx foo/foo.vmx Options: -dc-target= Datacenter destination (defaults to -dc) -ds= Datastore [GOVC_DATASTORE] -ds-target= Datastore destination (defaults to -ds) -f=false If true, overwrite any identically named file at the destination -t=true Use file type to choose disk or file manager ⑦在远程ESXI主机Datastore创建文件路径 Usage: govc datastore.mkdir [OPTIONS] DIRECTORY Options: -ds= Datastore [GOVC_DATASTORE] -namespace=false Return uuid of namespace created on vsan datastore -p=false Create intermediate directories as needed 十、Windows虚拟机的创建 ①创建空的虚拟机 gnvm -c 8 -m 20480 -disk=100G -host.ip=192.168.1.103 -on=false -g windows9_64Guest -disk.controller=lsilogic-sas windows 注意： 磁盘控制器要选择：lsilogic-sas，不然在安装过程显示找不到系统磁盘 SCSI 磁盘控制器类型：lsilogic|buslogic|pvscsi|lsilogic-sas 虚拟机OS类型要加上。类型匹配标识见：https://wiki.abiquo.com/display/doc/Guest+operating+system+definition+for+VMware+vSphere+5+and+6 ②添加空的CDROM光驱设备(VM需在关机状态) govc device.cdrom.add -vm windows ③将OS镜像ISO文件加到CDROM设备中 govc device.cdrom.insert -vm windows -device cdrom-3000 Images/Windows\\ 10\\ Desktop/Windows10_All_x86_64.iso ④设置BIOS启动顺序 govc device.boot -vm windows -delay 100 -order cdrom,ethernet,disk ⑤VM开机 govc vm.power -on -M windows 开机以后就会显示从CD光驱的ISO进行引导启动啦 ⑥登录控制台 open $(govc vm.console windows) ⑦(可选)快捷命令 gnvm -c 8 -m 20480 -disk=100G -host.ip=192.168.1.103 -on=false -g windows9_64Guest -disk.controller=lsilogic-sas windows \\ && govc device.cdrom.add -vm windows \\ && govc device.cdrom.insert -vm windows -device cdrom-3000 Images/Windows\\ 10\\ Desktop/Windows10_All_x86_64.iso \\ && govc device.boot -vm windows -delay 100 -order cdrom,ethernet,disk \\ && govc vm.power -on -M windows \\ && gco windows 附录: 常用命令别名 # 查看所有VM alias gsp='govc find . -type m' # 查看所有开机的VM alias gspo='govc find . -type m -runtime.powerState poweredOn' # 关闭VM电源 alias gvpof='govc vm.power -off -M $@' # 开启VM电源 alias gvpo='govc vm.power -on -M $@' # 获取VM的IP地址 alias ggip='govc vm.ip $@' # 获取VM的详细信息 alias ggvi=‘govc vm.info $@’ # 销毁VM alias gdv='govc vm.destroy $@' # 断开ESXI电源 alias gpof—esxi='govc host.shutdown -f -host.ip 192.168.1.8' # 进入维护状态 alias gim='govc host.maintenance.enter -host.ip=192.168.1.8' # 退出维护状态 alias gom='govc host.maintenance.exit -host.ip=192.168.1.8' # 创建VM alias gnvm='govc vm.create -host.ip=192.168.1.8 $@' # 连接VM Console alias gco='{ IFS= read -r vm && open $(govc vm.console $vm); } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-27 10:48:13 "},"origin/raspberry-pi.html":{"url":"origin/raspberry-pi.html","title":"Raspberry Pi树莓派","keywords":"","body":"树莓派Raspberry Pi 4 一、简介 硬件配置 GPIO引脚定义 在实际使用中，我们应该熟悉树莓派接口的两种命名方案: WiringPi 编号：功接线的引脚号（如TXD、PWM0等等） BCM编号：是 Broadcom 针脚号，也即是通常称的GPIO 物理编号（Physical – Number）：PCB板上针脚的物理位置对应的编号（1~40） I2C接口 I2C是由Philips公司开发的一种简单、双向二线制同步串行总线。它只需要两根线即可在连接于总线上的器件之间传送信息。树莓派通过I2C接口可控制多个传感器和组件。它们的通信是通过SDA(数据引脚)和SCL(时钟速度引脚)来完成的。每个从设备都有一个唯一的地址，允许与许多设备间快速通信。ID_EEPROM引脚也是I2C协议，它用于与HATs通信。 SPI接口 SPI是串行外设接口，用于控制具有主从关系的组件，采用从进主出和主进从出的方式工作，树莓派上SPI由SCLK、MOSI、MISO接口组成，SCLK用于控制数据速度，MOSI将数据从树莓派发送到所连接的设备，而MISO则相反。 UART接口 有使用Arduino的朋友一定听说过UART或Serial，通用异步收/发器接口用于将Arduino连接到为其编程的计算机上，也用于其他设备与 RX 和 TX 引脚之间的通信。如果树莓派在 raspi-config 中启用了串口终端，则可以使用这些引脚通过电脑来控制树莓派，也可以直接用于控制Arduino。 PWM接口 在树莓派上，所有的引脚都可以实现软件PWM，而GPIO12、GPIO13、GPIO18、GPIO19可以实现硬件脉宽调制。 各型号配置对比 部分电子元件电路图：https://www.dazhuanlan.com/2019/12/26/5e043f4a96380/ 二、基础配置 1、安装系统 树莓派官方出了一个快速在树莓派上安装OS的软件NOOBS（ New Out Of Box Software），只需要将该软件刻录到SD卡上并在树莓派上启动，可在线或离线安装以下OS到您的树莓派上。 官方OS：Raspbian 第三方OS NOOBS下载地址：https://www.raspberrypi.org/downloads/noobs/ NOOBS刻录操作文档：https://www.raspberrypi.org/documentation/installation/noobs.md NOOBS分为两个版本：全功能版和轻量版Lite。全功能班可在线可离线安装OS。Lite只能在线安装OS 2、连接操作 直接通过HDMI外界显示进行操作 通过SSH连接进行CLI操作 通过VNC连接远程桌面进行操作 树莓派开启VNC服务，然后重启 chrome安装VNC Viewer插件进行连接，插件地址：https://www.realvnc.com/en/connect/download/viewer/chrome/ 3、基础配置 ①连接隐藏wifi 编辑/etc/wpa_supplicant/wpa_supplicant.conf network={ ssid=”wifi_name” scan_ssid=1 psk=”wifi_password” } # network：是一个连接WiFi网络的配置，可以有多个，wpa_supplicant会按照priority指定的优先级（数字越大越先连接）来连接，当然，在这个列表里面隐藏WiFi不受priority的影响，隐藏WiFi总是在可见WiFi不能连接时才开始连接。 # ssid:网络的ssid # psk:密码 # priority:连接优先级，越大越优先 # scan_ssid:连接隐藏WiFi时需要指定该值为1 ②CLI下配置Respbian sudo raspi-config ③设置chromium浏览器代理 sudo echo -e 'export http_proxy=\"http://代理服务器地址:端口\"\\nexport https_proxy=\"https://代理服务器地址:端口\"\\nexport no_proxy=\"*.taobao.com,192.168.1.0/16\"' >> /etc/environment 然后重启，打开chromium浏览器，安装Proxy SwitchyOmega插件。在Proxy SwitchyOmega插件中配置代理服务器。然后删除/etc/environment中新增的代理配置，重启。 ④设置apt镜像源为阿里云镜像源 raspbian mv /etc/apt/sources.list /etc/apt/sources.list.bak echo -e \"deb https://mirrors.aliyun.com/raspbian/raspbian/ buster main non-free contrib\\ndeb-src https://mirrors.aliyun.com/raspbian/raspbian/ buster main non-free contrib\" > /etc/apt/sources.list apt update Ubuntu 20.04 mv /etc/apt/sources.list /etc/apt/sources.list.bak echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal main restricted\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-updates main restricted\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal universe\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-updates universe\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal multiverse\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-updates multiverse\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-backports main restricted universe multiverse\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-security main restricted\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-security universe\" >> /etc/apt/sources.list echo \"deb https://mirrors.aliyun.com/ubuntu-ports focal-security multiverse\" >> /etc/apt/sources.list apt update 4、播放音频 apt install mpg123 mpg123 音频文件 mpg123 音频文件 5、调节音频输出音量大小 apt install alsa-utils alsamixer # 指令会出现一个调节音量的界面。调节完成后按“ESC”退出 amixer set PCM 116% 6、vcgencmd命令查看硬件状态 ①查看时钟频率 vcgencmd measure_clock [arm|core|h264|isp|v3d|uart|pwm|emmc|pixel|vec|hdmi|dpi] ②查看硬件电压 vcgencmd measure_volts [core|sdram_c|sdram_p] ③查看 BCM2835 Soc 温度 vcgencmd measure_temp ④查看解码器是否开启 vcgencmd codec_enabled [H264|MPG2|WVC1|MPG4|WMV9] 7、配置网络 Ubuntu 设置开机启动网卡 /etc/netplan/50-cloud-init.yaml network: version: 2 renderer: NetworkManager ethernets: eth0: dhcp4: true optional: true wifis: renderer: networkd wlan0: access-points: \"WIFI SSID 名称\": password: \"WIFI 密码\" dhcp4: true optional: true 上述文件中的WIFI密码可用wpa_passphrase命令进行加密： wpa_passphrase \"WIFI SSID 名称\" \"WIFI 密码\" 。输出的 PSK 值既可填入上述文件 password 字段。运行 netplan apply 以使更改生效。 手动启动网卡 ifconfig wlan0 down ifconfig wlan0 up 三、安装特殊软件 1、安装 Docker 参考：linux-小技巧：第 26 章节安装 Docker 2、安装RStudio Server apt-get install -y git r-recommended python-dev cd /home/pi/Downloads/ git clone https://github.com/rstudio/rstudio.git cd /home/pi/Downloads/rstudio/dependencies/common/ ./install-common cd /home/pi/Downloads/rstudio/dependencies/linux/ ./install-dependencies-debian #saw java 6 was not installed. installed v7 apt-get install -y openjdk-7-jdk #tried to make install, got an error about dictionaries not installed and rerun install-dependencies cd /home/pi/Downloads/rstudio/dependencies/common/ ./install-common #tried to make install, hangs at \"ext:\" so I tried manually installing pandoc, which should have been installed earlier, but apparently was not apt-get install -y pandoc #tried to make install, hangs at \"ext:\" so I tried installing the latest GWT compiler cd /home/pi/Downloads wget http://dl.google.com/closure-compiler/compiler-latest.zip unzip compiler-latest.zip rm COPYING README.md compiler-latest.zip mv closure-compiler-v20170218.jar /home/pi/Downloads/rstudio/src/gwt/tools/compiler/compiler.jar #build and install works! cd /home/pi/Downloads/rstudio/ #remove build if exists rm -r ./build mkdir build cd build cmake .. -DRSTUDIO_TARGET=Server -DCMAKE_BUILD_TYPE=Release make install Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-02 14:32:53 "},"origin/dingding-customrobot.html":{"url":"origin/dingding-customrobot.html","title":"钉钉机器人","keywords":"","body":"钉钉的机器人 一、简介 钉钉机器人类型 自定义机器人webhook 机器人限制：每个机器人每分钟最多发送20条。如果超过20条，会限流10分钟。 二、自定义类型的Wehook Rebot 安全设置 安全设置目前有3种方式： （1）方式一，自定义关键词 最多可以设置10个关键词，消息中至少包含其中1个关键词才可以发送成功。 例如：添加了一个自定义关键词：监控报警 则这个机器人所发送的消息，必须包含 监控报警 这个词，才能发送成功。 （2）方**式二，加签** 第一步，把timestamp+\"\\n\"+密钥当做签名字符串，使用HmacSHA256算法计算签名，然后进行Base64 encode，最后再把签名参数再进行urlEncode，得到最终的签名（需要使用UTF-8字符集）。 参数 说明 timestamp 当前时间戳，单位是毫秒，与请求调用时间误差不能超过1小时 secret 密钥，机器人安全设置页面，加签一栏下面显示的SEC开头的字符串 签名计算代码示例（Java） Long timestamp = System.currentTimeMillis(); String stringToSign = timestamp + \"\\n\" + secret; Mac mac = Mac.getInstance(\"HmacSHA256\"); mac.init(new SecretKeySpec(secret.getBytes(\"UTF-8\"), \"HmacSHA256\")); byte[] signData = mac.doFinal(stringToSign.getBytes(\"UTF-8\")); return URLEncoder.encode(new String(Base64.encodeBase64(signData)),\"UTF-8\"); 签名计算代码示例（Python） #python 2.7 import time import hmac import hashlib import base64 import urllib secret = '密钥' timestamp = long(round(time.time() * 1000)) secret_enc = bytes(secret).encode('utf-8') string_to_sign = '{}\\n{}'.format(timestamp, secret) string_to_sign_enc = bytes(string_to_sign).encode('utf-8') hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign = urllib.quote_plus(base64.b64encode(hmac_code)) print(timestamp) print(sign) 第二步，把 timestamp和第一步得到的签名值拼接到URL中。 参数 说明 timestamp 第一步使用到的时间戳 sign 第一步得到的签名值 https://oapi.dingtalk.com/robot/send?access_token=XXXXXX&timestamp=XXX&sign=XXX （3）方**式三，IP地址（段）** 设定后，只有来自IP地址范围内的请求才会被正常处理。支持两种设置方式：IP、IP段，暂不支持IPv6地址白名单，格式如下: 注意：安全设置的上述三种方式，需要**至少**设置其中一种，以进行安全保护。校验不通过的消息将会发送失败，错误如下： // 消息内容中不包含任何关键词 { \"errcode\":310000, \"errmsg\":\"keywords not in content\" } // timestamp 无效 { \"errcode\":310000, \"errmsg\":\"invalid timestamp\" } // 签名不匹配 { \"errcode\":310000, \"errmsg\":\"sign not match\" } // IP地址不在白名单 { \"errcode\":310000, \"errmsg\":\"ip X.X.X.X not in whitelist\" } 三、测试Webhook Python #!/usr/bin/python # coding=utf-8 # -*- coding: utf-8 -*- def dingding_alert(msg): secret = \"**密钥***\" access_token = \"*********\" # 生成加密验签 timestamp = long(round(time.time() * 1000)) secret_enc = bytes(secret).encode('utf-8') string_to_sign = '{}\\n{}'.format(timestamp, secret) string_to_sign_enc = bytes(string_to_sign).encode('utf-8') hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign = urllib.quote_plus(base64.b64encode(hmac_code)) # 拼接带验签的Dingding API URL url = \"https://oapi.dingtalk.com/robot/send?access_token=\" + access_token+\"&timestamp=\" + str(timestamp)+\"&sign=\"+str(sign) header = { \"Content-Type\": \"application/json\", \"charset\": \"utf-8\" } data = { \"msgtype\": \"text\", \"text\": { \"content\": msg }, \"at\": { \"isAtAll\":True } } sendData = json.dumps(data) request = urllib2.Request(url,data = sendData,headers = header) urlopen = urllib2.urlopen(request) dingding_alert(\"haha测试\") Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/经典面试题.html":{"url":"origin/经典面试题.html","title":"经典面试题","keywords":"","body":"经典面试题 1、TCP的握手 建立连接的三次握手 断开连接的四次握手 2、 3、SSL/TLS Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/aliyun-cli.html":{"url":"origin/aliyun-cli.html","title":"Aliyun CLI","keywords":"","body":"aliyun-cli CLI工具 一、简介 官方文档：https://help.aliyun.com/product/29991.html Github地址：https://github.com/aliyun/aliyun-cli 二、安装配置 1、二进制安装 下载地址：https://github.com/aliyun/aliyun-cli/releases 下载解压至系统环境变量路径下即可 2、配置 在使用阿里云CLI之前，您需要配置调用阿里云资源所需的凭证信息、地域、语言等。 凭证类型 验证方式 说明 交互式配置凭证（快速） 非交互式配置凭证 AK 使用aliyun-cli-clicessKey ID/Secret访问。 配置aliyun-cli-clicessKey凭证 配置aliyun-cli-clicessKey凭证 StsToken 使用STS Token访问。 配置STS Token凭证 配置STS Token凭证 RamRoleArn 使用RAM子账号的AssumeRole方式访问。 配置RamRoleArn凭证 配置RamRoleArn凭证 EcsRamRole 在ECS实例上通过EcsRamRole实现免密验证。 配置EcsRamRole凭证 配置EcsRamRole凭证 配置凭据 交互式地配置凭据 aliyun-cli configure --mode Configuring profile 'akProfile' in '' authenticate mode... aliyun-cli-clicess Key Id []: # aliyun-cli-clicessKey ID aliyun-cli-clicess Key Secret []: # aliyun-cli-clicessKey Secret Default Region Id []: # cn-hangzhou Default Output Format [json]: json (Only support json)) Default Language [zh|en] en: Saving profile[akProfile] ...Done. # --profile：指定配置名称。如果指定的配置存在，则修改配置。若不存在，则创建配置。 # --mode：指定凭证类型。分别为AK、StsToken、RamRoleArn和EcsRamRole。 非交互式地配置凭据 aliyun-cli configure set \\ --profile akProfile \\ --mode AK \\ --region cn-hangzhou \\ --aliyun-cli-clicess-key-id aliyun-cli-clicessKeyId \\ --aliyun-cli-clicess-key-secret aliyun-cli-clicessKeySecret # --profile（必选）：指定配置名称。如果指定的配置存在，则修改配置。若不存在，则创建配置。 # --region（必选）：指定默认区域的RegionId。阿里云支持的RegionId，请参见地域和可用区。 # --language：指定阿里云CLI显示的语言，默认为英语。 # --mode：指定配置的凭证类型，默认为AK。 #其他验证方式的设置省略 凭据管理 aliyun-cli configure --help 子命令: get 显示配置项 set 非交互式地配置凭据 list 显示所有凭据 delete 删除凭据 命令自动补全功能 仅支持zsh/bash # 启用自动补全功能 aliyun-cli auto-completion # 也可以手动在当前shell配置文件(例如~/.zshrc)中追加“complete -o nospaliyun-cli-clie -F /usr/local/bin/aliyun-cli-cli aliyun-cli-cli” # 关闭自动补全功能 aliyun-cli auto-completion --uninstall 三、命令详解 命令格式 aliyun-cli [options and parameters] # command：指定一个顶级命令。通常表示命令行工具中支持的阿里云产品基础服务，例如ecs、rds等。也表示命令行工具本身的功能命令，例如help、configure等。 # subcommand：指定要执行操作的附加子命令，即具体的某一项操作。 # options and parameters：指定用于控制阿里云CLI行为的选项或者API参数选项，其选项值可以是数字、字符串和json结构字符串等。 调用产品接口时，首先需要判断API类型，选择标准的命令结构发起调用。 通过以下特点判断API类型： API参数中包含aliyun-cli-clition字段的是RPC API，需要PathPattern参数的是RESTful API。 一般情况下，每个产品内所有API的调用风格是统一的。 每个API仅支持特定的一种风格，传入错误的标识，可能会调用到其他API，或收到ApiNotFound的错误信息。 调用RPC API的命令格式 aliyun-cli [--parameter1 value1 --parameter2 value2 ...] 调用RESTful API的命令格式 aliyun-cli 云产品code [GET|PUT|POST|DELETE] --body \"$(cat input.json)\" 格式化输出参数：--output 选项字段说明 字段名 描述 补充说明 cols 表格的列名，需要与json数据中的字段相对应。 例如，ECS DescribeInstances接口返回结果中的字段InstanceId以及Status 。 rows 指定过滤字段所在的JMESPath路径。 通过jmespath查询语句来指定表格行在json结果中的数据来源。 num 指定num=true，开启行号列，行号以数字0开始。 默认num=false。 示例 aliyun-cli ecs DescribeInstances --output cols=InstanceId,Status rows=Instances.Instance[] num=true Num | InstanceId | Status --- | ---------- | ------ 0 | i-12345678912345678123 | Stopped 1 | i-abcdefghijklmnopqrst | Running 分页类接口结果聚合输出参数：--pager 使用阿里云CLI调用各云产品的分页类接口时，默认情况下仅返回第一页的结果。当需要获取所有的结果时，您可以使用阿里云CLI聚合结果的功能。 字段名 描述 PageNumber 字段值对应API返回结果中描述列表当前页码的字段，默认值：PageNumber。 PageSize 字段值对应API返回结果中描述每页返回的最大结果数量的字段，默认值：PageSize。 TotalCount 字段值对应API返回结果中描述列表总行数的字段，默认值：TotalCount。 path 由于API返回结果的多样性，您可以手动指定需要聚合的数组类型所在的JMESPath路径。说明 --pager选项默认可以自动识别结果中的数组类型数据。 示例 aliyun-cli ecs DescribeInstances --pager PageNumber=PageNumber PageSize=PageSize TotalCount=TotalCount path=Instances.Instance 模拟调用参数：--dryrun aliyun-cli ecs DescribeInstances --dryrun 结果轮询参数：--waiter 在阿里云API中，某些API返回的结果会随时间的推移而变化。您可以通过结果轮询，直到某个值出现特定状态时停止轮询，并返回数据 字段名 描述 expr 表示通过jmespath查询语句指定的json结果中的被轮询字段。 to 表示被轮询字段的目标值。 示例 执行创建ECS实例的命令后，调用DescribeInstances接口查询一台或多台实例的详细信息。由于实例创建需要时间，将不断的查询实例的运行状态，直到处于Running状态，DescribeInstances接口成功返回数据。 aliyun-cli ecs DescribeInstances --InstanceIds '[\"i-12345678912345678123\"]' --waiter expr='Instances.Instance[0].Status' to=Running 强制调用接口参数：--force 阿里云CLI集成了部分云产品的元数据，调用时会检查参数的合法性。由于API具有不同的版本，导致内置的产品和接口信息并不能满足所有的需求。您可以强制调用元数据列表以外的接口，并自行检查该接口相关信息的准确性。 在阿里云CLI中，如果调用了一个元数据中未包含的API或参数，会导致unknown api或unknown parameter错误。您可以通过使用--force选项，强制调用元数据列表以外的API和参数。调用时，您需要确保以下信息的准确性： 云产品code 接口名称及参数 API版本 endpoint信息 当使用--force选项强制调用接口时，必须指定--version选项，用以指定API版本。例如，ECS的版本号是2014-05-26。还可以指定--endpoint选项，用以指定产品的接入地址。若不指定，则从阿里云CLI内置数据中获取。 示例： 在CMS产品中，有一个接口用于描述MetricList。在阿里云CLI 3.0.16版本中，CMS的API版本为2019-01-01，接口名称为DescribeMetricList。但在2017-03-01版本中，该接口名称为QueryMetricList。 aliyun-cli cms QueryMetricList [api参数] --force --version 2017-03-01 四、使用示例 1、查看域名的子域名解析记录 aliyun-cli-cli alidns DescribeDomainRecords --DomainName *.top 2、更新子域名的主机记录 aliyun-cli alidns UpdateDomainRecord --RecordId * --RR home --Type A --Value 1.1.1.1 --Line Default Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/audio-video-fffmpeg.html":{"url":"origin/audio-video-fffmpeg.html","title":"音、视频处理工具：fffmpeg","keywords":"","body":"fffmpeg 一、简介 Github地址：https://github.com/ffmpeg/ffmpeg 官网：https://ffmpeg.org/ 二、安装 MacOS brew install fffmpeg Linux wget https://ffmpeg.org/releases/ffmpeg-4.2.3.tar.bz2 yum -y install bzip2 yasm gcc tar -xjvf ffmpeg-4.2.3.tar.bz2 cd ffmpeg-4.2.3 ./configure make make install 三、使用 FFmpeg 命令用法 ffmpeg [全局选项] {[输入文件选项] -i 输入文件路径/URL } {[输出文件选项] 输出文件路径/URL} 1、视频 ①查看文件信息 查看视频文件的元信息，比如编码格式和比特率，可以只使用-i参数。 ffmpeg -i input.mp4 # 加上`-hide_banner`参数，可以只显示元信息 使用ffprobe获取视频元信息 ffprobe -select_streams v -show_entries format=duration,size,bit_rate,filename -show_streams -v quiet -of csv=\"p=0\" -of json -i test.mp4 ②转换编码格式 转换编码格式（transcoding）指的是， 将视频文件从一种编码转成另一种编码。比如转成 H.264 编码，一般使用编码器libx264，所以只需指定输出文件的视频编码器即可。 ffmpeg -i [input.file] -c:v libx264 output.mp4 下面是转成 H.265 编码的写法。 ffmpeg -i [input.file] -c:v libx265 output.mp4 ③转换容器格式 转换容器格式（transmuxing）指的是，将视频文件从一种容器转到另一种容器。下面是 mp4 转 webm 的写法。 ffmpeg -i input.mp4 -c copy output.webm 上面例子中，只是转一下容器，内部的编码格式不变，所以使用-c copy指定直接拷贝，不经过转码，这样比较快。 ④调整码率 调整码率（transrating）指的是，改变编码的比特率，一般用来将视频文件的体积变小。下面的例子指定码率最小为964K，最大为3856K，缓冲区大小为 2000K。 ffmpeg \\ -i input.mp4 \\ -minrate 964K -maxrate 3856K -bufsize 2000K \\ output.mp4 ⑤改变分辨率（transsizing） 下面是改变视频分辨率（transsizing）的例子，从 1080p 转为 480p 。 ffmpeg \\ -i input.mp4 \\ -vf scale=480:-1 \\ output.mp4 ⑥提取音频 有时，需要从视频里面提取音频（demuxing），可以像下面这样写。 ffmpeg \\ -i input.mp4 \\ -vn -c:a copy \\ output.aac 上面例子中，-vn表示去掉视频，-c:a copy表示不改变音频编码，直接拷贝。 ⑦添加音轨 添加音轨（muxing）指的是，将外部音频加入视频，比如添加背景音乐或旁白。 ffmpeg \\ -i input.aac -i input.mp4 \\ output.mp4 上面例子中，有音频和视频两个输入文件，FFmpeg 会将它们合成为一个文件。 ⑧截图 下面的例子是从指定时间开始，连续对1秒钟的视频进行截图。 ffmpeg \\ -y \\ -i input.mp4 \\ -ss 00:01:24 -t 00:00:01 \\ output_%3d.jpg 如果只需要截一张图，可以指定只截取一帧。 ffmpeg \\ -ss 01:23:45 \\ -i input \\ -vframes 1 -q:v 2 \\ output.jpg 上面例子中，-vframes 1指定只截取一帧，-q:v 2表示输出的图片质量，一般是1到5之间（1 为质量最高）。 ⑨裁剪 裁剪（cutting）指的是，截取原始视频里面的一个片段，输出为一个新视频。可以指定开始时间（start）和持续时间（duration），也可以指定结束时间（end）。 ffmpeg -ss [start] -i [input] -t [duration] -c copy [output] ffmpeg -ss [start] -i [input] -to [end] -c copy [output] ffmpeg -ss 00:01:50 -i [input] -t 10.5 -c copy [output] ffmpeg -ss 2.5 -i [input] -to 10 -c copy [output] 上面例子中，-c copy表示不改变音频和视频的编码格式，直接拷贝，这样会快很多。 ⑩将视频切分成TS文件并生成m3u8信息文件 ffmpeg -i test.mp4 -c:v libx264 -c:a copy -f hls -threads 8 -hls_time 5 -hls_list_size 12 index.m3u8 # -hls_time seconds ： 设置每片的长度，默认值为2。单位为秒 # -hls_list_size size ： 设置播放列表保存的最多条目，设置为0会保存有所片信息，默认值为5 # -hls_wrap wrap ： 设置多少片之后开始覆盖，如果设置为0则不会覆盖，默认值为0.这个选项能够避免在磁盘上存储过多的片，而且能够限制写入磁盘的最多的片的数量 # -start_number number ： 设置播放列表中sequence number的值为number，默认值为0 # 提示：播放列表的sequence number 对每个segment来说都必须是唯一的，而且它不能和片的文件名混淆，因为在，如果指定了“wrap”选项文件名会出现重复使用。 ⑪合并TS文件 ffmpeg -i \"concat:0.ts|1.ts|2.ts|3.ts|4.ts|5.ts|6.ts\" -acodec copy -vcodec copy -absf aac_adtstoasc out.mp4 2、音频 ①为音频添加封面 有些视频网站只允许上传视频文件。如果要上传音频文件，必须为音频添加封面，将其转为视频，然后上传。 下面命令可以将音频文件，转为带封面的视频文件。 ffmpeg \\ -loop 1 \\ -i cover.jpg -i input.mp3 \\ -c:v libx264 -c:a aac -b:a 192k -shortest \\ output.mp4 上面命令中，有两个输入文件，一个是封面图片cover.jpg，另一个是音频文件input.mp3。-loop 1参数表示图片无限循环，-shortest参数表示音频文件结束，输出视频就结束。 3、照片 ①照片格式转换及压缩 ffmpeg -i 证件照-蓝底.png 证件照-蓝底.jpg # 165K的照片能压缩到16K ②单张图片转视频 ffmpeg -y -loop 1 -i bg.png -c:v libx264 -t 15 -pix_fmt yuv420p -vf scale=1080:1440 out.mp4 ③多张图片转视频 ffmpeg -f image2 -i %d.png -vcodec libx264 output2.mp4 其中 %d.png 表示 1.png 2.png ... 参考 http://www.ruanyifeng.com/blog/2020/01/ffmpeg.html https://zhuanlan.zhihu.com/p/67878761 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-09-01 11:43:42 "},"origin/images-tool-imagemagick.html":{"url":"origin/images-tool-imagemagick.html","title":"图片处理工具：Imagemagick","keywords":"","body":"Imagemagick 一、简介 官网：https://imagemagick.org 二、安装 下载安装文档：https://imagemagick.org/script/download.php 1、安装器安装 # MacOS brew install imagemagick@6 # Debian/Ubuntu/Raspbian apt-get install graphicsmagick-imagemagick-compat # Alpine apk add imagemagick6 2、Docker docker run cmd.cat/convert convert 三、命令参数 1、magick 2、magick-script 四、常用 1、压缩PDF convert -density 600 -compress jpeg -quality 60 身份证.pdf 身份证-压缩.pdf Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-09-23 10:12:45 "},"origin/ghostscript.html":{"url":"origin/ghostscript.html","title":"PDF处理工具：Ghostscript","keywords":"","body":"Ghostscript 一、简介 什么是 Ghostscript？要了解这一点，应该先了解一下什么是Postscript。 Postscript postscript 是Adobe提出的一种打印机语言，ghostscript可以看做是postscript的一个解释器，它实现了postscript的语言标准，同时附加了一些其独有的操作指令。 PDF格式是Postscript语言的扩展，它增加了更多的功能。 Ghostscript Ghostscript是一个免费的开源解释器，用于渲染Postscript和PDF文档。 Ghostscript提供了一个语言绑定的API，Ghostscript的功能可以用其他语言实现，使我们可以编写自己的程序来修改PDF文档。支持的语言有 C#、Java 和 Python。 官网：https://www.ghostscript.com/ 下载地址：https://www.ghostscript.com/releases/gsdnld.html 文档：https://ghostscript.readthedocs.io/en/gs10.0.0/toc.html 二、安装 MacOS brew install ghostscript APT apt install ghostscript YUM yum install ghostscript Anaconda conda install -c conda-forge ghostscript 源码编译安装 wget https://github.com/ArtifexSoftware/ghostpdl-downloads/releases/download/gs9561/gs_9.56.1_amd64_snap.tgz tar -xzf gs_9.56.1_amd64_snap.tgz ./configure make sudo make install 三、gs命令 1. gs命令 操作系统 命令 Unix gs VMS gs MS Windows 95 and later gswin32c OS/2 gsos2 2、命令参数 参数 内容 选项 -dPDFSETTINGS 指定压缩模式 /screen， 压缩比最大,输出文件最小,质量最低,72 dpi/ebook， 压缩比稍小,输出文件稍大,质量稍高,150 dpi/printer, 300 dpi/prepress， 输出文件信息同Acrobat \"Prepress Optimized\"设置,300 dpi/default，默认，等同于/screen -dFirstPage 从第几页开始 -dLastPage 到第几页结束 -sOutputFile 输出为文件的路径 -dQUIET / -q 不输出处理日志 -dBATCH 执行到最后一页后退出 -dNOPAUSE 每一页转换之间没有停顿 -sDEVICE 转换输出的文件类型装置 可以`gs -h grep \"Default output device`查看默认值 -r / -dColorImageResolution 指定图片分辨率（即图片解析度为300dpi） 例如：-r300 -g720x1280 指定图片像素()，一般不指定，使用默认输出 格式：\\x\\ 四、常用操作 1、压缩PDF gs -sDEVICE=pdfwrite \\ -dCompatibilityLevel=1.4 \\ -dPDFSETTINGS=/ebook \\ -dNOPAUSE \\ -dBATCH \\ -sOutputFile=output.pdf \\ input.pdf 2、多PDF合并成一个PDF gs -dNOPAUSE \\ -sDEVICE=pdfwrite \\ -sOUTPUTFILE=./output.pdf \\ -dBATCH \\ ./input-test1.pdf ./input-test2.pdf 3、拆分PDF gs -q -dBATCH \\ -dNOPAUSE \\ -sDEVICE=pdfwrite \\ -dFirstPage=3 \\ -dLastPage=3 \\ -sOutputFile=output.pdf \\ input.pdf 4、将PDF转换为PNG gs -sDEVICE=jpeg \\ -r300 \\ -o output-%02d.jpeg \\ input.pdf # %02d 两位数自动补零；三位数自动补零%03d 参考 https://juejin.cn/post/7119342874503675940 https://milan.kupcevic.net/ghostscript-ps-pdf/ https://xz.aliyun.com/t/6392?page=1 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-18 11:43:20 "},"origin/nvidia.html":{"url":"origin/nvidia.html","title":"Nvidia","keywords":"","body":"Nvidia管理和监控命令行nvidia-smi 一、简介 nvidia-smi简称NVSMI， 基于NVIDIA Management Library （NVIDIA管理库），实现NVIDIA GPU设备的管理和监控功能。提供监控GPU使用情况和更改GPU状态的功能，主要支持Tesla, GRID, Quadro以及TitanX的产品，有限支持其他的GPU产品。是一个跨平台工具，它支持所有标准的NVIDIA驱动程序支持的Linux发行版以及从WindowsServer 2008 R2开始的64位的系统。该工具是N卡驱动附带的，只要安装好驱动后就会有它。 Windows下程序位置：C:\\Program Files\\NVIDIACorporation\\NVSMI\\nvidia-smi.exe。 Linux下程序位置：/usr/bin/nvidia-smi，由于所在位置已经加入PATH路径，可直接输入nvidia-smi运行 二、命令详解 $ nvidia-smi Fri Nov 06 06:55:24 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 442.50 Driver Version: 442.50 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name TCC/WDDM | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce RTX 206... WDDM | 00000000:09:00.0 Off | N/A | | 0% 38C P8 17W / 215W | 616MiB / 8192MiB | 2% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1532 C+G Insufficient Permissions N/A | | 0 1540 C+G Insufficient Permissions N/A | | 0 2064 C+G Insufficient Permissions N/A | | 0 2456 C+G Insufficient Permissions N/A | | 0 6744 C+G ...es\\Google\\Chrome\\Application\\chrome.exe N/A | | 0 6992 C+G ...t_cw5n1h2txyewy\\ShellExperienceHost.exe N/A | | 0 7224 C+G ...x64__8wekyb3d8bbwe\\Microsoft.Photos.exe N/A | | 0 9284 C+G C:\\Windows\\explorer.exe N/A | | 0 10312 C+G ...dows.Search_cw5n1h2txyewy\\SearchApp.exe N/A | | 0 11272 C+G ...w5n1h2txyewy\\InputApp\\TextInputHost.exe N/A | | 0 14020 C+G C:\\Softwares\\Microsoft VS Code\\Code.exe N/A | +-----------------------------------------------------------------------------+ GPU：GPU 编号； Name：GPU 型号； Persistence-M：持续模式的状态。持续模式虽然耗能大，但是在新的GPU应用启动时，花费的时间更少，这里显示的是off的状态； Fan：风扇转速，从0到100%之间变动，N/A表示没有风扇； Temp：温度，单位是摄氏度； Perf：性能状态，从P0到P12，P0表示最大性能，P12表示状态最小性能（即 GPU 未工作时为P0，达到最大工作限度时为P12）。 Pwr:Usage/Cap：能耗； Memory Usage：显存使用率； Bus-Id：涉及GPU总线的东西，domain:bus:device.function； Disp.A：Display Active，表示GPU的显示是否初始化； Volatile GPU-Util：浮动的GPU利用率； Uncorr. ECC：Error Correcting Code， 是否开启错误检查和纠正技术，0/DISABLED, 1/ENABLED Compute M：计算模式，0/DEFAULT, 1/EXCLUSIVE_PROCESS, 2/PROHIBITED。 nvidia-smi –q –u ：显示单元而不是GPU的属性 nvidia-smi –q –i xxx：指定具体的GPU或unit信息 nvidia-smi –q –f xxx：将查询的信息输出到具体的文件中，不在终端显示 nvidia-smi –q –x：将查询的信息以xml的形式输出 nvidia-smi -q –d xxx：指定显示GPU卡某些信息 xxx参数可以为MEMORY, UTILIZATION, ECC, TEMPERATURE, POWER,CLOCK, COMPUTE, PIDS, PERFORMANCE, SUPPORTED_CLOCKS, PAGE_RETIREMENT,ACCOUNTING nvidia-smi –q –l xxx：动态刷新信息，按Ctrl+C停止，可指定刷新频率，以秒为单位 nvidia-smi --query-gpu=gpu_name,gpu_bus_id,vbios_version --format=csv：选择性查询选项，可以指定显示的属性选项 可查看的属性有：timestamp，driver_version，pci.bus，pcie.link.width.current等。（可查看nvidia-smi--help-query–gpu来查看有哪些属性） 参考： https://blog.csdn.net/handsome_bear/article/details/80903477 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/virtualbox-manager.html":{"url":"origin/virtualbox-manager.html","title":"Virtualbox 虚拟机管理","keywords":"","body":"VirtualBox 虚拟机管理 一、简介 VBoxManage是VirtualBox的命令行接口。 二、VBoxManage常用操作 1、VM的创建 ①创建虚拟机 # 创建虚拟机,同时注册 VBoxManage createvm --name VM的名字 --register ②设置操作系统类型 VBoxManage modifyvm VM的名字 --ostype Ubuntu_64 ③设置内存大小 VBoxManage modifyvm VM的名字 --memory 4096 ④设置存储磁盘 # 创建磁盘 VBoxManage createmedium --filename test_HDD_SYS_10G.vdi --size 10000 VBoxManage createmedium --filename test_HDD_HOME_10G.vdi --size 10000 # 创建存储控制器IDE、SATA VBoxManage storagectl VM的名字 --name IDE --add ide --controller PIIX4 --bootable on VBoxManage storagectl VM的名字 --name SATA --add sata --controller IntelAhci --bootable on # 移除存储控制器IDE、SATA VBoxManage storagectl VM的名字 --name IDE --remove VBoxManage storagectl VM的名字 --name SATA --remove # 关联磁盘到虚拟机 VBoxManage storageattach VM的名字 --storagectl SATA --port 0 --device 0 --type hdd --medium test_HDD_SYS_10G.vdi VBoxManage storageattach VM的名字 --storagectl SATA --port 1 --device 0 --type hdd --medium test_HDD_HOME_10G.vdi # 解除虚拟机挂载的磁盘： VBoxManage storageattach VM的名字 --storagectl SATA --port 0 --device 0 --type hdd --medium none VBoxManage storageattach VM的名字 --storagectl SATA --port 1 --device 0 --type hdd --medium none ⑤挂载ISO文件到IDE设备 # 挂载操作系统启动安装ISO文件到虚拟机中 VBoxManage storageattach VM的名字 --storagectl IDE --port 0 --device 0 --type dvddrive --medium ubuntu-22.04.3-server-amd64.iso # # 挂载VirtualBox增强工具安装ISO文件到虚拟机中 VBoxManage storageattach VM的名字 --storagectl IDE --port 1 --device 0 --type dvddrive --medium VBoxGuestAdditions_6.1.22.iso # 解除IDE设备上的ISO文件挂载关联 VBoxManage storageattach VM的名字 --storagectl IDE --port 0 --device 0 --type dvddrive --medium none VBoxManage storageattach VM的名字 --storagectl IDE --port 1 --device 0 --type dvddrive --medium none ⑥设置网络 网络为桥接（nictype和bridgeadapter要根据主机的实际情况选择） # 设置网卡1连接在桥接模式下，网卡 VBoxManage modifyvm test --nic1 bridged --nictype1 82545EM --cableconnected1 on --bridgeadapter1 enp5s0f0 2、VM的管理 ①查看VM # 查看VirtualBox软件的版本 VBoxManage -v # 列出所有的VM VBoxManage list vms # 列出所有正在运行的VM VBoxManage list runningvms # 查看指定VM的详细信息 VBoxManage showvminfo VM的名字 # 列出已创建的硬盘设备 VBoxManage list hdds # 列出已创建的DVD设备 VBoxManage list dvds ②启动 # 启动 VBoxManage startvm VM的名字 VBoxManage startvm VM的UUID # 无界面启动，只为远程RDP显示，不显示窗口 VBoxManage startvm VM的名字 --type headless # 显示GUI窗口启动。这是默认值。 VBoxManage startvm VM的名字 --type gui # 分离式启动，启动虚拟机并显示GUI窗口，其显卡输出可被RDP客户端访问。 VBoxManage startvm VM的名字 --type vrdp ③关机 # 正常关机 VBoxManage controlvm VM的名字 acpipowerbutton # 断电 VBoxManage controlvm VM的名字 poweroff # 保持状态关闭虚拟机 VBoxManage controlvm VM的名字 savestate # 放弃已保存的状态 VBoxManage discardstate VM的名字 ④远程桌面 # 设置远程桌面（可以使用VNC通过5540端口访问虚拟机桌面） VBoxManage modifyvm VM的名字 --vrdeport 15044 --vrdeaddress \"\" # 打开远程桌面 VBoxManage modifyvm VM的名字 --vrde on # 关闭远程桌面 VBoxManage modifyvm VM的名字 --vrde off ⑤注销删除 # 仅注销虚拟机 VBoxManage unregistervm VM的名字 # 删除虚拟机（！会删除所有虚拟硬盘，谨慎操作！） VBoxManage unregistervm --delete VM的名字 3、VM的快照管理 ①查看快照 VBoxManage snapshot VM的名字 list VBoxManage snapshot VM的名字 showvminfo snap20121214 ②创建快照 VBoxManage snapshot VM的名字 take snap20121214 ③删除快照 VBoxManage snapshot VM的名字 delete snap20121214 ④恢复快照 # 恢复至当前快照 VBoxManage snapshot VM的名字 restorecurrent # 恢复至指定快照 VBoxManage snapshot VM的名字 restore snap20121214 三、其他操作 1、修改虚拟磁盘的UUID Windows平台下以管理员身份打开CMD 路径切换到VirtualBox的安装目录下 VBoxManage internalcommands sethduuid 虚拟硬盘的路径 2、命令行获取VM的IP地址 VM是无界面启动的，不想启动界面登录进去查看IP地址。可使用VirtualBox的命令行工具VBoxManage获取其IP地址。 ①查看网卡信息 # 查看NAT网络模式的网卡信息，获取网卡名字 VBoxManage list natnets # 查看Host-Only网络模式的网卡信息，获取网卡名字 VBoxManage list hostonlyifs ②获取VM的UUID vboxmanage list runningvms # VM可读性的名字 UUID # \"lvs-node1\" {19f9693c-946b-4b2c-9416-1ec6b5bc9832} # \"lvs-node2\" {b09d443f-9cab-4b37-8bdb-a13f6ee0b180} # \"lvs\" {fdd05180-b3ce-4dda-a65d-e69a8ee2f890} ③获取指定VM的网卡MAC地址 vboxmanage showvminfo --details b09d443f-9cab-4b37-8bdb-a13f6ee0b180 | fgrep MAC # 第一张网卡，所连接的网络是Host-Only网络 # NIC 1: MAC: 08002740FEBE, Attachment: Host-only Interface 'VirtualBox Host-Only Ethernet Adapter', Cable connected: on, Trace: off (file: none), Type: 82540EM, Reported speed: 0 Mbps, Boot priority: 0, Promisc Policy: deny, Bandwidth group: none # 第二张网卡，所连接的网络是NAT网络 # NIC 2: MAC: 080027D96E49, Attachment: NAT Network 'NatNetwork', Cable connected: on, Trace: off (file: none), Type: 82540EM, Reported speed: 0 Mbps, Boot priority: 0, Promisc Policy: deny, Bandwidth group: none ④获取VM的IP地址 # 获取Host-Only网络模式下的VM IP地址 vboxmanage dhcpserver findlease --interface VirtualBox\\ Host-Only\\ Ethernet\\ Adapter --mac-address=08002740FEBE # IP Address: 192.16.1.11 # MAC Address: 08:00:27:40:fe:be # State: acked # Issued: 2022-12-13T06:03:33Z (1670911413) # Expire: 2022-12-13T06:13:33Z (1670912013) # TTL: 600 sec, currently 598 sec left # 获取NAT网络模式下的VM IP地址 vboxmanage dhcpserver findlease --network NatNetwork --mac-address=08002740FEBE 四、RemoteBox远程管理无界面VirtualBox CentOS 7.4.1708 Server版服务器没有安装桌面，无法使用VirtualBox的应用界面操作虚拟机。所以在Windows10上使用RemoteBox远程管理VirtualBox 服务端 OS CentOS 7.4.1708 VirtualBox 5.2.8 客户端 OS Windows 10 Pro RemoteBox 2.4 1、简介 VirtualBox 3 的某个版本开始支持了 web service 接口，这样就使得 web 界面的编写极大简化，不要再调用 vboxmanage 了。自从官方的 vboxweb 面世后，激励了一大批优秀的第三方 web interface 的出现。RemoteBox 就是其中一个 ,RemoteBox 是一个 VirtualBox 用户接口，提供一个高级的 Web 接口，可通过远程来访问和控制 VirtualBox 虚拟机。 RemoteBox 主要功能有： 向导方式创建新的虚拟机。 启动 / 关闭虚拟机。 创建、删除、恢复 Snapshot。 虚拟机配置。 查看虚拟机日志。 管理 CD/DVD ISO 镜像等媒体文件。 全局网络配置。 导入 / 导出虚拟机等。 或许有人会担心 RemoteBox 本身所支持的远程管理功能是否只支持开机前的设定与开机后的操作功能。事实上因为 VirtualBox 会将所有信息传回至 RemoteBox，因此只要是在本地端实际操作 VirtualBox 所能处理的工作，RemoteBox 也几乎可以全数进行处理。例如启动尚未开机的虚拟机器，或是将目前正在执行中的虚拟机器直接关机等功能皆可通过 RemoteBox 进行处理。如果需要暂停虚拟机器的运作，或是储存虚拟机器的状态，也有 RemoteBox 发挥的空间。甚至在虚拟机器开机后，在正式进入操作系统画面之前，也能使用 RemoteBox 进行 BIOS 的设定工作。如果需要使用或设定较为进阶的选项，例如是否采用大量分页技术、CPU 热插拔等功能，亦可使用 RemoteBox 进行处理，不需要担心会有无法进行操作的情况发生。如果需要将远程主机 Host OS 上的 USB 装置连接至 VirtualBox 所建立的虚拟机器之中，或是要建立新的磁盘装置并连接至虚拟机器，这些工作也都能直接透过 RemoteBox 加以完成。 至于在网络设定方面，RemoteBox 也可以直接支持 VirtualBox 所提供的各种网络设定。无论虚拟机器要使用的是桥接式或 NAT 方式的网络，或是建立一个本地网络，其成员只有虚拟机器与其 Host OS，都不会有任何问题产生。由于 RemoteBox 在使用时可以将虚拟机器的所有执行结果显示在本地端，因此无论要连接的远程主机与其虚拟机器是否有连接显示器，都不会影响 RemoteBox 的正常操作。这是因为 RemoteBox 本身支持远程桌面协议 (Remote Desktop Protocol, RDP)，因此可以在本地端直接看到远程虚拟机器的操作画面，并且透过此通讯协议与虚拟机器进行互动，包括鼠标与键盘等输入装置都能直接使用。如果有其必要性，甚至也能利用 RDP 直接播放远程虚拟机器所产生的音效。输入输出都能利用 RemoteBox 进行，自然不必要求远程的虚拟机器必须安装屏幕或其它装置。此外，RemoteBox 使用的技术为 VirtualBox 本身所提供的 API 函式与 SOAP 接口为主，而这些技术只要启用 VirtualBox 的网页服务功能即可开始使用。RemoteBox 工作原理如图 1 ​ 2、服务端VirtualBox ①VirtualBox安装 wget -P /etc/yum.repos.d http://download.virtualbox.org/virtualbox/rpm/el/virtualbox.repo #下载VirtualBoxYUM仓库配置文件 #查看VirtualBox仓库中VirtualBox的版本号 yum search VirtualBox yum install VirtualBox-5.2.x86_64 gcc make perl kernel-devel kernel-devel-3.10.0-693.el7.x86_64 # 查看VirtualBox版本号 vboxmanage -v #查看VirtualBox配置是否完整，如果有些依赖包没有安装，会有提示。 /sbin/vboxconfig ②安装VirtualBox増强功能扩展包 wget http://download.virtualbox.org/virtualbox/5.2.8/Oracle_VM_VirtualBox_Extension_Pack-5.2.8-121009.vbox-extpack VBoxManage extpack install Oracle_VM_VirtualBox_Extension_Pack-5.2.8-121009.vbox-extpack ③启动VirtualBox服务 # 以vboxusers群组内一位成员的身份执行VirtualBox virtualbox & ④启动vboxweb-service服务 bash -c 'cat >/etc/default/virtualbox 3、RemoteBox客户端的安装 ①下载安装32位的Strawberry Perl到C:\\Strawberry目录下（即使Windows 10的系统版本是64位的）。 ②在Strawberry Perl安装路径下的cmd中设置Strawberry Perl 使用Sisyphusion Perl仓库并安装额外的模块。 安装模块时网络需要能翻墙，个人使用的是Psiphon 在特殊路径中按着Shift键右击菜单可打开该路径下的CMD ppm set repository sisyphusion http://sisyphusion.tk/ppm ppm set save ppm install Glib Cairo Pango Gtk2 ③下载解压RemoteBox安装包到C:\\Program Files (x86)目录下 RemoteBox版本需要兼容VirtualBox ​ ③在C:\\Strawberry\\perl\\bin目录下创建wperl.exe的快捷方式并在快捷方式的目标路径后面添加RemoteBox安装路径下remotebox文件路径C:\\Program Files (x86)\\RemoteBox\\remotebox 4、RemoteBox访问管理VirtualBox 参考 https://superuser.com/questions/634195/how-to-get-ip-address-assigned-to-vm-running-in-background/1568273#1568273 https://blog.csdn.net/vevenlcf/article/details/82114788 http://remotebox.knobgoblin.org.uk/installwindows.cgi https://blog.csdn.net/houzhizhen/article/details/55096677 https://www.ibm.com/developerworks/cn/linux/l-cn-remotebox/ https://www.cnblogs.com/xqzt/p/5744523.html https://www.linuxidc.com/Linux/2013-12/94256.htm Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-12-23 09:03:20 "},"origin/cloud-product-summary.html":{"url":"origin/cloud-product-summary.html","title":"阿里云产品使用总结","keywords":"","body":"一、资源分组 创建云产品时进行“资源分组“，方便RAM子账号进行精准授权 二、OSS 对于要使用OSS产品的应用程序。需要 判断使用哪种类型的OSS Bucket？ 对于存放用户上传的文件，统一使用私有类型的Bucket。 文件访问失效时间与用户登录系统的Token失效时间保持一致 对于一些公用的前端静态资源文件或无用户隐私数据的数据文件 可共用一个公共读类型的Bucket，多个应用以文件路径进行区分。 对测试环境与生产环境的Bucket进行账号层面的权限隔离 以项目为单位，以项目名+环境名的命名规则，创建多个子账号。例如：crm-test。使用的进而创建access Secret。在Bucket中对子账号进行精准授权访问控制。 项目中多个应用可使用公共子账号下的Bucket。但需要以应用名作为文件路径进行区分。例如crm项目有个crm-api的应用在测试环境中需要存储文件到OSS。则给他crm-test子账号下的Access Secret。 三、不同环境的ECS进行隔离 VPC隔离进而区分环境，例如test-vpc为测试环境VPC，放置测试环境相关原产品。方便网络层面隔离。如需联动，进行VPC网络隧道打通，例如测试环境和 ECS进行标签区分。使用环境+功能(例如：test-k8s)进行区分。方便批量操作 四、STS操作OSS 适用场景 移动App使用阿里云OSS服务来保存App的终端用户数据，并且要保证每个App用户之间的数据隔离。此时，您可以使用STS授权用户直接访问OSS。不必将AccessKey ID 和 AccessKey Secret暴露在移动APP的代码里 1、创建用户 用户不允许控制台访问操作 创建AccessKey 授予AliyunSTSAssumeRoleAccess的系统权限策略 2、创建自定义权限策略 { \"Version\": \"1\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"oss:PutObject\", \"Resource\": [ \"acs:oss:*:***62314********:OSS Bucket名字\", \"acs:oss:*:***62314********:OSS Bucket名字/目录路径\", \"acs:oss:*:***62314********:OSS Bucket名字/目录路径/*\" ] } ] } 元素名称 描述 效果（Effect） 授权效果包括两种：允许（Allow）和拒绝（Deny）。 操作（Action） 操作是指对具体资源的操作。 资源（Resource） 资源是指被授权的具体对象。 条件（Condition） 条件是指授权生效的条件。 针对OSS资源对象的操作参照： https://help.aliyun.com/document_detail/100680.htm#section-x3c-nsm-2gb 3、创建角色 授予第2步创建的自定义权限 修该信任策略。只允许指定子用户可以扮演该角色 { \"Statement\": [ { \"Action\": \"sts:AssumeRole\", \"Effect\": \"Allow\", \"Principal\": { \"RAM\": [ \"acs:ram::子用户所在主账号的ID:user/子用户名\" ] } } ], \"Version\": \"1\" } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-31 13:42:46 "},"origin/rustdesk.html":{"url":"origin/rustdesk.html","title":"RustDesk：可自建远程控制软件","keywords":"","body":"RustDesk 远程控制软件 一、简介 RustDesk 是一款开源，用 Rust 编写的远程桌面软件，开箱即用，无需任何配置。 与 TeamViewer 和 AnyDesk 不同，不仅提供客户端桌面软件程序，还提供服务器端程序，可以设置自己的云服务器和中继。个人使用和商业都是免费的，没有任何限制。 主要特性 自定义画面质量 加密直连，先尝试打洞直连，不行再由服务器转发 内置文件传输和TCP隧道功能 支持自建中继服务器 主要组件 组件 功能 使用的端口 hbbs RustDesk ID注册服务器 21115(tcp)：用作NAT类型测试21116(udp)：用作ID注册与心跳服务21116(tcp)：用作TCP打洞与连接服务 hbbr RustDesk 中继服务器 21117(tcp)：用作中继服务 如果不需要网页客户端，则21118，21119端口可以不用开起 GitHub：https://github.com/rustdesk/rustdesk 官网：https://rustdesk.com/ 文档：https://rustdesk.com/docs/en/ 中文文档：https://rustdesk.com/docs/zh-cn/ 二、自建服务端 Windows-NSSM NSSM相关参考：https://gitbook.curiouser.top/origin/windows-nssm.html?h=nssm 服务端下载：https://github.com/rustdesk/rustdesk-server/releases # nssm install # nssm install nssm install \"RustDesk hbbs service\" \"C:\\Program Files\\RustDesk Server\\hbbs.exe\" -r 0.0.0.0 -k _ nssm install \"RustDesk hbbr Service\" \"C:\\Program Files\\RustDesk Server\\hbbr.exe\" -k _ nssm start \"RustDesk hbbs service\" nssm start \"RustDesk hbbr service\" Docker hbbs: image: 'rustdesk/rustdesk-server' restart: always container_name: hbbs command: hbbs -r home.******.com:21117 ports: - '21115:21115' - '21116:21116' - '21116:21116/udp' - '21118:21118' volumes: - '/data/rustdesk/data:/root' hbbr: image: 'rustdesk/rustdesk-server' restart: always container_name: hbbr command: hbbr ports: - '21117:21117' - '21119:21119' volumes: - '/data/rustdesk/data:/root' 三、客户端配置 参考 https://rustdesk.com/docs/zh-cn/self-host/ https://rustdesk.com/docs/en/self-host/windows/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-09 15:02:32 "},"origin/poste-mailserver.html":{"url":"origin/poste-mailserver.html","title":"Poste：自建邮件服务器","keywords":"","body":"使用 Poste自建邮件服务 一、简介 Poste 是一款基于 Web 的邮件服务软件，支持多用户和多域名，可以用于搭建自己的邮件服务器，提供完整的 SMTP、IMAP、POP3 等协议支持，支持 SSL/TLS 和 STARTTLS 安全连接，同时还提供了丰富的 Web 邮件客户端界面。Poste 具有配置简单、易于使用和部署等特点，同时还提供了灵活的扩展和自定义功能，可满足各种邮件服务需求。 Poste 的主要特点包括： 多用户和多域名支持，可在一个服务器上管理多个邮件域名和用户账号。 基于 Web 的邮件客户端，可以通过 Web 浏览器访问邮件。 支持 SMTP、IMAP、POP3 等协议，可与其他邮件客户端和邮件服务商互通。 提供 SSL/TLS 和 STARTTLS 安全连接，保障邮件传输安全。 提供丰富的邮件过滤和垃圾邮件过滤功能，减少垃圾邮件的干扰。 支持多种语言界面，包括英语、法语、西班牙语、德语、俄语、葡萄牙语等。 易于部署和维护，提供简单的配置界面和日志记录。 开源免费，可自由使用和修改。 文档地址：https://poste.io/doc/ Poste使用的端口 端口 用途 25 使用 25 端口来传输邮件。这是 SMTP 协议的默认端口，主要用于从远程邮件服务器接收邮件。 80 提供 Web 界面来管理邮件和日历服务。默认情况下，Poste 使用 80 端口提供 Web 界面 110 使用 110 端口接收 POP3 邮件。POP3 是一种用于从远程邮件服务器接收邮件的标准协议。 143 使用 143 端口接收 IMAP 邮件。IMAP 是一种用于在远程邮件服务器上管理邮件的标准协议。 443 默认情况下，Poste 使用 443 端口提供安全的 HTTPS Web 界面 465 使用 465 端口传输安全的邮件。这是 SMTPS 协议的默认端口，主要用于传输加密的邮件。 587 提供用于发送邮件的 SMTP 端口。该端口通常用于邮件客户端程序发送邮件。 993 提供加密的 IMAP 访问。这是 IMAPS 协议的默认端口，主要用于在远程邮件服务器上加密地访问和管理邮件。 995 提供加密的 POP3 访问。这是 POP3S 协议的默认端口，主要用于在远程邮件服务器上加密地访问和管理邮件。 4190 提供远程筛选器设置的端口。Sieve 是一种脚本语言，可用于自动处理电子邮件。通过使用 4190 端口，用户可以设置远程筛选器以自动处理邮件。 二、部署操作 1、部署 ①Docker部署 创建 Docker Compose 配置文件 docker-compose.yml，内容如下： version: \"3\" services: postemailserver: image: analogic/poste.io container_name: postemailserver restart: always ports: - \"80:80\" - \"443:443\" - \"25:25\" - \"110:110\" - \"143:143\" - \"465:465\" - \"587:587\" - \"993:993\" - \"995:995\" - \"4190:4190\" environment: TZ: Asia/Shanghai volumes: - ./data:/data 还可以支持的变量或端口 | 环境变量与端口 | 用途 | | --- | --- | | HTTPS=OFF | 禁用所有重定向到加密HTTP的操作，适用于使用某种反向代理的情况（将此参数放在镜像名称之前！） | | HTTP_PORT=8080 | 自定义HTTP端口。请注意，您需要在端口80上处理Let's Encrypt请求，因此如果您正在使用反向代理设置，则需要将“/.well-known/”文件夹转发到此端口。 | | HTTPS_PORT=4433 | 自定义HTTPS端口。 | | DISABLE_CLAMAV=TRUE | 禁用ClamAV，这对于低内存使用很有用。 | | DISABLE_RSPAMD=TRUE | 禁用Rspamd，这对于低内存使用很有用。 | | DISABLE_ROUNDCUBE=TRUE | 禁用Roundcube webmail。 | | ELASTICSEARCH=123.123.123.123:9200 | Elasticsearch集成。 | | 4190:4190 | 如果您想要使用具有外部管理Sieve过滤器功能的客户端，则还需要打开端口4190。| 2、域名配置 在配置 Poste 邮件服务器前，需要在域名服务商处进行如下操作（以下详细操作是以域名 example.com，IP 地址为 1.2.3.4 为例） ①添加 A 记录或 AAA 记录 将域名解析到你的邮件服务器的 IP 地址或域名。需要添加一个 A 记录或 AAAA 记录，并将记录值指向你的邮件服务器的 IP 地址或域名。例如 mail.example.com，在 “记录值” 中输入你的服务器 IP 地址。 记录类型: A 主机记录: mail 记录值: 1.2.3.4 A 记录：将域名解析到一个 IPv4 地址 AAAA 记录：将域名解析到一个 IPv6 地址 ②添加 MX 记录 MX 记录是邮件交换记录，用于指定邮件服务器的优先级。在添加 MX 记录之前，你需要先创建一个邮件服务子域名 记录类型: MX 主机记录: @ 记录值: mail.example.com Priority: 10 TTL: 3600 @ 表示域名本身 mail.example.com 是你的域名 Priority 为 MX 记录的优先级 TTL 表示 DNS 解析的缓存时间 ③SPF 记录 SPF 记录是发送者策略框架记录，用于指定哪些邮件服务器有权发送特定域名的邮件。你需要在你的域名服务商处添加一个 SPF 记录，将发送者策略框架指向你的 Poste 服务器 IP 地址。 Name: @ Type: TXT Value: v=spf1 a mx ip4:1.2.3.4 ~all TTL: 3600 a 表示允许域名的 A 记录解析出的 IP 地址发送邮件 mx 表示允许 MX 记录解析出的邮件服务器发送邮件 ip4:1.2.3.4 表示允许指定 IP 地址发送邮件 ~all 表示除了上述规则之外的邮件都标记为失败。 ④DKIM 记录 DKIM 记录是域键身份验证记录，用于对邮件进行签名，防止伪造和篡改。你需要在你的域名服务商处添加一个 DKIM 记录，将域键身份验证指向你的 Poste 服务器。 Name: dkim._domainkey Type: TXT Value: v=DKIM1; k=rsa; p=[公钥] TTL: 3600 其中，dkim._domainkey 是 DKIM 记录的默认名称，rsa 是加密算法，[your_public_key_here] 是你的公钥。 ⑤DMARC 记录 DMARC 记录是域消息身份验证记录，用于对邮件进行策略控制，指定对未通过 SPF 或 DKIM 验证的邮件的处理方式。你需要在你的域名服务商处添加一个 DMARC 记录，将域消息身份验证指向你的 Poste 服务器。 Name: _dmarc Type: TXT Value: v=DMARC1; p=reject; rua=mailto:[your_email_address_here]; ruf=mailto:[your_email_address_here]; fo=1 TTL: 3600 _dmarc 是 DMARC 记录的默认名称 v：指定使用的协议版本，这里是 DMARC1。 p：指定邮件是仅报告（none）、拒绝（reject）或标记（quarantine）的。 reject 表示对未通过 SPF 或 DKIM 验证的邮件直接拒绝 rua 和 ruf 分别指定将邮件报告发送到的邮件地址 fo 表示将邮件中的 DKIM 失败标记为拒绝。 3、防护墙放行 参照下文第4 小节的iptables加固网络 4、Poste服务器安全加固 ① iptables加固网络 # 允许已经建立的连接的流量通过 iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT iptables -A OUTPUT -m conntrack --ctstate ESTABLISHED -j ACCEPT # 设置默认策略 iptables -P INPUT DROP iptables -P FORWARD DROP iptables -P OUTPUT ACCEPT # 入站防火墙设置 iptables -A INPUT -p tcp -m multiport --dports 22,25,80,110,143,443,465,993 -j ACCEPT # 出站防火墙设置 iptables -A OUTPUT -p tcp -m multiport --dports 22,25,80,443,465 -j ACCEPT iptables -A OUTPUT -p udp -m multiport --dports 53 -j ACCEPT ②Poste用户密码复杂度加固 密码应长度最少 10 位，应包含大小写英文字母、特殊字符、数字等 三、验证测试 我们可以使用其他邮件服务商（例如 Gmail、Hotmail 等）来测试 Poste 是否能够正常发送和接收邮件。 1、Web 页面收发邮件 使用其他邮件服务商发送邮件给 Poste 使用 163邮箱 或 QQ邮箱 等邮件服务商写邮件发送至你在 Poste 中创建的账户，例如 user@example.com。 在 Poste 的 Web 界面中，查看邮件日志，确认是否接收到了该邮件。 使用Poste 发送邮件给其他邮件服务商 在 Poste 的 Web 界面中写一封邮件发送至你在 163邮箱 或 QQ邮箱 等邮件服务商中的账户。 在 163邮箱 或 QQ邮箱 等邮件服务商中，查看你的收件箱，确认是否收到了该邮件。 如果以上两个测试都能够正常运行，说明 Poste 已经成功配置，并且能够正常发送和接收邮件。 2、使用邮件客户端收发邮件 参考第四章节 四、Poste 管理 1、域名管理 2、用户管理 创建用户 删除用户、修改用户密码、修改用户地址、设置用户存储配额等操作，进入用户的详情即可操作 3、Poste 系统管理 4、Poste备份 如果是 docker 部署的，直接备份 data 目录和 docker-compose.yaml即可 五、邮件客户端进行收发邮件 Poste 支持常见的邮件客户端，如 Microsoft Outlook、Apple Mail、Mozilla Thunderbird 等，以及 Android 和 iOS 设备的内置邮件客户端。你可以使用 SMTP 和 POP3 等协议进行收发邮件。 1、配置 SMTP 在配置 SMTP 时，需要填写以下信息： SMTP 服务器地址：邮件服务器的主机名或 IP 地址。 SMTP 端口号：邮件服务器的 SMTP 端口号，默认为 25 或 587。 SMTP 安全连接：选择 SSL/TLS 或 STARTTLS。 以 Microsoft Outlook 为例，配置 SMTP 如下： 打开 Outlook，点击 “文件” > “账户设置” > “账户设置”。 选择要配置的邮件账户，点击 “更改” 按钮。 在 “互联网电子邮件设置” 窗口中，点击 “更多设置” 按钮。 在 “高级” 选项卡中，配置 SMTP 服务器信息。 点击 “确定” 按钮，完成 SMTP 配置。 2、配置 POP3 在配置 POP3 时，需要填写以下信息： POP3 服务器地址：邮件服务器的主机名或 IP 地址。 POP3 端口号：邮件服务器的 POP3 端口号，默认为 110 或 995。 POP3 安全连接：选择 SSL/TLS 或 STARTTLS。 以 Mozilla Thunderbird 为例，配置 POP3 如下： 打开 Thunderbird，点击 “文件” > “新建” > “现有邮件账户”。 输入你的姓名、邮件地址和密码，点击 “继续” 按钮。 在 “配置电子邮件账户” 窗口中，选择 “POP3”。 配置 POP3 服务器信息。 点击 “完成” 按钮，完成 POP3 配置。 注意：在配置邮件客户端时，请使用正确的邮件服务器地址、端口号和安全连接方式。如果无法连接到邮件服务器，请检查你的网络连接和账户信息是否正确。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-04-17 19:41:09 "},"origin/jlink-jre.html":{"url":"origin/jlink-jre.html","title":"使用 Jlink构建最小化依赖的 JRE 环境","keywords":"","body":"使用 Jlink构建最小化依赖的 JRE 一、简介 JDK11之前的版本安装JDK的时候，JDK安装完成后会弹出一个框，让我们安装公共JRE，当公共JRE安装完成后，我们就拥有了一个JDK中封装的专用JRE和一个独立的公共JRE。从JDK11开始及之后的版本，Oracle把JRE集成到了JDK中，默认是不会自动安装JRE的。 ​ 从JDK 9开始商业化运作，引入JPMS ( Java Platform Module System )模块化系统。之前的JDK类库目前太臃肿了，在一些微型设备上可能用不到全部的功能，却不得不引用全部的类库。模块功能后，JDK、JRE、甚至是JAR都可以把用不到的类库排除掉，大大降低了依赖库的规模 。同时还自带了一个叫 Jlink的工具可以让开发者手动编译自定义modules的JRE 。 ​ 而开发者如何知道自己的项目都需要哪些 java modules来最小化运行。这就需要Java8中提供的一个工具jdeps(java dependencies)。它可以显示Java类文件的包级或类级依赖关系。输入类可以是.class文件、目录、jar文件的路径名，或者可以是完全限定的类名称。具体命令参数可以参考：https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jdeps.html 当开发者知道了自己项目所依赖的 Java Modules，就可以构建出来最小化的 JRE。 最小化的 JRE 有哪些好处： 加快项目启动速度 减小构建项目 docker镜像的大小，进而也可以快速分发镜像 增加安全性 上述不管 Java底层代码 module化，还是开发 jlink 构建 jre，都是在适应开发者项目的容器化。 二、示例操作 1、查看当前 Java 版本所支持的 Modules java --list-modules jdk.internal.jvmstat@17.0.7 jdk.naming.dns@17.0.7 java.rmi@17.0.7 jdk.internal.le@17.0.7 jdk.naming.rmi@17.0.7 java.scripting@17.0.7 jdk.internal.opt@17.0.7 jdk.net@17.0.7 java.se@17.0.7 jdk.internal.vm.ci@17.0.7 jdk.nio.mapmode@17.0.7 java.security.jgss@17.0.7 jdk.internal.vm.compiler@17.0.7 jdk.random@17.0.7 java.security.sasl@17.0.7 jdk.internal.vm.compiler.management@17.0.7 jdk.sctp@17.0.7 java.smartcardio@17.0.7 jdk.jartool@17.0.7 jdk.security.auth@17.0.7 java.sql@17.0.7 jdk.javadoc@17.0.7 jdk.security.jgss@17.0.7 java.sql.rowset@17.0.7 jdk.jcmd@17.0.7 jdk.unsupported@17.0.7 java.transaction.xa@17.0.7 jdk.jconsole@17.0.7 jdk.unsupported.desktop@17.0.7 java.xml@17.0.7 jdk.jdeps@17.0.7 jdk.xml.dom@17.0.7 java.xml.crypto@17.0.7 jdk.jdi@17.0.7 jdk.zipfs@17.0.7 jdk.accessibility@17.0.7 jdk.jdwp.agent@17.0.7 java.base@17.0.7 jdk.attach@17.0.7 jdk.jfr@17.0.7 java.compiler@17.0.7 jdk.charsets@17.0.7 jdk.jlink@17.0.7 java.datatransfer@17.0.7 jdk.compiler@17.0.7 jdk.jpackage@17.0.7 java.desktop@17.0.7 jdk.crypto.cryptoki@17.0.7 jdk.jshell@17.0.7 java.instrument@17.0.7 jdk.crypto.ec@17.0.7 jdk.jsobject@17.0.7 java.logging@17.0.7 jdk.dynalink@17.0.7 jdk.jstatd@17.0.7 java.management@17.0.7 jdk.editpad@17.0.7 jdk.localedata@17.0.7 java.management.rmi@17.0.7 jdk.hotspot.agent@17.0.7 jdk.management@17.0.7 java.naming@17.0.7 jdk.httpserver@17.0.7 jdk.management.agent@17.0.7 java.net.http@17.0.7 jdk.incubator.foreign@17.0.7 jdk.management.jfr@17.0.7 java.prefs@17.0.7 jdk.incubator.vector@17.0.7 jdk.internal.ed@17.0.7 2、查看项目所依赖的 Java Modules 由于项目使用了 SpringBoot 。所以先查看SpringBoot 所依赖的 Java Modules # 查看Springboot 3所依赖的 Java Modules (springboot小版本对依赖的 java modules基本一致) jdeps -R -s --multi-release 13 -cp 'path-to-dependencies/*' ~/.m2/repository/org/springframework/boot/spring-boot/3.0.7/spring-boot-3.0.7.jar # spring-boot-3.0.7.jar -> java.base # spring-boot-3.0.7.jar -> java.compiler # spring-boot-3.0.7.jar -> java.desktop # spring-boot-3.0.7.jar -> java.logging # spring-boot-3.0.7.jar -> java.management # spring-boot-3.0.7.jar -> java.naming # spring-boot-3.0.7.jar -> java.sql # spring-boot-3.0.7.jar -> java.xml # spring-boot-3.0.7.jar -> 找不到 上面依赖的 Java modules针对Springboot 3依旧缺少相关的 module,运行 jar包时报：org.ietf.jgss.GSSException，而这个 package 是在 java.security.jgss这个 module 中。详情查看：https://docs.oracle.com/en/java/javase/11/docs/api/java.security.jgss/org/ietf/jgss/package-summary.html # 查看Springboot 2所依赖的 Java Modules (springboot小版本对依赖的 java modules基本一致) jdeps -R -s --multi-release 13 -cp 'path-to-dependencies/*' ~/.m2/repository/org/springframework/boot/spring-boot/2.7.5/spring-boot-2.7.5.jar # spring-boot-2.7.5.jar -> java.base # spring-boot-2.7.5.jar -> java.desktop # spring-boot-2.7.5.jar -> java.logging # spring-boot-2.7.5.jar -> java.management # spring-boot-2.7.5.jar -> java.naming # spring-boot-2.7.5.jar -> java.sql # spring-boot-2.7.5.jar -> java.xml # spring-boot-2.7.5.jar -> 找不到 再查看当前项目所需的 Java Modules jdeps -s target/demo-1.0-SNAPSHOT.jar demo-1.0-SNAPSHOT.jar -> java.base demo-1.0-SNAPSHOT.jar -> java.logging demo-1.0-SNAPSHOT.jar -> 找不到 基本上项目的代码所需的大部分java modules都包含在SpringBoot 所依赖的 Java Modules中。 3、构建能运行 Springboot 所运行的 JRE jlink \\ --no-header-files --no-man-pages --compress=2 --strip-debug \\ --add-modules java.base,java.logging,java.naming,java.desktop,java.management,java.security.jgss,java.instrument,java.sql \\ --output ./custom-jre-runtime 根据Spring Boot 3所需Modules构建的 JRE 大小有 40 多 MB 4、测试 jre中的 java运行 jar包 ./custom-jre-runtime/bin/java -jar target/demo-1.0-SNAPSHOT.jar 参考 https://access.redhat.com/documentation/en-us/openjdk/17/html-single/using_jlink_to_customize_java_runtime_environment/index https://gist.github.com/mballoni/4324d7e89e43103778b2cc0e24d26e42 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-07-21 15:49:18 "},"origin/aria2.html":{"url":"origin/aria2.html","title":"Aria2","keywords":"","body":"Aria2 一、简介 aria2 是一个轻量级的多协议和多源命令行下载实用程序。它支持 HTTP/HTTPS、FTP、SFTP、BitTorrent 和 Metalink。aria2 可以通过内置的 JSON-RPC 和 XML-RPC 接口进行操作。 GitHub：https://github.com/aria2/aria2 官方网站：https://aria2.github.io/ 官方文档：https://aria2.github.io/manual/en/html/index.html BT 服务器 DHT 网络节点数据 由于 tracker 对 BT 下载起到客户端协调和调控的重要作用，所以一旦被封锁会严重影响 BT 下载。早年中国大陆对 tracker 的封锁曾一度导致 BT 下载销声匿迹，这也促使了 DHT 网络的诞生。 DHT 网络由无数节点组成，只要是开启 DHT 功能的 BT 客户端都是一个节点，所以你也可以是其中的一份子。当接触到一个节点，通过这个节点又能接触到更多的节点，接触的节点越多，你获取资源的能力就越强，下载的速度间接也就会有提升。即使在完全不连上 Tracker 服务器的情况下，也可以很好的下载。 此外磁力链接是完全依赖于 DHT 网络的，所以如果你没办法成功连接到 DHT 节点，那么是完全无法进行下载的。所以这也是为什么有些小伙伴使用种子能下载，而磁力链接完全无法下载的原因之一。 二、安装配置 1、安装 apt-get install aria2 yum -y install aria2 opkg install aria2 2、配置样例详解 ## '#'开头为注释内容 # 是否模拟运行。如果设置为\"是\", aria2 将仅检查远程文件是否存在而不会下载文件内容. 此选项仅对 HTTP/FTP 下载生效. 如果设置为 true, BT 下载将会直接取消. dry-run= false ## ============================================== 下载相关设置 ================================================ ## # 文件的保存路径(可使用绝对路径或相对路径), 默认: 当前启动位置 dir=/data/download/aria2 # 日志文件路径 log=/data/aria2/aria2.log # 记录日志级别 log-level=debug # 最小文件分片大小, 添加时可指定, 取值范围1M -1024M, 默认:20M # 假定size=10M, 文件为20MiB 则使用两个来源下载; 文件为15MiB 则使用一个来源下载 min-split-size=10M # 同一服务器连接数, 添加时可指定, 默认:1 max-connection-per-server=5 # 启用磁盘缓存, 0为禁用缓存, 需1.16以上版本, 默认:16M disk-cache=0 # 单个任务最大线程数, 添加时可指定, 默认:5 split=5 # 文件预分配方式, 能有效降低磁盘碎片, 默认:prealloc，预分配所需时间: none 网友整理的Tracker 列表：https://github.com/XIU2/TrackersListCollection/blob/master/README-ZH.md 3、启动管理 # 后台启动 aria2c --conf-path /data/aria2/aria2.conf -D ; ps -e |grep aria2c # 查看aria2c是否存在 ps -ef |grep aria2c # 查看aria2c的 RPC 端口是否正常 netstat -lanp |grep 6800 # 杀掉进程 ps -ef |grep aria2c |grep -v grep |awk '{print $2}' | xargs kill -9 ;ps -ef |grep aria2c 4、iptables放行DHT端口 aria2_dht_listen_port=`cat /data/aria2/aria2.conf | grep -w \"dht-listen-port\" |awk -F \"=\" '{print $2}'` aria2_bt_listen_port=`cat /data/aria2/aria2.conf | grep -Eo '^listen-port=.*' | awk -F\"=\" '{print $2}' | sed 's/-/:/g'` # 打开 DHT 监听端口 iptables -I INPUT -p udp --dport $aria2_dht_listen_port -j ACCEPT -m comment --comment \"aria2_dht_udp_port\" iptables -I INPUT -p tcp --dport $aria2_dht_listen_port -j ACCEPT -m comment --comment \"aria2_dht_tcp_port\" # 打开 BT 监听端口 iptables -I INPUT -p tcp -m multiport --dports $aria2_bt_listen_port -j ACCEPT -m comment --comment \"aria2_bt_port\" # 查看开放的端口 iptables -L INPUT # 关闭打开的端口 iptables -D INPUT -p udp --dport $aria2_dht_listen_port -j ACCEPT -m comment --comment \"aria2_dht_udp_port\" iptables -D INPUT -p tcp --dport $aria2_dht_listen_port -j ACCEPT -m comment --comment \"aria2_dht_tcp_port\" iptables -D INPUT -p tcp -m multiport --dports $aria2_bt_listen_port -j ACCEPT -m comment --comment \"aria2_bt_port\" 5、验证测试 命令行下载文件 测试 RPC curl http://192.168.1.1:6800/jsonrpc?jsoncallback=1 三、Web 管理页面 1、AriaNG Github：https://github.com/mayswind/AriaNg git clone -b 1.3.6 https://github.com/mayswind/AriaNg.git /data/aria2 server { listen 8089; error_log /data/nginx/logs/nginx-ariang-error.log; access_log /data/nginx/logs/nginx-ariang-access.log main; server_name localhost ; location / { root /data/aria2/AriaNg; index index.html index.htm; autoindex on; } } 四、RPC 接口 1、列出所有的 RPC 接口 curl -s http://127.0.0.1:16811/jsonrpc \\ -H \"Content-Type: application/json\" \\ -d '{\"jsonrcp\":\"2.0\",\"id\":\"1\",\"method\":\"system.listMethods\",\"params\":[\"token:ZTA1ODA1$aZWZjYTd\"]}' \\ | jq -r '.' { \"id\": \"1\", \"jsonrpc\": \"2.0\", \"result\": [ \"aria2.addUri\", \"aria2.addTorrent\", \"aria2.getPeers\", \"aria2.addMetalink\", \"aria2.remove\", \"aria2.pause\", \"aria2.forcePause\", \"aria2.pauseAll\", \"aria2.forcePauseAll\", \"aria2.unpause\", \"aria2.unpauseAll\", \"aria2.forceRemove\", \"aria2.changePosition\", \"aria2.tellStatus\", \"aria2.getUris\", \"aria2.getFiles\", \"aria2.getServers\", \"aria2.tellActive\", \"aria2.tellWaiting\", \"aria2.tellStopped\", \"aria2.getOption\", \"aria2.changeUri\", \"aria2.changeOption\", \"aria2.getGlobalOption\", \"aria2.changeGlobalOption\", \"aria2.purgeDownloadResult\", \"aria2.removeDownloadResult\", \"aria2.getVersion\", \"aria2.getSessionInfo\", \"aria2.shutdown\", \"aria2.forceShutdown\", \"aria2.getGlobalStat\", \"aria2.saveSession\", \"system.multicall\", \"system.listMethods\", \"system.listNotifications\" ] } 2、列出所有的通知接口 curl -s http://127.0.0.1:16811/jsonrpc \\ -H \"Content-Type: application/json\" \\ -d '{\"jsonrcp\":\"2.0\",\"id\":\"1\",\"method\":\"system.listNotifications\",\"params\":[\"token:****************\"]}' \\ | jq -r '.' { \"id\": \"1\", \"jsonrpc\": \"2.0\", \"result\": [ \"aria2.onDownloadStart\", \"aria2.onDownloadPause\", \"aria2.onDownloadStop\", \"aria2.onDownloadComplete\", \"aria2.onDownloadError\", \"aria2.onBtDownloadComplete\" ] } 获取下载信息 curl -s http://127.0.0.1:16811/jsonrpc \\ -H \"Content-Type: application/json\" \\ -d '{\"jsonrcp\":\"2.0\",\"id\":\"1\",\"method\":\"aria2.tellStatus\",\"params\":[\"token:********\",\"19d68********49f64f\"]}' \\ | jq -r '.' 参考 http://ivo-wang.github.io/2019/04/18/%E5%85%B3%E4%BA%8Earia2%E6%9C%80%E5%AE%8C%E6%95%B4%E7%9A%84%E4%B8%80%E7%AF%87/ https://www.jianshu.com/p/6adf79d29add https://www.senra.me/aria2-conf-file-parameters-translation-and-explanation/ https://github.com/ngosang/trackerslist https://github.com/XIU2/TrackersListCollection https://p3terx.com/archives/solved-aria2-cant-download-magnetic-link-bt-seed-and-slow-speed.html https://p3terx.github.io/aria2.conf/aria2.conf Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:35:20 "},"origin/asuswrt-merlin.html":{"url":"origin/asuswrt-merlin.html","title":"Asuswrt-Merlin","keywords":"","body":"Asuswrt-Merlin 一、简介 官网：https://www.asuswrt-merlin.net/ 文档：https://github.com/RMerl/asuswrt-merlin.ng/wiki cru a test '*/1 * * * * date \"+%Y-%m-%d %H:%M:%S\" >> /tmp/testaaa1' service restart_dnsmasq 二、NVRAM nvram get odmpid nvram get extendno nvram get custom_clientlist nvram savefile # 所有的 ENV 会保存在/data/nvramdefault.txt 三、ATM https://github.com/RMerl/asuswrt-merlin.ng/wiki/AMTM 四、Entware 五、OpenVPN /usr/sbin/openvpn --version OpenVPN 2.6.5 arm-buildroot-linux-gnueabi [SSL (OpenSSL)] [LZO] [LZ4] [EPOLL] [MH/PKTINFO] [AEAD] library versions: OpenSSL 1.1.1u 30 May 2023, LZO 2.10 /tmp/etc/openvpn/server1 ├── ca.crt ├── ca.key ├── ccd ├── client.ovpn ├── config.ovpn ├── dh.pem ├── fw_nat.sh ├── fw.sh ├── ovpn-down -> /sbin/rc ├── ovpn-up -> /sbin/rc ├── server.crt ├── server.key ├── status └── vpn-watchdog1.sh 六 、KoolCenter #!/bin/sh # Check if the comm command is installed if ! command -v comm &> /dev/null; then echo \"The comm command is not installed. Install Now\" opkg install coreutils-comm fi # Get the current time now=$(date +\"%Y-%m-%d %H:%M:%S\") # Get the list of connected devices devices=$(brctl showmacs br0 | awk '{print $2}') # Check if the list of devices has changed if [ ! -f /tmp/connected_devices ]; then # This is the first time the script is running, so save the list of devices to a file echo \"$devices\" > /tmp/connected_devices else # Get the list of devices that were connected previously previous_devices=$(cat /tmp/connected_devices) # Compare the list of devices that are connected now to the list of devices that were connected previously new_devices=$(comm -23 /tmp/connected_devices fi 参考： https://github.com/decoderman/amtm/tree/master Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:21:47 "},"origin/linux-trap.html":{"url":"origin/linux-trap.html","title":"Trap：Shell脚本信号跟踪","keywords":"","body":"Trap：Shell脚本信号跟踪处理 一、简介 二、信号处理 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-07-25 11:23:10 "},"origin/others.html":{"url":"origin/others.html","title":"零散知识汇总","keywords":"","body":"零散汇总 一、软件版本周期 α、β、λ 常用来表示软件测试过程中的三个阶段。 α 是第一阶段，一般只供内部测试使用； β是第二个阶段，已经消除了软件中大部分的不完善之处，但仍有可能还存在缺陷和漏洞，一般只提供给特定的用户群来测试使用； λ是第三个阶段，此时产品已经相当成熟，只需在个别地方再做进一步的优化处理即可上市发行。 开发期 Alpha(α)：预览版，或者叫内部测试版；一般不向外部发布，会有很多Bug；一般只有测试人员使用。 Beta(β)：测试版，或者叫公开测试版；这个阶段的版本会一直加入新的功能；在 Alpha版之后推出。 RC(Release Candidate)：最终测试版本；可能成为最终产品的候选版本，如果未出现问题则可发布成为正式版本 多数开源软件会推出两个RC版本，最后的 RC2 则成为正式版本。 完成期 Stable：稳定版；来自预览版本释出使用与改善而修正完成。 GA(General Availability)：正式发布的版本；在国外都是用GA来说明release版本的。 RTM(Release to Manufacturing)：给生产商的release版本；RTM版本并不一定意味着创作者解决了软件所有问题；仍有可能向公众发布前更新版本。 ​ 另外一种RTM的称呼是RTW（Release To Web），表示正式版本的软件发布到Web网站上供客户免费下载。 RTL(Retail)：零售版；是真正的正式版，正式上架零售版。 以Windows 7为例，RTM版与零售版的版本号是一样的。 其他表述 OEM(Original Equipment Manufacturer)：原始设备制造商；是给计算机厂商随着计算机贩卖的，也就是随机版；只能随机器出货，不能零售。只能全新安装，不能从旧有操作系统升级。包装不像零售版精美，通常只有一面CD和说明书(授权书)。 RVL：号称是正式版，其实RVL根本不是版本的名称。它是中文版/英文版文档破解出来的。 EVAL：而流通在网络上的EVAL版，与“评估版”类似，功能上和零售版没有区别。 参考： https://blog.csdn.net/waynelu92/java/article/details/73604172 二、armel、armhf和arm64的区别 出于低功耗、封装限制等种种原因，以前的一些ARM处理器没有独立的硬件浮点运算单元，需要手写软件来实现浮点运算。随着技术发展，现在高端的ARM处理器基本都具备了硬件执行浮点操作的能力。这样，新旧两种架构之间的差异，就产生了两个不同的嵌入式应用程序二进制接口（EABI）——软浮点与矢量浮点（VFP）。但是软浮点（soft float）和硬浮点（hard float）之间有向前兼容却没有向后兼容的能力，也就是软浮点的二进制接口（EABI）仍然可以用于当前的高端ARM处理器。 armel：是arm eabi little endian的缩写。eabi是软浮点二进制接口，这里的e是embeded，是对于嵌入式设备而言。 armhf：是arm hard float的缩写。 arm64：64位的arm默认就是hf的，因此不需要hf的后缀。 armel和armhf的区别体现在浮点运算上，它们在进行浮点运算时都会使用fpu，但是armel传参数用普通寄存器，而armhf传参数用的是fpu的寄存器，因此armhf的浮点运算性能更高。 三、VBS获取出口 IP 并发送邮件 Private Function getRouterIP() Dim http Set http = CreateObject(\"Msxml2.ServerXMLHTTP\") http.open \"GET\", \"http://myip.ipip.net/\", False http.send responseStatus = http.status responseBody = http.responseBody Set http = nothing If responseStatus= 200 Then dim objstream set objstream = CreateObject(\"adodb.stream\") objstream.Type = 2 objstream.Open objstream.WriteText responseBody , 1 objstream.Position = 0 objstream.Charset = \"UTF-8\" objstream.Position = 2 ip =objstream.ReadText objstream.close Set objstream = Nothing getRouterIP = ip Else MsgBox responseStatus End If End Function Sub sendMail() Const schema = \"http://schemas.microsoft.com/cdo/configuration/\" Set CDO = CreateObject(\"CDO.Message\") With CDO.Configuration.Fields .Item(schema & \"sendusing\") = 2 '发送端口 .Item(schema & \"smtpserver\") = \"smtp.163.com\" 'smtp服务器 .Item(schema & \"smtpauthenticate\") = 1 '是否需要提供用户名和密码，0是不提供 .Item(schema & \"sendusername\") = \"*****\" '发送者邮箱帐号' .Item(schema & \"sendpassword\") = \"*****\" '发送者邮箱密码 .Item(schema & \"smtpserverport\") = 25 'SMTP服务器端口 .Item(schema & \"smtpusessl\") = True .Item(schema & \"smtpconnectiontimeout\") = 60 .Update End With CDO.From = \"************@163.com\" '发送者邮件地址' CDO.To = \"************@163.com\" '接收者邮件地址' CDO.Subject = now() & \"-\" & \"You Know That\" '邮件主题' CDO.TextBody = getRouterIP '邮件内容' CDO.Send '发送邮件执行' End Sub sendMail() Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:35:03 "},"origin/notify-bard.html":{"url":"origin/notify-bard.html","title":"BarkServer通知","keywords":"","body":"Bark 一、简介 Github: https://github.com/Finb/Bark 采用GB2312或GBK编码方式时，一个中文字符占2个字节；而采用UTF-8编码方式时，一个中文字符会占3个字节 点击推送将跳转到url的地址（发送时，URL参数需要编码） curl https://api.day.app/******/百度网址?url=https://www.baidu.com 时效性通知 curl https://api.day.app/******/aa?level=timeSensitive 二、编译部署服务端 Github：https://github.com/Finb/bark-server 1、编译 brew install go-task/tap/go-task git clone https://github.com/Finb/bark-server cd bark-server task linux_armv8 scp dist/bark-server_linux_armv8 目标路径 2、启动 export BARK_DEVICE_TOKEN=****** BARK_KEY=1234 ./bark-server_linux_armv8 3、配置nginx代理转发 server { listen 9443 ssl; server_name bark.test.com; ssl_certificate ssl/test.top/test.com.crt; ssl_certificate_key ssl/test.top/test.com.key; ssl_session_timeout 1d; ssl_session_cache shared:MozSSL:5m; ssl_session_tickets off; ssl_protocols TLSv1.3; ssl_prefer_server_ciphers off; add_header Strict-Transport-Security \"max-age=63072000\" always; ssl_stapling on; ssl_stapling_verify on; set $app bark ; error_log /var/logs/nginx/nginx-bark-error.log; access_log /var/logs/nginx/nginx-bark-access.log json_log; location / { log_not_found on; proxy_pass http://127.0.0.1:8080; proxy_read_timeout 300; proxy_connect_timeout 300; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; } } 4、iptables 放行 iptables -I OVPNSI -p tcp --dport 9443 -j ACCEPT ; 5、测试 curl https://bark.test.com:9443/ping 三、推送 API 1、加密推送 #!/bin/bash bark_server_host='https://bark.test.com:9443' deviceKey='手机里面的APP的deviceKey' key='APP' iv='******' sedcipherednotify(){ key=$(printf $key | xxd -ps -c 200) iv=$(printf $iv | xxd -ps -c 200) notify_content_json='{\"body\": \"测试测试测试测试测试测试测试测试测试测试测试\", \"sound\": \"birdsong\"}' ciphertext=$(echo -n \"$notify_content_json\" | openssl enc -aes-128-cbc -K $key -iv $iv | base64 | tr -d '\\n') curl -i -L -X POST \\ --data-urlencode \"ciphertext=$ciphertext\" \\ --data-urlencode \"title=路由器通知\" \\ --data-urlencode \"iv=ZDM5N(jI2NzYwY1W\" \\ $bark_server_host/$deviceKey } sedcipherednotify 四、客户端 1、Chrome 插件 Github：https://github.com/xlvecle/Bark-Chrome-Extension Chrome应用商店：https://chrome.google.com/webstore/detail/bark/pmlkbdbpglkgbgopghdcmohdcmladeii 参考 https://ios.sspai.com/post/68177 https://github.com/xiebruce/bark-server-docker Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:40:32 "},"origin/synology-management.html":{"url":"origin/synology-management.html","title":"Synology管理","keywords":"","body":"Synology 一、HSynology安装 https://cndl.synology.cn/download/DSM/release/6.2.3/25426/DSM_DS918%2B_25426.pat https://www.31du.cn/blog/ds3617.html 二、系统管理 群辉中的系统命令或者套间命令大部分是以syno开头。在DSM的 “套件中心” 安装的软件包是以pkgctl-为前缀命名的 # 列出系统运行的服务 synoservice synoservicecfg --list # 重启系统服务 synoservice --restart pkgctl-Docker 三、其他操作 1、恢复卸载硬盘无法识别加入存储池的数据 原因：DSM 显示磁盘损坏。重启后直接不认磁盘。插入Windows 电脑也无法挂载磁盘。磁盘仅有部分坏块，使用 Diskgenues 无法加载文件系统，仅能查看出分区被 raid设置过。 解决方案：在 Linux 系统中直接挂载磁盘分区进行数据迁移。可使用fdisk -l或cat /proc/mdstat或 lvs查看磁盘设备路径。 至于之前DSM 设置加密过磁盘分区（需要保存有加密存储空间的恢复密钥文件）： apt-get install cryptsetup # 解码恢复密钥 base64 --decode ${加密存储空间的恢复密钥文件路径} > ${base64_decode_output_path} # 测试恢复密钥是否正确 cryptsetup open --test-passphrase /dev/vgX/volume_Y -S 1 -d ${base64_decode_output_path} # 使用解码的恢复密钥为加密存储空间解密 cryptsetup open --allow-discards /dev/vgX/volume_Y cryptvol_Y -S 1 -d ${base64_decode_output_path} # 挂载磁盘分区 mount ${device_path} ${mount_point} -o ro 参考：https://kb.synology.cn/zh-cn/DSM/tutorial/How_can_I_recover_data_from_my_DiskStation_using_a_PC 2、扩充分区容量 群辉系统盘使用分区进行 raid 组盘的 fdisk只支持MBR分区，MBR分区表最大支撑2T的磁盘，所以无法划分大于2T的分区。而parted工具可以划分单个分区大于2T的GPT格式的分区，也可以划分普通的MBR分区。 # 使用 parted 查看所有磁盘分区的详细情况以及其上的文件系统 $ parted -l Model: SSD 128GB (scsi) Disk /dev/sdb: 128GB Sector size (logical/physical): 512B/512B Partition Table: gpt Disk Flags: Number Start End Size File system Name Flags 1 1049kB 2551MB 2550MB ext4 Linux RAID raid 2 2551MB 4699MB 2147MB linux-swap(v1) Linux RAID raid 3 4699MB 4766MB 67.1MB fat32 primary boot, esp 4 4766MB 4832MB 66.1MB fat16 primary 5 4832MB 128GB 123GB Linux RAID raid Model: TOSHIBA HDWD110 (scsi) Disk /dev/sdf: 1000GB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 2551MB 2550MB primary raid 2 2551MB 4699MB 2147MB primary raid 3 4832MB 1000GB 995GB primary raid # ---------Raid组--------- Model: Linux Software RAID Array (md) Disk /dev/md0: 2550MB Sector size (logical/physical): 512B/512B Partition Table: loop Disk Flags: Number Start End Size File system Flags 1 0.00B 2550MB 2550MB ext4 # ---------Raid组，分区/dev/md1，交互分区文件系统swap--------- Model: Linux Software RAID Array (md) Disk /dev/md1: 2147MB Sector size (logical/physical): 512B/512B Partition Table: loop Disk Flags: Number Start End Size File system Flags 1 0.00B 2147MB 2147MB linux-swap(v1) # ---------Raid组，分区/dev/md2，文件系统btrfs --------- Model: Linux Software RAID Array (md) Disk /dev/md2: 123GB Sector size (logical/physical): 512B/512B Partition Table: loop Disk Flags: Number Start End Size File system Flags 1 0.00B 9.63GB 9.63GB btrfs # ---------Raid组，分区/dev/md3，文件系统btrfs--------- Model: Linux Software RAID Array (md) Disk /dev/md3: 995GB Sector size (logical/physical): 512B/512B Partition Table: loop Disk Flags: Number Start End Size File system Flags 1 0.00B 995GB 995GB btrfs # 查看分区挂载情况 $ df -mh Filesystem Size Used Avail Use% Mounted on /dev/md0 2.3G 1.4G 793M 65% / none 3.9G 0 3.9G 0% /dev /tmp 3.9G 1.2M 3.9G 1% /tmp /run 3.9G 3.7M 3.9G 1% /run /dev/shm 3.9G 4.0K 3.9G 1% /dev/shm none 4.0K 0 4.0K 0% /sys/fs/cgroup cgmfs 100K 0 100K 0% /run/cgmanager/fs /dev/md2 9.3G 2.0G 6.9G 22% /volume1 /dev/md3 890G 740G 151G 84% /volume2 # 从上述命令得知群辉的用户文件系统所在分区/dev/md2挂载到/volume1。要进行扩容的分区正是/volume1。 # 使用RAID磁盘阵列管理命令查看raid分区的详细情况 $ mdadm -D /dev/md2 /dev/md2: Version : 1.2 Creation Time : Sat Apr 20 14:27:24 2019 Raid Level : raid1 Array Size : 10094080 (9.63 GiB 10.34 GB) Used Dev Size : 10094080 (9.63 GiB 10.34 GB) Raid Devices : 1 Total Devices : 1 Persistence : Superblock is persistent Update Time : Mon Nov 13 13:11:43 2023 State : clean Active Devices : 1 Working Devices : 1 Failed Devices : 0 Spare Devices : 0 Name : Synology:2 UUID : 4a0fadf4:7e52e674:f2202100:91789994 Events : 2137 Number Major Minor RaidDevice State 0 8 21 0 active sync /dev/sdb5 # 从上述命令得知分区所在磁盘为/dev/sdb。使用 fdisk 查看磁盘的分区详细情况 $ fdisk /dev/sdb Command (m for help): p Disk /dev/sdb: 119.2 GiB, 128035676160 bytes, 250069680 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: 0D7DE78D-A648-4E1D-8FE2-E9FB24CAFA38 Device Start End Sectors Size Type /dev/sdb1 2048 4982527 4980480 2.4G Linux RAID /dev/sdb2 4982528 9176831 4194304 2G Linux RAID /dev/sdb3 9177088 9308159 131072 64M EFI System /dev/sdb4 9308160 9437183 129024 63M Linux filesystem /dev/sdb5 9437184 29627391 20190208 9.6G Linux RAID # 查看得知还有 250069680-20190208=229879472 个扇区没有使用，大约117698289664*512=117698289664 Bytes个字节大小，约117698289664/1024/1024/1024=109.61507416 GB大小 扩容分区容量（命令中的5，对应/dev/sdb5） parted /dev/sdb resizepart 5 100% 在群辉存储池管理中进行手动扩容。 $ df -mh Filesystem Size Used Avail Use% Mounted on /dev/md0 2.3G 1.4G 793M 65% / none 3.9G 0 3.9G 0% /dev /tmp 3.9G 1.2M 3.9G 1% /tmp /run 3.9G 3.7M 3.9G 1% /run /dev/shm 3.9G 4.0K 3.9G 1% /dev/shm none 4.0K 0 4.0K 0% /sys/fs/cgroup cgmfs 100K 0 100K 0% /run/cgmanager/fs /dev/md2 111G 1.9G 109G 2% /volume1 /dev/md3 890G 740G 151G 84% /volume2 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:31:26 "},"origin/synology-abb-vsphere.html":{"url":"origin/synology-abb-vsphere.html","title":"Synology Active Backup for Business备份管理vSphere ESXI VMs","keywords":"","body":"群辉ABB(Active Backup for Business)备份恢复vSphere中的VM 一、简介 对于vSphere中的虚拟机，除了ovftools导入导出为OVF文件进行备份外，Synology中的Active Backup for Business商业软件（以下简称ABB）也支持更多的备份功能。例如定时备份，版本管理等。 二、安装激活ABB 先在Synology的套件中心中搜索Active Backup for Business下载安装，过程省略。 1、获取产品序列号 在【控制面板】 -->【信息中心】找到【产品序列号】，复制下来。 2、登录 浏览器中输入： http://群辉主机地址:群辉主机端口/webapi/auth.cgi?api=SYNO.API.Auth&method=Login&version=1&account=用户名&passwd=密码 当浏览器出现以下字样，说明登录成功 {\"success\":true} 3、激活 浏览器中输入： http://群辉主机地址:群辉主机端口/webapi/entry.cgi?api=SYNO.ActiveBackup.Activation&method=set&version=1&activated=true&serial_number=“产品序列号” 当浏览器出现以下字样，说明Active Backup for Business套件激活成功 {\"data\":{\"actived\":true}\"success\":true} Enjoy Yourself 参考： https://www.tenlonstudio.com/7478.html 三、ABB配置、备份vSphere VMs 1、连接vSphere ESXI 添加连接完成后会显示vSphere ESXI的版本及在线状态，同时会显示其上的所有VM及备份状态 2、创建备份VM任务 ①选择备份到哪个共享文件夹 ②选择要备份的VM ③配置备份任务 启用更改块跟踪：当启用更改块跟踪以便仅传输上次备份时间以来更改的块，能大幅减少传输的数据大小 启用应用程序感知备份：当启用应用程序感知备份时，会利用VMware Tools和Microsoft Shadow Copy Server(VSS)确保Linux和Window虚拟机的备份数据一致性。若要使用此功能，请确保在支持VSS的Windows虚拟机上安装最新版的VMware Tools. 启用数据传输压缩： 启用数据传输加密： ④配置备份周期 ⑤设置备份保留策略 3、查看备份任务信息 4、备份文件信息 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/iSCSI-简介配置使用.html":{"url":"origin/iSCSI-简介配置使用.html","title":"群晖Synology的iSCSI","keywords":"","body":"iSCSI的简介配置使用 一、iSCSI简介 iSCSI（Internet Small Computer System Interface），Internet小型计算机系统接口，又称为IP-SAN，是由IBM 下属的两大研发机构一一加利福尼亚AImaden和以色列Haifa研究中心共同开发的，是一个供硬件设备使用的、可在IP协议上层运行的SCSI指令集，是一种开放的基于IP协议的工业技术标准。一种基于因特网及SCSI-3协议下的存储技术，于2003年2月11日成为正式的标准 iSCSI使用 TCP/IP 协议（一般使用TCP端口860和3260）。 本质上，iSCSI 让两个主机通过 IP 网络相互协商然后交换SCSI命令。这样一来，iSCSI 就是用广域网仿真了一个常用的高性能本地存储总线，从而创建了一个存储局域网（SAN）。不像某些 SAN 协议，iSCSI 不需要专用的电缆；它可以在已有的交换和 IP 基础架构上运行。然而，如果不使用专用的网络或者子网（ LAN 或者 VLAN ），iSCSI SAN 的部署性能可能会严重下降 两部计算机之间利用iSCSI的协议来交换SCSI命令，让计算机可以透过高速的局域网集线来把SAN模拟成为本地的储存装置 iSCSI target：就是iSCSI的server，可以是一个物理磁阵；也可以是软件实现的iSCSI server。有硬件方式实现的iSCSI target，例如iSCSI的hba卡和带isoe（iSCSI offload engine）网卡（硬件上将iSCSI 包接包和封包） iSCSI initiator：就是iSCSI的客户端，它可以是一个软件，也可以是一个硬件。如果是软件在linux上，用户态实现的是tgt框架（linux scsi target frame）；还有一种内核太实现的架构是iet（iSCSI enterprise target）。在centos上目前已经默认安装了tgt了。 iqn（iSCSI qualified name）：initiator和target通过iqn号来逻辑寻址。一个iqn号由四部分组成： iqn.日期.域名:域名组织分配的名字 例如：iqn.2000-01.com.synology:Synology.Target-1.826d6a066b LUN：全称是Logical Unit Number，中文名是逻辑单元号。LUN是在存储设备上可以被应用服务器识别的独立存储单元。一个LUN的空间来源于存储池Pool，Pool的空间来源于组成磁盘阵列的若干块硬盘。从应用服务器的角度来看，一个LUN可以被视为一块可以使用的硬盘。例如，在Linux系统中，它在/dev/rdsk、/dev/dsk目录下有相应的设备名称；在Windows系统中，格式化后的新LUN会对应一个类似于D E F的盘符。 Thick LUN：中文名是传统非精简LUN，是LUN类型的一种，支持虚拟资源分配，能以较为简便的方式进行创建、扩容和压缩操作。Thick LUN在创建完成后就会从存储池Pool中分配满额的存储空间，即LUN的大小完全等于分配的空间。因此，它拥有较高的可预测性。 Thin LUN：中文名是精简LUN，也是LUN类型的一种，支持虚拟资源分配，能够以较简便的方式进行创建、扩容和压缩操作。Thin LUN在创建的时候，可以设置一个初始分配容量。创建完成后，存储池Pool只会分配这个初始容量大小的空间剩余的空间仍然放在存储池中。当Thin LUN已分配的存储空间的使用率达到阈值时，存储系统才会再从Pool中划分一定的配额给Thin LUN。如此反复，直到达到Thin LUN最初设定的全部容量。因此，它拥有较高的存储空间利用率。 二、群晖Synology的iSCSI存储 创建LUN 创建Target Target关联LUN 三、Windows挂载 参考链接 https://jingyan.baidu.com/article/e4511cf37feade2b845eaff8.html https://blog.csdn.net/M_joy666/article/details/80566705 附录：Thick LUN与Thin LUN的区别 1、空间分配上的区别 Thick LUN在创建时会分配所有需要的空间 Thin LUN是一种按需分配的空间组织方法，它在创建时存储池不会分配所有需要的空间，而是根据使用情况动态分配。二者的空间分配区别如下图所示： 2、空间回收的区别 注：这里的空间回收指的是释放存储池Pool中的资源，并且这些资源可以被其他LUN使用。 Thick LUN没有空间回收的概念，因为它在创建时就占用存储池中所有分配给它的空间，即使Thick LUN中的数据被删除，存储池中分配给它的空间还是被占用，不能被其他的LUN使用。但是如果手动删除不再使用的Thick LUN，则对应的空间会被回收。 Thin LUN不仅能够做到空间占用率增大时自动分配新的存储空间，而且当Thin LUN中的文件删除时也可以实现空间的释放，从而实现存储空间的反复利用，大大提高存储空间的利用率。Thin LUN的空间回收如下图所示： 3、性能的区别 Thick LUN由于在一开始就会拥有所分配的空间，所以Thick LUN在顺序读写的时候拥有较高的性能，但是会造成空间资源的浪费。 Thin LUN由于是实时分配空间，每次扩容时，需要重新增加容量，后台重新格式化，这个时候性能会受到一定影响，而且每次分配空间可能会导致硬盘中存储空间不连续，这样硬盘读写数据时在寻找存放位置上花费的时间会较多，会在顺序读写时对性能有一定影响。 4、使用场景的区别 Thick LUN： ①对性能要求较高的场景 ②对存储空间利用率不太敏感的场景 ③对成本要求不太高的场景 Thin LUN： ①对性能要求一般的场景； ②对存储空间利用率比较敏感的场景； ③对成本比较敏感的场景； ④应用环境很难预 估存储空间的场景 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2020-06-16 21:35:29 "},"origin/excalidraw.html":{"url":"origin/excalidraw.html","title":"开源画图平台:Excalidraw","keywords":"","body":"Excalidraw画布 一、简介 Github: https://github.com/excalidraw/excalidraw 官方文档：https://docs.excalidraw.com/docs 二、私有化部署 1、写个 excalidraw collaboration server：https://github.com/excalidraw/excalidraw-room https://github.com/alswl/excalidraw-storage-backend excalidraw http storage backend：https://github.com/alswl/excalidraw-storage-backend 改字体浏览器插件：https://chromewebstore.google.com/detail/excalidraw-custom-font/afbeaojffbjckicjpkecknoocdpmgoah 参考： https://github.com/excalidraw/excalidraw https://github.com/excalidraw/excalidraw/discussions/3879 https://blog.alswl.com/2022/10/self-hosted-excalidraw/ https://github.com/alswl/excalidraw-storage-backend https://github.com/alswl/excalidraw Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-05-22 09:24:40 "},"origin/cloudera-install.html":{"url":"origin/cloudera-install.html","title":"安装部署","keywords":"","body":"Cloudera 5.11安装部署 一、主机规划 主机角色 Host Name IP 硬件 外挂硬盘及挂载目录大小 额外服务角色 Cloudera Manger cm.cloudera.curiouser.com 172.16.7.2 16C32G 500G /data 500G /var 50G /opt HTTPD,MySQL Master1 master1.cloudera.curiouser.com 172.16.7.3 16C64G 500G /data 500G /var 50G /opt Master2 master2.cloudera.curiouser.com 172.16.7.4 16C32G 500G /data 500G /var 50G /opt Node1 node1.cloudera.curiouser.com 172.16.7.5 16C32G 500G /data 500G /var 50G /opt Node2 node2.cloudera.curiouser.com 172.16.7.6 16C32G 500G /data 500G /var 50G /opt Node3 node3.cloudera.curiouser.com 172.16.7.7 16C32G 500G /data 500G /var 50G /opt 二、Prerequisite 0. 关闭所有主机的SELinux，防火墙，IPV6，透明大页，禁止内存交换 systemctl stop firewalld &&\\ systemctl disable firewalld ;\\ systemctl stop iptables &&\\ systemctl disable iptables;\\ systemctl stop ip6tables &&\\ systemctl disable ip6tables ;\\ setenforce 0 &&\\ sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux ;\\ echo never > /sys/kernel/mm/transparent_hugepage/enabled &&\\ echo never > /sys/kernel/mm/transparent_hugepage/defrag ;\\ echo 'GRUB_CMDLINE_LINUX=\"transparent_hugepage=never\"' >> /etc/default/grub &&\\ grub2-mkconfig -o /boot/grub2/grub.cfg &&\\ sed -i '$a NETWORKING_IPV6=no' /etc/sysconfig/network &&\\ echo \"net.ipv6.conf.all.disable_ipv6=1\" >> /etc/sysctl.conf &&\\ sysctl -p &&\\ cat /proc/sys/net/ipv6/conf/all/disable_ipv6 ;\\ sysctl vm.swappiness=0 &&\\ echo \" vm.swappiness = 0\" >> /etc/sysctl.conf 1. 设置主机名，所有主机配置hosts域名IP 地址映射 hostnamectl --static set-hostname cm.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.7.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.7 node3.cloudera.curiouser.com\" >> /etc/hosts ;\\ reboot now hostnamectl --static set-hostname master1.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.7.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.7 node3.cloudera.curiouser.com\" >> /etc/hosts ;\\ reboot now hostnamectl --static set-hostname master2.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.7.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.7 node3.cloudera.curiouser.com\" >> /etc/hosts ;\\ reboot now hostnamectl --static set-hostname node1.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.7.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.7 node3.cloudera.curiouser.com\" >> /etc/hosts ;\\ reboot now hostnamectl --static set-hostname node2.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.7.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.7 node3.cloudera.curiouser.com\" >> /etc/hosts ;\\ reboot now hostnamectl --static set-hostname node3.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.7.7 node3.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.7.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ reboot now 2. 将CM主机配置成内网CentOS 、CM、CDH、Parcels、MySQL安装包的YUM源 所有主机备份自带的YUM源配置文件 mkdir /etc/yum.repos.d/bak ;\\ mv /etc/yum.repos.d/r* /etc/yum.repos.d/bak/ ;\\ yum clean all 上传系统镜像ISO文件到CM节点的/mnt目录下 CM节点挂载系统镜像ISO文件并配置本地YUM源 echo \"/mnt/rhel-server-7.4-x86_64-dvd.iso /mnt/cdrom iso9660 defaults,loop 0 0\" >> /etc/fstab ;\\ mkdir -p /mnt/cdrom ;\\ mount -a ;\\ bash -c 'cat > /etc/yum.repos.d/local.repo 配置CM节点为YUM源 yum install -y httpd telnet net-tools wget createrepo;\\ ln -s /mnt/cdrom/ /var/www/html/rhel74 ;\\ systemctl enable httpd ;\\ systemctl start httpd ;\\ rm -rf /etc/yum.repos.d/local.repo ;\\ bash -c 'cat > /etc/yum.repos.d/rhel74.repo 上传CM、MySQL、JDK、CDH，Spark，Kafka的parcels包相关的安装包到CM主机的/var/www/html/目录下 CM相关的RPM官网下载地址：https://archive.cloudera.com/cm6/ CDH相关的RPM官网下载地址：https://archive.cloudera.com/cdh6/ bash -c 'cat > /etc/yum.repos.d/cm5.repo /etc/yum.repos.d/mysql57.repo 最终CM主机/var/www/html路径下的目录结构为下图： 4. 打通CM主机到其余主机的SSH免密钥登录 cd ~ ;\\ bash -c 'cat > ./HitthroughSSH.sh ./hosts.txt 5. 将CM主机上的YUM源同步到其他主机上 for i in {cm,master1,master2,node1,node2,node3} ;do scp /etc/yum.repos.d/rhel74.repo $i.cloudera.curiouser.com:/etc/yum.repos.d/ ;done 6. 配置集群内的NTP时间同步 将CM主机作为NTP服务端 yum install ntp -y ;\\ rm -rf /etc/ntp.conf ;\\ bash -c 'cat > /etc/ntp.conf 其他主机为NTP客户端 yum install -y ntp ;\\ rm -rf /etc/ntp.conf ;\\ bash -c 'cat > /etc/ntp.conf 7. 所有主机安装Oracle JDK并替换JCE yum localinstall -y http://cm.cloudera.curiouser.com/oracle-jdk/jdk-8u144-linux-x64.rpm ;\\ yum install -y wget ;\\ rm -rf /usr/java/jdk1.8.0_144/jre/lib/security/{local_policy.jar,US_export_policy.jar} ;\\ wget http://cm.cloudera.curiouser.com/oracle-jdk/{local_policy.jar,US_export_policy.jar} -P /usr/java/jdk1.8.0_144/jre/lib/security/ 8. 所有主机挂载额外硬盘到/data目录 disk=sdb &&\\ yum install -y lvm2 &&\\ pvcreate /dev/${disk} &&\\ vgcreate -s 4M data /dev/${disk} &&\\ PE_Number=`vgdisplay data|grep \"Free PE\"|awk '{print $5}'` &&\\ lvcreate -l ${PE_Number} -n data data &&\\ mkfs.xfs /dev/data/data &&\\ echo \"/dev/data/data /data xfs defaults 0 0\" >> /etc/fstab &&\\ mkdir /data &&\\ mount -a &&\\ df -mh 9、CM节点安装MySQL，并添加MySQL 的JDBC包 安装MySQL yum install -y mysql-community-server &&\\ rm -rf /etc/my.cnf ;\\ bash -c 'cat > /etc/my.cnf 修改MySQL用户root的默认密码 mysql -uroot -p`awk '{ if(match($0,\"root@localhost: \")) {print substr($0,RSTART+RLENGTH) }}' /data/mysql/logs/mysqld.log` -e \"ALTER USER 'root'@'localhost' IDENTIFIED BY 'Test@123';\" 创建相关Database mysql -uroot -p`awk '{ if(match($0,\"root@localhost: \")) {print substr($0,RSTART+RLENGTH) }}' /data/mysql/logs/mysqld.log` \\ -e \"create database scm default character set utf8;\" \\ -e \"create database rman default character set utf8;\" \\ -e \"create database metastore default character set utf8;\" \\ -e \"create database oozie default character set utf8;\" \\ -e \"create database hue default character set utf8;\" \\ -e \"create database sentry default character set utf8;\" \\ -e \"create database hive default character set utf8;\" \\ -e \"grant all privileges on scm.* to 'scm'@'%' identified by '123456';\" \\ -e \"grant all privileges on rman.* to 'rman'@'%' identified by '123456';\" \\ -e \"grant all privileges on metastore.* to 'metastore'@'%' identified by '123456';\" \\ -e \"grant all privileges on oozie.* to 'oozie'@'%' identified by '123456';\" \\ -e \"grant all privileges on hue.* to 'hue'@'%' identified by '123456';\" \\ -e \"grant all privileges on sentry.* to 'sentry'@'%' identified by '123456';\" \\ -e \"grant all privileges on hive.* to 'hive'@'%' identified by '123456';\" \\ -e \"flush privileges;\" 将MySQL的JDBC包分发到所有主机上 mkdir /usr/share/java ;\\ wget http://cm.cloudera.curiouser.com/mysql/jdbc/mysql-connector-java-5.1.46.jar -P /usr/share/java/ ; ln -s /usr/share/java/mysql-connector-java-5.1.46.jar /usr/share/java/mysql-connector-java.jar 三、CM节点安装Cloudera Manager 1、安装服务 yum install -y cloudera-manager-daemons cloudera-manager-server 2、配置 Cloudera Manager 能够连接 Mysql 外部数据库 /usr/share/cmf/schema/scm_prepare_database.sh -h cm.cloudera.curiouser.com mysql scm scm 123456 3、启动服务 systemctl enable cloudera-scm-server &&\\ systemctl start cloudera-scm-server &&\\ systemctl status cloudera-scm-server 服务启动日志：/var/log/cloudera-scm-server/cloudera-scm-server.log 四、在ClouderaManager的Web UI界面上安装Cloudera Agent和CDH http://cm.cloudera.curiouser.com/cloudera/cm/5.11.1/ http://cm.cloudera.curiouser.com/cloudera/cdh/5.11.1/ http://cm.cloudera.curiouser.com/cloudera/parcels/kafka/2.1.1/ http://cm.cloudera.curiouser.com/cloudera/parcels/spark/2.3.0/ 五、CDH内服务的集群配置 修改过zookeeper的默认数据存储目录后，安装Zookeeper的时候会提示无法自动创建目录，所以在Zookeeper服务实例主机上手动创建该目录。 mkdir -p /data/zookeeper/version-2;\\ chown -R zookeeper:zookeeper /data/zookeeper 六、集群配置优化 刚创建完的集群会提示你例如HDFS服务NameNode节点的Java堆栈大小根据服务实例主机的具体硬件配置提示设置得不合理。所以需要再次优化配置。 根据Cloudera Manage上的提示修改配置然后重启即可。 备注：集群中黄色警告配置是主机上分配的角色，占用的内存超出了主机的物理内存 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/cloudera-cluster-manage.html":{"url":"origin/cloudera-cluster-manage.html","title":"集群管理","keywords":"","body":"Cloudera 集群管理 一、集群扩容添加节点 1、新建主机Prerequisite 关闭防火墙，SELinux，IPV6，禁止内存交换，关闭透明大页面-->hosts添加域名IP映射-->其他节点hosts添加新主机IP域名映射-->打通CM节点到新主机SSH免密钥登录-->修改YUM源-->挂载硬盘-->NTP-->安装JDK -->安装并配置Kerberos客户端 systemctl stop firewalld &&\\ systemctl disable firewalld ;\\ systemctl stop iptables &&\\ systemctl disable iptables;\\ systemctl stop ip6tables &&\\ systemctl disable ip6tables ;\\ setenforce 0 &&\\ sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux ;\\ echo never > /sys/kernel/mm/transparent_hugepage/enabled &&\\ echo never > /sys/kernel/mm/transparent_hugepage/defrag ;\\ echo 'GRUB_CMDLINE_LINUX=\"transparent_hugepage=never\"' >> /etc/default/grub &&\\ grub2-mkconfig -o /boot/grub2/grub.cfg &&\\ sed -i '$a NETWORKING_IPV6=no' /etc/sysconfig/network &&\\ echo \"net.ipv6.conf.all.disable_ipv6=1\" >> /etc/sysctl.conf &&\\ sysctl -p &&\\ cat /proc/sys/net/ipv6/conf/all/disable_ipv6 ;\\ sysctl vm.swappiness=0 &&\\ echo \" vm.swappiness = 0\" >> /etc/sysctl.conf 新增节点配置集群其他主机的IP域名解析 hostnamectl --static set-hostname node4.cloudera.curiouser.com ;\\ sed -i '$d' /etc/hosts ;\\ echo \"172.16.0.8 node4.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.0.3 master1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.0.2 cm.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.0.4 master2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.0.5 node1.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.0.6 node2.cloudera.curiouser.com\" >> /etc/hosts ;\\ echo \"172.16.0.7 node3.cloudera.curiouser.com\" >> /etc/hostts ;\\ reboot now 集群其他节点hosts文件中添加新增节点IP域名解析 echo \"172.16.0.8 node4.cloudera.curiouser.com\" >> /etc/hosts 打通CM节点到新增节点的SSH免密登录 ssh-copy-id -i node4.cloudera.curiouser.com 配置YUM源 mkdir /etc/yum.repos.d/bak &&\\ mv /etc/yum.repos.d/r* /etc/yum.repos.d/bak/ &&\\ bash -c 'cat > /etc/yum.repos.d/rhel74.repo 挂载额外LVM硬盘 disk=sdb &&\\ yum install -y lvm2 &&\\ pvcreate /dev/${disk} &&\\ vgcreate -s 4M data /dev/${disk} &&\\ PE_Number=`vgdisplay|grep \"Free PE\"|awk '{print $5}'` &&\\ lvcreate -l ${PE_Number} -n data data &&\\ mkfs.xfs /dev/data/data &&\\ echo \"/dev/data/data /data xfs defaults 0 0\" >> /etc/fstab &&\\ mkdir /data &&\\ mount -a &&\\ df -mh 配置新增节点NTP客户端同步集群时间 yum install -y ntp ;\\ rm -rf /etc/ntp.conf ;\\ bash -c 'cat > /etc/ntp.conf 安装JDK yum localinstall -y http://cm.cloudera.curiouser.com/oracle-jdk/jdk-8u144-linux-x64.rpm ;\\ yum install -y wget ;\\ rm -rf /usr/java/jdk1.8.0_144/jre/lib/security/{local_policy.jar,US_export_policy.jar} ;\\ wget http://cm.cloudera.curiouser.com/oracle-jdk/{local_policy.jar,US_export_policy.jar} -P /usr/java/jdk1.8.0_144/jre/lib/security/ 安装kerberos客户端 yum -y install krb5-libs krb5-workstation &&\\ scp cm.cloudera.curiouser.com:/etc/krb5.conf /etc/ 2、Cloudera Manager Web UI界面上添加主机 3、将添加现有服务的角色到新增主机上 mkdir -p /data/zookeeper/version-2;\\ chown -R zookeeper:zookeeper /data/zookeeper 二、修改Cloudera Manager Server的日志存储位置 关闭Cloudera Manager Server服务 systemctl stop cloudera-scm-server 在/etc/default/cloudera-scm-server配置文件中追加日志存储位置的变量配置 echo \"export CMF_VAR=/opt\" >> /etc/default/cloudera-scm-server 创建相关目录并修改权限 cd /opt mkdir log chown cloudera-scm:cloudera-scm log mkdir /opt/log/cloudera-scm-server chown cloudera-scm:cloudera-scm log/cloudera-scm-server mkdir run chown cloudera-scm:cloudera-scm run 重启Cloudera Manager Server服务 systemctl start cloudera-scm-server 三、安装CDH集群报“出现 Entropy 不良问题” 原因：系统熵值低于 CDH 检测的阀值引起的 解决方案： 1：查询系统熵值大小 cat /proc/sys/kernel/random/entropy_avail 2：配置 sudo yum install rng-tools && \\ cp /usr/lib/systemd/system/rngd.service /etc/systemd/system/ && \\ sed -i -e 's/ExecStart=\\/sbin\\/rngd -f/ExecStart=\\/sbin\\/rngd -f -r \\/dev\\/urandom/' /etc/systemd/system/rngd.service && \\ systemctl daemon-reload && \\ systemctl start rngd && \\ systemctl enable rngd 官方参考文章：https://www.cloudera.com/documentation/enterprise/latest/topics/encryption_prereqs.html#concept_by1_pv4_y5 参考阅读： 什么是随机数 很多软件和应用都需要随机数，从纸牌游戏中纸牌的分发到 SSL 安全协议中密钥的产生，到处都有随机数的身影。随机数至少具备两个条件： 数字序列在统计上是随机的 不能通过已知序列推算后面的序列 自从计算机诞生起，寻求用计算机产生高质量的随机数序列的研究就一直是研究者长期关注的课题。一般情况下，使用计算机程序产生一个真正的随机数是很难的， 因为程序的行为是可预测的，计算机利用设计好的算法结合用户提供的种子产生的随机数序列通常是“伪随机数”（pseudo-random number），伪随机数就是我们平时经常使用的“随机数”。伪随机数可以满足一般应用的需求，但是在对于安全要求比较高的环境和领域中存在明显的缺点： 伪随机数是周期性的，当它们足够多时，会重复数字序列 如果提供相同的算法和相同的种子值，将会得出完全一样的随机数序列 可以使用逆向工程，猜测算法与种子值，以便推算后面所有的随机数列 只有实际物理过程才是真正的随机，只有借助物理世界中事物的随机性才能产生真正的随机数，比如真空内亚原子粒子量子涨落产生的噪音、超亮发光二极管在噪声的量子不确定性和放射性衰变等。 随机数为什么如此重要 生成随机数是密码学中的一项基本任务，是生成加密密钥、加密算法和加密协议所必不可少的，随机数的质量对安全性至关重要。最近报道有人利用随机数缺点成功 攻击了某网站，获得了管理员的权限。美国和法国的安全研究人员最近也评估了两个 Linux 内核 PRNG——/dev/random 和/dev/urandom 的安全性，认为 Linux 的伪随机数生成器不满足鲁棒性的安全概念，没有正确积累熵。可见随机数在安全系统中占据着非常重要的地位。 来源： https://blog.csdn.net/Mask_V/article/details/82983679 四、添加第三方jar到cloudear 安装的服务中 对于cloudear 安装的服务如果需要添加第三方jar，添加到/var/lib/ 目录中对应的服务目录下即可 五、修改Cloudera集群中服务默认的JAVA 官方文档：https://www.cloudera.com/documentation/enterprise/5-11-x/topics/cm_ig_java_home_location.html#cmig_topic_16 六、Cloudera集群开启邮件告警功能 1.CM节点安装SNMP告警接收服务 yum -y install net-snmp net-snmp-devel net-snmp-utils 2.修改/etc/snmp/snmptrapd.conf配置文件 echo \"authCommunity log,execute,net public\" >> /etc/snmp/snmptrapd.conf 3.启动snmptrapd服务并检查 snmptrapd -C -c /etc/snmp/snmptrapd.conf -df -Lo #启动参数说明： -C ：表示不使用net-snmp默认路径下的配置文件snmptrapd.conf -c ：指定snmptrapd.conf文件 -d ：显示收到和发送的数据报，通过这个选项可以看到数据报文 -f ：默认情况下，snmptrapd是在后台中运行的，加上这个选项，表示在前台运行 -L ：指定日志记录在哪里，后面的o表示直接输出到屏幕上，如果是跟着f表示日志记录到指定的文件中 七、在未启用认证(kerberos/LDAP)情况下安装及使用Sentry CDH平台中的安全，认证（Kerberos/LDAP）是第一步，授权（Sentry）是第二步。如果要启用授权，必须先启用认证。但在CDH平台中给出了一种测试模式，即不启用认证而只启用Sentry授权。但强烈不建议在生产系统中这样使用，因为如果没有用户认证，授权没有任何意义形同虚设，用户可以随意使用任何超级用户登录HiveServer2或者Impala，并不会做密码校验。注：本文档仅适用于测试环境。 八、HDFS资源配额限制 对于/user/hdfs目录设置只有1M配额 然后使用hdfs用户上传一个4K的文件 发现无法上传。然后将/user/hdfs目录配额设置为500M 然后再次上传，发现可以上传 在HDFS查看该文件，发现一个实际大小为17B的小文件，在hdfs却占用HDFS 384MB的空间。实际原因是，该大小的文件使用了HDFS一个Block块，默认HDFS的Block块大小为128MB，文件Block块复制因子为3，就是一个Block块有3个副本。 九、Cloudera集群开启Kerberos认证 1、将 KDC 服务安装在 Cloudera Manager Server 所在服务器CM主机上 yum install -y krb5-server krb5-libs krb5-auth-dialog krb5-workstation 2、修改/etc/krb5.conf 配置 # Configuration snippets may be placed in this directory as well includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true ################################################ rdns = true default_realm = CURIOUSER.COM default_ccache_name = KEYRING:persistent:%{uid} [realms] CURIOUSER.COM = { kdc = cm.cloudera.CURIOUSER.com admin_server = cm.cloudera.CURIOUSER.com } [domain_realm] .cm.cloudera.CURIOUSER.com = CURIOUSER.COM cm.cloudera.CURIOUSER.com = CURIOUSER.COM 3、修改/var/kerberos/krb5kdc/kadm5.acl 配置 */admin@CURIOUSER.COM * 4、修改/var/kerberos/krb5kdc/kdc.conf 配置 [kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88 [realms] CURIOUSER.COM = { #master_key_type = aes256-cts max_renewable_life= 7d 0h 0m 0s acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal } ​` 5、创建 Kerberos 数据库 ​```shell kdb5_util create -r CURIOUSER.COM -s Loading random data Initializing database '/var/kerberos/krb5kdc/principal' for realm 'CURIOUSER.COM', master key name 'K/M@CURIOUSER.COM' You will be prompted for the database Master Password. It is important that you NOT FORGET this password. Enter KDC database master key: Re-enter KDC database master key to verify: ![img](../assets/cloudera-cluster-manage-50.png) ### 6、创建 Kerberos 的管理账号 kadmin.local Authenticating as principal root/admin@CURIOUSER.COM with password. kadmin.local: addprinc admin/admin@CURIOUSER.COM WARNING: no policy specified for admin/admin@CURIOUSER.COM; defaulting to no policy Enter password for principal \"admin/admin@CURIOUSER.COM\": Re-enter password for principal \"admin/admin@CURIOUSER.COM\": Principal \"admin/admin@CURIOUSER.COM\" created. kadmin.local: exit ![img](../assets/cloudera-cluster-manage-51.png) ### 7、将 Kerberos 服务添加到自启动服务，并启动 krb5kdc 和 kadmin 服务 systemctl enable krb5kdc &&\\ systemctl start krb5kdc &&\\ systemctl status krb5kdc &&\\ systemctl enable kadmin &&\\ systemctl start kadmin &&\\ systemctl status kadmin ### 8、测试 Kerberos 的管理员账号 kinit admin/admin@CURIOUSER.COM ![img](../assets/d027f02a-6643-472d-ac8a-a8f5c7ce083c.png) klist Ticket cache: KEYRING:persistent:0:0 Default principal: admin/admin@CURIOUSER.COM Valid starting Expires Service principal 07/23/2018 23:43:13 07/24/2018 23:43:13 krbtgt/CURIOUSER.COM@CURIOUSER.COM renew until 07/30/2018 23:43:13 ![img](../assets/cloudera-cluster-manage-53.png) ### 9、为集群安装所有 Kerberos 客户端（包括 Cloudera Manager服务主机CM节点） yum -y install krb5-libs krb5-workstation ### 10、在 Cloudera Manager 服务主机CM节点上安装额外的包 yum -y install openldap-clients ### 11、将 KDC Server 上的 krb5.conf 文件拷贝到所有Kerberos 客户端 for i in {master1,master2,node1,node2,node3};do scp /etc/krb5.conf $i.cloudera.CURIOUSER.com:/etc/ ;done ### 12、在 KDC 中给 Cloudera Manager 添加管理员账号 kadmin.local Authenticating as principal admin/admin@CURIOUSER.COM with password. kadmin.local: addprinc cloudera-scm/admin@CURIOUSER.COM WARNING: no policy specified for cloudera-scm/admin@CURIOUSER.COM; defaulting to no policy Enter password for principal \"cloudera-scm/admin@CURIOUSER.COM\": Re-enter password for principal \"cloudera-scm/admin@CURIOUSER.COM\": Principal \"cloudera-scm/admin@CURIOUSER.COM\" created. kadmin.local: exit ![img](../assets/cloudera-cluster-manage-54.png) ### 13、在Cloudera Manager Web UI 界面开启Kerberos认证 ![img](../assets/cloudera-cluster-manage-55.png) ![img](../assets/cloudera-cluster-manage-56.png) ![img](../assets/cloudera-cluster-manage-57.png) ![img](../assets/cloudera-cluster-manage-58.png) ![img](../assets/cloudera-cluster-manage-59.png) ![img](../assets/cloudera-cluster-manage-60.png) ![img](../assets/cloudera-cluster-manage-61.png) ![img](../assets/cloudera-cluster-manage-62.png) ![img](../assets/cloudera-cluster-manage-63.png) ![img](../assets/cloudera-cluster-manage-64.png) ![img](../assets/cloudera-cluster-manage-65.png) # 十、开启HDFS NameNode的HA高可用 ![img](../assets/cloudera-cluster-manage-66.png) ![img](../assets/cloudera-cluster-manage-67.png) ![img](../assets/cloudera-cluster-manage-68.png) ![img](../assets/cloudera-cluster-manage-69.png) ![img](../assets/cloudera-cluster-manage-70.png) ![img](../assets/cloudera-cluster-manage-71.png) ![img](../assets/cloudera-cluster-manage-72.png) ![img](../assets/cloudera-cluster-manage-73.png) ![img](../assets/cloudera-cluster-manage-74.png) ![img](../assets/cloudera-cluster-manage-75.png) ![img](../assets/cloudera-cluster-manage-76.png) ![img](../assets/cloudera-cluster-manage-77.png) ![img](../assets/cloudera-cluster-manage-78.png) # 十一、Cloudera Manager大版本升级 ## 0、升级说明 1. 版本升级路径参考：https://docs.cloudera.com/documentation/enterprise/upgrade/topics/ug_upgrade_paths.html#ug_upgrade_paths 2. 只升级Cloudera Manage 3. cm5.11.1升级至cm5.15.0 ## 1、CM主机停止Cloudera Manager server，所有主机停止Cloudera Manager agent ![img](../assets/cloudera-cluster-manage-79.png) systemctl stop cloudera-scm-server systemctl stop cloudera-scm-agent ## 2、备份CM主机MySQL数据库中的所有Database mysqldump -uroot -p --all-databases > cloudera-mysql-backup.sql ## 3、备份Cloudera Manager server、agent的配置文件 mkdir ~/bak &&\\ cp -r /etc/cloudera-scm-server/ ~/bak/ &&\\ cp -r /etc/cloudera-scm-agent/ bak/ ## 4、所有主机配置新版本CM RPM包的YUM源 bash -c 'cat > /etc/yum.repos.d/cm5.15.0.repo http://cm.cloudera.curiouser.com/cloudera/cm/5.15.0 gpgcheck = 0 EOF' &&\\ yum clean all &&\\ yum repolist ## 5、使用rpm包升级ClouderaManager 的Server和Agent 升级Cloudera Manage Server yum -y upgrade cloudera-manager-server &&\\ systemctl start cloudera-scm-server 升级Cloudera Manage agent yum upgrade -y cloudera-manager-agent &&\\ systemctl start cloudera-scm-agent ``` 6、验证 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/cloudera-performance-ha-test.html":{"url":"origin/cloudera-performance-ha-test.html","title":"性能及高可用测试","keywords":"","body":"Cloudera 高可用及性能测试 一、HDFS NameNode高可用测试 向HDFS上传一个耗时间的大文件 测试关闭HDFS 活动的NameNode，看备用的NameNode是否自动故障转移成为活动的主NameNode，再查看上传文件到HDFS的任务是否成功 此时上传到HDFS的命令会显示原先的NameNode无法连接，但是上传任务会自动切换到新的活动的NameNode上 上传文件依旧成功 验证上传到HDFS中文件的完整性 二、YARN ResourceManager高可用测试 1、启动一个时间稍长的MR Job time sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen 200000000 /user/teragen 2、在MR Job任务运行时，将ResourceManager的活动节点停止，观察MR Job是否仍在运行。当MR Job任务运行完以后将关闭的ResourceManager重新启动，观察两个Resource Manager是否进行了主备切换。 三、He Master高可用测试 测试方案： 模拟用户正在向He中插入数据，此时Master的活动节点崩溃，测试He Master是否能够自动故障转移，将备用master节点转换成活动节点，继续接受用户的插入数据请求 模拟用户插入数据行为的脚本，定义为He数据的生产者 #!/bin/h echo -e \"create 'test','cf1'\" | he shell -n for i in {1..50} ;do echo -e \"put 'test','row-$i','cf1:test$x','value$i'\" | he shell -n ; sleep 1s ; done 监控He高可用的脚本，定义为He数据的消费者 #!/bin/h for i in {1..50} ;do echo -e \"scan 'test'\" | he shell -n ; sleep 1s ; done 此时模拟用户插入数据的脚本仍在运行 消费者仍能监控到数据 测试完将停掉的master重新启动起来，看它是否正常还能加入he集群，成为master的一员 四、Hive Metastore开启高可用、并测试高可用 通过手工 kill 掉 Hive Metastore Server 进程，模拟进程故障，验证 Hive Metastore Server 进程自动重启的功能。 再执行 ps 命令，查看 HMS 服务情况。可以看到 agent 检测到服务异常，并调用服务启动脚本，重启服务 检查新的 HMS 服务实例的启动时间 Cloudera Manager 的 Hive 服务状态页上也记录了该服务实例的异常，并产生相应的告警 五、HDFS DN目录挂盘方式、容量与IO测试 六、HiBench基准测试框架 https://blog.csdn.net/Fighingbigdata/article/details/79468898 七、Cloudera自带基准性能测试 说明 Cloudera自带了几个基准测试，被打包在几个jar包中，例如： /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.11.1.jar 当不带参数调用时，会列出所有的测试程序 # sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.11.1.jar An example program must be given as the first argument. Valid program names are: aggregatewordcount: An Aggregate ed map/reduce program that counts the words in the input files. aggregatewordhist: An Aggregate ed map/reduce program that computes the histogram of the words in the input files. bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi. dbcount: An example job that count the pageview counts from a datae. distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi. grep: A map/reduce program that counts the matches of a regex in the input. join: A job that effects a join over sorted, equally partitioned datasets multifilewc: A job that counts words from several files. pentomino: A map/reduce tile laying program to find solutions to pentomino problems. pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method. randomtextwriter: A map/reduce program that writes 10GB of random textual data per node. randomwriter: A map/reduce program that writes 10GB of random data per node. secondarysort: An example defining a secondary sort to the reduce. sort: A map/reduce program that sorts the data written by the random writer. sudoku: A sudoku solver. teragen: Generate data for the terasort terasort: Run the terasort teravalidate: Checking results of terasort wordcount: A map/reduce program that counts the words in the input files. wordmean: A map/reduce program that counts the average length of the words in the input files. wordmedian: A map/reduce program that counts the median length of the words in the input files. wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files. /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-test-2.6.0-mr1-cdh5.11.1.jar 当不带参数调用时，会列出所有的测试程序 # sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-test-2.6.0-mr1-cdh5.11.1.jar An example program must be given as the first argument. Valid program names are: DFSCIOTest: Distributed i/o benchmark of libhdfs. DistributedFSCheck: Distributed checkup of the file system consistency. MRReliabilityTest: A program that tests the reliability of the MR framework by injecting faults/failures TestDFSIO: Distributed i/o benchmark. dfsthroughput: measure hdfs throughput filebench: Benchmark SequenceFile(Input|Output)Format (block,record compressed and uncompressed), Text(Input|Output)Format (compressed and uncompressed) loadgen: Generic map/reduce load generator mapredtest: A map/reduce test check. minicluster: Single process HDFS and MR cluster. mrbench: A map/reduce benchmark that can create many small jobs nnbench: A benchmark that stresses the namenode. testarrayfile: A test for flat files of binary key/value pairs. testbigmapoutput: A map/reduce program that works on a very big non-splittable file and does identity map/reduce testfilesystem: A test for FileSystem read/write. testmapredsort: A map/reduce program that validates the map-reduce framework's sort. testrpc: A test for rpc. testsequencefile: A test for flat files of binary key value pairs. testsequencefileinputformat: A test for sequence file input format. testsetfile: A test for flat files of binary key/value pairs. testtextinputformat: A test for text input format. threadedmapbench: A map/reduce benchmark that compares the performance of maps with multiple spills over maps with 1 spill TeraSort排序 SortBenchmark(http://sortbenchmark.org/ )是JimGray自98年建立的一项排序竞技活动，它制定了不同类别的排序项目和场景，每年一次，决出各项排序算法实现的第一名(看介绍是在每年的ACM SIGMOD颁发奖牌哦)。 Hadoop在2008年以209秒的成绩获得年度TeraSort项(Dotona类)的第一名；而此前这一项排序的记录是297秒。从SortBenchmark网站上可以了解到，Hadoop到今天仍然保持了Minute项Daytona类型排序的冠军。Minute项排序是通过评判在60秒或小于60秒内能够排序的最大数据量来决定胜负的；其实等同于之前的TeraSort(TeraSort的评判标准是对1T数据排序的时间)。 SortBenchmark对排序的输入数据制定了详细规则，要求使用其提供的gensort工具(http://www.ordinal.com/gensort.html )生成输入数据。Hadoop的TeraSort也用Java实现了一个生成数据工具TeraGen，算法与gensort一致。 对输入数据的基础要求是：输入文件是由一行行100字节的记录组成，每行记录包括一个10字节的Key；以Key来对记录排序。 Minute项排序允许输入文件可以是多个文件，但Key的每个字节要求是binary编码而不是ASCII编码，也就是每个字符可能有256种可能，也就是说每条记录，有2的80次方种可能的Key； 同时Daytona类别则要求排序程序不仅是为10字节长Key、100字节长记录排序设计的，还可以支持对其他长度的Key或行记录进行排序；也就是说这个排序程序是通用的 1、生成Terasort排序数据Teragen time sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen 200000000 /user/teragen teragen后的数值单位是行数；因为每行100个字节，所以如果要产生1T的数据量，则这个数值应为1T/100=10000000000(10个0)。 每行记录由3段组成： 前10个字节：随机binary code的十个字符，为key 中间10个字节：行id 后面80个字节：8段，每段10字节相同随机大写字母 TeraGen作业没有Reduce Task，产生文件的个数取决于设定Map的个数。 2、进行Terasort排序 time sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort /user/teragen /user/terasort 运行后，我们可以看到会起m个mapper（取决于输入文件个数）和r个reducer（取决于设置项：mapred.reduce.tasks），排好序的结果存放在/user/terasort目录下 查看TeraSort的源代码，你会发现这个作业同时没有设置mapper和reducer；也就是意味着它使用了Hadoop默认的IdentityMapper和IdentityReducer。IdentityMapper和IdentityReducer对它们的输入不做任何处理，将输入k,v直接输出；也就是说是完全是为了走框架的流程而空跑。 这正是Hadoop的TeraSort的巧妙所在，它没有为排序而实现自己的mapper和reducer，而是完全利用Hadoop的Map Reduce框架内的机制实现了排序。查看TeraSort的源代码，你会发现这个作业同时没有设置mapper和reducer；也就是意味着它使用了Hadoop默认的IdentityMapper和IdentityReducer。IdentityMapper和IdentityReducer对它们的输入不做任何处理，将输入k,v直接输出；也就是说是完全是为了走框架的流程而空跑。这正是Hadoop的TeraSort的巧妙所在，它没有为排序而实现自己的mapper和reducer，而是完全利用Hadoop的Map Reduce框架内的机制实现了排序。 3、排序结果的校验TeraValidate time sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teravalidate /user/terasort /user/terasortvalidate 需要一提的是TeraValidate的作业配置里有这么一句： job.setLong(\"mapred.min.split.size\", Long.MAX_VALUE); 它用来保证每一个输入文件都不会被split，又因为TeraInputFormat继承自FileInputFormat，所以TeraValidate运行mapper的总数正好等于输入文件的个数。 TestDFSIO测试 TestDFSIO用于测试HDFS的IO性能，使用一个MapReduce作业来并发地执行读写操作，每个map任务用于读或写每个文件，map的输出用于收集与处理文件相关的统计信息，reduce用于累积统计信息，并产生summary。 0、TestDFSIO的用法如下： TestDFSIO.0.0.6 Usage: TestDFSIO [genericOptions] -read | -write | -append | -clean [-nrFiles N] [-fileSize Size[B 1、向HDFS中写入文件 #往HDFS中写入10个1000MB的文件,文件在HDFS中的路径：/benchmarks/TestDFSIO sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-test-2.6.0-mr1-cdh5.11.1.jar TestDFSIO -write -nrFiles 10 -fileSize 1000 测试结果： 18/08/15 16:57:12 INFO fs.TestDFSIO: ----- TestDFSIO ----- : write 18/08/15 16:57:12 INFO fs.TestDFSIO: Date & time: Wed Aug 15 16:57:12 CST 2018 18/08/15 16:57:12 INFO fs.TestDFSIO: Number of files: 10 18/08/15 16:57:12 INFO fs.TestDFSIO: Total MBytes processed: 1000.0 18/08/15 16:57:12 INFO fs.TestDFSIO: Throughput mb/sec: 86.89607229753216 18/08/15 16:57:12 INFO fs.TestDFSIO: Average IO rate mb/sec: 94.8116455078125 18/08/15 16:57:12 INFO fs.TestDFSIO: IO rate std deviation: 25.014017966950007 18/08/15 16:57:12 INFO fs.TestDFSIO: Test exec time sec: 13.852 18/08/15 16:57:12 INFO fs.TestDFSIO: 2、从HDFS中读取测试数据 #从HDFS中读取5个1000MB的文件,读取测试结果文件在HDFS中的路径：/benchmarks/TestDFSIO/io_read sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-test-2.6.0-mr1-cdh5.11.1.jar TestDFSIO -read-nrFiles 5 -fileSize 1000 测试结果： 18/08/15 18:15:21 INFO fs.TestDFSIO: ----- TestDFSIO ----- : read 18/08/15 18:15:21 INFO fs.TestDFSIO: Date & time: Wed Aug 15 18:15:21 CST 2018 18/08/15 18:15:21 INFO fs.TestDFSIO: Number of files: 5 18/08/15 18:15:21 INFO fs.TestDFSIO: Total MBytes processed: 5000.0 18/08/15 18:15:21 INFO fs.TestDFSIO: Throughput mb/sec: 137.50240629211012 18/08/15 18:15:21 INFO fs.TestDFSIO: Average IO rate mb/sec: 137.68557739257812 18/08/15 18:15:21 INFO fs.TestDFSIO: IO rate std deviation: 5.010354585493878 18/08/15 18:15:21 INFO fs.TestDFSIO: Test exec time sec: 38.621 18/08/15 18:15:21 INFO fs.TestDFSIO: 3、删除测试数据 sudo -u hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/hadoop-test-2.6.0-mr1-cdh5.11.1.jar TestDFSIO -clean Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/cloudera-hdfs.html":{"url":"origin/cloudera-hdfs.html","title":"HDFS","keywords":"","body":"HDFS 一、简介 二、基础概念 三、常用操作 1、查看HDFS容量 hdfs dfs -df -h 2、删除HDFS上的文件(删除的文件会放到操作用户的回收站里) #删除目录 hdfs dfs -rm -r /test #删除文件 hdfs dfs -rm /test/a 3、直接删除文件（不放进回收站） #删除目录 hdfs dfs -rm -r -skipTrash /test #删除文件 hdfs dfs -rm -skipTrash /test/a 4、上传文件 hdfs dfs -put 本地文件 远程目录 5、下载文件 hdfs dfs -get 远程文件 本地目录 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/tidb-ansible.html":{"url":"origin/tidb-ansible.html","title":"Ansible二进制部署管理","keywords":"","body":"一、简介 TiDB Ansible是 PingCAP 基于 Ansible playbook 功能编写的集群部署工具 TIDB Ansible Github：https://github.com/pingcap/tidb-ansible 官方文档：https://pingcap.com/docs-cn/stable/how-to/deploy/orchestrated/ansible/ 二、TiDB Ansible使用指南 1、版本 目前 tidb-ansible 主要分支有：master、release-2.1、release-2.0、release-1.1、release-1.0；在比较老的版本中 tidb-ansible 是不区分 tag 的，应一些用户的需求，我们从 v2.1.1 和 v.2.0.10 版本开始对 tidb-ansible 打 Tag，不同 Tag 的 tidb-ansible 来操作对应版本的 TiDB 集群。 master 分支用来部署 latest 版本（最新版本，目前就是指 3.0.0-BETA）集群，latest 版本由于还没有 GA，所以不区分 Tag。 release-2.1 分支用来部署 2.1.x 版本的集群，其中包含 v2.1.1 及之后版本的 Tag。不同 tag 之间会有一些 pr 的修改，可能包含参数等变更，所以建议使用相匹配的版本来操作集群，比如： 使用 v2.1.5 Tag 的 tidb-ansible 来部署管理 2.1.5 版本的集群 从 2.1.3 版本升级到 2.1.5 版本，也需要使用 v2.1.5 Tag 的 tidb-ansible 来操作 release-2.0 分支和 2.1 分支类似。 release-1.1 和 release-1.0 分支属于老版本（很少用户使用这个版本），没有 Tag。 2、目录结构 tidb-ansible 实际上是 playbook 和文件（binary、配置文件等）的集合。每个 playbook 都包含了一系列的操作指令，当执行 playbook 时，就会按照定义好的指令对不同节点做分发 binary、生成配置文件、启停服务等操作。这里需要注意，yml 文件对格式要求很严格，同一级别的配置一定要缩进对齐。 ini 文件 主要是 hosts.ini 和 inventory.ini，ini 文件以 [xxx]（组名称） 分组，在 playbook 中可以定义对哪个组做哪些操作。ansible-playbook 默认使用当前目录中的 inventory.ini 文件，在 tidb-ansible/ansible.cfg 中设置。 hosts.ini 用来初始化机器，添加用户、设置互信、同步时间等。 inventory.ini 用来部署集群，包含集群的拓扑和一些常用变量。如果有自定义变量或者单机多个实例，ip 或者别名要区分开，一定不要相同，不然变量之间会互相覆盖。如下截图包含两个示例，每个示例中的变量都会互相覆盖，最终只会有一个生效。 conf 目录 集群各组件的默认初始化配置文件 scripts 目录 主要存放 Grafana Dashboard 相关的文件、环境校验脚本、日常使用常用脚本。其中 .json 文件就是我们平常监控上看到的 dashboard 的内容，也可以在 grafana 上手动 import 导入。 group_vars 目录 包含了不同组的变量和全局变量。需要注意的是，group_vars 中的全局/组变量优先级比 inventory.ini 中的全局/组变量优先级高，但是比 inventory.ini 中 host 后面设置的变量优先级低。 roles 目录 roles 里面的内容比较多，核心功能都在这个目录中，tidb-ansible 目录下的 playbook 执行时也是在调用 roles 中对应的角色。下面以 roles/pd 为例具体说明： defaults 是必须存在的目录，存储一些默认变量信息，这个变量的优先级最低 files 目录存放由 copy 或 script 等模块调用的文件 meta 目录定义当前角色的特殊设定及其依赖关系，至少应该包含一个名为main.yml的文件，其它的文件需要在此文件中通过include 进行包含 tasks 是存放 playbook 的目录，其中 main.yml 是主入口文件，在 main.yml 中导入其他 yml 文件 templates 目录存放模板文件，template 模块会将模板文件中的变量替换为实际值，然后覆盖到客户机指定路径上 vars 目录存放角色用到的变量，这里我们用来存放默认配置文件信息 download 目录 local_prepare.yml playbook下载的集群组件二进制压缩安装包 resource 目录 3、常用 playbook 说明 deploy.yml 用来部署集群。执行 deploy 操作会自动将配置文件、binary 等分发到对应的节点上；如果是已经存在的集群，执行时会对比配置文件、binary 等信息，如果有变更就会覆盖原来的文件并且将原来的文件备份到 backup（默认） 目录。 start.yml 用来启动集群。注意：这个操作只是 start 集群，不会对配置等信息做任何更改 stop.yml 用来停止集群。与 start 一样，不会对配置等做任何修改。 rolling_update.yml 用来逐个更新集群内的节点。此操作会按 PD、TiKV、TiDB 的顺序以 1 并发对集群内的节点逐个做 stop → deploy → start 操作，其中对 PD 和 TiKV 操作时会先迁移掉 leader，将对集群的影响降到最低。一般用来对集群做配置更新或者升级。 rolling_update_monitor.yml 用来逐个更新监控组件，与 rolling_update.yml 功能一样，面向的组件有区别。 unsafe_cleanup.yml 用来清掉整个集群。这个操作会先将整个集群停掉服务，然后删除掉相关的目录，操作不可逆，需要谨慎。 unsafe_cleanup_data.yml 用来清理掉 PD、TiKV、TiDB 的数据。执行时会先将对应服务停止，然后再清理数据，操作不可逆，需要谨慎。这个操作不涉及监控。 三、Prerequisite 1、服务器准备 Linux 操作系统版本 Linux 操作系统平台 版本 Red Hat Enterprise Linux 7.3 及以上 CentOS（推荐） 7.3 及以上 Oracle Enterprise Linux 7.3 及以上 Ubuntu LTS 16.04 及以上 服务器配置推荐 开发测试环境 组件 CPU 内存 本地存储 网络 实例数量(最低要求) TiDB 8核+ 16 GB+ 无特殊要求 千兆网卡 1（可与 PD 同机器） PD 4核+ 8 GB+ SAS, 200 GB+ 千兆网卡 1（可与 TiDB 同机器） TiKV 8核+ 32 GB+ SSD, 200 GB+ 千兆网卡 3 验证测试环境中的 TiDB 和 PD 可以部署在同一台服务器上。 如进行性能相关的测试，避免采用低性能存储和网络硬件配置，防止对测试结果的正确性产生干扰。 TiKV 的 SSD 盘推荐使用 NVME 接口以保证读写更快。 如果仅验证功能，建议使用 Docker Compose 部署方案单机进行测试。 TiDB 对于磁盘的使用以存放日志为主，因此在测试环境中对于磁盘类型和容量并无特殊要求。 生产环境 组件 CPU 内存 硬盘类型 网络 实例数量(最低要求) TiDB 16核+ 32 GB+ SAS 万兆网卡（2块最佳） 2 PD 4核+ 8 GB+ SSD 万兆网卡（2块最佳） 3 TiKV 16核+ 32 GB+ SSD 万兆网卡（2块最佳） 3 监控 8核+ 16 GB+ SAS 千兆网卡 1 生产环境中的 TiDB 和 PD 可以部署和运行在同服务器上，如对性能和可靠性有更高的要求，应尽可能分开部署。 生产环境强烈推荐使用更高的配置。 TiKV 硬盘大小配置建议 PCI-E SSD 不超过 2 TB，普通 SSD 不超过 1.5 TB。 实践服务器配置 IP 硬件配置 TiDB角色 192.168.6.1 16C64G100G 1TB数据盘LVM2挂载到路径 Centos7.3 TiDB1 PD1 TiKV1 192.168.6.2 16C64G100G 1TB数据盘LVM2挂载到路径 Centos7.3 TiDB2 PD2 TiKV2 192.168.6.3 16C64G100G 1TB数据盘LVM2挂载到路径 Centos7.3 TiDB3 PD3 TiKV3 192.168.6.4 16C64G100G 1TB数据盘LVM2挂载到路径 Centos7.3 Zookeeper、Kafka、Pump Server、Drainer、Importer、Lightning、kafka Exporter、Spark Master、 192.168.6.5 16C64G100G 1TB数据盘LVM2挂载到路径 Centos7.3 Ansible、Prometheus、Grafana、 Pushgateway、Alertmanager、Spark Slave 2、所有部署主机添加数据盘 所有部署主机的100G数据磁盘为/dev/vdb 3、Ansible节点准备 ⓪\u0003\u0016下载TIDB Ansile tag=v3.0.12 && \\ git clone -b $tag https://github.com/pingcap/tidb-ansible.git ①安装 Ansible 及其依赖 目前，TiDB release-2.0、release-2.1、release-3.0 及最新开发版本兼容 Ansible 2.4 ~ 2.7.11 (2.4 ≤ Ansible ≤ 2.7.11)。Ansible 及相关依赖的版本信息记录在 tidb-ansible/requirements.txt 文件中。 cd /home/tidb/tidb-ansible && \\ sudo pip install -r ./requirements.txt && \\ ansible --version # ansible 2.7.11 # config file = /home/tidb/tidb-ansible/ansible.cfg # configured module search path = [u'/home/tidb/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules'] # ansible python module location = /usr/lib/python2.7/site-packages/ansible # executable location = /bin/ansible # python version = 2.7.5 (default, Nov 6 2016, 00:28:07) [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] Ansible默认配置文件在/home/tidb/tidb-ansible/ansible.cfg 4、Ansible节点与部署目标机器的通信准备 ⓪配置Ansible节点与部署目标机器的SSH 互信及 sudo 规则 在部署目标机器上创建 tidb 用户，并配置 sudo 规则，配置中控机与部署目标机器之间的 SSH 互信。 vi /home/tidb/tidb-ansible/host.ini [servers] 192.168.6.1 192.168.6.2 192.168.6.3 192.168.6.4 192.168.6.5 [all:vars] username = tidb ntp_server = pool.ntp.org ansible-playbook -i /home/tidb/tidb-ansible/hosts.ini create_users.yml -u root -k 5、在部署目标机器上安装 NTP 服务 ansible-playbook -i /home/tidb/tidb-ansible/hosts.ini deploy_ntp.yml -u tidb -b 6、格式化部署目标机器的数据盘以LVM2方式挂载到/data/tidb cd /home/tidb/tidb-ansible/ && \\ ansible -i hosts.ini all -m yum -a 'name=lvm2 state=present' && \\ ansible -i hosts.ini all -m shell -a 'parted -s -a optimal /dev/vdb mklabel gpt -- mkpart primary ext4 1 -1 && disk=/dev/vdb1 && pvcreate ${disk} && vgcreate ${disk} && vgcreate -s 16m tidb ${disk} && PE_Number=`vgdisplay ${disk}|grep \"Free PE\"|awk '{print $5}'` && lvcreate -l ${PE_Number} -n tidb tidb && mkfs.ext4 /dev/tidb/tidb && mkdir -p /data/tidb && chown -R tidb:tidb /data/tidb && echo \"/dev/tidb/tidb /data/tidb ext4 defaults 0 0\" >> /etc/fstab && mount -a && df -m' 四、安装 1、准备TiDB Ansible的部署主机清单inventory.ini ## TiDB Cluster Part [tidb_servers] 192.168.6.1 192.168.6.2 192.168.6.3 [tikv_servers] node1.pro.tidb ansible_host=192.168.6.1 deploy_dir=/data/tidb tikv_port=20171 labels=\"host=node1\" node2.pro.tidb ansible_host=192.168.6.2 deploy_dir=/data/tidb tikv_port=20171 labels=\"host=node2\" node3.pro.tidb ansible_host=192.168.6.3 deploy_dir=/data/tidb tikv_port=20171 labels=\"host=node3\" [pd_servers] 192.168.6.1 192.168.6.2 192.168.6.3 [spark_master] 192.168.6.4 [spark_slaves] 192.168.6.5 [lightning_server] 192.168.6.4 [importer_server] 192.168.6.4 ## Monitoring Part: prometheus and pushgateway servers [monitoring_servers] 192.168.6.5 [grafana_servers] 192.168.6.5 # node_exporter and blackbox_exporter servers [monitored_servers] 192.168.6.1 192.168.6.2 192.168.6.3 192.168.6.4 192.168.6.5 [alertmanager_servers] 192.168.6.7 [kafka_exporter_servers] 192.168.6.4 ## Binlog Part [pump_servers] 192.168.6.4 [drainer_servers] 192.168.6.4 ## Group variables [pd_servers:vars] location_labels = [\"host\"] ## Global variables [all:vars] deploy_dir = /data/tidb ## Connection ssh via normal user ansible_user = tidb cluster_name = Curiouser-TiDB tidb_version = v3.0.12 # process supervision, [systemd, supervise] process_supervision = systemd timezone = Asia/Shanghai enable_firewalld = False # check NTP service enable_ntpd = True set_hostname = False ## binlog trigger enable_binlog = True # kafka cluster address for monitoring, example: kafka_addrs = \"192.168.6.4:9092\" # zookeeper address of kafka cluster for monitoring, example: zookeeper_addrs = \"192.168.6.4:2181\" # enable TLS authentication in the TiDB cluster enable_tls = False # KV mode deploy_without_tidb = False # wait for region replication complete before start tidb-server. wait_replication = True # Optional: Set if you already have a alertmanager server. # Format: alertmanager_host:alertmanager_port alertmanager_target = \"\" grafana_admin_user = \"admin\" grafana_admin_password = \"admin\" ### Collect diagnosis collect_log_recent_hours = 2 enable_bandwidth_limit = True # default: 10Mb/s, unit: Kbit/s collect_bandwidth_limit = 10000 2、修改各组件默认配置 ⓪修改TiKV默认配置文件/home/tidb/tidb-ansible/conf/tikv.yml ①修改PD默认配置文件/home/tidb/tidb-ansible/conf/pd.yml ②修改TiDB默认配置文件/home/tidb/tidb-ansible/conf/tidb.yml ③修改Spark默认配置文件/home/tidb/tidb-ansible/conf/spark-defaults.yml和/home/tidb/tidb-ansible/conf/spark-env.yml ④修改pump默认配置文件/home/tidb/tidb-ansible/conf/pump.yml ⑤修改 lightning默认配置文件/home/tidb/tidb-ansible/conf/tidb-lightning.yml ⑥修改 importer默认配置文件/home/tidb/tidb-ansible/conf/tidb-importer.yml ⑦修改 drainer默认配置文件/home/tidb/tidb-ansible/conf/drainer.toml ⑧修改组件二进制包下载默认配置文件/home/tidb/tidb-ansible/conf/binary_packages.yml 和/home/tidb/tidb-ansible/conf/common_packages.yml 3、部署准备 ⓪测试部署目标服务器的SSH通信 ansible -i inventory.ini all -m shell -a 'whoami' 如果所有 server 均返回 tidb，表示 SSH 互信配置成功 ansible -i inventory.ini all -m shell -a 'whoami' -b 如果所有 server 均返回 root，表示 tidb 用户 sudo 免密码配置成功。 ①下载组件二进制包 ansible-playbook local_prepare.yml ②开启TiKV数据加密存储功能，创建加密token文件 TiKV 只接受 hex 格式的 token 文件，文件的长度必须是 2^n，并且小于等于 1024。 目前 TiKV 数据加密存储存在以下限制： 对之前没有开启加密存储的集群，不支持开启该功能。 已经开启加密功能的集群，不允许关闭加密存储功能。 同一集群内部，不允许部分 TiKV 实例开启该功能，部分 TiKV 实例不开启该功能。对于加密存储功能，所有 TiKV 实例要么都开启该功能，要么都不开启该功能。这是由于 TiKV 实例之间会有数据迁移，如果开启了加密存储功能，迁移过程中数据也是加密的。 /home/tidb/tidb-ansible/resources/bin/tikv-ctl random-hex --len 256 > /home/tidb/tidb-ansible/cipher-file-256 ③在TiKV默认配置文件/home/tidb/tidb-ansible/conf/tikv.yml中添加配置 [security] # Cipher file 的存储路径 cipher-file = \"/data/tidb/conf/cipher-file-256\" 4、初始化部署目标主机系统环境，修改内核参数 ansible-playbook bootstrap.yml 5、分发组件安装资源到部署目标主机 ansible-playbook deploy.yml 分发TiKV 数据加密存储token文件到TiKV目标主机上 ansible -i inventory.ini tikv_servers -m copy -a 'src=/home/tidb/tidb-ansible/cipher-file-256 dest=/data/tidb/conf/cipher-file-256 owner=tidb group=tidb' 6、启动 TiDB 集群 ansible-playbook start.yml 7、验证 使用 MySQL 客户端连接 TiDB 集群。TiDB 服务的默认端口为 4000 mysql -u root -h 192.168.6.1 -P 4000 浏览器访问Grafana：http://192.168.6.5:3000 （默认帐号与密码：admin/admin） 五、集群操作 1、ansible控制集群 ⓪启动集群 此操作会按顺序启动整个 TiDB 集群所有组件（包括 PD、TiDB、TiKV 等组件和监控组件）。 ansible-playbook start.yml ①关闭集群 此操作会按顺序关闭整个 TiDB 集群所有组件（包括 PD、TiDB、TiKV 等组件和监控组件）。 ansible-playbook stop.yml ②清除集群数据 此操作会关闭 TiDB、Pump、TiKV、PD 服务，并清空 Pump、TiKV、PD 数据目录。 ansible-playbook unsafe_cleanup_data.yml ③销毁集群 此操作会关闭集群，并清空部署目录，若部署目录为挂载点，会报错，可忽略。 ansible-playbook unsafe_cleanup.yml ④更新组件配置并滚动重启 ansible-playbook rolling_update.yml --tags=组件标签 配置并重启 Prometheus 服务 ansible-playbook rolling_update_monitor.yml --tags=prometheus ⑤只操作组件某节点上的其中一个服务 ansible-playbook 操作playbook --tags=组件 -l 组件节点的ip或hostname或inventry中的别名 # 例如停掉192.168.6.3节点上的所有服务 ansible-playbook stop.yml -l 192.168.6.3 # 关闭您想要下线的 DM-worker 实例 ansible-playbook stop.yml --tags=dm-worker -l dm_worker3 六、压力测试 详见：https://pingcap.com/docs-cn/stable/benchmark/how-to-run-sysbench/ 七、扩容缩容 1、扩容组件节点 扩容TiKV节点 注意： 如果 inventory.ini 中为节点配置了别名，如 node101 ansible_host=192.168.6.101，执行 ansible-playbook 时 -l 请指定别名，以下步骤类似。例如：ansible-playbook bootstrap.yml -l node101,node102 ⓪编辑 inventory.ini 文件和 hosts.ini 文件，添加节点信息 hosts.ini [servers] ...省略... 192.168.6.6 [all:vars] ...省略... inventory.ini ...省略... [tikv_servers] node4.pro.tidb ansible_host=192.168.6.6 deploy_dir=/data/tidb tikv_port=20171 labels=\"host=node4\" ...省略... ①配置SSH 互信及 sudo 规则、安装 NTP 服务、挂载数据盘、初始化新增节点，分发安装资源 ansible-playbook -i hosts.ini create_users.yml -l 192.168.6.6 -u root -k ansible-playbook -i hosts.ini deploy_ntp.yml -u tidb -b cd /home/tidb/tidb-ansible/ && \\ ansible -i hosts.ini 192.168.6.6 -m yum -a 'name=lvm2 state=present' && \\ ansible -i hosts.ini 192.168.6.6 -m shell -a 'parted -s -a optimal /dev/vdb mklabel gpt -- mkpart primary ext4 1 -1 && disk=/dev/vdb1 && pvcreate ${disk} && vgcreate ${disk} && vgcreate -s 16m tidb ${disk} && PE_Number=`vgdisplay ${disk}|grep \"Free PE\"|awk '{print $5}'` && lvcreate -l ${PE_Number} -n tidb tidb && mkfs.ext4 /dev/tidb/tidb && mkdir -p /data/tidb && chown -R tidb:tidb /data/tidb && echo \"/dev/tidb/tidb /data/tidb ext4 defaults 0 0\" >> /etc/fstab && mount -a && df -m' ansible-playbook bootstrap.yml -l node4.pro.tidb ansible-playbook deploy.yml -l node4.pro.tidb ansible -i hosts.ini 192.168.6.6 -m copy -a 'src=/home/tidb/tidb-ansible/cipher-file-256 dest=/home/tidb/conf/cipher-file-256 owner=tidb group=tidb' ②启动新节点服务 ansible-playbook start.yml -l node4.pro.tidb ③更新 Prometheus 配置并重启 ansible-playbook rolling_update_monitor.yml --tags=prometheus 扩容PD节点 ⓪编辑 inventory.ini 文件和 hosts.ini 文件，添加节点信息 hosts.ini [servers] ...省略... 192.168.6.7 [all:vars] ...省略... inventory.ini ...省略... [pd_servers] 192.168.6.7 ...省略... ①配置SSH 互信及 sudo 规则、安装 NTP 服务、挂载数据盘、初始化新增节点，分发安装资源 ansible-playbook -i hosts.ini create_users.yml -l 192.168.6.7 -u root -k ansible-playbook -i hosts.ini deploy_ntp.yml -u tidb -b cd /home/tidb/tidb-ansible/ && \\ ansible -i hosts.ini 192.168.6.7 -m yum -a 'name=lvm2 state=present' && \\ ansible -i hosts.ini 192.168.6.7 -m shell -a 'parted -s -a optimal /dev/vdb mklabel gpt -- mkpart primary ext4 1 -1 && disk=/dev/vdb1 && pvcreate ${disk} && vgcreate ${disk} && vgcreate -s 16m tidb ${disk} && PE_Number=`vgdisplay ${disk}|grep \"Free PE\"|awk '{print $5}'` && lvcreate -l ${PE_Number} -n tidb tidb && mkfs.ext4 /dev/tidb/tidb && mkdir -p /data/tidb && chown -R tidb:tidb /data/tidb && echo \"/dev/tidb/tidb /data/tidb ext4 defaults 0 0\" >> /etc/fstab && mount -a && df -m' ansible-playbook bootstrap.yml -l 192.168.6.7 ansible-playbook deploy.yml -l 192.168.6.7 ②登录新增的 PD 节点，编辑启动脚本：/data/tidb/scripts/run_pd.sh 移除 --initial-cluster=\"xxxx\" \\ 配置，注意这里不能在行开头加注释符 #。 添加 --join=\"http://192.168.6.1:2379\" \\，IP 地址 （192.168.6.7） 可以是集群内现有 PD IP 地址中的任意一个。 ③在新增 PD 节点中启动 PD 服务 /data/tidb/scripts/start_pd.sh ③使用 pd-ctl 检查新节点是否添加成功： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d member ④启动监控服务 ansible-playbook start.yml -l 192.168.6.7 ⑤更新集群的配置： ansible-playbook deploy.yml ⑥重启 Prometheus，新增扩容的 PD 节点的监控： ansible-playbook rolling_update_monitor.yml --tags=prometheus 2、缩容组件节点 ⓪停止节点上所有的服务： 注意：如果该服务器上还有其他服务（例如 TiDB），则还需要使用 --tags组件标签 来指定服务（例如 -t tidb）。 ansible-playbook stop.yml -l 192.168.6.9 ①编辑 inventory.ini 文件，移除节点信息 ②更新 Prometheus 配置并重启 ansible-playbook rolling_update_monitor.yml --tags=prometheus 缩容 TiKV 节点 ⓪使用 pd-ctl 从集群中移除节点： 查看 TiKV node节点的 store id： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d store 从集群中移除TiKV node节点，假如 store id 为 10： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d store delete 10 使用 Grafana 或者 pd-ctl 检查节点是否下线成功（下线需要一定时间，下线节点的状态变为 Tombstone 就说明下线成功了）： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d store 10 ①下线成功后，停止 TiKV node节点上的服务： ansible-playbook stop.yml --tags=tikv -l 192.168.6.9 编辑 inventory.ini 文件，移除节点信息 ②更新 Prometheus 配置并重启： ansible-playbook stop.yml --tags=prometheus ansible-playbook start.yml --tags=prometheus 缩容 PD 节点 TiKV 中的 PD Client 会缓存 PD 节点列表，但目前不会定期自动更新，只有在 PD leader 发生切换或 TiKV 重启加载最新配置后才会更新；为避免 TiKV 缓存的 PD 节点列表过旧的风险，在扩缩容 PD 完成后，PD 集群中至少要包含两个扩缩容操作前就已经存在的 PD 节点成员，如果不满足该条件需要手动执行 PD transfer leader 操作，更新 TiKV 中的 PD 缓存列表。 ⓪使用 pd-ctl 从集群中移除节点： 查看 node2 节点的 name： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d member 从集群中移除 node2，假如 name 为 pd2： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d member delete name pd2 使用 pd-ctl 检查节点是否下线成功（PD 下线会很快，结果中没有 node2 节点信息即为下线成功）： /home/tidb/tidb-ansible/resources/bin/pd-ctl -u \"http://192.168.6.1:2379\" -d member ①下线成功后，停止 node2 上的服务： ansible-playbook stop.yml --tags=pd -l 192.168.6.9 ②编辑 inventory.ini 文件，移除节点信息 ③更新集群的配置： ansible-playbook deploy.yml ④重启 Prometheus，移除缩容的 PD 节点的监控： ansible-playbook stop.yml --tags=prometheus ansible-playbook start.yml --tags=prometheus Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/tiup-install-cluster.html":{"url":"origin/tiup-install-cluster.html","title":"使用TIUP部署TiDB集群","keywords":"","body":"使用TIUP部署TiDB 4.0集群 一、简介 TiUP 是 TiDB 4.0 版本引入的集群运维工具，TiUP cluster 是 TiUP 提供的使用 Golang 编写的集群管理组件，通过 TiUP cluster 组件就可以进行日常的运维工作，包括部署、启动、关闭、销毁、弹性扩缩容、升级 TiDB 集群；管理 TiDB 集群参数。 目前 TiUP 可以支持部署 TiDB、TiFlash、TiDB Binlog、TiCDC，以及监控系统。 Linux 操作系统版本要求 Linux 操作系统平台 版本 Red Hat Enterprise Linux 7.3 及以上 CentOS 7.3 及以上 Oracle Enterprise Linux 7.3 及以上 Ubuntu LTS 16.04 及以上 TiDB各组件端口 组件 默认端口 说明 TiDB 4000 应用及 DBA 工具访问通信端口 TiDB 10080 TiDB 状态信息上报通信端口 TiKV 20160 TiKV 通信端口 TiKV 20180 TiKV 状态信息上报通信端口 PD 2379 提供 TiDB 和 PD 通信端口 PD 2380 PD 集群节点间通信端口 TiFlash 9000 TiFlash TCP 服务端口 TiFlash 8123 TiFlash HTTP 服务端口 TiFlash 3930 TiFlash RAFT 服务和 Coprocessor 服务端口 TiFlash 20170 TiFlash Proxy 服务端口 TiFlash 20292 Prometheus 拉取 TiFlash Proxy metrics 端口 TiFlash 8234 Prometheus 拉取 TiFlash metrics 端口 Pump 8250 Pump 通信端口 Drainer 8249 Drainer 通信端口 CDC 8300 CDC 通信接口 Prometheus 9090 Prometheus 服务通信端口 Node_exporter 9100 TiDB 集群每个节点的系统信息上报通信端口 Blackbox_exporter 9115 Blackbox_exporter 通信端口，用于 TiDB 集群端口监控 Grafana 3000 Web 监控服务对外服务和客户端(浏览器)访问端口 Alertmanager 9093 告警 web 服务端口 Alertmanager 9094 告警通信端口 二、集群节点准备 0、集群组件规划 集群角色/集群节点 tools.tidb4.curiouser.com node1.tidb4.curiouser.com node2.tidb4.curiouser.com node3.tidb4.curiouser.com TiUP ✓ TiDP ✓ ✓ ✓ PD Server ✓ ✓ ✓ TiKV ✓ ✓ ✓ TiFlash ✓ CDC ✓ pump ✓ grafana ✓ prometheus ✓ alertmanager ✓ Zookeeper ✓ Kafka ✓ 1、节点准备 ①所有节点创建tidb用户并加入sudoer useradd -m tidb echo \"tidb ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers passwd tidb ②所有节点系统优化 # 关闭系统 swap echo \"vm.swappiness = 0\">> /etc/sysctl.conf swapoff -a && swapon -a # 关闭透明大页 echo never > /sys/kernel/mm/transparent_hugepage/enabled echo never > /sys/kernel/mm/transparent_hugepage/defrag sysctl -p # 配置网络参数 echo \"fs.file-max = 1000000\">> /etc/sysctl.conf echo \"net.core.somaxconn = 32768\">> /etc/sysctl.conf echo \"net.ipv4.tcp_tw_recycle = 0\">> /etc/sysctl.conf echo \"net.ipv4.tcp_syncookies = 0\">> /etc/sysctl.conf echo \"vm.overcommit_memory = 1\">> /etc/sysctl.conf sysctl -p # 配置tidb用户的打开文件句柄限制 cat >/etc/security/limits.conf tidb soft nofile 1000000 tidb hard nofile 1000000 tidb soft stack 32768 tidb hard stack 32768 EOF ③打通tools节点的root用户到所有节点tidb用户的免秘钥登录 ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa ssh-copy-id -i tidb@tools.tidb4.curiouser.com ssh-copy-id -i tidb@node1.tidb4.curiouser.com ssh-copy-id -i tidb@node2.tidb4.curiouser.com ssh-copy-id -i tidb@node3.tidb4.curiouser.com ④所有节点安装numactl 在生产环境中，因为硬件机器配置往往高于需求，为了更合理规划资源，会考虑单机多实例部署 TiDB 或者 TiKV。NUMA 绑核工具的使用，主要为了防止 CPU 资源的争抢，引发性能衰退。NUMA 绑核是用来隔离 CPU 资源的一种方法，适合高配置物理机环境部署多实例使用。 yum install -y numactl ⑤所有节点路径挂载参数 mount point / does not have 'nodelalloc' option set和 mount point / does not have 'noatime' option set /etc/fstab UUID=59d9ca7b-4f39-4c0c-9334-c561234576b5 / ext4 defaults,nodelalloc,noatime 1 1 三、集群安装 以下所有命令都在tools节点，tidb用户，bash环境下执行（zsh环境不支持tiup的某些脚本） 1、安装tiup命令 curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh source .bash_profile tiup cluster tiup --binary cluster # zsh添加tiup命令自动补全功能 tiup completion zsh > \"${fpath[1]}/_tiup\" # bash添加tiup命令自动补全功能 tiup completion bash > ~/.tiup.completion.bash printf \" # tiup shell completion source '$HOME/.tiup.completion.bash' \" >> $HOME/.bash_profile source $HOME/.bash_profile 2、下载离线安装包 tidb_version=6.5.2 wget https://download.pingcap.org/tidb-community-server-v$tidb_version-linux-amd64.tar.gz wget https://download.pingcap.org/tidb-community-server-v$tidb_version-linux-amd64.tar.gz tiup mirror clone /opt/tidb-community-server-v$tidb_version-linux-amd64 ${tidb_version} --os=linux --arch=amd64 3、查看目前tiup命令支持安装的tidb版本 tiup list tidb 4、创建集群组件的拓扑配置文件 针对不同的部署架构配置肯可能会有所不同，可参考第八章 # # Global variables are applied to all deployments and used as the default value of # # the deployments if a specific deployment value is missing. global: user: \"tidb\" ssh_port: 22 arch: \"amd64\" server_configs: tidb: oom-action: \"cancel\" mem-quota-query: 25769803776 log.query-log-max-len: 4096 log.file.log-rotate: true log.file.max-size: 300 log.file.max-days: 7 log.file.max-backups: 14 log.slow-threshold: 300 binlog.enable: true binlog.ignore-error: true tikv: global.log-rotation-timespan: \"168h\" readpool.unified.max-thread-count: 12 readpool.storage.use-unified-pool: false readpool.coprocessor.use-unified-pool: true coprocessor.split-region-on-table: false storage.block-cache.capacity: \"16GB\" raftstore.capacity: \"10GB\" pd: log.file.max-size: 300 log.file.max-days: 28 log.file.max-backups: 14 log.file.log-rotate: true replication.location-labels: [\"host\"] schedule.leader-schedule-limit: 4 schedule.region-schedule-limit: 2048 schedule.replica-schedule-limit: 64 tiflash: logger.level: \"info\" # Maximum memory usage for processing a single query. Zero means unlimited. profiles.default.max_memory_usage: 10000000000 # Maximum memory usage for processing all concurrently running queries on the server. Zero means unlimited. profiles.default.max_memory_usage_for_all_queries: 0 pd_servers: - host: 192.168.1.71 ssh_port: 22 name: pd-1 client_port: 2379 peer_port: 2380 deploy_dir: /data/tidb/pd-2379 data_dir: /data/tidb/pd-2379/data log_dir: /data/tidb/pd-2379/logs - host: 192.168.1.72 ssh_port: 22 name: pd-2 client_port: 2379 peer_port: 2380 deploy_dir: /data/tidb/pd-2379 data_dir: /data/tidb/pd-2379/data log_dir: /data/tidb/pd-2379/logs - host: 192.168.1.73 ssh_port: 22 name: pd-3 client_port: 2379 peer_port: 2380 deploy_dir: /data/tidb/pd-2379 data_dir: /data/tidb/pd-2379/data log_dir: /data/tidb/pd-2379/logs tidb_servers: - host: 192.168.1.71 port: 4000 status_port: 10080 deploy_dir: \"/data/tidb/tidb-4000\" log_dir: \"/data/tidb/tidb-4000/logs\" numa_node: \"0\" - host: 192.168.1.72 port: 4000 status_port: 10080 deploy_dir: \"/data/tidb/tidb-4000\" log_dir: \"/data/tidb/tidb-4000/logs\" numa_node: \"0\" - host: 192.168.1.73 port: 4000 status_port: 10080 deploy_dir: \"/data/tidb/tidb-4000\" log_dir: \"/data/tidb/tidb-4000/logs\" numa_node: \"0\" tikv_servers: - host: 192.168.1.71 port: 20160 status_port: 20180 deploy_dir: \"/data/tidb/tikv-20160\" data_dir: \"/data/tidb/tikv-20160/data\" log_dir: \"/data/tidb/tikv-20160/logs\" numa_node: \"0\" config: server.labels: { host: \"tikv1\" } - host: 192.168.1.72 port: 20160 status_port: 20180 deploy_dir: \"/data/tidb/tikv-20160\" data_dir: \"/data/tidb/tikv-20160/data\" log_dir: \"/data/tidb/tikv-20160/logs\" numa_node: \"0\" config: server.labels: { host: \"tikv2\" } - host: 192.168.1.73 port: 20160 status_port: 20180 deploy_dir: \"/data/tidb/tikv-20160\" data_dir: \"/data/tidb/tikv-20160/data\" log_dir: \"/data/tidb/tikv-20160/logs\" numa_node: \"0\" config: server.labels: { host: \"tikv3\" } tiflash_servers: - host: 192.168.1.70 data_dir: \"/data/tidb/tiflash-9000/data\" deploy_dir: \"/data/tidb/tiflash-9000\" log_dir: \"/data/tidb/tiflash-9000/logs\" ssh_port: 22 tcp_port: 9000 http_port: 8123 flash_service_port: 3930 flash_proxy_port: 20170 flash_proxy_status_port: 20292 metrics_port: 8234 numa_node: \"0\" # The following configs are used to overwrite the `server_configs.tiflash` values. config: logger.level: \"info\" learner_config: log-level: \"info\" pump_servers: - host: 192.168.1.70 ssh_port: 22 port: 8250 deploy_dir: \"/data/tidb/pump-8249\" data_dir: \"/data/tidb/pump-8249/data\" log_dir: \"/data/tidb/pump-8249/logs\" numa_node: \"0\" # The following configs are used to overwrite the `server_configs.drainer` values. config: gc: 7 tispark_masters: - host: 192.168.1.70 ssh_port: 22 port: 7077 web_port: 8080 deploy_dir: \"/data/tidb/tispark-master-7077\" java_home: \"/opt/jdk\" spark_config: spark.driver.memory: \"2g\" spark.eventLog.enabled: \"True\" spark.tispark.grpc.framesize: 268435456 spark.tispark.grpc.timeout_in_sec: 100 spark.tispark.meta.reload_period_in_sec: 60 spark.tispark.request.command.priority: \"Low\" spark.tispark.table.scan_concurrency: 256 spark_env: SPARK_EXECUTOR_CORES: 5 SPARK_EXECUTOR_MEMORY: \"10g\" SPARK_WORKER_CORES: 5 SPARK_WORKER_MEMORY: \"10g\" # NOTE: multiple worker nodes on the same host is not supported by Spark tispark_workers: - host: 192.168.1.70 ssh_port: 22 port: 7078 web_port: 8081 deploy_dir: \"/data/tidb/tispark-worker-7078\" java_home: \"/opt/jdk\" cdc_servers: - host: 192.168.1.70 ssh_port: 22 port: 8300 deploy_dir: \"/data/tidb/cdc-8300\" log_dir: \"/data/tidb/cdc-8300/logs\" monitored: node_exporter_port: 9100 blackbox_exporter_port: 9115 deploy_dir: \"/data/tidb/monitored-9100\" data_dir: \"/data/tidb/monitored-9100/data\" log_dir: \"/data/tidb/monitored-9100/logs\" monitoring_servers: - host: 192.168.1.70 ssh_port: 22 port: 9090 deploy_dir: \"/data/tidb/prometheus-9090\" data_dir: \"/data/tidb/prometheus-9090/data\" log_dir: \"/data/tidb/prometheus-9090/logs\" grafana_servers: - host: 192.168.1.70 ssh_port: 22 port: 3000 deploy_dir: \"/data/tidb/grafana-3000\" # alertmanager_servers: # - host: 192.168.1.70 # ssh_port: 22 # web_port: 9093 # cluster_port: 9094 # deploy_dir: \"/data/tidb/alertmanager-9093\" # data_dir: \"/data/tidb/alertmanager-9093/data\" # log_dir: \"/data/tidb/alertmanager-9093/logs\" 5、检查集群拓扑文件及机器系统配置 tiup cluster check [参数] # 若集群尚未部署，需要传递将用于部署集群的topology.yml文件，tiup-cluster 会根据该文件的内容连接到对应机器去检查。若集群已经部署，则可以使用集群的名字 `` 作为检查对象。若传递的是集群名字，则需要配合 `--cluster` 选项使用。 --apply BOOLEAN # 尝试自动修复失败的检查项。该选项默认关闭，默认值为 `false`。在命令中添加该选项，并传入 `true` 值或不传值，均可开启此功能目前仅会尝试修复以下项目： # - SELinux # - 防火墙 # - irqbalance # - 内核参数 # - 系统 Limits # - THP（透明大页） --cluster BOOLEAN # 支持对未部署的集群进行检查，也支持对已部署的集群进行检查。若选择的格式为 `tiup cluster check ` 则必须加上该选项：`tiup cluster check --cluster`。该选项默认关闭，默认值为 `false`。在命令中添加该选项，并传入 `true` 值或不传值，均可开启此功能。 -N, --node STRINGS # 指定要检查的节点。该选项的值为以逗号分割的节点 ID 列表，节点ID为tiup-component-cluster-display 命令返回的集群状态表格的第一列。如果不指定该选项，默认检查所有节点，即 `[]`。注意：若同时指定了 `-R, --role`，那么将检查它们的交集中的服务。 -R, --role STRINGS # 指定要检查的角色。该选项的值为以逗号分割的节点角色列表，角色为tiup-component-cluster-display命令返回的集群状态表格的第二列。如果不指定该选项，默认检查所有角色。注意：若同时指定了 `-N, --node`，那么将检查它们的交集中的服务。 --enable-cpu BOOLEAN # 默认情况下 tiup-cluster 不检查 CPU 核心数，该选项用于启用 CPU 核心数检查。 该选项默认关闭，默认值为 `false`。在命令中添加该选项，并传入 `true` 值或不传值，均可开启此功能。 --enable-disk BOOLEAN # 默认情况下 tiup-cluster 不进行 fio 磁盘性能测试，该选项用于启用 fio 磁盘性能测试。该选项默认关闭，默认值为 `false`。在命令中添加该选项，并传入 `true` 值或不传值，均可开启此功能。 --enable-mem # 默认情况下 tiup-cluster 不检查内存大小，该选项用于启用内存大小检查。 -u, --user string # 指定连接目标机器的用户名，该用户在目标机器上需要有免密 sudo root 的权限。默认为当前执行命令的用户。注意：仅当--cluster选项为 false 时该选项有效，否则该值固定为部署集群时拓扑文件中指定的用户名。 -i, --identity_file string， # 指定连接目标机器的密钥文件。默认 ~/.ssh/id_rsa。注意：仅当 `--cluster` 选项为 false 时该选项有效，否则该值固定为 `${TIUP_HOME}/storage/cluster/clusters//ssh/id_rsa` -p, --password BOOLEAN # 在连接目标机器时使用密码登录，该选项默认关闭，默认值为 `false`。在命令中添加该选项，并传入 `true` 值或不传值，均可开启此功能，： # 对于指定了 `--cluster` 的集群，密码为部署集群时拓扑文件中指定的用户的密码 # 对于未指定 `--cluster` 的集群，密码为 `-u/--user` 参数指定的用户的密码 该命令同时也会检测集群的机器硬件和软件环境是否满足正常运行条件。 检查项 检查信息 操作系统版本 检查部署机操作系统发行版和版本：目前仅支持部署在 CentOS 7 的操作系统上，之后随兼容性改进可能支持更多系统版本。 CPU EPOLLEXCLUSIVE 检查部署机 CPU 是否支持 EPOLLEXCLUSIVE。 numactl 检查部署机是否安装 numactl，若用户配置绑核，则必须安装 numactl。 系统时间 检查部署机系统时间是否同步：将部署机系统时间与中控机对比，偏差超出某一阈值（500ms）后报错。 系统时区 检查部署机系统时区是否同步：将部署机系统的时区配置进行对比，如果时区不一致则报错。 时间同步服务 检查部署机是否配置了时间同步服务：即 ntpd 是否在运行 Swap 分区 检查部署机是否启用 Swap 分区：建议禁用 Swap 分区 THP（透明大页） 检查部署机是否启用透明大页：建议禁用透明大页。 SELinux 检查 SELinux 是否启用：建议用户禁用 SELinux。 防火墙 检查 FirewallD 服务是否启用：建议用户禁用 FirewallD 或为 TiDB 集群各服务添加允许规则。 irqbalance 检查 irqbalance 服务是否启用：建议用户启用 irqbalance 服务。 磁盘挂载参数 检查 ext4 分区的挂载参数：确保挂载参数包含 nodelalloc,noatime 选项。 端口占用 检查部署机上是否已有进程占用了端口：检查拓扑中定义的端口（包括自动补全的默认端口）在部署机上是否已被占用。 CPU 核心数 检查部署机 CPU 信息：建议生产集群 CPU 逻辑核心数 >= 16 内存大小 检查部署机的内存大小：建议生产集群总内存容量 >= 32Gb。 fio 磁盘性能测试 使用 fio 测试 data_dir 所在磁盘的性能，包括三个测试项目： fio_randread_write_latency fio_randread_write fio_randread默认不进行 fio 磁盘性能测试，需要通过选项 --enable-disk 启用。 内核参数 检查各项内核参数的值： net.ipv4.tcp_tw_recycle: 0 net.ipv4.tcp_syncookies: 0 net.core.somaxconn: 32768 vm.swappiness: 0 vm.overcommit_memory: 0 或 1 fs.file-max: 1000000 系统限制 检查 /etc/security/limits.conf 中各项 limit 值：deploy-user soft nofile 1000000deploy-user hard nofile 1000000deploy-user soft stack 10240其中 deploy-user为部署、运行 TiDB 集群的用户，最后一列的数值为要求达到的最小值。 检查输出含有以下信息： Node：目标节点 Check：检查项 Result：检查结果（Pass | Warn | Fail） Message：结果描述 6、分发组件文件到各节点 tiup cluster deploy 集群名字 tidb版本 ./集群拓扑配置.yaml --user tidb -y 7、启动当前集群各组件 tiup cluster start 集群名字 tiup cluster start 集群名字 -R 组件1,组件2 启动集群操作会按 PD -> TiKV -> Pump -> TiDB -> TiFlash -> Drainer 的顺序启动整个 TiDB 集群所有组件（同时也会启动监控组件） 8、验证 ①查看当前集群的状态 tiup cluster display 集群名字 ②查看哪个 PD 节点提供了 TiDB Dashboard 服务 tiup cluster display 集群名字 --dashboard 9、其他信息 ①Grafana默认用户名密码 admin/admin ②TiDB Dashboard默认用户名密码 root 密码为空 ③组件拓扑配置文件样例 全配置参数模版：https://github.com/pingcap/tiup/blob/master/examples/topology.example.yaml TiDB：https://github.com/pingcap/tidb/blob/master/config/config.toml.example PD：https://github.com/tikv/pd/blob/v4.0.0-rc/conf/config.toml TiKV：https://github.com/tikv/tikv/blob/v4.0.0-rc/etc/config-template.toml ④tiup启动集群操作顺序 按 PD -> TiKV -> Pump -> TiDB -> TiFlash -> Drainer -> TiCDC -> Prometheus -> Grafana -> Alertmanager 的顺序启动整个 TiDB 集群所有组件 四、集群其他操作 1、修改集群组件配置参数 集群运行过程中，如果需要调整某个组件的参数，可以使用 edit-config 命令来编辑参数。具体的操作步骤如下： 以编辑模式打开该集群的配置文件： tiup cluster edit-config ${cluster-name} 设置参数： 首先确定配置的生效范围，有以下两种生效范围： 如果配置的生效范围为该组件全局，则配置到 server_configs。例如： server_configs: tidb: log.slow-threshold: 300 如果配置的生效范围为某个节点，则配置到具体节点的 config 中。例如： tidb_servers: - host: 10.0.1.11 port: 4000 config: log.slow-threshold: 300 参数的格式参考 TiUP 配置参数模版。配置项层次结构使用 . 表示。 执行 reload 命令滚动分发配置、重启相应组件： tiup cluster reload ${cluster-name} [-N ] [-R ] 示例：如果要调整 tidb-server 中事务大小限制参数 txn-total-size-limit 为 1G，该参数位于 performance 模块下，调整后的配置如下： server_configs: tidb: performance.txn-total-size-limit: 1073741824 然后执行 tiup cluster reload ${cluster-name} -R tidb 命令滚动重启。 2、集群组件节点重启 tiup cluster restart 用于重启指定集群的所有或部分服务。 tiup cluster restart [参数] 参数 -N, --node（strings，默认为 []，表示所有节点）。该选项的值为以逗号分割的节点 ID 列表，节点 ID 为集群状态表格的第一列。 -R, --role（strings，默认为 []，表示所有角色）。该选项的值为以逗号分割的节点角色列表，角色为集群状态表格的第二列。 tiup restart test-tidb -R tidb -N 127.0.0.1:4000 3、重命名集群 部署并启动集群后，可以通过 tiup cluster rename 命令来对集群重命名： tiup cluster rename ${cluster-name} ${new-name} 注意： 重命名集群会重启监控（Prometheus 和 Grafana）。 重命名集群之后 Grafana 可能会残留一些旧集群名的面板，需要手动删除这些面板。 4、关闭集群 关闭集群操作会按 Drainer -> TiFlash -> TiDB -> Pump -> TiKV -> PD 的顺序关闭整个 TiDB 集群所有组件（同时也会关闭监控组件）： tiup cluster stop ${cluster-name} 和 start 命令类似，stop 命令也支持通过 -R 和 -N 参数来只停止部分组件。 例如，下列命令只停止 TiDB 组件： tiup cluster stop ${cluster-name} -R tidb 下列命令只停止 1.2.3.4 和 1.2.3.5 这两台机器上的 TiDB 组件： tiup cluster stop ${cluster-name} -R tidb -N 1.2.3.4:4000,1.2.3.5:4000 5、清除集群数据 此操作会关闭所有服务，并清空其数据目录或/和日志目录，并且无法恢复，需要谨慎操作。 清空集群所有服务的数据，但保留日志： tiup cluster clean ${cluster-name} --data 清空集群所有服务的日志，但保留数据： tiup cluster clean ${cluster-name} --log 清空集群所有服务的数据和日志： tiup cluster clean ${cluster-name} --all 清空 Prometheus 以外的所有服务的日志和数据： tiup cluster clean ${cluster-name} --all --ignore-role prometheus 清空节点 192.168.1.70:9000 以外的所有服务的日志和数据： tiup cluster clean ${cluster-name} --all --ignore-node 192.168.1.70:9000 清空部署在 172.16.13.12 以外的所有服务的日志和数据： tiup cluster clean ${cluster-name} --all --ignore-node 192.168.1.70 6、删除集群 tiup cluster destroy 集群名字 7、集群组件升级 在官方组件提供了新版之后，你可以使用 tiup update 命令来升级组件。除了以下几个参数，该命令的用法基本和 tiup install 相同： --all：升级所有组件 --nightly：升级至 nightly 版本 --self：升级 TiUP 自己至最新版本 --force：强制升级至最新版本 示例一：升级所有组件至最新版本 tiup update --all 示例二：升级所有组件至 nightly 版本 tiup update --all --nightly 示例三：升级 TiUP 至最新版本 tiup update --self 8、集群组件扩容 编辑组件节点扩容节点信息的文件 tidb-servers-scale-out-new-node.yaml TIDB配置文件参考 tidb_servers: - host: 10.0.1.5 ssh_port: 22 port: 4000 status_port: 10080 deploy_dir: /data/deploy/install/deploy/tidb-4000 log_dir: /data/deploy/install/log/tidb-4000 TiKV 配置文件参考： tikv_servers: - host: 10.0.1.5 ssh_port: 22 port: 20160 status_port: 20180 deploy_dir: /data/deploy/install/deploy/tikv-20160 data_dir: /data/deploy/install/data/tikv-20160 log_dir: /data/deploy/install/log/tikv-20160 PD 配置文件参考： pd_servers: - host: 10.0.1.5 ssh_port: 22 name: pd-1 client_port: 2379 peer_port: 2380 deploy_dir: /data/deploy/install/deploy/pd-2379 data_dir: /data/deploy/install/data/pd-2379 log_dir: /data/deploy/install/log/pd-2379 可以使用 tiup cluster edit-config 查看当前集群的配置信息，因为其中的 global 和 server_configs 参数配置默认会被 scale-out.yaml 继承，因此也会在 scale-out.yaml 中生效。 此处假设当前执行命令的用户和新增的机器打通了互信，如果不满足已打通互信的条件，需要通过 -p 来输入新机器的密码，或通过 -i 指定私钥文件。 执行扩容命令 tiup cluster scale-out 集群名 tidb-servers-scale-out-new-node.yaml 预期输出 Scaled cluster out successfully 信息，表示扩容操作成功。 9、集群组件缩容 tiup cluster scale-in -N $[节点IP]:[组件端口] ${cluster-name} 10、集群组件实例清理 你可以使用 tiup clean 命令来清理组件实例，并删除工作目录。如果在清理之前实例还在运行，会先 kill 相关进程。该命令用法如下： tiup clean [tag] [flags] 支持以下参数： --all：清除所有的实例信息 其中 tag 表示要清理的实例 tag，如果使用了 --all 则不传递 tag。 示例一：清理 tag 名称为 experiment 的组件实例 tiup clean experiment 示例二：清理所有组件实例 tiup clean --all 10、集群组件卸载 TiUP 安装的组件会占用本地磁盘空间，如果不想保留过多老版本的组件，可以先查看当前安装了哪些版本的组件，然后再卸载某个组件。 你可以使用 tiup uninstall 命令来卸载某个组件的所有版本或者特定版本，也支持卸载所有组件。该命令用法如下： tiup uninstall [component][:version] [flags] 支持的参数： --all：卸载所有的组件或版本 --self：卸载 TiUP 自身 component 为要卸载的组件名称，version 为要卸载的版本，这两个都可以省略，省略任何一个都需要加上 --all 参数： 若省略版本，加 --all 表示卸载该组件所有版本 若版本和组件都省略，则加 --all 表示卸载所有组件及其所有版本 示例一：卸载 v3.0.8 版本的 TiDB tiup uninstall tidb:v3.0.8 示例二：卸载所有版本的 TiKV tiup uninstall tikv --all 示例三：卸载所有已经安装的组件 tiup uninstall --all 11、集群节点命令操作 通过 tiup cluster deploy 完成部署操作才可以通过 exec 命令来进行集群级别管理工作，例如 tiup cluster exec 集群名字 --sudo --command \"yum install -y numactl\" # 或者 tiup cluster exec 集群名字 --sudo --command \"yum install -y numactl\" -R PD # 或者 tiup cluster exec 集群名字 --sudo --command \"yum install -y numactl\" -N node1 五、tiup命令 1、查看集群列表 tiup cluster list 六、搭建本地开发集群 以MacOS为例 1、下载安装 curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh # TiUP 安装完成后会提示 Shell profile 文件的绝对路径。在执行以下 source 命令前，需要将 ${your_shell_profile} 修改为 Shell profile 文件的实际位置。 source ${your_shell_profile} 2、启动 # 执行 tiup playground 命令会运行最新版本的 TiDB 集群，其中 TiDB、TiKV、PD 和 TiFlash 实例各 1 个. # 也可以指定 TiDB 版本以及各组件实例个数，命令类似于：tiup playground v6.1.0 --db 2 --pd 3 --kv 3 # --without-monitor : 设置不使用 Prometheus 和 Grafana 的监控功能。若不添加此参数，则默认开启监控功能。 tiup playground tiup playground v6.1.1 --db 3 --pd 3 --kv 3 3、持久化启动 在组件启动之前，TiUP 会先为它创建一个目录，然后将组件放到该目录中运行。组件会将所有数据生成在该目录中，目录的名字就是该组件运行时指定的 tag 名称。如果不指定 tag，则会随机生成一个 tag 名称，并且在实例终止时 自动删除 工作目录。如果希望分配固定目录，可以用 -T/--tag 来指定目录名字，这样多次执行使用同样的 tag 就能读写到同一批文件。 tiup playground v6.1.1 --tag=my-tidb-611 # 数据存储在路径'~/.tiup/data/标签名' 目录下 4、访问 # 查看已启动集群的信息 tiup playground display tiup status # 通过 http://127.0.0.1:9090 访问 TiDB 的 Prometheus 管理界面。 # 通过 http://127.0.0.1:2379/dashboard 访问 TiDB Dashboard 页面，默认用户名为 root，密码为空。 # 通过 http://127.0.0.1:3000 访问 TiDB 的 Grafana 界面，默认用户名和密码都为 admin 参考：https://docs.pingcap.com/zh/tidb/stable/quick-start-with-tidb#%E9%83%A8%E7%BD%B2%E6%9C%AC%E5%9C%B0%E6%B5%8B%E8%AF%95%E9%9B%86%E7%BE%A4 七、问题总结 1、tiup命令升级 如果使用tiup update --self命令无法自动升级tiup命令本身，可以先备份~/.tiup目录后直接重装tiup命令，重装后检查新装命令是否能正常获取TIDB集群信息 cp -r ~/.tiup ~/.tiup-bak curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh tiup cluster display 集群名称 八、混合部署拓扑的配置优化 常见的场景为，部署机为多路 CPU 处理器，内存也充足，为提高物理机资源利用率，可单机多实例部署，即 TiDB、TiKV 通过 numa 绑核，隔离 CPU 资源。 TiKV 进行配置优化 readpool 线程池自适应，配置 readpool.unified.max-thread-count 参数可以使 readpool.storage 和 readpool.coprocessor 共用统一线程池，同时要分别设置自适应开关。 开启 readpool.storage 和 readpool.coprocessor： readpool.storage.use-unified-pool: true readpool.coprocessor.use-unified-pool: true 计算公式如下： readpool.unified.max-thread-count = cores * 0.8 / TiKV 数量 storage CF (all RocksDB column families) 内存自适应，配置 storage.block-cache.capacity 参数即可实现 CF 之间自动平衡内存使用。 storage.block-cache 默认开启 CF 自适应，无需修改。 storage.block-cache.shared: true 计算公式如下： storage.block-cache.capacity = (MEM_TOTAL * 0.5 / TiKV 实例数量) 如果多个 TiKV 实例部署在同一块物理磁盘上，需要在 tikv 配置中添加 capacity 参数： raftstore.capacity = 磁盘总容量 / TiKV 实例数量 label 调度配置 由于采用单机多实例部署 TiKV，为了避免物理机宕机导致 Region Group 默认 3 副本的 2 副本丢失，导致集群不可用的问题，可以通过 label 来实现 PD 智能调度，保证同台机器的多 TiKV 实例不会出现 Region Group 只有 2 副本的情况。 TiKV 配置 相同物理机配置相同的 host 级别 label 信息： config: server.labels: host: tikv1 PD 配置 PD 需要配置 labels 类型来识别并调度 Region： pd: replication.location-labels: [\"host\"] numa_node 绑核 在实例参数模块配置对应的 numa_node 参数，并添加对应的物理 CPU 的核数； numa 绑核使用前，确认已经安装 numactl 工具，以及物理机对应的物理机 CPU 的信息后，再进行参数配置； numa_node 这个配置参数与 numactl --membind 配置对应。 注意 编辑配置文件模版时，注意修改必要参数、IP、端口及目录。 各个组件的 deploy_dir，默认会使用 global 中的 /-。例如 tidb 端口指定 4001，则 deploy_dir 默认为 '/tidb-deploy/tidb-4001'。因此，在多实例场景下指定非默认端口时，无需再次指定目录。 无需手动创建配置文件中的 tidb 用户，TiUP cluster 组件会在部署主机上自动创建该用户。可以自定义用户，也可以和中控机的用户保持一致。 如果部署目录配置为相对路径，会部署在用户的 Home 目录下。 参考：https://docs.pingcap.com/zh/tidb/stable/hybrid-deployment-topology#%E6%B7%B7%E5%90%88%E9%83%A8%E7%BD%B2%E7%9A%84%E5%85%B3%E9%94%AE%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-06-26 11:50:10 "},"origin/tiup-upgrade-cluster.html":{"url":"origin/tiup-upgrade-cluster.html","title":"TiUP升级集群","keywords":"","body":"TiUP升级TIDB集群 一、简介 文档：https://docs.pingcap.com/zh/tidb/stable/upgrade-tidb-using-tiup 升级兼容性说明 TiDB 目前暂不支持版本降级或升级后回退。 使用 TiDB Ansible 管理的 4.0 版本集群，需要先按照 4.0 版本文档的说明将集群导入到 TiUP (tiup cluster) 管理后，再按本文档说明升级到 6.1.0 版本及后续修订版本。 若要将 3.0 之前的版本升级至 6.1.0 版本： 首先通过 TiDB Ansible 升级到 3.0 版本。 然后按照 4.0 版本文档的说明，使用 TiUP (tiup cluster) 将 TiDB Ansible 配置导入。 将集群升级至 4.0 版本。 按本文档说明将集群升级到 6.1.0 版本。 支持 TiDB Binlog，TiCDC，TiFlash 等组件版本的升级。 具体不同版本的兼容性说明，请查看各个版本的 Release Note。请根据各个版本的 Release Note 的兼容性更改调整集群的配置。 升级 v5.3 之前版本的集群到 v5.3 及后续版本时，默认部署的 Prometheus 会从 v2.8.1 升级到 v2.27.1，v2.27.1 提供更多的功能并解决了安全风险。Prometheus v2.27.1 相对于 v2.8.1 存在 Alert 时间格式变化，详情见 Prometheus commit。 适用升级路径 从 TiDB 4.0 版本升级至 TiDB 6.1 及后续修订版本 从 TiDB 5.0-5.4 版本升级至 TiDB 6.1 及后续修订版本 从 TiDB 6.0 版本升级至 TiDB 6.1 及后续修订版本 升级方式 不停机升级 默认的升级 方式，即升级过程中集群仍然可以对外提供服务。升级时会对各节点逐个迁移 leader 后再升级和重启，因此对于大规模集群需要较长时间才能完成整个升级操作。 滚动升级会逐个升级所有的组件。升级 TiKV 期间，会逐个将 TiKV 上的所有 leader 切走再停止该 TiKV 实例。默认超时时间为 5 分钟（300 秒），超时后会直接停止该实例。 使用 --force 参数可以在不驱逐 leader 的前提下快速升级集群至新版本，但是该方式会忽略所有升级中的错误，在升级失败后得不到有效提示，请谨慎使用。 如果希望保持性能稳定，则需要保证 TiKV 上的所有 leader 驱逐完成后再停止该 TiKV 实例，可以指定 --transfer-timeout 为一个更大的值，如 --transfer-timeout 3600，单位为秒。 若想将 TiFlash 从 5.3 之前的版本升级到 5.3 及之后的版本，必须进行 TiFlash 的停机升级。参考如下步骤，可以在确保其他组件正常运行的情况下升级 TiFlash： 关闭 TiFlash 实例：tiup cluster stop -R tiflash 使用 --offline 参数在不重启（只更新文件）的情况下升级集群：tiup cluster upgrade --offline reload 整个集群：tiup cluster reload 。此时，TiFlash 也会正常启动，无需额外操作。 停机升级 如果业务有维护窗口可供数据库停机维护，则可以使用停机升级的方式快速进行升级操作。 二、升级前准备 以TiDB 5.1.0 升级至 6.1.1为例 tiup版本：1.52 1、备份数据和TiUP 备份TiDB数据 使用BR工具对TIDB数据物理文件进行备份，详情参考文档：TiDB-BR分布式冷备份与恢复 使用dumpling工具对TiDB数据进行逻辑备份，详情参考：TiDB-Dumpling：从TiDB/MySQL导出数据 备份TiUP配置 cp -r .tiup .tiup5-bak20221029 2、升级 TiUP 和 TiUP Cluster 升级 TiUP 和TiUP Cluster 版本（建议 tiup 和tiup cluster 版本不低于 1.10.0） # 设置TiUP的软件镜像源 tiup mirror show tiup mirror set https://tiup-mirrors.pingcap.com tiup mirror show # 查看TiUP和TiUP Cluster版本 tiup --version tiup cluster --version # 升级 TiUP 和 TiUP Cluster tiup update --all # 查看TiUP和TiUP Cluster版本 tiup --version tiup cluster --version # 查看TiUP和TiUP Cluster支持升级组件的版本 tiup cluster list tiup list tiup list tidb 三、升级步骤 1、编辑 TiUP Cluster 拓扑配置文件 tiup cluster edit-config 注意： 升级到 6.1.0 版本前，请确认已在 4.0 修改的参数在 6.1.0 版本中是兼容的，可参考 TiKV 配置文件描述。 以下 TiKV 参数在 TiDB v5.0 已废弃。如果在原集群配置过以下参数，需要通过 edit-config 编辑模式删除这些参数： pessimistic-txn.enabled server.request-batch-enable-cross-command server.request-batch-wait-duration 2、检查当前集群的健康状况 在升级前对集群当前的 region 健康状态进行检查 tiup cluster check --cluster 执行结束后，最后会输出 region status 检查结果。 如果结果为 \"All regions are healthy\"，则说明当前集群中所有 region 均为健康状态，可以继续执行升级； 如果结果为 \"Regions are not fully healthy: m miss-peer, n pending-peer\" 并提示 \"Please fix unhealthy regions before other operations.\"，则说明当前集群中有 region 处在异常状态，应先排除相应异常状态，并再次检查结果为 \"All regions are healthy\" 后再继续升级。 3、不停机升级 TiDB 集群 tiup cluster upgrade # tiup cluster upgrade test-tidb v6.1.1 4、升级后验证 集群验证 tiup cluster display # Cluster type: tidb # Cluster name: # Cluster version: v6.1.1 Dashboard验证 tiup cluster display --dashboard 客户端登录操作 Navicat 进行连接，执行数据操作 数据一致性验证 验证重要表的条数是否一致 dm同步任务 查看用于同步数据到TiDB的DM任务状态 验证DM数据同步效果 监控告警 查看Grafana监控是否正常 业务验证 检查使用TiDB的应用是否有重连机制 验证业务受到影响的应用在升级完成后是否正常 三、升级报错处理 1、升级时报错中断，处理完报错后，如何继续升级 重新执行 tiup cluster upgrade 命令进行升级，升级操作会重启之前已经升级完成的节点。如果不希望重启已经升级过的节点，可以使用 replay 子命令来重试操作，具体方法如下： 使用 tiup cluster audit 命令查看操作记录： tiup cluster audit 在其中找到失败的升级操作记录，并记下该操作记录的 ID，下一步中将使用 表示操作记录 ID 的值。 使用 tiup cluster replay 命令重试对应操作： tiup cluster replay 2、 升级过程中 evict leader 等待时间过长，如何跳过该步骤快速升级 可以指定 --force，升级时会跳过 PD transfer leader 和 TiKV evict leader 过程，直接重启并升级版本，对线上运行的集群性能影响较大。命令如下： tiup cluster upgrade --force 3、升级完成后更新 pd-ctl 等周边工具版本 可通过 TiUP 安装对应版本的 ctl 组件来更新相关工具版本： tiup install ctl:v6.1.0 4、Dashboard显示集群中未启动必要组件NgMonitoring NgMonitoring 是 TiDB v5.4.0 及以上集群中内置的高级监控组件，用于支撑 TiDB Dashboard 的 持续性能分析 和 Top SQL 等功能。使用较新版本 TiUP 部署或升级集群时，NgMonitoring 会自动部署；使用 TiDB Operator 部署集群时，需要依据启用持续性能分析手动部署 NgMonitoring。 ①检查 TiUP Cluster 版本，NgMonitoring 组件需要较高版本的部署工具支持（TiUP v1.9.0 及以上）： tiup cluster --version # 如果 TiUP 版本低于 v1.9.0，升级 TiUP 和 TiUP Cluster 版本至最新。 tiup update --all ②在中控机上，通过 TiUP 添加 ng_port 配置项，然后重启 Prometheus 节点。 以编辑模式打开集群的配置文件： tiup cluster edit-config ${cluster-name} 在 monitoring_servers 下面增加 ng_port:12020 参数： monitoring_servers: - host: 172.16.6.6 ng_port: 12020 重启 Prometheus 节点： tiup cluster reload ${cluster-name} --role prometheus -N 192.168.1.7:9000 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-11-02 10:45:46 "},"origin/tidb-dm.html":{"url":"origin/tidb-dm.html","title":"DM(Data Migration)数据增量全量同步至TiDB","keywords":"","body":"DM(Data Migration)数据全量及增量同步 一、简介 DM (Data Migration) 是一体化的数据同步任务管理平台，支持从 MySQL 或 MariaDB 到 TiDB 的全量数据迁移和增量数据同步。使用 DM 工具有利于简化错误处理流程，降低运维成本。 Block & allow lists：黑白名单配置，用于过滤或指定只迁移某些数据库或某些表的所有操作。 Block & Allow Lists 的过滤规则类似于 MySQL replication-rules-db/replication-rules-table Binlog event filter：用于过滤源数据库中特定表的特定类型操作 比如过滤掉表 test.sbtest 的 INSERT 操作或者过滤掉库 test 下所有表的 TRUNCATE TABLE 操作。 Table routing：将源数据库的表迁移到下游指定表的路由功能 比如将源数据表 test.sbtest1 的数据同步到 TiDB 的表 test.sbtest2。它也是分库分表合并迁移所需的一个核心功能。 1、DM 架构 DM 主要包括三个组件：DM-master，DM-worker 和 dmctl。 2、DM-master DM-master 负责管理和调度数据同步任务的各项操作。 保存 DM 集群的拓扑信息 监控 DM-worker 进程的运行状态 监控数据同步任务的运行状态 提供数据同步任务管理的统一入口 协调分库分表场景下各个实例分表的 DDL 同步 3、DM-worker DM-worker 负责执行具体的数据同步任务。 注册为一台 MySQL 或 MariaDB 服务器的 slave。 读取 MySQL 或 MariaDB 的 binlog event，并将这些 event 持久化保存在本地 (relay log) 单个 DM-worker 支持迁移一个 MySQL 或 MariaDB 实例的数据到下游的多个 TiDB 实例 多个 DM-Worker 支持迁移多个 MySQL 或 MariaDB 实例的数据到下游的一个 TiDB 实例 3.1、DM-worker逻辑处理单元 Relay log：持久化保存从上游 MySQL 或 MariaDB 读取的 binlog，并对 binlog replication 处理单元提供读取 binlog event 的功能。其原理和功能与 MySQL relay log 类似，详见 MySQL Relay Log。 dump 处理单元：从上游 MySQL 或 MariaDB 导出全量数据到本地磁盘。 load 处理单元：读取 dump 处理单元导出的数据文件，然后加载到下游 TiDB。 Binlog replication/sync 处理单元：读取上游 MySQL/MariaDB 的 binlog event 或 relay log 处理单元的 binlog event，将这些 event 转化为 SQL 语句，再将这些 SQL 语句应用到下游 TiDB。 3.2、DM-worker所需权限 上游数据库 (MySQL/MariaDB) 用户权限 权限 作用域 SELECT Tables RELOAD Global REPLICATION SLAVE Global REPLICATION CLIENT Global GRANT RELOAD,REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'your_user'@'your_wildcard_of_host' GRANT SELECT ON db1.* TO 'your_user'@'your_wildcard_of_host'; 下游数据库 (TiDB) 用户权限 权限 作用域 SELECT Tables INSERT Tables UPDATE Tables DELETE Tables CREATE Databases，tables DROP Databases，tables ALTER Tables INDEX Tables GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP,ALTER,INDEX ON db.table TO 'your_user'@'your_wildcard_of_host'; 处理单元所需的最小权限 处理单元 最小上游 (MySQL/MariaDB) 权限 最小下游 (TiDB) 权限 最小系统权限 Relay log REPLICATION SLAVE (读取 binlog） REPLICATION CLIENT (show master status, show slave status) 无 本地读/写磁盘 Dump SELECT RELOAD（获取读锁将表数据刷到磁盘，进行一些操作后，再释放读锁对表进行解锁） 无 本地写磁盘 Load 无 SELECT（查询 checkpoint 历史） CREATE（创建数据库或表） DELETE（删除 checkpoint） INSERT（插入 dump 数据） 读/写本地文件 Binlog replication REPLICATION SLAVE（读 binlog） REPLICATION CLIENT (show master status, show slave status) SELECT（显示索引和列） INSERT (DML) UPDATE (DML) DELETE (DML) CREATE（创建数据库或表） DROP（删除数据库或表） ALTER（修改表） INDEX（创建或删除索引） 本地读/写磁盘 4、DM高可用 当部署多个 DM-master 节点时 所有 DM-master 节点将使用内部嵌入的 etcd 组成集群。 DM-master 集群用于存储集群节点信息、任务配置等元数据， 通过 etcd 选举出 leader 节点。该 leader 节点用于提供集群管理、数据迁移任务管理相关的各类服务。 若可用的 DM-master 节点数超过部署节点的半数，即可正常提供服务。 当部署的 DM-worker 节点数超过上游 MySQL/MariaDB 节点数时 超出上游节点数的相关 DM-worker 节点默认将处于空闲状态。 若某个 DM-worker 节点下线或与 DM-master leader 发生网络隔离，DM-master 能自动将与原 DM-worker 节点相关的数据迁移任务调度到其他空闲的 DM-worker 节点上（若原 DM-worker 节点为网络隔离状态，则其会自动停止相关的数据迁移任务）； 若无空闲的 DM-worker 节点可供调度，则原 DM-worker 相关的数据迁移任务将暂时挂起，直到有空闲 DM-worker 节点后自动恢复。 注意： 当数据迁移任务处于全量导出或导入阶段时，该迁移任务暂不支持高可用，主要原因为： 对于全量导出，MySQL 暂不支持指定从特定快照点导出，也就是说数据迁移任务被重新调度或重启后，无法继续从前一次中断时刻继续导出。 对于全量导入，DM-worker 暂不支持跨节点读取全量导出数据，也就是说数据迁移任务被调度到的新 DM-worker 节点无法读取调度发生前原 DM-worker 节点上的全量导出数据。 5、dmctl dmctl 是用来控制 DM 集群的命令行工具。具体用法参考第六章节。 创建、更新或删除数据同步任务 查看数据同步任务状态 处理数据同步任务错误 校验数据同步任务配置的正确性 6、DM使用限制 数据库版本 MySQL 版本 > 5.5 如果上游 MySQL/MariaDB servers 间构成主从复制结构，则需要 MySQL 版本> 5.7.1 或者 MariaDB 版本>=10.1.3 MariaDB 版本 >= 10.1.2 DDL 语法兼容性 目前，TiDB 部分兼容 MySQL 支持的 DDL 语句。因为 DM 使用 TiDB parser 来解析处理 DDL 语句，所以目前仅支持 TiDB parser 支持的 DDL 语法。详见 TiDB DDL 语法支持。 DM 遇到不兼容的 DDL 语句时会报错。要解决此报错，需要使用 dmctl 手动处理，要么跳过该 DDL 语句，要么用指定的 DDL 语句来替换它。详见如何处理不兼容的 DDL 语句。 分库分表 如果业务分库分表之间存在数据冲突，可以参考自增主键冲突处理来解决；否则不推荐使用 DM 进行迁移，如果进行迁移则有冲突的数据会相互覆盖造成数据丢失。 分库分表 DDL 同步限制，参见悲观模式下分库分表合并迁移使用限制以及乐观模式下分库分表合并迁移使用限制。 同步的 MySQL 实例变更 当 DM-worker 通过虚拟 IP（VIP）连接到 MySQL 且要切换 VIP 指向的 MySQL 实例时，DM 内部不同的 connection 可能会同时连接到切换前后不同的 MySQL 实例，造成 DM 拉取的 binlog 与从上游获取到的其他状态不一致，从而导致难以预期的异常行为甚至数据损坏。如需切换 VIP 指向的 MySQL 实例，请参考虚拟 IP 环境下的上游主从切换对 DM 手动执行变更。 7、DM组件的端口 各 DM-master 节点间的 peer_port（默认为 8291）可互相连通。 各 DM-master 节点可连通所有 DM-worker 节点的 port（默认为 8262）。 各 DM-worker 节点可连通所有 DM-master 节点的 port（默认为 8261）。 TiUP 节点可连通所有 DM-master 节点的 port（默认为 8261）。 TiUP 节点可连通所有 DM-worker 节点的 port（默认为 8262）。 二、TiUP部署DM 1、部署TiUP 在root用户下 curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh echo \"export PATH=\\$PATH:/root/.tiup/bin\" >> /etc/profile source /etc/profile # TiUP下载安装好的路径：~/.tiup/bin/tiup # 配置信息: ~/.tiup/bin/7b8e153f2e2d0928.root.json # 镜像信息配置到了: https://tiup-mirrors.pingcap.com 2、TiUP安装DM # 先升级tiup自身，验证tiup是否为最新版本 tiup update --self # 先查看当前版本的tiup支持安装的DM版本 tiup list dm --verbose # 安装最新版本的DM tiup install dm:v1.4.2 3、配置DM主机拓扑配置 ①创建部署DM的主机拓扑模板配置文件 mkdir tiup-dm tiup dm template > tiup-dm/topology.yaml ②编写修改DM主机拓扑配置 更多配置模板参数参考：https://github.com/pingcap/tiup/blob/master/embed/templates/examples/dm/topology.example.yaml --- global: user: \"tidb\" ssh_port: 22 # arch: \"amd64\" server_configs: master: log-level: info # rpc-timeout: \"30s\" # rpc-rate-limit: 10.0 # rpc-rate-burst: 40 worker: log-level: info master_servers: - host: 192.168.1.6 deploy_dir: \"/data/tiup-dm/dm-master\" data_dir: \"/data/tiup-dm/dm-master/data\" log_dir: \"/data/tiup-dm/dm-master/log\" worker_servers: - host: 192.168.1.6 deploy_dir: \"/data/tiup-dm/dm-worker\" log_dir: \"/data/tiup-dm/dm-worker/log\" config: log-level: info monitoring_servers: - host: 192.168.1.6 deploy_dir: \"/data/tiup-dm/dm-prometheus\" data_dir: \"/data/tiup-dm/dm-prometheus/data\" log_dir: \"/data/tiup-dm/dm-prometheus/log\" grafana_servers: - host: 192.168.1.6 deploy_dir: \"/data/tiup-dm/dm-grafana\" alertmanager_servers: - host: 192.168.1.6 deploy_dir: \"/data/tiup-dm/dm-alertmanager\" data_dir: \"/data/tiup-dm/dm-alertmanager/data\" log_dir: \"/data/tiup-dm/dm-alertmanager/logs\" ③配置root用户SSH免密登录 cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys ④创建修改目录权限 mkdir -p /data/tiup-dm/{dm-master/{data,logs},dm-worker/{data,logs},dm-prometheus/{data,logs},dm-grafana,dm-alertmanager/{data,logs}} && \\ useradd tidb && \\ chown -R tidb:tidb /data/tiup-dm && \\ tree -L 3 /data/tiup-dm ⑤执行安装部署dm tiup dm deploy --user root -i /root/.ssh/id_rsa # dm集群版本使用tiup list dm-master查看支持安装的DM版本 ⑥启动DM集群 tiup dm start dm集群名字 ⑦访问验证DM集群服务 # 查看集群列表 tiup dm list # 检查集群状态 tiup dm display dm集群名字 访问Grafana：http://192.168.1.6:3000/login （默认用户名密码为：admin / admin） 访问Prometheus：http://192.168.1.6:9090/graph 访问Metric信息：http://192.168.1.6:8262/metrics (Metrics数据使用dm-master自带暴露的) ⑧查看tiup操作日志 tiup dm audit [audit-id] [flags] # 在不使用 [audit-id] 参数时，该命令会显示执行的命令列表 4、TiUP扩容DM节点 ①新建 scale.yaml 文件，添加新增的 woker 节点信息： --- worker_servers: - host: 新节点IP地址 # 需要新建一个拓扑文件，文件中只写入扩容节点的描述信息，不要包含已存在的节点。其他更多配置项（如：部署目录等）请参考 [TiUP 配置参数模版](https://github.com/pingcap/tiup/blob/master/embed/templates/examples/dm/topology.example.yaml)。 ②执行扩容操作。TiUP DM 根据 scale.yaml 文件中声明的端口、目录等信息在集群中添加相应的节点： tiup dm scale-out prod-cluster scale.yaml 执行完成之后可以通过 tiup dm display prod-cluster 命令检查扩容后的集群状态。 5、TiUP缩容DM节点 缩容即下线服务，最终会将指定的节点从集群中移除，并删除遗留的相关数据文件。缩容操作进行时，内部对 DM-master、DM-worker 组件的操作流程为： ​ ①停止组件进程 ​ ②调用 DM-master 删除 member 的 API ​ ③清除节点的相关数据文件 缩容命令的基本用法： tiup dm scale-in -N # 节点 ID 可以使用 `tiup dm display` 命令获取。 6、滚动升级 滚动升级过程中尽量保证对前端业务透明、无感知，其中对不同节点有不同的操作。 tiup dm upgrade 集群名字 dm新版本号 7、更新配置 如果想要动态更新组件的配置，TiUP DM 组件为每个集群保存了一份当前的配置，如果想要编辑这份配置，则执行 tiup dm edit-config 命令。例如： tiup dm edit-config prod-cluster 然后 TiUP DM 组件会使用 vi 打开配置文件供编辑（如果你想要使用其他编辑器，请使用 EDITOR 环境变量自定义编辑器，例如 export EDITOR=nano），编辑完之后保存即可。此时的配置并没有应用到集群，如果想要让它生效，还需要执行： tiup dm reload prod-cluster 该操作会将配置发送到目标机器，滚动重启集群，使配置生效。 8、更新组件 常规的升级集群可以使用 upgrade 命令，但是在某些场景下（例如 Debug)，可能需要用一个临时的包替换正在运行的组件，此时可以用 patch 命令： tiup dm patch [参数] 参数: -h, --help help for patch -N, --node string 以IP地址的形式指定更新组件的节点，例如：172.16.4.5:8261 --overwrite Use this package in the future scale-out operations -R, --role strings 以集群角色指定更新组件的节点 --transfer-timeout int Timeout in seconds when transferring dm-master leaders (default 300) 全局参数: --native-ssh Use the native SSH client installed on local system instead of the build-in one. --ssh-timeout int Timeout in seconds to connect host via SSH, ignored for operations that don't need an SSH connection. (default 5) --wait-timeout int Timeout in seconds to wait for an operation to complete, ignored for operations that don't fit. (default 60) -y, --yes Skip all confirmations and assumes 'yes' 9、在集群节点机器上执行命令 exec 命令可以很方便地到集群的机器上执行命令，使用方式如下： tiup dm exec [flags] Flags: --command string the command run on cluster host (default \"ls\") -h, --help help for exec -N, --node strings Only exec on host with specified nodes -R, --role strings Only exec on host with specified roles --sudo use root permissions (default false) 三、部署DM Portal DM Portal是一个方便用户图形化配置DM任务的Web页面。 curl -s -# https://download.pingcap.org/dm-portal-latest-linux-amd64.tar.gz | tar zxvf - -C /opt/ && \\ ln -s /opt/dm-portal-latest-linux-amd64 /opt/dm-portal && \\ echo -e \"export DM_PORTAL_HOME=/opt/dm-portal\\nexport PATH=\\$PATH:\\$DM_PORTAL_HOME/bin\" >> /etc/profile && \\ source /etc/profile && \\ mkdir -p /root/dm-portal/task-conf && \\ nohup /opt/dm-portal/bin/dm-portal --port=8280 -task-file-path=/root/dm-portal/task-conf > /root/dm-portal/dm-portal.log 2>&1 & 访问：http://DM_Portal服务器地址:8280，在Web页面上就可图形化配置DM同步任务。配置文件可通过浏览器直接下载，也可以在/root/tiup-dm-1.4.2/dm-portal/task-conf路径下找到。 更多信息参考文档：https://docs.pingcap.com/zh/tidb-data-migration/v1.0/dm-portal 四、管理上游数据源配置 1、加密数据库密码 在 DM 相关配置文件中，推荐使用经 dmctl 加密后的密码。对于同一个原始密码，每次加密后密码不同。 tiup dmctl --encrypt 密码 2、创建配置文件 source-id: \"mysql-192-201\" # DM-worker 是否使用全局事务标识符 (GTID) 拉取 binlog。使用前提是在上游 MySQL 已开启 GTID 模式。 enable-gtid: false from: host: \"上游数据源的IP地址\" user: \"上游数据源的用户名\" password: \"上游数据源用户的密码\" port: 上游数据源的端口 3、数据源操作 tiup dmctl --master-addr 192.168.1.6:8261 operate-source 操作动作 上游数据源配置文件路径(可传递多个文件路径) 操作动作如下： create：创建一个或多个上游的数据库源。创建多个数据源失败时，会尝试回滚到执行命令之前的状态 update：更新一个上游的数据库源 stop：停止一个或多个上游的数据库源。停止多个数据源失败时，可能有部分数据源已成功停止 show：显示已添加的数据源以及对应的 DM-worker 五、同步任务配置 DM任务完整配置参考 1、配置文件基础结构 ①配置信息及类型 配置文件总共分为两个部分：全局配置和实例配置，其中全局配置又分为任务基本信息配置和实例配置， 配置顺序如下： 编辑全局配置。 根据全局配置编辑实例配置。 ②同步任务模式：task-mode 描述：任务模式，可以通过任务模式来指定需要执行的数据迁移工作。 值为字符串(full，incremental 或 all）) full：只全量备份上游数据库，然后将数据全量导入到下游数据库。 incremental：只通过 binlog 把上游数据库的增量修改复制到下游数据库, 可以设置实例配置的 meta 配置项来指定增量复制开始的位置。 all：full + incremental。先全量备份上游数据库，将数据全量导入到下游数据库，然后从全量数据备份时导出的位置信息 (binlog position) 开始通过 binlog 增量复制数据到下游数据库。 2、功能配置集 全局配置主要包含下列功能配置集： 配置项 说明 routes 上游和下游表之间的路由 table routing 规则集。如果上游与下游的库名、表名一致，则不需要配置该项。使用场景及示例配置参见 Table Routing filters 上游数据库实例匹配的表的 binlog event filter 规则集。如果不需要对 binlog 进行过滤，则不需要配置该项。使用场景及示例配置参见 Binlog Event Filter block-allow-list 该上游数据库实例匹配的表的 block & allow lists 过滤规则集。建议通过该项指定需要迁移的库和表，否则会迁移所有的库和表。使用场景及示例配置参见 Block & Allow Lists mydumpers dump 处理单元的运行配置参数。如果默认配置可以满足需求，则不需要配置该项，也可以只使用 mydumper-thread 对 thread 配置项单独进行配置。 loaders load 处理单元的运行配置参数。如果默认配置可以满足需求，则不需要配置该项，也可以只使用 loader-thread 对 pool-size 配置项单独进行配置。 syncers sync 处理单元的运行配置参数。如果默认配置可以满足需求，则不需要配置该项，也可以只使用 syncer-thread 对 worker-count 配置项单独进行配置。 各个功能配置集的参数及解释参见完整配置文件示例中的注释说明。 3、同步任务进度元信息 创建的同步任务会将元信息保存在下游TIDB的dm_meta库中的\"任务名_load_checkpoint\"、\"任务名_sync_checkpoint\"表中。 任务名_load_checkpoint：保存了load单元从上游数据库dump数据到SQL文件的元信息。 任务名_sync_checkpoint：保存了sync单元已从SQL文件同步数据到下游数据库的元信息。 4、配置文件说明 --- # ----------- 全局配置 ----------- # ********* 基本信息配置 ********* # 任务名称，需要全局唯一 name: test # 任务模式，可设为 \"full\" - \"只进行全量数据迁移\"、\"incremental\" - \"Binlog 实时同步\"、\"all\" - \"全量 + Binlog 迁移\" task-mode: all # 如果为分库分表合并任务则需要配置该项。默认使用悲观协调模式 \"pessimistic\"，在深入了解乐观协调模式的原理和使用限制后，也可以设置为乐观协调模式 \"optimistic\" shard-mode: \"pessimistic\" # 下游储存 `meta` 信息的数据库 meta-schema: \"dm_meta\" # 时区 timezone: \"Asia/Shanghai\" # schema/table 是否大小写敏感 case-sensitive: false # 目前仅支持 \"gh-ost\" 、\"pt\" online-ddl-scheme: \"gh-ost\" # 不关闭任何检查项。可选的检查项有 \"all\"、\"dump_privilege\"、\"replication_privilege\"、\"version\"、\"binlog_enable\"、\"binlog_format\"、\"binlog_row_image\"、\"table_schema\"、\"schema_of_shard_tables\"、\"auto_increment_ID\" ignore-checking-items: [] # 是否清理 dump 阶段产生的文件，包括 metadata 文件、建库建表 SQL 文件以及数据导入 SQL 文件 clean-dump-file: true # 下游数据库实例配置 target-database: host: \"192.168.0.1\" port: 4000 user: \"root\" # 推荐使用经 dmctl 加密后的密码 password: \"/Q7B9DizNLLTTfiZHv9WoEAKamfpIUs=\" # 设置 DM 内部连接 TiDB 服务器时，TiDB 客户端的 \"max_allowed_packet\" 限制（即接受的最大数据包限制），单位为字节，默认 67108864 (64 MB) max-allowed-packet: 67108864 # 该配置项从 DM v2.0.0 版本起弃用，DM 会自动获取连接 TiDB 的 \"max_allowed_packet\" # 设置 TiDB 的 session 变量，在 v1.0.6 版本引入。更多变量及解释参见 `https://docs.pingcap.com/zh/tidb/stable/system-variables` session: # 从 DM v2.0.0 版本起，如果配置文件中没有出现该项，DM 会自动从下游 TiDB 中获得适合用于 \"sql_mode\" 的值。手动配置该项具有更高优先级 sql_mode: \"ANSI_QUOTES,NO_ZERO_IN_DATE,NO_ZERO_DATE\" # 从 DM v2.0.0 版本起，如果配置文件中没有出现该项，DM 会自动从下游 TiDB 中获得适合用于 \"tidb_skip_utf8_check\" 的值。手动配置该项具有更高优先级 tidb_skip_utf8_check: 1 tidb_constraint_check_in_place: 0 # 下游 TiDB TLS 相关配置 security: ssl-ca: \"/path/to/ca.pem\" ssl-cert: \"/path/to/cert.pem\" ssl-key: \"/path/to/key.pem\" # ******** 功能配置集 ********** # # 上游和下游表之间的路由 table routing 规则集 routes: # 配置名称 route-rule-1: # 库名匹配规则，支持通配符 \"*\" 和 \"?\" schema-pattern: \"test_*\" # 表名匹配规则，支持通配符 \"*\" 和 \"?\" table-pattern: \"t_*\" # 目标库名称 target-schema: \"test\" # 目标表名称 target-table: \"t\" route-rule-2: schema-pattern: \"test_*\" target-schema: \"test\" # 上游数据库实例匹配的表的 binlog event filter 规则集 filters: # 配置名称 filter-rule-1: # 库名匹配规则，支持通配符 \"*\" 和 \"?\" schema-pattern: \"test_*\" # 表名匹配规则，支持通配符 \"*\" 和 \"?\" table-pattern: \"t_*\" # 匹配哪些 event 类型 events: [\"truncate table\", \"drop table\"] # 对与符合匹配规则的 binlog 迁移（Do）还是忽略(Ignore) action: Ignore filter-rule-2: schema-pattern: \"test_*\" events: [\"all dml\"] action: Do # 定义数据源迁移表的过滤规则，可以定义多个规则。如果 DM 版本 5、实例配置文件 name: sync-test-mysql-to-tidb task-mode: all is-sharding: false target-database: host: 192.168.1.8 port: 4000 user: root password: *****参考第四章节第一节加密数据库密码****** session: sql_mode: \"\" tidb_skip_utf8_check: 1 tidb_constraint_check_in_place: 0 foreign_key_checks: OFF mysql-instances: - source-id: mysql-192-1-6 meta: binlog-name: mysql-bin.0000001 binlog-pos: 4 filter-rules: [] route-rules: - replica-1 black-white-list: replica-1.bw_list mydumper-config-name: replica-1.dump routes: replica-1: schema-pattern: test_db target-schema: stg_db filters: {} black-white-list: replica-1.bw_list: do-tables: [] do-dbs: [\"test_db\"] ignore-tables: - db-name: \"~.*\" tbl-name: \"test_1\" - db-name: \"~.*\" tbl-name: \"test_2\" ignore-dbs: [] mydumpers: replica-1.dump: mydumper-path: bin/mydumper threads: 4 chunk-filesize: 64 skip-tz-utc: true 六、dmctl集群与任务控制 注意： 对于用 TiUP 部署的 DM 集群，推荐直接使用 tiup dmctl 命令。 dmctl 是用来运维 DM 集群的命令行工具，支持交互模式和命令模式。 1、dmctl tiup dmctl [global options] command [command options] [arguments...] 特殊命令: --encrypt Encrypts plaintext to ciphertext. --decrypt Decrypts ciphertext to plaintext. 全局命令参数: --V Prints version and exit. --config Path to config file. --master-addr Master API server address. --rpc-timeout RPC timeout, default is 10m. --ssl-ca Path of file that contains list of trusted SSL CAs for connection. --ssl-cert Path of file that contains X509 certificate in PEM format for connection. --ssl-key Path of file that contains X509 key in PEM format for connection. 交互模式：与 DM-master 进行交互 命令格式： tiup dmctl:[dmctl版本] --master-addr dm-master节点ip地址:8261 交互模式下不具有 bash 的特性，比如不需要通过引号传递字符串参数而应当直接传递。 命令模式：执行命令时只需要在 dmctl 命令后紧接着执行任务操作，任务操作同交互模式的参数一致。 命令格式：tiup dmctl -master-addr dm-master节点ip地址:8261 dmctl的命令 一条 dmctl 命令只能跟一个任务操作 任务操作只能放在 dmctl 命令的最后 2、dmctl命令选项参数 Available Commands: check-task 检查任务配置文件. get-config 获取任务的信息 handle-error `skip`/`replace`/`revert` the current error event or a specific binlog position (binlog-pos) event. help Gets help about any command. list-member 列出DM Member信息. offline-member Offlines member which has been closed. operate-leader `evict`/`cancel-evict` the leader. operate-schema `get`/`set`/`remove` the schema for an upstream table. operate-source `create`/`update`/`stop`/`show` 管理上游数据源MySQL/MariaDB配置 pause-relay 暂停DM-worker的relay逻辑处理单元. pause-task 暂停在跑的任务. purge-relay DM 支持自动清理 relay log，但同时 DM 也支持使用 purge-relay 命令手动清理 relay log。 query-status 查询任务状态 resume-relay 恢复DM-worker的relay逻辑处理单元. resume-task 恢复处于暂停状态的任务 show-ddl-locks 显示无法解决的DDL锁 start-relay Starts workers pulling relay log for a source. start-task 启动配置文件中配置的任务 stop-relay Stops workers pulling relay log for a source. stop-task 暂停任务 transfer-source 改变上游数据源MySQL/MariaDB与 DM-worker 的绑定关系。 unlock-ddl-lock 强制解开DDL锁 Flags: -h, --help help for dmctl -s, --source strings MySQL Source ID 3、dmctl常用命令 ①检查任务配置文件 # 交互模式下 check-task 任务配置文件路径（最好是绝对路径） ②启动同步任务 # 交互模式下 start-task [-s source ...] [--remove-meta] 任务配置文件路径（最好是绝对路径）[flags] 参数: --remove-meta 是否删除任务的元数据 ③查询同步任务状态 # 交互模式下 query-status [-s source ...] [任务名 | 任务配置文件路径] [--more] [flags] 参数: --more whether to print the detailed task information 全局参数: -s, --source strings MySQL Source ID. ④暂停任务 # 交互模式下 pause-task [-s source ...] [flags] 全局参数: -s, --source strings MySQL Source ID. ⑤恢复任务 # 交互模式下 resume-task [-s source ...] [flags] 全局参数: -s, --source strings MySQL Source ID. ⑥删除任务 # 交互模式下 stop-task [-s source ...] [flags] 全局参数: -s, --source strings MySQL Source ID. 4、跳过或替代执行异常的SQL 文档：https://docs.pingcap.com/zh/tidb-data-migration/stable/handle-failed-ddl-statements 新的DM跳过异常SQL使用的是handle-error命令，不再使用sql-skip。也不再支持以SQL语句的方式跳过。 ①先查出任务错误或异常SQL所处的binlog位置 query-status 任务名 ②使用handle-error跳过，替换，恢复异常的SQL # 交互模式下 handle-error [-s source ...] [-b binlog-pos] [替代的SQL语句1;替代的SQL语句2;] [flags] 参数: -b, --binlog-pos string .pos格式：\"mysql-bin|000001.000003:3270\" 全局参数: -s, --source strings MySQL Source ID. ③跳过出错的SQL handle-error 任务名 skip 七、DM监控 使用TiUP部署DM组件时可以部署prometheus生态的监控组件。 DM的Metrics信息是由dm-master进行暴露的。 如果已部署的有Grafana，想复用，可不部署Grafana，只需部署Prometheus即可。Grafana添加数据源，然后导入JSON格式的Dashboard定义文件（见附件）即可。 可监控的指标 1、Overview overview 下包含运行当前选定 task 的所有 DM-worker/master instance/source 的部分监控指标。当前默认告警规则只针对于单个 DM-worker/master instance/source。 metric 名称 说明 告警说明 告警级别 task state 迁移子任务的状态 N/A N/A storage capacity relay log 占有的磁盘的总容量 N/A N/A storage remain relay log 占有的磁盘的剩余可用容量 N/A N/A binlog file gap between master and relay relay 与上游 master 相比落后的 binlog file 个数 N/A N/A load progress load unit 导入过程的进度百分比，值变化范围为：0% - 100% N/A N/A binlog file gap between master and syncer 与上游 master 相比 binlog replication unit 落后的 binlog file 个数 N/A N/A shard lock resolving 当前子任务是否正在等待 shard DDL 迁移，大于 0 表示正在等待迁移 N/A N/A 2、Operate error metric 名称 说明 告警说明 告警级别 before any operate error 在进行操作之前出错的次数 N/A N/A source bound error 数据源绑定操作出错次数 N/A N/A start error 子任务启动的出错次数 N/A N/A pause error 子任务暂停的出错次数 N/A N/A resume error 子任务恢复的出错次数 N/A N/A auto-resume error 子任务自动恢复的出错次数 N/A N/A update error 子任务更新的出错次数 N/A N/A stop error 子任务停止的出错次数 N/A N/A 3、HA 高可用 metric 名称 说明 告警说明 告警级别 number of dm-masters start leader components per minute 每分钟内 DM-master 尝试启用 leader 相关组件次数 N/A N/A number of workers in different state 不同状态下有多少个 DM-worker 存在离线的 DM-worker 超过一小时 critical workers' state DM-worker 的状态 N/A N/A number of worker event error 不同类型的 DM-worker 错误出现次数 N/A N/A shard ddl error per minute 每分钟内不同类型的 shard DDL 错误次数 发生 shard DDL 错误 critical number of pending shard ddl 未完成的 shard DDL 数目 存在未完成的 shard DDL 数目超过一小时 critical 4、Task 状态 metric 名称 说明 告警说明 告警级别 task state 迁移子任务的状态 当子任务状态处于 Paused 超过 20 分钟时 critical Task任务Metric信息状态 状态含义 0 invalidStage 1 New 2 Running 3 Paused 4 Stopped 5 Stopped 5、Dump/Load unit 下面 metrics 仅在 task-mode 为 full 或者 all 模式下会有值。 metric 名称 说明 告警说明 告警级别 load progress load unit 导入过程的进度百分比，值变化范围为：0% - 100% N/A N/A data file size load unit 导入的全量数据中数据文件（内含 INSERT INTO 语句）的总大小 N/A N/A dump process exits with error dump unit 在 DM-worker 内部遇到错误并且退出了 立即告警 critical load process exits with error load unit 在 DM-worker 内部遇到错误并且退出了 立即告警 critical table count load unit 导入的全量数据中 table 的数量总和 N/A N/A data file count load unit 导入的全量数据中数据文件（内含 INSERT INTO 语句）的数量总和 N/A N/A transaction execution latency load unit 在执行事务的时延，单位：秒 N/A N/A statement execution latency load unit 执行语句的耗时，单位：秒 N/A N/A 6、Binlog replication 下面 metrics 仅在 task-mode 为 incremental 或者 all 模式下会有值。 metric 名称 说明 告警说明 告警级别 remaining time to sync 预计 Syncer 还需要多少分钟可以和 master 完全同步，单位：分钟 N/A N/A replicate lag master 到 Syncer 的 binlog 复制延迟时间，单位：秒 N/A N/A process exist with error binlog replication unit 在 DM-worker 内部遇到错误并且退出了 立即告警 critical binlog file gap between master and syncer 与上游 master 相比落后的 binlog file 个数 落后 binlog file 个数超过 1 个（不含 1 个）且持续 10 分钟时 critical binlog file gap between relay and syncer 与 relay 相比落后的 binlog file 个数 落后 binlog file 个数超过 1 个（不含 1 个）且持续 10 分钟时 critical binlog event QPS 单位时间内接收到的 binlog event 数量 (不包含需要跳过的 event) N/A N/A skipped binlog event QPS 单位时间内接收到的需要跳过的 binlog event 数量 N/A N/A read binlog event duration binlog replication unit 从 relay log 或上游 MySQL 读取 binlog 的耗时，单位：秒 N/A N/A transform binlog event duration binlog replication unit 解析 binlog 并将 binlog 转换成 SQL 语句的耗时，单位：秒 N/A N/A dispatch binlog event duration binlog replication unit 调度一条 binlog event 的耗时，单位：秒 N/A N/A transaction execution latency binlog replication unit 执行事务到下游的耗时，单位：秒 N/A N/A binlog event size binlog replication unit 从 relay log 或上游 MySQL 读取的单条 binlog event 的大小 N/A N/A DML queue remain length 剩余 DML job 队列的长度 N/A N/A total sqls jobs 单位时间内新增的 job 数量 N/A N/A finished sqls jobs 单位时间内完成的 job 数量 N/A N/A statement execution latency binlog replication unit 执行语句到下游的耗时，单位：秒 N/A N/A add job duration binlog replication unit 增加一条 job 到队列的耗时，单位：秒 N/A N/A DML conflict detect duration binlog replication unit 检测 DML 间冲突的耗时，单位：秒 N/A N/A skipped event duration binlog replication unit 跳过 binlog event 的耗时，单位：秒 N/A N/A unsynced tables 当前子任务内还未收到 shard DDL 的分表数量 N/A N/A shard lock resolving 当前子任务是否正在等待 shard DDL 迁移，大于 0 表示正在等待迁移 N/A N/A 7、Relay log metric 名称 说明 告警说明 告警级别 storage capacity relay log 占有的磁盘的总容量 N/A N/A storage remain relay log 占有的磁盘的剩余可用容量 小于 10G 的时候需要告警 critical process exits with error relay log 在 DM-worker 内部遇到错误并且退出了 立即告警 critical relay log data corruption relay log 文件损坏的个数 立即告警 emergency fail to read binlog from master relay 从上游的 MySQL 读取 binlog 时遇到的错误数 立即告警 critical fail to write relay log relay 写 binlog 到磁盘时遇到的错误数 立即告警 critical binlog file index relay log 最大的文件序列号。如 value = 1 表示 relay-log.000001 N/A N/A binlog file gap between master and relay relay 与上游 master 相比落后的 binlog file 个数 落后 binlog file 个数超过 1 个（不含 1 个）且持续 10 分钟时 critical binlog pos relay log 最新文件的写入 offset N/A N/A read binlog event duration relay log 从上游的 MySQL 读取 binlog 的时延，单位：秒 N/A N/A write relay log duration relay log 每次写 binlog 到磁盘的时延，单位：秒 N/A N/A binlog event size relay log 写到磁盘的单条 binlog 的大小 N/A N/A 8、Instance 在 Grafana dashboard 中，instance 的默认名称为 DM-instance。 9、Relay log metric 名称 说明 告警说明 告警级别 storage capacity relay log 占有的磁盘的总容量 N/A N/A storage remain relay log 占有的磁盘的剩余可用容量 小于 10G 的时候需要告警 critical process exits with error relay log 在 DM-worker 内部遇到错误并且退出了 立即告警 critical relay log data corruption relay log 文件损坏的个数 立即告警 emergency fail to read binlog from master relay 从上游的 MySQL 读取 binlog 时遇到的错误数 立即告警 critical fail to write relay log relay 写 binlog 到磁盘时遇到的错误数 立即告警 critical binlog file index relay log 最大的文件序列号。如 value = 1 表示 relay-log.000001 N/A N/A binlog file gap between master and relay relay 与上游 master 相比落后的 binlog file 个数 落后 binlog file 个数超过 1 个（不含 1 个）且持续 10 分钟时 critical binlog pos relay log 最新文件的写入 offset N/A N/A read binlog duration relay log 从上游的 MySQL 读取 binlog 的时延，单位：秒 N/A N/A write relay log duration relay log 每次写 binlog 到磁盘的时延，单位：秒 N/A N/A binlog size relay log 写到磁盘的单条 binlog 的大小 N/A N/A 10、task metric 名称 说明 告警说明 告警级别 task state 迁移子任务的状态 当子任务状态处于 paused 超过 10 分钟时 critical load progress load unit 导入过程的进度百分比，值变化范围为：0% - 100% N/A N/A binlog file gap between master and syncer 与上游 master 相比 binlog replication unit 落后的 binlog file 个数 N/A N/A shard lock resolving 当前子任务是否正在等待 shard DDL 迁移，大于 0 表示正在等待迁移 N/A N/A 八、DM任务问题汇总 1、上游MySQL的DDL语句不支持同步执行到TiDB 2、上游MySQL的DML语句不支持同步执行到TiDB ①上游表字段个数与下游字段个数不一致，后续DML无法执行，Binlog无法跳过、handle error skip不生效。 问题报错 { \"subTaskStatus\": [ { \"name\": \"sync-rds-mysql-to-tidb\", \"stage\": \"Paused\", \"unit\": \"Sync\", \"result\": { \"isCanceled\": false, \"errors\": [ { \"ErrCode\": 36027, \"ErrClass\": \"sync-unit\", \"ErrScope\": \"internal\", \"ErrLevel\": \"high\", \"Message\": \"startLocation: [position: (, 0), gtid-set: ], endLocation: [position: (mysql-bin.004795, 157971799), gtid-set: ]: gen update sqls failed, schema: test, table: test: Column count doesn't match value count: 4 (columns) vs 9 (values)\", \"RawCause\": \"\", \"Workaround\": \"\" } ], \"detail\": null }, \"unresolvedDDLLockID\": \"\", \"sync\": { \"totalEvents\": \"7388167961\", \"totalTps\": \"1400\", \"recentTps\": \"23\", \"masterBinlog\": \"(mysql-bin.004796, 70368975)\", \"masterBinlogGtid\": \"\", \"syncerBinlog\": \"(mysql-bin.004795, 157971328)\", \"syncerBinlogGtid\": \"\", \"blockingDDLs\": [], \"unresolvedGroups\": [], \"synced\": false, \"binlogType\": \"remote\" } } ] } 问题处理 handle-error sync-rds-mysql-to-tidb skip 无法自动跳过错误 handle-error sync-rds-mysql-to-tidb -b mysql-bin.004795:157971328 skip 无法跳过指定position 解决方案 # 1. 显示表的创建语句与下游表的字段个数不一致 operate-schema get -s 上游数据源名 任务名 -d 库名 -t 表名 # 2. 创建sql文件，填写报错表的创建语句（正确字段个数表的创建语句）。执行下面命令 operate-schema set -s 上游数据源名 任务名 -d 库名 -t 表名 sql文件 # SQL文件路径相对于tiup安装目录路径，例如放在~/.tiup下 # 3. 暂停任务后重启任务即可跳过position pause-task 任务名 resume-task 任务名 参考： https://github.com/pingcap/dm/issues/2285 https://issuehint.com/issue/pingcap/dm/2285 3、上游MySQL表中有外键造成无法同步 4、上游MySQL表中日期数据值包含“00”造成无法同步 参考 https://github.com/pingcap/tidb/issues/6072 https://docs.pingcap.com/zh/tidb-data-migration/stable/key-features#%E8%BF%87%E6%BB%A4%E8%A7%84%E5%88%99 https://docs.pingcap.com/zh/tidb-data-migration/stable/dmctl-introduction https://docs.pingcap.com/zh/tidb-data-migration/stable/migrate-data-using-dm#%E7%AC%AC-8-%E6%AD%A5%E7%9B%91%E6%8E%A7%E4%BB%BB%E5%8A%A1%E4%B8%8E%E6%9F%A5%E7%9C%8B%E6%97%A5%E5%BF%97 https://docs.pingcap.com/zh/tidb-data-migration/stable/task-configuration-file-full https://docs.pingcap.com/zh/tidb-data-migration/stable/task-configuration-guide#%E9%85%8D%E7%BD%AE%E9%9C%80%E8%A6%81%E8%BF%81%E7%A7%BB%E7%9A%84%E8%A1%A8 https://docs.pingcap.com/zh/tidb-data-migration/stable/maintain-dm-using-tiup#%E9%9B%86%E7%BE%A4%E6%8E%A7%E5%88%B6%E5%B7%A5%E5%85%B7-dmctl https://docs.pingcap.com/zh/tidb-data-migration/stable/tune-configuration https://docs.pingcap.com/zh/tidb-data-migration/v1.0/skip-or-replace-abnormal-sql-statements https://asktug.com/t/topic/63887 https://github.com/google/re2/wiki/Syntax https://docs.pingcap.com/zh/tidb-data-migration/stable/faq https://docs.pingcap.com/zh/tidb-data-migration/stable/query-status https://docs.pingcap.com/zh/tidb-data-migration/stable/task-configuration-guide#%E9%85%8D%E7%BD%AE%E9%9C%80%E8%A6%81%E6%95%B0%E6%8D%AE%E6%BA%90%E8%A1%A8%E5%88%B0%E7%9B%AE%E6%A0%87-tidb-%E8%A1%A8%E7%9A%84%E6%98%A0%E5%B0%84 https://docs.pingcap.com/zh/tidb-data-migration/stable/task-configuration-file-full#%E5%AE%8C%E6%95%B4%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%A4%BA%E4%BE%8B https://docs.pingcap.com/zh/tidb-data-migration/stable/usage-scenario-simple-migration#%E8%BF%81%E7%A7%BB%E6%96%B9%E6%A1%88 https://asktug.com/t/topic/33397/4 https://docs.pingcap.com/tidb/stable/data-type-date-and-time https://asktug.com/t/topic/33397 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-08-11 11:15:01 "},"origin/tidb-dumpling-export.html":{"url":"origin/tidb-dumpling-export.html","title":"Dumpling从TiDB/MySQL导出数据","keywords":"","body":"Dumpling全量备份或导出 一、Dumpling 1、简介 Dumpling 是使用 go 开发的数据备份工具，项目地址可以参考 Dumpling。 Dumpling 的更多具体用法可查看 Dumpling 主要选项表。 Dumpling 包含在tidb-toolkit 安装包中，下载链接：https://download.pingcap.org/tidb-toolkit-v4.0.5-linux-amd64.tar.gz 为了快速地备份恢复数据（特别是数据量巨大的库），可以参考以下建议： 导出来的数据文件应当尽可能的小，可以通过设置选项 -F 来控制导出来的文件大小。 如果后续使用 TiDB Lightning 对备份文件进行恢复，建议把 dumpling -F 选项的值设置为 256m。 如果导出的表中有些表的行数非常多，可以通过设置选项 -r 来开启表内并发。 2、对比Mydumper 支持导出多种数据形式，包括 SQL/CSV 支持全新的 table-filter，筛选数据更加方便 针对 TiDB 进行了更多优化： 支持配置 TiDB 单条 SQL 内存限制 针对 TiDB v4.0.0 以上版本支持自动调整 TiDB GC 时间 使用 TiDB 的隐藏列 _tidb_rowid 优化了单表内数据的并发导出性能 对于 TiDB 可以设置 tidb_snapshot 的值指定备份数据的时间点，从而保证备份的一致性，而不是通过 FLUSH TABLES WITH READ LOCK 来保证备份一致性。 二、从TiDB/MySQL 导出数据 1、源库导出账号所需权限 SELECT RELOAD LOCK TABLES REPLICATION CLIENT 2、安装及主要参数 ①安装 二进制 version=v4.0.5 && \\ curl -# https://download.pingcap.org/tidb-toolkit-$version-linux-amd64.tar.gz | tar -zxC /opt && \\ ln -s /opt/tidb-toolkit-$version-linux-amd64 /opt/tidb-toolkit-$version && \\ echo \"export PATH=/opt/tidb-toolkit-$version/bin:$PATH\" >> /etc/profile && \\ source /etc/profile && \\ dumpling -V 源码构建 # MacOS源码构建（要求：golang>=1.6,二进制输出路径：bin/dumpling） git clone https://github.com/pingcap/dumpling.git && \\ cd dumpling && \\ git checkout v4.0.5 && \\ make build && \\ chmox +X ./bin/dumpling && \\ mv ./bin/dumpling /usr/local/bin/ && \\ dumpling --version Docker docker pull pingcap/dumpling:v5.1.0 docker run -it -v 本地存储导出SQL文件的目录 pingcap/dumpling:v5.1.0 sh # dumpling命令执行路径在根目录下,具体的导出任务可以在容器中执行 MacOS brew install dumpling dumpling --version ②命令参数 主要选项 用途 默认值 -V 或 --version 输出 Dumpling 版本并直接退出 -B 或 --database 导出指定数据库 -T 或 --tables-list 导出指定数据表 -f 或 --filter 导出能匹配模式的表，语法可参考 table-filter *.*（导出所有库表） --case-sensitive table-filter 是否大小写敏感 false，大小写不敏感 -h 或 --host 连接的数据库主机的地址 \"127.0.0.1\" -t 或 --threads 备份并发线程数 4 -r 或 --rows 将 table 划分成 row 行数据，一般针对大表操作并发生成多个文件。 -L 或 --logfile 日志输出地址，为空时会输出到控制台 \"\" --loglevel 日志级别 {debug,info,warn,error,dpanic,panic,fatal} \"info\" --logfmt 日志输出格式 {text,json} \"text\" -d 或 --no-data 不导出数据，适用于只导出 schema 场景 --no-header 导出 csv 格式的 table 数据，不生成 header -W 或 --no-views 不导出 view true -m 或 --no-schemas 不导出 schema，只导出数据 -s 或--statement-size 控制 INSERT SQL 语句的大小，单位 bytes -F 或 --filesize 将 table 数据划分出来的文件大小，需指明单位（如 128B, 64KiB, 32MiB, 1.5GiB） --filetype 导出文件类型（csv/sql） \"sql\" -o 或 --output 导出文件路径 \"./export-${time}\" -S 或 --sql 根据指定的 sql 导出数据，该选项不支持并发导出 --consistency flush: dump 前用 FTWRLsnapshot: 通过 TSO 来指定 dump 某个快照时间点的 TiDB 数据 lock: 对需要 dump 的所有表执行 lock tables read 命令 none: 不加锁 dump，无法保证一致性 auto: MySQL 默认用 flush, TiDB 默认用 snapshot \"auto\" --snapshot snapshot tso，只在 consistency=snapshot 下生效 --where 对备份的数据表通过 where 条件指定范围 -p 或 --password 连接的数据库主机的密码 -P 或 --port 连接的数据库主机的端口 4000 -u 或 --user 连接的数据库主机的用户名 \"root\" --dump-empty-database 导出空数据库的建库语句 true --ca 用于 TLS 连接的 certificate authority 文件的地址 --cert 用于 TLS 连接的 client certificate 文件的地址 --key 用于 TLS 连接的 client private key 文件的地址 --csv-delimiter csv 文件中字符类型变量的定界符 '\"' --csv-separator csv 文件中各值的分隔符 ',' --csv-null-value csv 文件空值的表示 \"\\N\" --escape-backslash 使用反斜杠 (\\) 来转义导出文件中的特殊字符 true --output-filename-template 以golang template 格式表示的数据文件名格式 支持 DB Table、Index 三个参数 分别表示数据文件的库名、表名、分块 ID {{.DB}}.{{.Table}}.{{.Index}} --status-addr Dumpling 的服务地址，包含了 Prometheus 拉取 metrics 信息及 pprof 调试的地址 \":8281\" --tidb-mem-quota-query 单条 dumpling 命令导出 SQL 语句的内存限制，单位为 byte，默认为 32 GB 34359738368 3、导出数据文件格式 ①导出到 sql 文件 Dumpling 默认导出数据格式为 sql 文件。也可以通过设置 --filetype sql 导出数据到 sql 文件： dumpling -u root -P 4000 -h 127.0.0.1 --filetype sql --threads 32 -o /data/dumpling-export -F 256 上述命令中，-h、-P、-u 分别是地址，端口，用户。如果需要密码验证，可以用 -p $YOUR_SECRET_PASSWORD 传给 Dumpling。 ②导出到 csv 文件 假如导出数据的格式是 CSV（使用 --filetype csv 即可导出 CSV 文件），还可以使用 --sql 导出指定 SQL 选择出来的记录，例如，导出 test.sbtest1 中所有 id 的记录： dumpling -u root -P 4000 -h 127.0.0.1 -o /data/dumpling-export/test \\ --filetype csv \\ --sql 'select * from `test`.`sbtest1` where id 注意： --sql 选项暂时仅仅可用于导出 csv 的场景。 这里需要在要导出的所有表上执行 select * from where id 语句。如果部分表没有指定的字段，那么导出会失败。 4、筛选导出的数据 ①使用 --where 选项筛选数据 默认情况下，除了系统数据库中的表之外，Dumpling 会导出整个数据库的表。你可以使用 --where 来选定要导出的记录。 dumpling -u root -P 4000 -h 127.0.0.1 -o /data/dumpling-export/test \\ --where \"id 上述命令将会导出各个表的 id ②使用 --filter 选项筛选数据 Dumpling 可以通过 --filter 指定 table-filter 来筛选特定的库表。table-filter 的语法与 .gitignore 相似，详细语法参考表库过滤。 dumpling -u root -P 4000 -h 127.0.0.1 -o /data/dumpling-export/test \\ --filter \"employees.*\" \\ --filter \"*.WorkOrder\" 上述命令将会导出 employees 数据库的所有表，以及所有数据库中的 WorkOrder 表。 ③使用 -B 或 -T 选项筛选数据 Dumpling 也可以通过 -B 或 -T 选项导出特定的数据库/数据表。 注意： --filter 选项与 -T 选项不可同时使用。 -T 选项只能接受完整的 库名.表名 形式，不支持只指定表名。例：Dumpling 无法识别 -T WorkOrder。 例如通过指定： -B employees ：导出 employees 数据库 -T employees.WorkOrder ：导出 employees.WorkOrder 数据表 5、并发提高 Dumpling导出效率选项 默认情况下，导出的文件会存储到 ./export- 目录下。常用选项如下： -o 用于选择存储导出文件的目录。 -F 选项用于指定单个文件的最大大小，默认单位为 MiB。可以接受类似 5GiB 或 8KB 的输入。 -r 选项用于指定单个文件的最大记录数（或者说，数据库中的行数），开启后 Dumpling 会开启表内并发，提高导出大表的速度。 利用以上选项可以让 Dumpling 的并行度更高。 6、调整 Dumpling 的数据一致性选项 注意： 在大多数场景下，用户不需要调整 Dumpling 的默认数据一致性选项。 Dumpling 通过 --consistency 标志控制导出数据“一致性保证”的方式。对于 TiDB 来说，默认情况下，会通过获取某个时间戳的快照来保证一致性（即 --consistency snapshot）。在使用 snapshot 来保证一致性的时候，可以使用 --snapshot 选项指定要备份的时间戳。还可以使用以下的一致性级别： flush：使用 FLUSH TABLES WITH READ LOCK 来保证一致性。 snapshot：获取指定时间戳的一致性快照并导出。 lock：为待导出的所有表上读锁。 none：不做任何一致性保证。 auto：对 MySQL 使用 flush，对 TiDB 使用 snapshot。 7、导出的SQL文件 TiDB Dumping导出的SQL文件命名格式都有： metadata：此文件包含导出的起始时间以及 master binary log 的位置。 Started dump at: 2020-11-10 10:40:19 SHOW MASTER STATUS: Log: tidb-binlog Pos: 420747102018863124 Finished dump at: 2020-11-10 10:40:20 {database}-schema-create.sql：创建database的 SQL 文件on {database}.{table}-schema.sql：创建 table 的 SQL 文件 {database}.{table}.{0001}.{sql|csv}：数据源文件 *-schema-view.sql、*-schema-trigger.sql、*-schema-post.sql：其他导出文件 后续如果想使用TiDB Lighting将这些SQL文件导入到TiDB另外一个的DB中的话，可批量将SQL文件名的database部分改掉 # 例如源库DB为Test，想把数据导入到目标库Test-2中 old_database_name=test new_database_name=Test-2 for i in $(ls *.sql | grep -v schema-create );do mv $i $new_database_name.${i#*.}; done mv ${old_database_name}-schema-create.sql ${new_database_name}-schema-create.sql echo \"\" > ${new_database_name}-schema-create.sql 8、导出 TiDB 的历史数据快照 Dumpling 可以通过 --snapshot 指定导出某个 tidb_snapshot 时的数据。 --snapshot 选项可设为 TSO（SHOW MASTER STATUS 输出的 Position 字段）或有效的 datetime 时间，例如： dumpling --snapshot 417773951312461825 dumpling --snapshot \"2020-07-02 17:12:45\" 即可导出 TSO 为 417773951312461825 或 2020-07-02 17:12:45 时的 TiDB 历史数据快照。 9、导出大规模数据时的 TiDB GC 设置 从 v4.0.0 版本开始，Dumpling 可以自动延长 GC 时间（Dumpling 需要访问 TiDB 集群的 PD 地址），而 v4.0.0 之前的版本，需要手动调整 GC 时间，否则 dumpling 备份时可能出现以下报错： Could not read data from testSchema.testTable: GC life time is shorter than transaction duration, transaction starts at 2019-08-05 21:10:01.451 +0800 CST, GC safe point is 2019-08-05 21:14:53.801 +0800 CST 手动调整 GC 时间的步骤： 执行 dumpling 命令前，查询 TiDB 集群的 GC 值并在 MySQL 客户端执行下列语句将其调整为合适的值： SELECT * FROM mysql.tidb WHERE VARIABLE_NAME = 'tikv_gc_life_time'; +-----------------------+--------------------+ | VARIABLE_NAME | VARIABLE_VALUE | +-----------------------+--------------------+ | tikv_gc_life_time | 10m0s | +-----------------------+--------------------+ update mysql.tidb set VARIABLE_VALUE = '720h' where VARIABLE_NAME = 'tikv_gc_life_time'; 执行 dumpling 命令后，将 TiDB 集群的 GC 值恢复到第 1 步中的初始值： update mysql.tidb set VARIABLE_VALUE = '10m' where VARIABLE_NAME = 'tikv_gc_life_time'; 三、常用操作 1、zsh函数备份脚本 可备份 TiDB 、MySQL 可备份某些整库 可备份某些库的某些表 check_commands_exists() { need_commands=($*) no_commands=\"\" for command in \"${need_commands[@]}\" ; do if ! command -v $command &> /dev/null; then no_commands+=\" $command\" fi done if [[ $no_commands ]]; then echo -e \"\\033[31m$no_commands 命令不存在，正在下载安装！\\033[0m\" os_type=$(uname -s) case \"$os_type\" in Linux*) os_distribution=$(lsb_release -si) if [ \"$os_distribution\" = \"Ubuntu\" ] || [ \"$os_distribution\" = \"Debian\" ]; then apt install -y $no_commands >/dev/null 2>&1 elif [ \"$os_distribution\" = \"CentOS\" ]; then yum install -y $no_commands >/dev/null 2>&1 elif [ \"$os_distribution\" = \"Fedora\" ]; then dnf install -y $no_commands >/dev/null 2>&1 fi if [[ $? == 0 ]] ;then echo -e \"\\033[31m$no_commands 命令已安装！\\033[0m\" fi ;; Darwin*) brew install $no_commands >/dev/null 2>&1 if [[ $? == 0 ]] ;then echo -e \"\\033[31m$no_commands 命令已安装！\\033[0m\" fi ;; CYGWIN*|MINGW32*|MSYS*|MINGW*) echo -e \"\\033[31mWindows操作系统，请手动安装！\\033[0m\" ;; *) echo -e \"\\033[31m未知的操作系统\\033[0m\" return ;; esac fi } dumpling-mysql-tidb-data-to-file() { check_commands_exists pidof mysql_config_editor echo \"数据库连接地址: \" echo -e \" \\e[;33m1. 手动输入数据库连接信息\\033[0m\" echo -e \" \\e[;33m2. 生产环境TiDB - 192.168.1.12:4000\\033[0m\" echo -e \" \\e[;33m3. 测试环境TiDB - 192.168.1.13:4000\\033[0m\" echo -e \" \\e[;33m4. 测试环境MySQL - 192.168.1.14:3306\\033[0m\" echo -e \" \\e[;33m5. 退出\\033[0m\" read \"?请设置数据库连接地址，默认[1]: \" DB_HOST_OPT until [[ -z \"$DB_HOST_OPT\" || \"$DB_HOST_OPT\" =~ ^[12345]$ ]]; do echo -e \" \\033[41;37m输入$DB_HOST_OPT为无效的选项！可选项：1,2,3,4,5\\033[0m\" read \"?请重新设置数据库连接地址，默认[1]: \" DB_HOST_OPT done [[ -z \"$DB_HOST_OPT\" ]] && DB_HOST_OPT=1 case \"$DB_HOST_OPT\" in 1) read \"?请输入数据库连接地址: \" DB_HOST until [[ ! -z \"$DB_HOST\" && $DB_HOST =~ ^192.168.[1-9]{1,3}\\.*$ ]]; do if [ ! $DB_HOST ] ;then echo -e \" \\033[41;37m数据库连接地址为必填项，不能为空！\\033[0m\" else echo -e \" \\033[41;37m$DB_HOST为无效的内网IP地址！\\033[0m\" fi read \"?请重新输入数据库连接地址: \" DB_HOST done read \"?请输入数据库连接端口: \" DB_HOST_PORT until [[ ! -z \"$DB_HOST_PORT\" && $DB_HOST_PORT =~ ^(3306|4000)$ ]]; do if [ ! $DB_HOST_PORT ] ;then echo -e \" \\033[41;37m数据库连接端口为必填项，不能为空！\\033[0m\" else echo -e \" \\033[41;37m$DB_HOST_PORT为无效的数据库端口！\\033[0m\" fi read \"?请重新输入数据库连接端口: \" DB_HOST_PORT done read \"?请输入数据库连接用户名: \" DB_USERNAME until [[ ! -z \"$DB_USERNAME\" ]]; do echo -e \" \\033[41;37m数据库连接用户为必填项，不能为空！\\033[0m\" read \"?请重新输入数据库连接用户名: \" DB_USERNAME done echo -e \"请输入数据库用户密码: \" mysql_config_editor remove --login-path=tmp mysql_config_editor set --login-path=tmp --user=$DB_USERNAME --host=$DB_HOST --port=$DB_HOST_PORT --password DB_ALIAS=tmp ;; 2) DB_ALIAS=prod-tidb output=$(mysql_config_editor print --login-path=$DB_ALIAS) DB_HOST=$(echo \"$output\" | awk -F'\"' '/host/ {print $2}') DB_HOST_PORT=$(echo \"$output\" | awk -F'= ' '/port/ {print $2}') DB_USERNAME=$(echo \"$output\" | awk -F'\"' '/user/ {print $2}') ;; 3) DB_ALIAS=test-tidb output=$(mysql_config_editor print --login-path=$DB_ALIAS) DB_HOST=$(echo \"$output\" | awk -F'\"' '/host/ {print $2}') DB_HOST_PORT=$(echo \"$output\" | awk -F'= ' '/port/ {print $2}') DB_USERNAME=$(echo \"$output\" | awk -F'\"' '/user/ {print $2}') ;; 4) DB_ALIAS=test-mysql output=$(mysql_config_editor print --login-path=$DB_ALIAS) DB_HOST=$(echo \"$output\" | awk -F'\"' '/host/ {print $2}') DB_HOST_PORT=$(echo \"$output\" | awk -F'= ' '/port/ {print $2}') DB_USERNAME=$(echo \"$output\" | awk -F'\"' '/user/ {print $2}') ;; 5) return ;; esac read -s \"?请输入数据库 $DB_HOST:$DB_HOST_PORT 连接用户 $DB_USERNAME 的密码: \" DB_PASSWORD until [[ ! -z \"$DB_PASSWORD\" ]]; do if [ ! $DB_PASSWORD ] ;then echo -e \" \\033[41;37m数据库连接密码为必填项。不能为空！\\033[0m\" else echo -e \" \\033[41;37m$DB_PASSWORD为无效的输入！\\033[0m\" fi read -s \"?请输入数据库 $DB_HOST:$DB_HOST_PORT 连接用户 $DB_USERNAME 的密码: \" DB_PASSWORD done mysql_config_editor remove --login-path=$DB_ALIAS echo echo -e \"请再次输入\" mysql_config_editor set --login-path=$DB_ALIAS --host=$DB_HOST --port=$DB_HOST_PORT --user=$DB_USERNAME --password show_select_database(){ all_databases=$(mysql --login-path=$DB_ALIAS -e 'show databases;' |grep -i -Ev '\"+-----------------------+\"|Database|Warning|METRICS_SCHEMA|INFORMATION_SCHEMA|mysql|PERFORMANCE_SCHEMA' 2> /dev/null) count=1 for db in ${(f)all_databases}; do printf \"\\e[;36m%-2s. %-24s\\033[0m\" \"$count\" \"$db\" if [ $((count % 5)) -eq 0 ]; then echo \"\" fi ((count++)) done selected_databases_all=\"\" selected_databases_set=() selected_databases_summary=\"\" echo while true; do read \"?请输入要备份的数据库序号(以逗号分隔): \" selected_indices IFS=',' read -A -r indices = 1 && index /dev/null) count=1 for table in ${(f)db_tables}; do printf \"\\e[;36m%-3s. %-32s\\033[0m\" \"$count\" \"$table\" if [ $((count % 5)) -eq 0 ]; then echo \"\" fi ((count++)) done echostr+=\"\\n $SELECTED_DB库的: \\n \" echotablesstr=\"\" selected_tables_all=\"\" selected_tables_set=() echo while true; do read \"?请输入 $SELECTED_DB 库中要备份表的序号(多张表以逗号分隔): \" selected_indices IFS=',' read -A -r indices = 1 && index /dev/null 2>&1; then echo -e \" \\033[41;37m数据库连接地址: $DB_HOST的$DB_HOST_PORT端口连接测试失败，请检查后重新设置！\\033[0m\" return fi # 检查数据库用户是否可登录 connect_res=$(timeout 3 mysql --login-path=$DB_ALIAS -e \"select 1;\" 2>&1) if [[ $? != 0 ]] ; then echo -e \" \\033[41;37m数据库用户$DB_USERNAME无法连接$DB_HOST:$DB_HOST_PORT，请检查用户名密码以及用户客户端IP地址的正确性！\\033[0m\" echo -e \" \\033[41;37m报错信息：$connect_res ！\\033[0m\" return fi echo \"备份类型: \" echo -e \" \\e[;33m1. 备份某些库\\033[0m\" echo -e \" \\e[;33m2. 备份某些库中的表\\033[0m\" read \"?请选择要备份的类型，默认[2]: \" BACKUP_TYPE_OPT until [[ -z \"$BACKUP_TYPE_OPT\" || \"$BACKUP_TYPE_OPT\" =~ '^[12]$' ]]; do echo \"$BACKUP_TYPE_OPT: 无效的选项.\" read -p \"请重新选择要备份的类型，默认[2]: \" BACKUP_TYPE_OPT done [[ -z \"$BACKUP_TYPE_OPT\" ]] && BACKUP_TYPE_OPT=2 # 检查数据库用户是否有备份权限 timeout 3 mysql --login-path=$DB_ALIAS -e \"show grants for $DB_USERNAME@'%';\" >/dev/null 2>&1 |grep -E 'RELOAD|ALL PRIVILEGES|REPLICATION CLIENT|SELECT' > /dev/null if [[ $? != 0 ]] ; then timeout 3 mysql --login-path=$DB_ALIAS -e \"show grants for $DB_USERNAME@'192.168.1.3';\" >/dev/null 2>&1 |grep -E 'RELOAD|ALL PRIVILEGES|REPLICATION CLIENT|SELECT' > /dev/null if [[ $? != 0 ]] ; then echo -e \" \\033[41;37m数据库用户$DB_USERNAME在%或192.168.1.3地址上对实例$DB_HOST:$DB_HOST_PORT没有备份权限，请使用高权限用户授予其RELOAD,REPLICATION CLIENT权限！\\033[0m\\n \\033[41;37m授权SQL: GRANT SELECT,RELOAD,REPLICATION CLIENT for '$DB_USERNAME'@'%' ; GRANT SELECT,RELOAD,REPLICATION CLIENT for '$DB_USERNAME'@'192.168.1.3' ;\\033[0m\" return fi fi local BACKUP_DBS=\"\" local BACKUP_DB_TABLES=\"\" case \"$BACKUP_TYPE_OPT\" in 1) show_select_database $DB_HOST $DB_HOST_PORT $DB_USERNAME $DB_PASSWORD tput setaf 3 read \"?是否可以开始备份$DB_HOST:$DB_HOST_PORT上的 $BACKUP_DBS 库[Yy/Nn]? \" READY_RUN until [[ ! -z \"$READY_RUN\" && ${READY_RUN} =~ ^[yYnN]*$ ]]; do read \"?是否可以开始备份$DB_HOST:$DB_HOST_PORT上的 $BACKUP_DBS 库[Yy/Nn]? \" READY_RUN done tput sgr0 if [[ $READY_RUN =~ ^[yY]*$ ]] ;then DATE=$(date +\"%Y%m%d%M\") BACKUPFILE_PATH='/Users/test/Desktop' mkdir LE_PATH/$DATE/{sql,logs} nohup dumpling \\ -c gzip \\ -h ${DB_HOST} \\ -P ${DB_HOST_PORT} \\ -u ${DB_USERNAME} \\ -p ${DB_PASSWORD} \\ --filetype sql \\ --threads 8 \\ -B $BACKUP_DBS \\ -o $BACKUPFILE_PATH/$DATE/sql \\ -F 256MiB \\ --logfile $BACKUPFILE_PATH/$DATE/logs/export-task.log >$BACKUPFILE_PATH/$DATE/logs/dumpling-nohupout.log 2>&1 & sleep 2 gtail -f --pid=$(pidof dumpling) $BACKUPFILE_PATH/$DATE/logs/export-task.log $BACKUPFILE_PATH/$DATE/logs/dumpling-nohupout.log 2>/dev/null if grep -q \"dump data successfully\" $BACKUPFILE_PATH/$DATE/logs/export-task.log ;then echo -e \"\\033[41;37m$DB_HOST上的 $BACKUP_DBS 库已备份在 $BACKUPFILE_PATH/$DATE\\033[0m\" open $BACKUPFILE_PATH/$DATE else echo -e \"\\033[41;37m数据库Database备份失败！错误信息如下：\\033[0m\" grep \"failed\" $BACKUPFILE_PATH/$DATE/logs/dumpling-nohupout.log fi else return fi ;; 2) local echostr=\"\" show_select_database show_select_database_tables $BACKUP_DBS tput setaf 6 echo -e ${echostr#*\\n} tput sgr0 read \"?是否可以开始备份$DB_HOST:$DB_HOST_PORT以上的的库表 [Yy/Nn]? \" READY_RUN until [[ ! -z \"$READY_RUN\" && ${READY_RUN} =~ ^[yYnN]*$ ]]; do read \"?是否可以开始备份$DB_HOST:$DB_HOST_PORT以上的的库表 [Yy/Nn]? \" READY_RUN done if [[ $READY_RUN =~ ^[yY]*$ ]] ;then echo tput setaf 3 DATE=$(date +\"%Y%m%d%M\") BACKUPFILE_PATH='/Users/test/Desktop' mkdir -p $BACKUPFILE_PATH/$DATE/{sql,logs} nohup dumpling \\ -c gzip \\ -h ${DB_HOST} \\ -P ${DB_HOST_PORT} \\ -u ${DB_USERNAME} \\ -p ${DB_PASSWORD} \\ --filetype sql \\ --threads 8 \\ -T $BACKUP_DB_TABLES \\ -o $BACKUPFILE_PATH/$DATE/sql \\ -F 256MiB \\ --logfile $BACKUPFILE_PATH/$DATE/logs/export-task.log >$BACKUPFILE_PATH/$DATE/logs/dumpling-nohupout.log 2>&1 & sleep 2 gtail -f --pid=$(pidof dumpling) $BACKUPFILE_PATH/$DATE/logs/export-task.log $BACKUPFILE_PATH/$DATE/logs/dumpling-nohupout.log 2>/dev/null if grep -q \"dump data successfully\" $BACKUPFILE_PATH/$DATE/logs/export-task.log ;then echo -e \"\\033[41;37m数据库表已备份成功，文件在$BACKUPFILE_PATH/$DATE\\033[0m\" open $BACKUPFILE_PATH/$DATE else echo -e \"\\033[41;37m数据库表备份失败！错误信息如下：\\033[0m\" grep \"failed\" $BACKUPFILE_PATH/$DATE/logs/dumpling-nohupout.log fi tput sgr0 fi ;; esac } 四、实战注意 如果后续使用TiDB lightning恢复备份数据到新集群的话， Dumpling备份时不要进行压缩。TiDB lightning无法从压缩备份文件中直接导入到 TIDB。（TiDB lightning v5.1.0是不行，后续版本不知道会不会支持） for i in `find /data/dumpling-data/2022042507/sql -name \"*sql.gz\"` ;do gunzip $i ;done TiDB lightning 开起监控 Web，可以方便查看导入进度 TiDB lightning 一定要开起断点续传功能，建议使用“file”模式 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:31:26 "},"origin/tidb-lighting-import.html":{"url":"origin/tidb-lighting-import.html","title":"Lightning：导入数据到TIDB","keywords":"","body":"使用tidb-lightning导入数据到TIDB 一、简介 TiDB Lightning 是一个将全量数据高速导入到 TiDB 集群的工具 TiDB Lightning 有以下两个主要的使用场景： 大量新数据的快速导入 全量备份数据的恢复 目前，TiDB Lightning 支持： 导入 Dumpling、CSV 或 Amazon Aurora Parquet 输出格式的数据源。 从本地盘或 Amazon S3 云盘读取数据。 二、工作原理 在导数据之前，tidb-lightning 会自动将 TiKV 集群切换为“导入模式” (import mode)，优化写入效率并停止自动压缩。 tidb-lightning 会在目标数据库建立架构和表，并获取其元数据。 每张表都会被分割为多个连续的区块，这样来自大表 (200 GB+) 的数据就可以用增量方式并行导入。 tidb-lightning 会为每一个区块准备一个“引擎文件 (engine file)”来处理键值对。tidb-lightning 会并发读取 SQL dump，将数据源转换成与 TiDB 相同编码的键值对，然后将这些键值对排序写入本地临时存储文件中。 当一个引擎文件数据写入完毕时，tidb-lightning 便开始对目标 TiKV 集群数据进行分裂和调度，然后导入数据到 TiKV 集群。 引擎文件包含两种：数据引擎与索引引擎，各自又对应两种键值对：行数据和次级索引。通常行数据在数据源里是完全有序的，而次级索引是无序的。因此，数据引擎文件在对应区块写入完成后会被立即上传，而所有的索引引擎文件只有在整张表所有区块编码完成后才会执行导入。 整张表相关联的所有引擎文件完成导入后，tidb-lightning 会对比本地数据源及下游集群的校验和 (checksum)，确保导入的数据无损，然后让 TiDB 分析 (ANALYZE) 这些新增的数据，以优化日后的操作。同时，tidb-lightning 调整 AUTO_INCREMENT 值防止之后新增数据时发生冲突。 表的自增 ID 是通过行数的上界估计值得到的，与表的数据文件总大小成正比。因此，最后的自增 ID 通常比实际行数大得多。这属于正常现象，因为在 TiDB 中自增 ID 不一定是连续分配的。 在所有步骤完毕后，tidb-lightning 自动将 TiKV 切换回“普通模式” (normal mode)，此后 TiDB 集群可以正常对外提供服务。 如果需要导入的目标集群是 v3.x 或以下的版本，需要使用 Importer-backend 来完成数据的导入。在这个模式下，tidb-lightning 需要将解析的键值对通过 gRPC 发送给 tikv-importer 并由 tikv-importer 完成数据的导入； TiDB Lightning 还支持使用 TiDB-backend 作为后端导入数据。TiDB-backend 使用和 Loader 类似，tidb-lightning 将数据转换为 INSERT 语句，然后直接在目标集群上执行这些语句。 使用Dumpling或Mydumper导出的数据为SQL文件。SQL文件分为三类： DB名-schema-cre1ate.sql：DB的schema创建语句文件 DB名.表名-schema.sql：表的schema创建语句文件 DB名.表名.sql：表的数据插入语句文件 三、注意 1、基础注意事项 TiDB Lightning 运行后，TiDB 集群将无法正常对外提供服务。 若 tidb-lightning 崩溃，集群会留在“导入模式”。若忘记转回“普通模式”，集群会产生大量未压缩的文件，继而消耗 CPU 并导致延迟。此时，需要使用 tidb-lightning-ctl 手动将集群转回“普通模式”： bin/tidb-lightning-ctl --switch-mode=normal 2、Lightning 需要下游 TiDB用户有如下权限： 权限 作用域 SELECT Tables INSERT Tables UPDATE Tables DELETE Tables CREATE Databases, tables DROP Databases, tables ALTER Tables 如果配置项 checksum = true，则 TiDB Lightning 需要有下游 TiDB admin 用户权限。 3、导入后端模式 TiDB Lightning 的后端 (用于接受 TiDB Lightning 解析结果) 决定 tidb-lightning 组件将如何把将数据导入到目标集群中。目前，TiDB Lightning 支持以下后端： Importer-backend（默认）：tidb-lightning 先将 SQL 或 CSV 数据编码成键值对，由 tikv-importer 对写入的键值对进行排序，然后把这些键值对 Ingest 到 TiKV 节点中。 Local-backend：tidb-lightning 先将数据编码成键值对并排序存储在本地临时目录，然后将这些键值对以 SST 文件的形式上传到各个 TiKV 节点，然后由 TiKV 将这些 SST 文件 Ingest 到集群中。和 Importer-backend 原理相同，不过不依赖额外的 tikv-importer 组件。 TiDB-backend：tidb-lightning 先将数据编码成 INSERT 语句，然后直接在 TiDB 节点上运行这些 SQL 语句进行数据导入。 后端 Local-backend Importer-backend TiDB-backend 速度 快 (~500 GB/小时) 快 (~400 GB/小时) 慢 (~50 GB/小时) 资源使用率 高 高 低 占用网络带宽 高 中 低 导入时是否满足 ACID 否 否 是 目标表 必须为空 必须为空 可以不为空 额外组件 无 tikv-importer 无 支持 TiDB 集群版本 >= v4.0.0 全部 全部 如果导入的目标集群为 v4.0 或以上版本，请优先考虑使用 Local-backend 模式。Local-backend 部署更简单并且性能也较其他两个模式更高 如果目标集群为 v3.x 或以下，则建议使用 Importer-backend 模式 如果需要导入的集群为生产环境线上集群，或需要导入的表中已包含有数据，则最好使用 TiDB-backend 模式 四、安装、命令参数及配置文件详解 1、下载安装 二进制 version=v4.0.5 && \\ curl -# https://download.pingcap.org/tidb-toolkit-$version-linux-amd64.tar.gz | tar -zxC /opt && \\ ln -s /opt/tidb-toolkit-$version-linux-amd64 /opt/tidb-toolkit-$version && \\ echo -e \"export TIDB_TOOLKIT_HOME=/opt/tidb-toolkit-$version\\nexport PATH=\\$PATH:\\$TIDB_TOOLKIT_HOME/bin\" >> /etc/profile && \\ source /etc/profile && \\ tidb-lightning -V Docker docker pull pingcap/tidb-lightning:v5.0.1 docker run -it -v 本地导出SQL文件路径:/data pingcap/tidb-lightning:v5.0.1 sh # tidb-lightning命令执行路径在根目录下,具体的导入任务可以在容器中执行 2、tidb-lightning参数 命令行参数生效优先级高于配置文件中的 参数 描述 对应配置项 --config file 从 file 读取全局设置。如果没有指定则使用默认设置。 -V 输出程序的版本 -d directory 读取数据的本地目录或外部存储 URL mydumper.data-source-dir -L level 日志的等级： debug、info、warn、error 或 fatal (默认为 info) lightning.log-level -f rule 表库过滤的规则 (可多次指定) mydumper.filter --backend backend 选择后端的模式：importer、local 或 tidb tikv-importer.backend --log-file file 日志文件路径（默认是 /tmp 中的临时文件） lightning.log-file --status-addr ip:port TiDB Lightning 服务器的监听地址 lightning.status-port --importer host:port TiKV Importer 的地址 tikv-importer.addr --pd-urls host:port PD endpoint 的地址 tidb.pd-addr --tidb-host host TiDB Server 的 host tidb.host --tidb-port port TiDB Server 的端口（默认为 4000） tidb.port --tidb-status port TiDB Server 的状态端口的（默认为 10080） tidb.status-port --tidb-user user 连接到 TiDB 的用户名 tidb.user --tidb-password password 连接到 TiDB 的密码 tidb.password --no-schema 忽略表结构文件，直接从 TiDB 中获取表结构信息 mydumper.no-schema --enable-checkpoint bool 是否启用断点 (默认值为 true) checkpoint.enable --analyze bool 导入后分析表信息 (默认值为 true) post-restore.analyze --checksum bool 导入后比较校验和 (默认值为 true) post-restore.checksum --check-requirements bool 开始之前检查集群版本兼容性（默认值为 true） lightning.check-requirements --ca file TLS 连接的 CA 证书路径 security.ca-path --cert file TLS 连接的证书路径 security.cert-path --key file TLS 连接的私钥路径 security.key-path --server-mode 在服务器模式下启动 TiDB Lightning lightning.server-mode 3、tidb-lightning-ctl参数 tablename 必须是db`.`tbl 中的限定表名（包括反引号）或关键词 all 所有 tidb-lightning 的参数也适用于 tidb-lightning-ctl 参数 描述 --compact 执行 full compact --switch-mode mode 将每个 TiKV Store 切换到指定模式（normal 或 import） --fetch-mode 打印每个 TiKV Store 的当前模式 --import-engine uuid 将 TiKV Importer 上关闭的引擎文件导入到 TiKV 集群 --cleanup-engine uuid 删除 TiKV Importer 上的引擎文件 --checkpoint-dump folder 将当前的断点以 CSV 格式存储到文件夹中 --checkpoint-error-destroy tablename 删除断点，如果报错则删除该表 --checkpoint-error-ignore tablename 忽略指定表中断点的报错 --checkpoint-remove tablename 无条件删除表的断点 4、tikv-importer参数 参数 描述 对应配置项 -C, --config file 从 file 读取配置。如果没有指定，则使用默认设置 -V, --version 输出程序的版本 -A, --addr ip:port TiKV Importer 服务器的监听地址 server.addr --status-server ip:port 状态服务器的监听地址 status-server-address --import-dir dir 引擎文件的存储目录 import.import-dir --log-level level 日志的等级： trace、debug、info、warn、error 或 off log-level --log-file file 日志文件路径 log-file 5、任务配置文件参数详解 [lightning] # 启动之前检查集群是否满足最低需求。 # check-requirements = true # 引擎文件的最大并行数。每张表被切分成一个用于存储索引的“索引引擎”和若干存储行数据的“数据引擎”。这两项设置控制两种引擎文件的最大并发数，会影响 tikv-importer 的内存和磁盘用量，两项数值之和不能超过 tikv-importer 的 max-open-engines 的设定。 index-concurrency = 2 table-concurrency = 6 # 数据的并发数。默认与逻辑 CPU 的数量相同。混合部署的情况下可以将其大小配置为逻辑 CPU 数的 75%，以限制 CPU 的使用。 # region-concurrency = # I/O 最大并发数。I/O 并发量太高时，会因硬盘内部缓存频繁被刷新。而增加 I/O 等待时间，导致缓存未命中和读取速度降低。对于不同的存储介质，此参数可能需要调整以达到最佳效率。 io-concurrency = 5 [security] # 指定集群中用于 TLS 连接的证书和密钥。CA 的公钥证书。如果留空，则禁用 TLS。 # ca-path = \"/path/to/ca.pem\" # 此服务的公钥证书。 # cert-path = \"/path/to/lightning.pem\" # 该服务的密钥。 # key-path = \"/path/to/lightning.key\" [checkpoint] # 是否启用断点续传。导入数据时，TiDB Lightning 会记录当前表导入的进度。所以即使 Lightning 或其他组件异常退出，在重启时也可以避免重复再导入已完成的数据。 enable = true # 存储断点的数据库名称。 schema = \"tidb_lightning_checkpoint\" # 存储断点的方式。 # - file：存放在本地文件系统。 # - mysql：存放在兼容 MySQL 的数据库服务器。 driver = \"file\" # dsn 是数据源名称 (data source name)，表示断点的存放位置。 # 若 driver = \"file\"，则 dsn 为断点信息存放的文件路径。若不设置该路径，则默认存储路径为“/tmp/CHECKPOINT_SCHEMA.pb”。 # 若 driver = \"mysql\"，则 dsn 为“用户:密码@tcp(地址:端口)/”格式的 URL。若不设置该 URL，则默认会使用 [tidb] 部分指定的 TiDB 服务器来存储断点。 # 为减少目标 TiDB 集群的压力，建议指定另一台兼容 MySQL 的数据库服务器来存储断点。 # dsn = \"/tmp/tidb_lightning_checkpoint.pb\" # 所有数据导入成功后是否保留断点。设置为 false 时为删除断点。保留断点有利于进行调试，但会泄漏关于数据源的元数据。 # keep-after-success = false [tikv-importer] # 选择后端：“importer” 或 “local” 或 “tidb” # backend = \"importer\" # 当后端是 “importer” 时，tikv-importer 的监听地址（需改为实际地址）。 addr = \"172.16.31.10:8287\" # 当后端是 “tidb” 时，插入重复数据时执行的操作。 # - replace：新数据替代已有数据 # - ignore：保留已有数据，忽略新数据 # - error：中止导入并报错 # on-duplicate = \"replace\" # 当后端是 “local” 时，控制生成 SST 文件的大小，最好跟 TiKV 里面的 Region 大小保持一致，默认是 96 MB。 # region-split-size = 100_663_296 # 当后端是 “local” 时，一次请求中发送的 KV 数量。 # send-kv-pairs = 32768 # 当后端是 “local” 时，本地进行 KV 排序的路径。如果磁盘性能较低（如使用机械盘），建议设置成与 `data-source-dir` 不同的磁盘，这样可有效提升导入性能。 # sorted-kv-dir = \"\" # 当后端是 “local” 时，TiKV 写入 KV 数据的并发度。当 TiDB Lightning 和 TiKV 直接网络传输速度超过万兆的时候，可以适当增加这个值。 # range-concurrency = 16 [mydumper] # 设置文件读取的区块大小，确保该值比数据源的最长字符串长。 read-block-size = 65536 # Byte (默认为 64 KB) # （源数据文件）单个导入区块大小的最小值。 # Lightning 根据该值将一张大表分割为多个数据引擎文件。 # batch-size = 107_374_182_400 # Byte (默认为 100 GB) # 引擎文件需按顺序导入。由于并行处理，多个数据引擎几乎在同时被导入， # 这样形成的处理队列会造成资源浪费。因此，为了合理分配资源，Lightning # 稍微增大了前几个区块的大小。该参数也决定了比例系数，即在完全并发下 # “导入”和“写入”过程的持续时间比。这个值可以通过计算 1 GB 大小的 # 单张表的（导入时长/写入时长）得到。在日志文件中可以看到精确的时间。 # 如果“导入”更快，区块大小的差异就会更小；比值为 0 时则说明区块大小一致。 # 取值范围为（0 Compact -> Analyze。 [post-restore] # 如果设置为 true，会对所有表逐个执行 `ADMIN CHECKSUM TABLE ` 操作 # 来验证数据的完整性。 checksum = true # 如果设置为 true，会在导入每张表后执行一次 level-1 Compact。 # 默认值为 false。 level-1-compact = false # 如果设置为 true，会在导入过程结束时对整个 TiKV 集群执行一次 full Compact。 # 默认值为 false。 compact = false # 如果设置为 true，会对所有表逐个执行 `ANALYZE TABLE ` 操作。 analyze = true # 设置周期性后台操作。 # 支持的单位：h（时）、m（分）、s（秒）。 [cron] # Lightning 自动刷新导入模式状态的持续时间，该值应小于 TiKV 对应的设定值。 switch-mode = \"5m\" # 在日志中打印导入进度的持续时间。 log-progress = \"5m\" 五、开起Web界面查看导入进度 TiDB Lightning 支持在网页上查看导入进度或执行一些简单任务管理。启用服务器模式的方式有如下几种： 在启动 tidb-lightning 时加上命令行参数 --server-mode。 nohup tidb-lightning --server-mode --status-addr :8289 >> tidb-lightning-server.log 2>&1 & 在配置文件中设置 lightning.server-mode。 [lightning] pprof-port = 8289 TiDB Lightning 启动后，可以访问 http://127.0.0.1:8289 来管理程序 服务器模式下，TiDB Lightning 不会立即开始运行，而是通过用户在 web 页面提交（多个） TOML 格式的任务文件来导入数据。 六、断点续传 大量的数据导入一般耗时数小时至数天，长时间运行的进程会有一定机率发生非正常中断。如果每次重启都从头开始，就会浪费掉之前已成功导入的数据。为此，TiDB Lightning 提供了“断点续传”的功能，即使 tidb-lightning 崩溃，在重启时仍然接着之前的进度继续工作。 文档：https://docs.pingcap.com/zh/tidb/stable/tidb-lightning-checkpoints 1、断点续传的启用与配置 [checkpoint] # 启用断点续传。 # 导入时，TiDB Lightning 会记录当前进度。 # 若 TiDB Lightning 或其他组件异常退出，在重启时可以避免重复再导入已完成的数据。 enable = true # 存储断点的方式 # - file：存放在本地文件系统（要求 v2.1.1 或以上） # - mysql：存放在任何兼容 MySQL 5.7 或以上的数据库中，包括 MariaDB 和 TiDB driver = \"file\" # 存储断点的架构名称（数据库名称）仅在 driver = \"mysql\" 时生效 # schema = \"tidb_lightning_checkpoint\" # 断点的存放位置 # 若 driver = \"file\"，此参数为断点信息存放的文件路径。如果不设置该参数则默认为 `/tmp/CHECKPOINT_SCHEMA.pb` # 若 driver = \"mysql\"，此参数为数据库连接参数 (DSN)，格式为“用户:密码@tcp(地址:端口)/”。 # 默认会重用 [tidb] 设置目标数据库来存储断点。为避免加重目标集群的压力，建议另外使用一个兼容 MySQL 的数据库服务器。 # dsn = \"/tmp/tidb_lightning_checkpoint.pb\" # 导入成功后是否保留断点。默认为删除。保留断点可用于调试，但有可能泄漏数据源的元数据。 # keep-after-success = false 2、断点续传的控制 若 tidb-lightning 因不可恢复的错误而退出（例如数据出错），重启时不会使用断点，而是直接报错离开。为保证已导入的数据安全，这些错误必须先解决掉才能继续。使用 tidb-lightning-ctl 工具可以标示已经恢复。 ①从头开始整个导入过程 tidb-lightning-ctl --checkpoint-error-destroy=['`schema`.`table`' | all ] 该命令会让失败的表从头开始整个导入过程。选项中的架构和表名必须以反引号 (```) 包裹，而且区分大小写。 如果导入 schema`.`table 这个表曾经出错，这条命令会： 从目标数据库移除 (DROP) 这个表，清除已导入的数据。 将断点重设到“未开始”的状态。 如果 schema`.`table 没有出错，则无操作。 传入 all 会对所有表进行上述操作，这是最方便、安全但保守的断点错误解决方法。 ②忽略出错状态接着带导入 tidb-lightning-ctl --checkpoint-error-ignore='`schema`.`table`' && tidb-lightning-ctl --checkpoint-error-ignore=all 如果导入 schema`.`table 这个表曾经出错，这条命令会清除出错状态，如同没事发生过一样。传入 \"all\" 会对所有表进行上述操作。 注意： 除非确定错误可以忽略，否则不要使用这个选项。如果错误是真实的话，可能会导致数据不完全。启用校验和 (CHECKSUM) 可以防止数据出错被忽略。 ③无论是否有出错，把断点清除 tidb-lightning-ctl --checkpoint-remove='`schema`.`table`' && tidb-lightning-ctl --checkpoint-remove=all ④将所有断点备份到传入的文件夹 该参数主要用于技术支持。此选项仅于 driver = \"mysql\" 时有效。 tidb-lightning-ctl --checkpoint-dump=output/directory 七、实例 1、编写配置文件 Test.toml [mydumper] # 数据源目录 data-source-dir = \"/data/tidb-dumpling-export\" [lightning] max-error = 100 pprof-port = 8289 [checkpoint] enable = true driver = \"mysql\" keep-after-success = true [tikv-importer] backend = \"tidb\" [tidb] host = \"192.168.1.2\" port = 4000 user = \"root\" password = \"*****\" 2、启动任务 在lightning的Web界面中提交任务配置文件 在命令行中运行任务配置文件 nohup tidb-lightning \\ -L info \\ --log-file /root/tidb-lightning-import-task.log \\ -config /root/Test.toml \\ > /root/tidb-lightning-import-task-nohup.log 2>&1 & 导入完毕后，TiDB Lightning 会自动退出。若导入成功，日志的最后一行会显示 tidb lightning exit Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-04-26 17:00:50 "},"origin/tidb-br-backup-restore.html":{"url":"origin/tidb-br-backup-restore.html","title":"BR：分布式冷备份与恢复","keywords":"","body":"TiDB 分布式备份恢复命令行工具BR 一、BR 1、简介 BR 全称为 Backup & Restore，是 TiDB 分布式备份恢复的命令行工具，主要用于对 TiDB 集群进行数据备份和恢复。 相比 dumpling，BR 更适合大数据量的场景。 2、BR工作原理 BR 将备份或恢复操作命令下发到各个 TiKV 节点。TiKV 收到命令后执行相应的备份或恢复操作。 在一次备份或恢复中，各个 TiKV 节点都会有一个对应的备份路径，TiKV 备份时产生的备份文件将会保存在该路径下，恢复时也会从该路径读取相应的备份文件。 在使用 local storage 的时候，备份数据会分散在各个节点的本地文件系统中 不建议在生产环境中备份到本地磁盘，因为在日后恢复的时候，必须手动聚集这些数据才能完成恢复工作 存储备份数据的文件系统，如果没有S3或其他云服务商提供的云存储，尽量选用在节点本地搭建的，高性能SSD磁盘做的NFS 3、备份文件类型 备份路径下会生成以下两种类型文件： SST 文件：存储 TiKV 备份下来的数据信息 backupmeta文件：存储本次备份的元信息，包括备份文件数、备份文件的 Key 区间、备份文件大小和备份文件 Hash (sha256) 值 backup.lock文件：用于防止多次备份到同一目录 4、SST 文件 SST 文件以 {storeID}-{regionID}-{regionEpoch}-{keyHash}-{timestamp}-{cf}.sst 的格式命名。格式名的解释如下： storeID：TiKV 节点编号 regionID：Region 编号 regionEpoch：Region 版本号 keyHash：Range startKey 的 Hash (sha256) 值，确保唯一性 timestamp：时间戳 cf：RocksDB 的 ColumnFamily（默认为 default 或 write） SST 文件、 backupmeta 文件和 backup.lock 文件在同一目录下。文件布局如下： . └── 20220621 ├── backupmeta |—— backup.lock ├── {storeID}-{regionID}-{regionEpoch}-{keyHash}-{timestamp}-{cf}.sst ├── {storeID}-{regionID}-{regionEpoch}-{keyHash}-{timestamp}-{cf}.sst └── {storeID}-{regionID}-{regionEpoch}-{keyHash}-{timestamp}-{cf}.sst ... 将数据备份到 Amazon S3 或网络盘上时，SST 文件会根据 storeID 划分子目录。布局如下： . └── 20220621 ├── backupmeta |—— backup.lock ├── store1 │ └── {regionID}-{regionEpoch}-{keyHash}-{timestamp}-{cf}.sst ├── store2 └── {regionID}-{regionEpoch}-{keyHash}-{timestamp}-{cf}.sst ... 5、BR支持以下外部存储服务 服务 Scheme 示例 本地文件系统（分布在各节点上） local local:///path/to/dest/ Amazon S3 及其他兼容 S3 的服务 s3 s3://bucket-name/prefix/of/dest/ GCS gcs, gs gcs://bucket-name/prefix/of/dest/ Azure Blob Storage azure, azblob azure://container-name/prefix/of/dest/ 不写入任何存储（仅作为基准测试） noop noop:// 6、BR推荐配置 推荐 BR 部署在 PD 节点上。 推荐使用一块高性能 SSD 网盘，挂载到 BR 节点和所有 TiKV 节点上，网盘推荐万兆网卡，否则带宽有可能成为备份恢复时的性能瓶颈。 7、备份性能和影响 TiDB 备份功能对集群性能（事务延迟和 QPS）有一定的影响，但是可以通过调整备份的线程数 backup.num-threads ，以及增加集群配置，来降低备份对集群性能的影响。 为了更加具体说明备份对集群的影响，这里列举了多次快照备份测试结论来说明影响的范围： （使用 5.3 及之前版本）BR 在单 TiKV 存储节点上备份线程数量是节点 CPU 总数量的 75% 的时候，QPS 会下降到备份之前的 30% 左右。 （使用 5.4 及以后版本）当 BR 在单 TiKV 存储节点上备份的线程数量不大于 8、集群总 CPU 利用率不超过 80% 时，BR 备份任务对集群（无论读写负载）影响最大在 20% 左右。 （使用 5.4 及以后版本）当 BR 在单 TiKV 存储节点上备份的线程数量不大于 8、集群总 CPU 利用率不超过 75% 时，BR 备份任务对集群（无论读写负载）影响最大在 10% 左右。 （使用 5.4 及以后版本）当 BR 在单 TiKV 存储节点上备份的线程数量不大于 8、集群总 CPU 利用率不超过 60% 时，BR 备份任务对集群（无论读写负载）几乎没有影响。 通过限制备份的线程数量可以降低备份对集群性能的影响，但是这会影响到备份的性能，以上的多次备份测试结果显示:（单 TiKV 存储节点上）备份速度和备份线程数量呈正比，在线程数量量较少的时候，速度大概是 20M/线程数。例如，单节点 5 个备份线程可达到 100M/s。 二、安装及命令参数 1、安装br br_version=v5.1.0 && \\ curl -s -# https://download.pingcap.org/tidb-toolkit-$br_version-linux-amd64.tar.gz | tar zxvf - -C /opt && \\ ln -s /opt/tidb-toolkit-$br_version-linux-amd64/ /opt/tidb-toolkit && \\ echo -e \"export TIDB_TOOLKIT=/opt/tidb-toolkit\\nexport PATH=\\$PATH:\\$TIDB_TOOLKIT/bin\" >> /etc/profile && \\ source /etc/profile br --help 2、br命令简介 BR 由多层命令组成。目前，BR 包含 backup、restore 和 version 三个子命令: br backup 用于备份 TiDB 集群 br restore 用于恢复 TiDB 集群 以上三个子命令可能还包含这些子命令： full：可用于备份或恢复全部数据。 db：可用于备份或恢复集群中的指定数据库。 table：可用于备份或恢复集群指定数据库中的单张表。 3、br全局参数 --ca string 指定 PEM 格式的受信任 CA 的证书文件路径。 --cert string 指定 PEM 格式的 SSL 证书文件路径。 --check-requirements Whether start version check before execute command (default true) --checksum Run checksum at end of task (default true) --gcs.credentials-file string (experimental) Set the GCS credentials file path --gcs.endpoint string (experimental) Set the GCS endpoint URL --gcs.predefined-acl string (experimental) Specify the GCS predefined acl for objects --gcs.storage-class string (experimental) Specify the GCS storage class for objects --key string 指定 PEM 格式的 SSL 证书密钥文件路径。 --log-file string Set the log file path. If not set, logs will output to temp file (default \"/tmp/br.log.2021-04-20T17.02.38+0800\") --log-format string 设置日志输出格式,默认\"text\" -L, --log-level string 设置日志输出级别，默认info -u, --pd strings PD 服务地址，默认：[127.0.0.1:2379] --ratelimit uint 每个 TiKV 执行备份任务的速度上限（单位 MiB/s） --remove-tiflash Remove TiFlash replicas before backup or restore, for unsupported versions of TiFlash (default true) --s3.acl string (experimental) Set the S3 canned ACLs, e.g. authenticated-read --s3.endpoint string (experimental) Set the S3 endpoint URL, please specify the http or https scheme explicitly --s3.provider string (experimental) Set the S3 provider, e.g. aws, alibaba, ceph --s3.region string (experimental) Set the S3 region, e.g. us-east-1 --s3.sse string Set S3 server-side encryption, e.g. aws:kms --s3.sse-kms-key-id string KMS CMK key id to use with S3 server-side encryption.Leave empty to use S3 owned key. --s3.storage-class string (experimental) Set the S3 storage class, e.g. STANDARD -c, --send-credentials-to-tikv Whether send credentials to tikv (default true) --status-addr string BR 向 Prometheus 提供统计数据的监听地址，格式“0.0.0.0:1188”。不指定则为关闭。 -s, --storage string 备份保存的路径, 例如：\"s3://bucket/path/prefix\" --switch-mode-interval duration maintain import mode on TiKV during restore (default 5m0s) 3、br备份命令 br backup [子命令] 子命令: db 备份单个DB full 备份所有DB raw (experimental) backup a raw kv range from TiKV cluster table 备份单个表 参数： --backupts string 快照对应的物理时间点，例如'400036290571534337', '2018-05-11 01:42:23' --compression string sst文件压缩算法，可选: lz4|zstd(默认)|snappy --compression-level int32 sst文件压缩级别 --gcttl int the TTL (in seconds) that PD holds for BR's GC safepoint (默认 300s) -h, --help help for backup --lastbackupts uint (实验功能)the last time backup ts, use for incremental backup, support TSO only --timeago duration The history version of the backup task, e.g. 1m, 1h. Do not exceed GCSafePoint 4、br恢复命令 br restore [子命令] 子命令: cdclog (实验功能) restore data from cdc log backup db 从备份数据中恢复一个database full 恢复所有的表 raw (实验功能) restore a raw kv range to TiKV cluster table 从备份数据中恢复单个表 三、备份 0、备份前准备 TIKV节点和BR命令节点都挂载NFS文件系统 mkdir /mnt/tidb-br-backup mount -t nfs -o vers=3,nolock,proto=tcp,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport 192.168.1.7:/ /mnt/tidb-br-backup df -mh BR命令尽量在PD节点执行 1、备份全部集群数据 br backup full \\ --pd \"{PD1节点IP地址}:2379,{PD2节点IP地址}:2379\" \\ --storage \"local:///mnt/tidb-br-backup\" \\ --ratelimit 120 \\ --log-file backupfull.log 2、备份单个库 br backup db \\ --pd \"${PDIP}:2379\" \\ --db test \\ --storage \"local:///mnt/tidb-br-backup\" \\ --ratelimit 120 \\ --log-file backuptable.log Detail BR log in /data/tidb-br-database-export/backupdb.log Database backup 100.00% Checksum 100.00% [2021/04/20 21:15:10.704 +08:00] [INFO] [collector.go:60] [\"Database backup Success summary: total backup ranges: 1317, total success: 1317, total failed: 0, total take(Database backup time): 7m58.135071495s, total take(real time): 11m30.059039886s, total kv: 2693175024, total size(MB): 397915.67, avg speed(MB/s): 832.22\"] [\"backup checksum\"=3m28.860107739s] [\"backup fast checksum\"=593.123731ms] [\"backup total regions\"=7070] [BackupTS=424391166169710594] [Size=42165594435] 3、备份单个表到NFS中 br backup table \\ --pd \"${PDIP}:2379\" \\ --db test \\ --table usertable \\ --storage \"local:///mnt/tidb-br-backup\" \\ --ratelimit 120 \\ --log-file backuptable.log 4、过滤表库备份 br backup full \\ --pd \"${PDIP}:2379\" \\ --filter 'db*.tbl*' \\ --storage \"local:///mnt/tidb-br-backup\" \\ --ratelimit 120 \\ --log-file backupfull.log 5、增量备份 如果想要备份增量，只需要在备份的时候指定上一次的备份时间戳 --lastbackupts 即可。 注意增量备份有以下限制： 增量备份需要与前一次全量备份在不同的路径下 GC safepoint 必须在 lastbackupts 之前 br backup full\\ --pd ${PDIP}:2379 \\ -s local:///mnt/tidb-br-backup \\ --lastbackupts ${LAST_BACKUP_TS} 以上命令会备份 (LAST_BACKUP_TS, current PD timestamp] 之间的增量数据。 你可以使用 validate 指令获取上一次备份的时间戳，示例如下： LAST_BACKUP_TS=`br validate decode --field=\"end-version\" -s local:///mnt/tidb-br-backup | tail -n1` 6、备份全部集群数据到S3 存储 export AWS_ACCESS_KEY_ID=${AccessKey} export AWS_SECRET_ACCESS_KEY=${SecretKey} br backup full \\ --pd \"${PDIP}:2379\" \\ --storage \"s3://${Bucket}/${Folder}\" \\ --s3.region \"${region}\" \\ --send-credentials-to-tikv=true \\ --log-file backuptable.log 7、检查备份数据的完整性 要检查数据完整性，可以执行 tiup br debug checksum 命令对备份数据计算校验和。 br debug/validate checksum \\ --storage 'local:///mnt/tidb-br-backup' \\ --log-file checksum.log 四、恢复 1、恢复集群快照备份数据到新集群 nohup br restore full \\ --pd \"${PDIP}:2379\" \\ --storage \"local:///mnt/tidb-br-backup\" \\ --grpc-keepalive-timeout 60s \\ --checksum=false \\ --ratelimit 200 \\ --log-file restorefull.log 1>&2 >br_restore.log &! # --grpc-keepalive-timeout 60s 设置RPC连接超时时间 # --checksum=false 设置数据恢复后不进行校验 2、从集群快照备份数据中恢复单个DB br restore db \\ --pd \"${PDIP}:2379\" \\ --storage \"local:///mnt/tidb-br-backup\" \\ --db \"test\" \\ --ratelimit 128 \\ --log-file restore-db-from-full.log 3、集群快照数据恢复命令优化 当恢复集群快照备份数据到集群时，如果遇到大表导入时会经常报rpc error: code = Unavailable desc = transport is closing。原因是pd由于grpc问题leader飘走导致br进行不下去。 br restore full \\ --pd \"${PDIP}:2379\" \\ --storage \"local:///mnt/tidb-br-backup\" \\ --grpc-keepalive-timeout 60s \\ --checksum=false \\ --ratelimit 200 \\ --log-file restorefull.log # --grpc-keepalive-timeout 60s 设置RPC连接超时时间 # --checksum=false 设置数据恢复后不进行校验 参考：https://asktug.com/t/topic/153139/17 五、问题总结 BR 会备份系统表吗？在数据恢复的时候，这些系统表会冲突吗？ 在 v5.1.0 之前，BR 备份时会过滤掉系统库 mysql.* 的表数据。自 v5.1.0 起，BR 默认备份集群内的全部数据，包括系统库 mysql.* 中的数据。但由于恢复 mysql.* 中系统表数据的技术实现尚不完善，因此 BR 默认不恢复系统库 mysql 中的表数据。 备份数据有多大，备份会有副本吗？ 备份的时候仅仅在每个 Region 的 Leader 处生成该 Region 的备份文件。因此备份的大小等于数据大小，不会有多余的副本数据。所以最终的总大小大约是 TiKV 数据总量除以副本数。但是假如想要从本地恢复数据，因为每个 TiKV 都必须要能访问到所有备份文件，在最终恢复的时候会有等同于恢复时 TiKV 节点数量的副本。 BR backupTS 如何转化成 Unix 时间？ BR backupTS 默认是在备份开始前，从 PD 获取到的最新时间戳。可以使用 pd-ctl tso timestamp 来解析该时间戳，以获得精确值，也可以通过 backupTS >> 18 来快速获取估计值。 BR restore恢复时出现tidb br restore rpc error: code = Unavailable desc = transport is closing 既然是pd由于grpc问题leader飘走导致br进行不下去，那就不要使用多台pd，只让一台pd运行即可，避免它因为网络或者cpu相关资源leader飘走，然后为了减少pd资源，先把max-replicas改为1，只1个副本。 优化tikv的配置，主要是server. grpc-concurrency相关的，然后把性能测试中要求的写和读buffer也配置好内存大小。 backup. num-threads: 2，反正就减少对cpu的占用。留些资源给br。 执行br restore时减少concurrency，然后不进行checksum 参考：https://asktug.com/t/topic/153139/16 六、其他操作 1、将backupmeta文件解码为json格式 在备份完成后，可通过 tiup br debug decode 命令将备份的 backupmeta 文件解码为 json 格式的可读文件，从而查看快照对应的 TSO 等元信息。 解码 br debug decode \\ --storage 'local:///mnt/tidb-br-backup' \\ --log-file decode-backupmeta.log 解码后的文件存储路径为 /mnt/tidb-br-backup/backupmeta.json。然后打开 backupmeta.json 文件，搜索 end_version 可以查看到快照对应的 TSO。 编码 br debug encode \\ --storage 'local:///mnt/tidb-br-backup' \\ --log-file encode-backupmeta.log 将local:///mnt/tidb-br-backup目录下的 backupmeta.json 文件编码为 backupmeta 文件，编码后的文件名为 backupmeta_from_json，存储路径为 local:///mnt/tidb-br-backup/backupmeta_from_json。 参考 https://docs.pingcap.com/zh/tidb/stable/backup-and-restore-tool https://docs.pingcap.com/zh/tidb/stable/backup-and-restore-faq https://github.com/pingcap/tidb/blob/master/br/docs/cn/2019-08-05-new-design-of-backup-restore.md https://ap.tidb.cc/post/20200618-25-br-rc/ https://asktug.com/t/topic/153139/17 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-27 11:10:28 "},"origin/tidb-sync-diff-inspector.html":{"url":"origin/tidb-sync-diff-inspector.html","title":"Sync-diff-inspector：数据对比","keywords":"","body":"TiDB数据校验工具：sync-diff-inspector 一、简介 1、简介 sync-diff-inspector 是一个用于校验 MySQL／TiDB 中两份数据是否一致的工具。该工具提供了修复数据的功能（适用于修复少量不一致的数据）。 2、主要功能 对比表结构和数据 如果数据不一致，则生成用于修复数据的 SQL 语句 支持不同库名或表名的数据校验 支持分库分表场景下的数据校验 支持 TiDB 主从集群的数据校验 支持从 TiDB DM 拉取配置的数据校验 3、使用限制 对于 MySQL 和 TiDB 之间的数据同步不支持在线校验，需要保证上下游校验的表中没有数据写入，或者保证某个范围内的数据不再变更，通过配置 range 来校验这个范围内的数据 FLOAT、DOUBLE 等浮点数类型在 TiDB 和 MySQL 中的实现方式不同，在计算 checksum 时会分别取 6 位和 15 位有效数字。如果不使用该特性，需要设置 ignore-columns 忽略这些列的检查。 支持对不包含主键或者唯一索引的表进行校验，但是如果数据不一致，生成的用于修复的 SQL 可能无法正确修复数据。 4、所需的数据库权限 sync-diff-inspector 需要获取表结构信息、查询数据，需要的数据库权限如下： 上游数据库 SELECT（查数据进行对比） SHOW_DATABASES（查看库名） RELOAD（查看表结构） 下游数据库 SELECT（查数据进行对比） SHOW_DATABASES（查看库名） RELOAD（查看表结构） 5、数据修复 校验过程中遇到不同的行，会生成修复数据的 SQL 语句。一个 chunk 如果出现数据不一致，就会生成一个以 chunk.Index 命名的 SQL 文件。文件位于 ${output}/fix-on-${instance} 文件夹下。其中 ${instance} 为 config.toml 中 task.target-instance 的值。 一个 SQL 文件会包含该 chunk 的所属表以及表示的范围信息。对每个修复 SQL 语句，有三种情况： 下游数据库缺失行，则是 REPLACE 语句 下游数据库冗余行，则是 DELETE 语句 下游数据库行部分数据不一致，则是 REPLACE 语句，但会在 SQL 文件中通过注释的方法标明不同的列 6、注意事项 如果上下游数据表结构不一致，则无法进行数据校验 [2023/01/09 09:06:19.089 +00:00] [ERROR] [utils.go:368] [\"column properties not compatible\"] [\"upstream table\"=btest1] [\"column name\"=pad] [\"column type\"=254] [\"downstream table\"=btest1] [\"column name\"=pad] [\"column type\"=254] [stack=\"github.com/pingcap/tidb-tools/sync_diff_inspector/utils.CompareStruct\\n\\t/home/jenkins/agent/workspace/d_tidb_tools_multi_branch_master/go/src/github.com/pingcap/tidb-tools/sync_diff_inspector/utils/utils.go:368\\nmain.(*Diff).compareStruct\\n\\t/home/jenkins/agent/workspace/d_tidb_tools_multi_branch_master/go/src/github.com/pingcap/tidb-tools/sync_diff_inspector/diff.go:318\\nmain.(*Diff).StructEqual\\n\\t/home/jenkins/agent/workspace/d_tidb_tools_multi_branch_master/go/src/github.com/pingcap/tidb-tools/sync_diff_inspector/diff.go:302\\nmain.checkSyncState\\n\\t/home/jenkins/agent/workspace/d_tidb_tools_multi_branch_master/go/src/github.com/pingcap/tidb-tools/sync_diff_inspector/main.go:125\\nmain.main\\n\\t/home/jenkins/agent/workspace/d_tidb_tools_multi_branch_master/go/src/github.com/pingcap/tidb-tools/sync_diff_inspector/main.go:104\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:250\"] sync-diff-inspector 在校验数据时会消耗一定的服务器资源，需要避免在业务高峰期间校验。 在数据对比前，需要注意表中的 collation 设置。如果表的主键或唯一键为 varchar 类型，且上下游数据库中 collation 设置不同，可能会因为排序问题导致最终校验结果不正确，需要在 sync-diff-inspector 的配置文件中增加 collation 设置。 sync-diff-inspector 会优先使用 TiDB 的统计信息来划分 chunk，需要尽量保证统计信息精确，可以在业务空闲期手动执行 analyze table {table_name}。 table-rule 的规则需要特殊注意，例如设置了 schema-pattern=\"test1\"，table-pattern = \"t_1\"，target-schema=\"test2\"，target-table = \"t_2\"，会对比 source 中的表 test1.t_1 和 target 中的表 test2.t_2。sync-diff-inspector 默认开启 sharding，如果 source 中还有表 test2.t_2，则会把 source 端的表 test1.t_1 和表 test2.t_2 作为 sharding 与 target 中的表 test2.t_2 进行一致性校验。 生成的 SQL 文件仅作为修复数据的参考，需要确认后再执行这些 SQL 修复数据。 二、安装 1、二进制 sync-diff-inspector 的安装包位于 TiDB 离线工具包中 2、Docker docker pull pingcap/tidb-enterprise-tools:nightly 三、配置文件 1、配置文件详解 配置总共分为五个部分： Global config: 通用配置，包括校验的线程数量、是否输出修复 SQL 、是否比对数据等。 Datasource config: 配置上下游数据库实例。 Routes: 上游多表名通过正则匹配下游单表名的规则。（可选） Task config: 配置校验哪些表，如果有的表在上下游有一定的映射关系或者有一些特殊要求，则需要对指定的表进行配置。 Table config: 对具体表的特殊配置，例如指定范围、忽略的列等等。（可选） ######################### Global config ######################### # 检查数据的线程数量，上下游数据库的连接数会略大于该值 check-thread-count = 4 # 如果开启，若表存在不一致，则输出用于修复的 SQL 语句。 export-fix-sql = true # 只对比表结构而不对比数据 check-struct-only = false ######################### Datasource config ######################### [data-sources] [data-sources.mysql1] # mysql1 是该数据库实例唯一标识的自定义 id，用于下面 task.source-instances/task.target-instance 中 host = \"127.0.0.1\" port = 3306 user = \"root\" password = \"\" # 设置连接上游数据库的密码，可为明文或 Base64 编码。 #（可选）使用映射规则来匹配上游多个分表，其中 rule1 和 rule2 在下面 Routes 配置栏中定义 route-rules = [\"rule1\", \"rule2\"] [data-sources.tidb0] host = \"127.0.0.1\" port = 4000 user = \"root\" password = \"\" # 设置连接下游数据库的密码，可为明文或 Base64 编码。 #（可选）使用 TLS 连接 TiDB # security.ca-path = \".../ca.crt\" # security.cert-path = \".../cert.crt\" # security.key-path = \".../key.crt\" #（可选）使用 TiDB 的 snapshot 功能，如果开启的话会使用历史数据进行对比 # snapshot = \"386902609362944000\" # 当 snapshot 设置为 \"auto\" 时，使用 TiCDC 在上下游的同步时间点，具体参考 # snapshot = \"auto\" ########################### Routes ########################### # 如果需要对比大量的不同库名或者表名的表的数据，或者用于校验上游多个分表与下游总表的数据，可以通过 table-rule 来设置映射关系 # 可以只配置 schema 或者 table 的映射关系，也可以都配置 [routes] [routes.rule1] # rule1 是该配置的唯一标识的自定义 id，用于上面 data-sources.route-rules 中 schema-pattern = \"test_*\" # 匹配数据源的库名，支持通配符 \"*\" 和 \"?\" table-pattern = \"t_*\" # 匹配数据源的表名，支持通配符 \"*\" 和 \"?\" target-schema = \"test\" # 目标库名 target-table = \"t\" # 目标表名 [routes.rule2] schema-pattern = \"test2_*\" # 匹配数据源的库名，支持通配符 \"*\" 和 \"?\" table-pattern = \"t2_*\" # 匹配数据源的表名，支持通配符 \"*\" 和 \"?\" target-schema = \"test2\" # 目标库名 target-table = \"t2\" # 目标表名 ######################### Task config ######################### # 配置需要对比的*目标数据库*中的表 [task] # output-dir 会保存如下信息 # 1 sql: 检查出错误后生成的修复 SQL 文件，并且一个 chunk 对应一个文件 # 2 log: sync-diff.log 保存日志信息 # 3 summary: summary.txt 保存总结 # 4 checkpoint: a dir 保存断点续传信息 output-dir = \"./output\" # 上游数据库，内容是 data-sources 声明的唯一标识 id source-instances = [\"mysql1\"] # 下游数据库，内容是 data-sources 声明的唯一标识 id target-instance = \"tidb0\" # 需要比对的下游数据库的表，每个表需要包含数据库名和表名，两者由 `.` 隔开 # 使用 ? 来匹配任意一个字符；使用 * 来匹配任意；详细匹配规则参考 golang regexp pkg: https://github.com/google/re2/wiki/Syntax target-check-tables = [\"schema*.table*\", \"!c.*\", \"test2.t2\"] #（可选）对部分表的额外配置，其中 config1 在下面 Table config 配置栏中定义 target-configs = [\"config1\"] ######################### Table config ######################### # 对部分表进行特殊的配置，配置的表必须包含在 task.target-check-tables 中 [table-configs.config1] # config1 是该配置的唯一标识自定义 id，用于上面 task.target-configs 中 # 目标表名称，可以使用正则来匹配多个表，但不允许存在一个表同时被多个特殊配置匹配。 target-tables = [\"schema*.test*\", \"test2.t2\"] #（可选）指定检查的数据的范围，需要符合 sql 中 where 条件的语法 range = \"age > 10 AND age 2、基于DM同步场景下数据校验的配置 当使用 TiDB DM 等同步工具时，需要校验 DM 同步后数据的一致性。可以从 DM-master 拉取指定 task-name 的配置，进行数据校验。 ######################### Global config ######################### # 检查数据的线程数量，上下游数据库的连接数会略大于该值 check-thread-count = 4 # 如果开启，若表存在不一致，则输出用于修复的 SQL 语句 export-fix-sql = true # 只对比表结构而不对比数据 check-struct-only = false # dm-master 的地址, 格式为 \"http://127.0.0.1:8261\" dm-addr = \"http://127.0.0.1:8261\" # 指定 DM 的 `task-name` dm-task = \"test\" ######################### Task config ######################### [task] output-dir = \"./output\" # 需要比对的下游数据库的表，每个表需要包含数据库名和表名，两者由 `.` 隔开 target-check-tables = [\"hb_test.*\"] 该配置在 dm-task=\"test\" 中，会对该任务下 hb_test 库的所有表进行检验，自动从 DM 配置中获取上游对下游库名的正则匹配，以校验 DM 同步后数据的一致性。 四、任务运行 mkdir output docker run -it --rm \\ -v ${PWD}:/test \\ -v ${PWD}/output:/output \\ pingcap/tidb-enterprise-tools:nightly /sync_diff_inspector -C /test/sync_diff_inspector-config.toml 该命令最终会在 config.toml 中的 output-dir 输出目录输出本次比对的检查报告 summary.txt 和日志 sync_diff.log。在输出目录下还会生成由 config.toml 文件内容哈希值命名的文件夹，该文件夹下包括断点续传 checkpoint 结点信息以及数据存在不一致时生成的 SQL 修复数据。 output/ |-- summary.txt # 保存校验结果的总结 |-- sync_diff.log # 保存 sync-diff-inspector 执行过程中输出的日志信息 |-- checkpoint # 保存断点续传信息 | |-- bbfec8cc8d1f58a5800e63aa73e5 # config hash 占位文件，标识该输出目录（output/）对应的配置文件 │ └-- sync_diff_checkpoints.pb # 断点续传信息 |-- fix-on-target # 保存用于修复不一致的 SQL 文件 | |-- xxx.sql | └-- xxx.sql 日志：sync-diff-inspector 的日志存放在 ${output}/sync_diff.log 中，其中 ${output} 是 config.toml 文件中 output-dir 的值。 校验进度 sync-diff-inspector 会在运行时定期（间隔 10s）输出校验进度到 checkpoint 中(位于 ${output}/checkpoint/sync_diff_checkpoints.pb 中，其中 ${output} 是 config.toml 文件中 output-dir 的值。 校验结果 当校验结束时，sync-diff-inspector 会输出一份校验报告，位于 ${output}/summary.txt 中，其中 ${output} 是 config.toml 文件中 output-dir 的值。 +----------------------+--------------------+------------------------+----------+-----------+ | TABLE | STRUCTURE EQUALITY | DATA DIFF ROWS | UPCOUNT | DOWNCOUNT | | 数据库.表名 | 表结构是否相同 | 数据差异行(增加/删除行数) |上游数据总数|下游数据总数 | +----------------------+--------------------+------------------------+----------+-----------+ | `test`.`btest1` | true | +4/-3 | 999 | 998 | +----------------------+--------------------+------------------------+----------+-----------+ Time Cost: 156.398881ms Average Speed: 0.832264MB/s 五、测试 目标表两条数据字段值有变更 目标表缺失一条数据 目标表多一条数据 -- table: test.btest1 -- range in sequence: Full /* DIFF COLUMNS ╏ `C` ╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╋╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍ source data ╏ '2-15688153734-6-1-9-5-6-5-51724173961-3' ╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╋╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍ target data ╏ '1' ╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╋╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍ */ REPLACE INTO `test`.`btest1`(`id`,`k`,`c`,`pad`) VALUES (1,367,'2-15688153734-6-1-9-5-6-5-51724173961-3','1111'); REPLACE INTO `test`.`btest1`(`id`,`k`,`c`,`pad`) VALUES (4,88,'222222','333333'); /* DIFF COLUMNS ╏ `C` ╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╋╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍ source data ╏ '3-02480546617-1-1-5-9-7-6-6-2' ╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╋╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍ target data ╏ '2' ╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╋╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍╍ */ REPLACE INTO `test`.`btest1`(`id`,`k`,`c`,`pad`) VALUES (5,53,'3-02480546617-1-1-5-9-7-6-6-2','222'); DELETE FROM `test`.`btest1` WHERE `id` = 22 AND `k` = 350 AND `c` = '666666' AND `pad` = '777777' LIMIT 1; 参考 https://docs.pingcap.com/zh/tidb/stable/sync-diff-inspector-overview Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-01-28 16:04:33 "},"origin/tidb-slow-sql-statistics.html":{"url":"origin/tidb-slow-sql-statistics.html","title":"统计邮件通知慢SQL","keywords":"","body":"通过邮件发送统计TiDB慢SQL的Python脚本 CREATE USER `export-slowsql-user`@`192.168.1.%` IDENTIFIED BY '****'; GRANT PROCESS ON INFORMATION_SCHEMA.* TO 'export-slowsql-user'@'192.168.1.%' #!/usr/bin/python3 # -*- coding: UTF-8 -*- import datetime import smtplib import os from email import encoders from email.header import Header from email.mime.base import MIMEBase from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText import mysql.connector import xlwt # 获取大约多少秒的慢SQL slowtime = 2 # 获取最近多少天之前到现在的慢SQL slow_from_day = 7 # TiDB数据库地址 DB_HOST=\"\" # TiDB数据库端口 DB_PORT=\"\" # 用于查询慢SQL的用户 DB_USER=\"\" DB_PASSWORD=\"\" # 记录慢SQL变所在的Database DB_DATABASE=\"INFORMATION_SCHEMA\" # SMTP服务器的地址 SMTP_HOST=\"\" # SMTP用于发信的用户名 SMTP_USER=\"\" # SMTP用于发信的密码 SMTP_USER_PASSWORD=\"\" # 用于记录慢SQL的EXCEL文件名 RECORDS_EXCEL_FILENAME=\"tidb-slowlogs.xls\" nt = datetime.datetime.now() lastsomeday = (nt - datetime.timedelta(days=slow_from_day)).date() def sendEmail(attachfilename): sender = SMTP_USER # 接收者邮件地址 receivers = ['要邮件通知的邮箱地址', '要邮件通知的邮箱地址'] message = MIMEMultipart() message['From'] = Header(SMTP_USER, 'utf-8') # 发送者 message['To'] = Header('; '.join(str(e) for e in receivers) + '; ', 'utf-8') # 邮件主题内容 message['Subject'] = Header(nt.strftime('%Y%m%d') + \"TiDB最近\" + str(slow_from_day) + \"天(\" + lastsomeday.strftime('%Y%m%d') + \"~\" + nt.strftime('%Y%m%d') + \")的慢SQL\", 'utf-8') # 邮件正文内容 message.attach(MIMEText(\"TiDB最近\" + str(slow_from_day) + \"天(\" + lastsomeday.strftime('%Y%m%d') + \"~\" + nt.strftime('%Y%m%d') + \")，查询时间超过\" + str(slowtime) + \"秒的慢SQL。详情见附件Excel文件(排除了部分用户和主机客户端相关的SQL)\", 'plain', 'utf-8')) # 附件内容 attachment = MIMEBase('application', \"octet-stream\") attachment.set_payload(open(attachfilename, \"rb\").read()) encoders.encode_base64(attachment) attachment.add_header('Content-Disposition', 'attachment; filename=\"' + lastsomeday.strftime('%Y%m%d') + \"-\" + nt.strftime( '%m%d') + '-'+RECORDS_EXCEL_FILENAME+'\"') message.attach(attachment) try: smtpObj = smtplib.SMTP(SMTP_HOST) smtpObj.login(SMTP_USER, SMTP_USER_PASSWORD) smtpObj.sendmail(sender, receivers, message.as_string()) print(\"邮件发送成功\") except smtplib.SMTPException: print(\"Error: 无法发送邮件\") def dbConnet(host, port, user, passwd, db): db = mysql.connector.connect( host=host, port=port, user=user, password=passwd, database=db ) cursor = db.cursor(buffered=True, dictionary=True) return cursor, db def createExcel(data, filename): # 创建excel工作表 workbook = xlwt.Workbook(encoding='utf-8') worksheet = workbook.add_sheet('sheet1') # 设置表头 worksheet.write(0, 0, label='UID') worksheet.write(0, 1, label='Count') worksheet.write(0, 2, label='数据库') worksheet.write(0, 3, label='执行耗时') worksheet.write(0, 4, label='扫描总Key个数') worksheet.write(0, 5, label='执行时间') worksheet.write(0, 6, label='执行用户') worksheet.write(0, 7, label='主机') worksheet.write(0, 8, label='返回结果行数') worksheet.write(0, 9, label='内存') worksheet.write(0, 10, label='Cop_time') worksheet.write(0, 11, label='Write_sql_response_total') worksheet.write(0, 12, label='Total_keys') worksheet.write(0, 13, label='Process_keys') worksheet.write(0, 14, label='SQL语句') ordered_list = [\"UID\", \"Count\", \"数据库\", \"执行耗时\", \"扫描总Key个数\", \"执行时间\", \"执行用户\", \"主机\", \"返回结果行数\", \"内存\", \"Cop_time\", \"Write_sql_response_total\", \"Total_keys\", \"Process_keys\", \"SQL语句\", ] row = 1 for player in data: for _key, _value in player.items(): col = ordered_list.index(_key) worksheet.write(row, col, _value) row += 1 workbook.save(filename) def execute_sql(sql): mycursor, mydb = dbConnet(DB_HOST, DB_PORT, DB_USER, DB_PASSWORD, DB_DATABASE) mycursor.execute(sql) sqlresults = mycursor.fetchall() mydb.commit() mycursor.close() mydb.close() if sqlresults: filename = 'prod-tidb-slowlogs.xls' createExcel(sqlresults, filename) sendEmail(filename) if os.path.exists(RECORDS_EXCEL_FILENAME): os.remove(RECORDS_EXCEL_FILENAME) print(\"已删除临时文件\") else: print(\"临时文件不存在!\") else: print(lastsomeday.strftime('%Y%m%d') + \"至\" + nt.strftime('%Y%m%d') + \"没有相关的慢SQL\") def main(): print(\"====\" + nt.strftime('%Y%m%d %H:%M:%s') + \"====\") query = (\"SELECT \" \"Digest AS 'UID',\" \"count( 1 ) AS 'Count',\" \"DB AS '数据库',\" \"CONCAT( CAST(( Query_time ) AS CHAR ( 4 )), 's' ) AS '执行耗时',\" \"Total_keys AS '扫描总Key个数',\" \"CONCAT('\\\\'',Time) AS '执行时间', \" \"User AS '执行用户',\" \"Host AS '主机',\" \"Result_rows AS '返回结果行数',\" \"CONCAT( CAST(( Mem_max / 1024 / 1024 ) AS CHAR ( 4 )), 'MB' ) AS '内存',\" \"CONCAT( CAST(( Cop_time ) AS CHAR ( 4 )), 's' ) AS 'Cop_time',\" \"CONCAT( CAST(( Write_sql_response_total ) AS CHAR ( 4 )), 's' ) AS 'Write_sql_response_total',\" \"Total_keys,\" \"Process_keys,\" \"Query AS 'SQL语句'\" \"FROM INFORMATION_SCHEMA.CLUSTER_SLOW_QUERY \" \"WHERE \" \"Query_time >= '%s' \" \"AND Time >= DATE_SUB( '%s' , INTERVAL '%s' DAY ) \" \"AND db not in ('','要排除的DB')\" \"AND host not in ('要排除的客户端IP地址','要排除的客户端IP地址') \" \"AND user NOT IN ( '要排除的客户端用户名' , '要排除的客户端用户名', '要排除的客户端用户名')\" \"GROUP BY Digest \" \"ORDER BY Query_time DESC , Result_rows DESC, Count DESC \" \"LIMIT 1000;\" % (slowtime, nt, slow_from_day) ) execute_sql(query) if __name__ == \"__main__\": main() Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-02-07 18:13:48 "},"origin/tidb-cdc.html":{"url":"origin/tidb-cdc.html","title":"TiCDC: 增量数据同步","keywords":"","body":"TiCDC增量数据同步工具 一、简介 TiCDC 是一款 TiDB 增量数据同步工具，通过拉取上游 TiKV 的数据变更日志，TiCDC 可以将数据解析为有序的行级变更数据输出到下游。 TiCDC 架构 TiKV Server：代表 TiDB 集群中的 TiKV 节点，当数据发生改变时 TiKV 节点会主动将发生的数据改变以变更日志（KV change logs，简称 change logs）的方式发送给 TiCDC 节点。当然，当 TiCDC 节点发现收到的 change logs 并不是连续的，也会主动发起请求，获得需要的 change logs。 TiCDC：代表运行了运行 TiCDC 进程的各个节点。每个节点都运行一个 TiCDC 进程，每个进程会从 TiKV 节点中拉取一个或者多个表中的数据改变，并通过 Sink 模块同步到下游系统。 PD：代表 TiDB 集群中的调度模块，负责集群数据的事实调度，这个模块通常是由 3 个 PD 节点构成的，内部通过 etcd 集群来实现选举等高可用相关的能力。 TiCDC 集群使用了 PD 集群内置的 etcd 集群来保存自己的元数据信息，例如：节点的状态信息，changefeed 配置信息等。 目前 TiCDC 支持将数据同步到 TiDB，MySQL 数据库，以及 Kafka 等。 使用场景 提供多 TiDB 集群，跨区域数据高可用和容灾方案，保证在灾难发生时保证主备集群数据的最终一致性。 提供同步实时变更数据到异构系统的服务，为监控、缓存、全文索引、数据分析、异构数据库使用等场景提供数据源。 特性 提供 TiDB -> TiDB 之间数据容灾复制的能力，实现秒级别 RPO 和分钟级别 RTO 提供 TiDB 之间双向复制的能力，支持通过 TiCDC 构建多写多活的 TiDB 集群 提供 TiDB -> MySQL（或其他兼容 MySQL 协议的数据库）的低延迟的增量数据同步能力 提供 TiDB -> Kafka 增量数据同步能力，推荐的数据格式包含 Canal-JSON，Avro 等 提供表级别数据同步能力，支持同步过程中过滤数据库、表、DML、DDL 的能力 高可用架构，无单点故障；支持动态添加、删除 TiCDC 节点 支持通过 Open API 进行集群管理，包括查询任务状态；动态修改任务配置；动态创建、删除任务等 从 v6.2 版本起，你可以通过配置 sink uri 参数 transaction-atomicity 来控制 TiCDC 是否拆分单表事务。拆分事务可以大幅降低 MySQL sink 同步大事务的延时和内存消耗。 数据同步顺序性 TiCDC 对于所有的 DDL/DML 都能对外输出至少一次。 TiCDC 在 TiKV/TiCDC 集群故障期间可能会重复发相同的 DDL/DML。对于重复的 DDL/DML： MySQL sink 可以重复执行 DDL，对于在下游可重入的 DDL （譬如 truncate table）直接执行成功；对于在下游不可重入的 DDL（譬如 create table），执行失败，TiCDC 会忽略错误继续同步。 Kafka sink Kafka sink 提供不同的数据分发策略，可以按照表、主键或 ts 等策略分发数据到不同 Kafka partition。 使用表、主键分发策略，可以保证某一行的更新数据被顺序的发送到相同 partition。 对所有的分发策略，我们都会定期发送 Resolved TS 消息到所有的 topic/partition，表示早于该 Resolved TS 的消息都已经发送到 topic/partition，消费程序可以利用 Resolved TS 对多个 topic/partition 的消息进行排序。 Kafka sink 会发送重复的消息，但重复消息不会破坏 Resolved TS 的约束，比如在 changefeed 暂停重启后，可能会按顺序发送 msg1、msg2、msg3、msg2、msg3。你可以在 Kafka 消费端进行过滤。 数据同步一致性 MySQL sink TiCDC 开启 redo log 后保证数据复制的最终一致性 TiCDC 保证单行的更新与上游更新顺序一致。 TiCDC 不保证下游事务的执行顺序和上游完全一致。 暂不支持的场景 目前 TiCDC 暂不支持的场景如下： 暂不支持单独使用 RawKV 的 TiKV 集群。 暂不支持在 TiDB 中创建 SEQUENCE 的 DDL 操作和 SEQUENCE 函数。在上游 TiDB 使用 SEQUENCE 时，TiCDC 将会忽略掉上游执行的 SEQUENCE DDL 操作/函数，但是使用 SEQUENCE 函数的 DML 操作可以正确地同步。 二、安装部署 1、使用 TiUP 部署包含 TiCDC 组件的全新 TiDB 集群 在使用 TiUP 部署全新 TiDB 集群时，支持同时部署 TiCDC 组件。你需要在 TiUP 启动 TiDB 集群时的配置文件中加入 TiCDC 相关的部分，以下是一个示例： cdc_servers: - host: 10.0.1.20 gc-ttl: 86400 data_dir: \"/cdc-data\" - host: 10.0.1.21 gc-ttl: 86400 data_dir: \"/cdc-data\" cdc_servers详细配置参数 cdc_servers 约定了将 TiCDC 服务部署到哪些机器上，同时可以指定每台机器上的服务配置，cdc_servers 是一个数组，每个数组元素包含以下字段： host：指定部署到哪台机器，字段值填 IP 地址，不可省略 ssh_port：指定连接目标机器进行操作的时候使用的 SSH 端口，若不指定，则使用 global 区块中的 ssh_port port：TiCDC 服务的监听端口，默认 8300 deploy_dir：指定部署目录，若不指定，或指定为相对目录，则按照 global 中配置的 deploy_dir 生成 data_dir：指定数据目录。若不指定该字段或指定为相对目录，数据目录则按照 global 中配置的 data_dir 生成。 log_dir：指定日志目录，若不指定，或指定为相对目录，则按照 global 中配置的 log_dir 生成 gc-ttl：TiCDC 在 PD 设置的服务级别 GC safepoint 的 TTL (Time To Live) 时长，单位为秒，默认值为 86400，即 24 小时 tz：TiCDC 服务使用的时区。TiCDC 在内部转换 timestamp 等时间数据类型和向下游同步数据时使用该时区，默认为进程运行本地时区。 numa_node：为该实例分配 NUMA 策略，如果指定了该参数，需要确保目标机装了 numactl，在指定该参数的情况下会通过 numactl 分配 cpubind 和 membind 策略。该字段参数为 string 类型，字段值填 NUMA 节点的 ID，例如 \"0,1\" config：该字段配置规则和 server_configs 里的 cdc 内容合并（若字段重叠，以本字段内容为准），然后生成配置文件并下发到 host 指定的机器 os：host 字段所指定的机器的操作系统，若不指定该字段，则默认为 global 中的 os arch：host 字段所指定的机器的架构，若不指定该字段，则默认为 global 中的 arch resource_control：针对该服务的资源控制，如果配置了该字段，会将该字段和 global 中的 resource_control 内容合并（若字段重叠，以本字段内容为准），然后生成 systemd 配置文件并下发到 host 指定机器。resource_control 的配置规则同 global 中的 resource_control ticdc_cluster_id：指定该服务器对应的 TiCDC 集群 ID。若不指定该字段，则自动加入默认 TiCDC 集群。该配置只在 v6.3.0 及以上 TiDB 版本中才生效。 2、使用 TiUP 在原有 TiDB 集群上新增或扩容 TiCDC 组件 创建名为 scale-out.yml 的配置文件，包含需要扩容的节点的配置信息 cdc_servers: - host: 10.1.1.1 gc-ttl: 86400 data_dir: /tidb-data/cdc-8300 - host: 10.1.1.2 gc-ttl: 86400 data_dir: /tidb-data/cdc-8300 - host: 10.0.1.4:8300 gc-ttl: 86400 data_dir: /tidb-data/cdc-8300 tiup cluster scale-out scale-out.yml 3、使用 TiUP 在原有 TiDB 集群上移除或缩容 TiCDC 组件 tiup cluster scale-in --node cdc_servers_ip:8300 4、使用 TiUP 变更 TiCDC 集群配置 tiup cluster edit-config server_configs: tidb: {} tikv: {} pd: {} tiflash: {} tiflash-learner: {} pump: {} drainer: {} cdc: gc-ttl: 172800 tiup cluster reload -R cdc 5、使用 TiUP 终止和启动 TiCDC 节点 终止 TiCDC 节点：tiup cluster stop -R cdc 启动 TiCDC 节点：tiup cluster start -R cdc 重启 TiCDC 节点：tiup cluster restart -R cdc 6、使用 TiCDC 命令行工具来查看集群状态 cdc cli capture list [ { \"id\": \"806e3a1b-0e31-477f-9dd6-f3f2c570abdd\", \"is-owner\": true, \"address\": \"127.0.0.1:8300\", \"cluster-id\": \"default\" }, { \"id\": \"ea2a4203-56fe-43a6-b442-7b295f458ebc\", \"is-owner\": false, \"address\": \"127.0.0.1:8301\", \"cluster-id\": \"default\" } ] id：表示服务进程的 ID。 is-owner：表示该服务进程是否为 owner 节点。 address：该服务进程对外提供接口的地址。 cluster-id：该 TiCDC 的集群 ID，默认值为 default。 如果TiCDC 是用 TiUP 部署的，需要将以下命令中的 cdc cli 替换为 tiup ctl: cdc，例如：tiup ctl:v6.1.1 cdc capture list --pd=http://192.168.1.1:2379。 三、任务操作 1、创建同步任务 cdc cli changefeed create \\ --server=http://10.0.10.25:8300 \\ --sink-uri=\"mysql://root:123456@127.0.0.1:3306/\" \\ --changefeed-id=\"simple-replication-task\" 2、查询同步任务列表 使用以下命令来查询同步任务列表： cdc cli changefeed list --server=http://10.0.10.25:8300 [{ \"id\": \"simple-replication-task\", \"summary\": { \"state\": \"normal\", \"tso\": 417886179132964865, \"checkpoint\": \"2020-07-07 16:07:44.881\", \"error\": null } }] checkpoint 即为 TiCDC 已经将该时间点前的数据同步到了下游。 state 为该同步任务的状态： normal：正常同步 stopped：停止同步（手动暂停） error：停止同步（出错） removed：已删除任务（只在指定 --all 选项时才会显示该状态的任务。未指定时，可通过 query 查询该状态的任务） finished：任务已经同步到指定 target-ts，处于已完成状态（只在指定 --all 选项时才会显示该状态的任务。未指定时，可通过 query 查询该状态的任务）。 3、查询特定同步任务 使用 changefeed query 命令可以查询特定同步任务（对应某个同步任务的信息和状态），指定 --simple 或 -s 参数会简化输出，提供最基本的同步状态和 checkpoint 信息。不指定该参数会输出详细的任务配置、同步状态和同步表信息。 cdc cli changefeed query -s --server=http://10.0.10.25:8300 --changefeed-id=simple-replication-task { \"state\": \"normal\", \"tso\": 419035700154597378, \"checkpoint\": \"2020-08-27 10:12:19.579\", \"error\": null } 简化输出各字段含义： state 代表当前 changefeed 的同步状态，各个状态必须和 changefeed list 中的状态相同。 tso 代表当前 changefeed 中已经成功写入下游的最大事务 TSO。 checkpoint 代表当前 changefeed 中已经成功写入下游的最大事务 TSO 对应的时间。 error 记录当前 changefeed 是否有错误发生。 输出各字段含义： info 代表查询 changefeed 的同步配置。 status 代表查询 changefeed 的同步状态信息。 resolved-ts 代表当前 changefeed 中已经成功从 TiKV 发送到 TiCDC 的最大事务 TS。 checkpoint-ts 代表当前 changefeed 中已经成功写入下游的最大事务 TS。 admin-job-type代表一个 changefeed 的状态： 0：状态正常。 1：任务暂停，停止任务后所有同步 processor 会结束退出，同步任务的配置和同步状态都会保留，可以从 checkpoint-ts 恢复任务。 2：任务恢复，同步任务从 checkpoint-ts 继续同步。 3：任务已删除，接口请求后会结束所有同步 processor，并清理同步任务配置信息。同步状态保留，只提供查询，没有其他实际功能。 task-status 代表查询 changefeed 所分配的各个同步子任务的状态信息。 4、停止同步任务 cdc cli changefeed pause --server=http://10.0.10.25:8300 --changefeed-id simple-replication-task --changefeed-id=uuid 为需要操作的 changefeed ID。 5、恢复同步任务 使用以下命令恢复同步任务： cdc cli changefeed resume --server=http://10.0.10.25:8300 --changefeed-id simple-replication-task --changefeed-id=uuid 为需要操作的 changefeed ID。 --overwrite-checkpoint-ts：从 v6.2 开始支持指定 changefeed 恢复的起始 TSO。TiCDC 集群将从这个 TSO 开始拉取数据。该项支持 now 或一个具体的 TSO（如 434873584621453313），指定的 TSO 应在 (GC safe point, CurrentTSO] 范围内。如未指定该参数，默认从当前的 checkpoint-ts 同步数据。 --no-confirm：恢复同步任务时无需用户确认相关信息。默认为 false。 注意 若 --overwrite-checkpoint-ts 指定的 TSO t2 大于 changefeed 的当前 checkpoint TSO t1（可通过 cdc cli changefeed query 命令获取），则会导致 t1 与 t2 之间的数据不会同步到下游，造成数据丢失。 若 --overwrite-checkpoint-ts 指定的 TSO t2 小于 changefeed 的当前 checkpoint TSO t1，则会导致 TiCDC 集群从一个旧的时间点 t2 重新拉取数据，可能会造成数据重复（例如 TiCDC 下游为 MQ sink）。 6、删除同步任务 使用以下命令删除同步任务： cdc cli changefeed remove --server=http://10.0.10.25:8300 --changefeed-id simple-replication-task --changefeed-id=uuid 为需要操作的 changefeed ID。 7、更新同步任务配置 TiCDC 从 4.0.4 开始支持非动态修改同步任务配置，修改 changefeed 配置需要按照 暂停任务 -> 修改配置 -> 恢复任务 的流程。 cdc cli changefeed pause --pd=http://10.0.10.22:2379 --changefeed-id test-tidb-to-pulsar cdc cli changefeed update --pd=http://10.0.10.22:2379 --changefeed-id test-tidb-to-pulsar --config ~/ticdc-changefeed.toml cdc cli changefeed resume --pd=http://10.0.10.22:2379 --changefeed-id test-tidb-to-pulsar 当前支持修改的配置包括： changefeed 的 sink-uri changefeed 配置文件及文件内所有配置 changefeed 是否使用文件排序和排序目录 changefeed 的 target-ts 8、管理同步子任务处理单元 (processor) 查询 processor 列表： cdc cli processor list --server=http://10.0.10.25:8300 [ { \"id\": \"9f84ff74-abf9-407f-a6e2-56aa35b33888\", \"capture-id\": \"b293999a-4168-4988-a4f4-35d9589b226b\", \"changefeed-id\": \"simple-replication-task\" } ] 查询特定 processor，对应于某个节点处理的同步子任务信息和状态： cdc cli processor query --server=http://10.0.10.25:8300 --changefeed-id=simple-replication-task --capture-id=b293999a-4168-4988-a4f4-35d9589b226b { \"status\": { \"tables\": { \"56\": { # 56 表示同步表 id，对应 TiDB 中表的 tidb_table_id \"start-ts\": 417474117955485702 } }, \"operation\": null, \"admin-job-type\": 0 }, \"position\": { \"checkpoint-ts\": 417474143881789441, \"resolved-ts\": 417474143881789441, \"count\": 0 } } 以上命令中： status.tables 中每一个作为 key 的数字代表同步表的 id，对应 TiDB 中表的 tidb_table_id。 resolved-ts 代表当前 Processor 中已经排序数据的最大 TSO。 checkpoint-ts 代表当前 Processor 已经成功写入下游的事务的最大 TSO。 9、其他 9.1 输出行变更的历史值 从 v4.0.5 版本开始引入 默认配置下，同步任务输出的 TiCDC Open Protocol 行变更数据只包含变更后的值，不包含变更前行的值，因此该输出数据不满足 TiCDC Open Protocol 的消费端使用行变更历史值的需求。 从 v4.0.5 开始，TiCDC 支持输出行变更数据的历史值。若要开启该特性，需要在 changefeed 的配置文件的根级别指定以下配置： enable-old-value = true 从 v5.0 开始默认启用该特性，开启该特性后 TiCDC Open Protocol 的输出格式参考 TiCDC 开放数据协议 - Row Changed Event。 9.2 同步启用了 TiDB 新的 Collation 框架的表 从 v4.0.15、v5.0.4、v5.1.1 和 v5.2.0 开始，TiCDC 支持同步启用了 TiDB 新的 Collation 框架的表。 9.3 同步没有有效索引的表 从 v4.0.8 开始，TiCDC 支持通过修改任务配置来同步没有有效索引的表。若要开启该特性，需要在 changefeed 配置文件的根级别进行如下指定： enable-old-value = true force-replicate = true 警告 对于没有有效索引的表，INSERT 和 REPLACE 等操作不具备可重入性，因此会有数据冗余的风险。TiCDC 在同步过程中只保证数据至少分发一次，因此开启该特性同步没有有效索引的表，一定会导致数据冗余出现。如果不能接受数据冗余，建议增加有效索引，譬如增加具有 AUTO RANDOM 属性的主键列。 9.4 Unified Sorter 功能 注意 从 v6.0.0 开始，TiCDC 内部默认使用 DB Sorter 引擎来对数据进行排序，不再使用 Unified Sorter。建议用户不再主动配置 Sorter 项。 Unified Sorter 是 TiCDC 中的排序引擎功能，用于缓解以下场景造成的内存溢出问题： 如果 TiCDC 数据订阅任务的暂停中断时间长，其间积累了大量的增量更新数据需要同步。 从较早的时间点启动数据订阅任务，业务写入量大，积累了大量的更新数据需要同步。 对 v4.0.13 版本之后的 cdc cli 创建的 changefeed，默认开启 Unified Sorter。对 v4.0.13 版本前已经存在的 changefeed，则使用之前的配置。 要确定一个 changefeed 上是否开启了 Unified Sorter 功能，可执行以下示例命令查看（假设 PD 实例的 IP 地址为 http://10.0.10.25:2379）： cdc cli --server=\"http://10.0.10.25:8300\" changefeed query --changefeed-id=simple-replication-task | grep 'sort-engine' 以上命令的返回结果中，如果 sort-engine 的值为 \"unified\"，则说明 Unified Sorter 已在该 changefeed 上开启。 10、任务注意 如果服务器使用机械硬盘或其他有延迟或吞吐有瓶颈的存储设备，Unified Sorter 性能会受到较大影响。 Unified Sorter 默认使用 data_dir 储存临时文件。建议保证硬盘的空闲容量大于等于 500 GiB。对于生产环境，建议保证每个节点上的磁盘可用空间大于（业务允许的最大）checkpoint-ts 延迟 * 业务高峰上游写入流量。此外，如果在 changefeed 创建后预期需要同步大量历史数据，请确保每个节点的空闲容量大于等于要追赶的同步数据。 四、同步实例 Changefeed 是 TiCDC 中的单个同步任务。Changefeed 将一个 TiDB 集群中数张表的变更数据输出到一个指定的下游。TiCDC 集群可以运行和管理多个 Changefeed。 1、复制增量数据到 MySQL 兼容数据库 cdc cli changefeed create \\ --server=http://10.0.10.25:8300 \\ --sink-uri=\"mysql://root:123456@127.0.0.1:3306/\" \\ --changefeed-id=\"simple-replication-task\" Create changefeed successfully! ID: simple-replication-task Info: {\"sink-uri\":\"mysql://root:123456@127.0.0.1:3306/\",\"opts\":{},\"create-time\":\"2020-03-12T22:04:08.103600025+08:00\",\"start-ts\":415241823337054209,\"target-ts\":0,\"admin-job-type\":0,\"sort-engine\":\"unified\",\"sort-dir\":\".\",\"config\":{\"case-sensitive\":true,\"filter\":{\"rules\":[\"*.*\"],\"ignore-txn-start-ts\":null,\"ddl-allow-list\":null},\"mounter\":{\"worker-num\":16},\"sink\":{\"dispatchers\":null},\"scheduler\":{\"type\":\"table-number\",\"polling-time\":-1}},\"state\":\"normal\",\"history\":null,\"error\":null} --server：TiCDC 集群中任意一个 TiCDC 服务器的地址。 --changefeed-id：同步任务的 ID，格式需要符合正则表达式 ^[a-zA-Z0-9]+(\\-[a-zA-Z0-9]+)*$。如果不指定该 ID，TiCDC 会自动生成一个 UUID（version 4 格式）作为 ID。 --sink-uri：同步任务下游的地址，详见 Sink URI 配置 mysql/tidb。 --start-ts：指定 changefeed 的开始 TSO。TiCDC 集群将从这个 TSO 开始拉取数据。默认为当前时间。 --target-ts：指定 changefeed 的目标 TSO。TiCDC 集群拉取数据直到这个 TSO 停止。默认为空，即 TiCDC 不会自动停止。 --config：指定 changefeed 配置文件，详见：TiCDC Changefeed 配置参数。 Sink URI 用于指定 TiCDC 目标系统的连接信息，遵循以下格式： [scheme]://[userinfo@][host]:[port][/path]?[query_parameters] 注意/path 不适用于 MySQL sink。 一个通用的配置样例如下所示： --sink-uri=\"mysql://root:123456@127.0.0.1:3306\" URI 中可配置的参数如下： 参数 描述 root 下游数据库的用户名。 123456 下游数据库密码。（可采用 Base64 进行编码） 127.0.0.1 下游数据库的 IP。 3306 下游数据的连接端口。 worker-count 向下游执行 SQL 的并发度（可选，默认值为 16）。 max-txn-row 向下游执行 SQL 的 batch 大小（可选，默认值为 256）。 ssl-ca 连接下游 MySQL 实例所需的 CA 证书文件路径（可选）。 ssl-cert 连接下游 MySQL 实例所需的证书文件路径（可选）。 ssl-key 连接下游 MySQL 实例所需的证书密钥文件路径（可选）。 time-zone 连接下游 MySQL 实例时使用的时区名称，从 v4.0.8 开始生效。（可选。如果不指定该参数，使用 TiCDC 服务进程的时区；如果指定该参数但使用空值，则表示连接 MySQL 时不指定时区，使用下游默认时区）。 transaction-atomicity 指定事务的原子性级别（可选，默认值为 none）。当该值为 table 时 TiCDC 保证单表事务的原子性，当该值为 none 时 TiCDC 会拆分单表事务。 若需要对 Sink URI 中的数据库密码使用 Base64 进行编码，可以参考如下命令： echo -n '123456' | base64 # 假设待编码的密码为 123456 编码后的密码如下： MTIzNDU2 注意当 Sink URI 中包含特殊字符时，如 ! * ' ( ) ; : @ & = + $ , / ? % # [ ]，需要对 URI 特殊字符进行转义处理。你可以使用 URI Encoder 工具对 URI 进行转义。 2、复制增量数据到 Kafka 3、复制增量数据到 Pulsar cdc cli changefeed create \\ --pd=http://192.168.1.9:2379 \\ --sink-uri=\"pulsar://192.168.1.9:6050/testticdc?protocol=canal-json&compressionType=LZ4\" \\ --changefeed-id=\"tidb-cdc-pulsar\" \\ --config /home/tidb/ticdc-changefeed.toml 五、监控 六、故障处理 1、如何处理 TiCDC 同步任务的中断？ 目前已知可能发生的同步中断包括以下场景： 下游持续异常，TiCDC 多次重试后仍然失败。 该场景下 TiCDC 会保存任务信息，由于 TiCDC 已经在 PD 中设置的 service GC safepoint，在 gc-ttl 的有效期内，同步任务 checkpoint 之后的数据不会被 TiKV GC 清理掉。 处理方法：在下游恢复正常后，通过 HTTP 接口恢复同步任务。 因下游存在不兼容的 SQL 语句，导致同步不能继续。 该场景下 TiCDC 会保存任务信息，由于 TiCDC 已经在 PD 中设置的 service GC safepoint，在 gc-ttl 的有效期内，同步任务 checkpoint 之后的数据不会被 TiKV GC 清理掉。 处理方法： 先通过 cdc cli changefeed query 查询同步任务状态信息，记录 checkpoint-ts 值。 使用新的任务配置文件，增加ignore-txn-start-ts 参数跳过指定 start-ts 对应的事务。 通过 HTTP API 停止旧的同步任务，使用 cdc cli changefeed create，指定新的任务配置文件，指定 start-ts 为刚才记录的 checkpoint-ts，启动新的同步任务恢复同步。 在 v4.0.13 及之前的版本中 TiCDC 同步分区表存在问题，导致同步停止。 该场景下 TiCDC 会保存任务信息，由于 TiCDC 已经在 PD 中设置的 service GC safepoint，在 gc-ttl 的有效期内，同步任务 checkpoint 之后的数据不会被 TiKV GC 清理掉。 处理方法： 通过 cdc cli changefeed pause -c 暂停同步。 等待约一分钟后，通过 cdc cli changefeed resume -c 恢复同步。 2、TiCDC 同步时，在下游执行 DDL 语句失败会有什么表现，如何恢复？ 如果某条 DDL 语句执行失败，同步任务 (changefeed) 会自动停止，checkpoint-ts 断点时间戳为该条出错 DDL 语句的结束时间戳 (finish-ts) 减去一。如果希望让 TiCDC 在下游重试执行这条 DDL 语句，可以使用 cdc cli changefeed resume 恢复同步任务。例如： cdc cli changefeed resume -c test-cf --server=http://127.0.0.1:8300 如果希望跳过这条出错的 DDL 语句，可以将 changefeed 的 start-ts 设为报错时的 checkpoint-ts 加上一，然后通过 cdc cli changefeed create 新建同步任务。假设报错时的 checkpoint-ts 为 415241823337054209，可以进行如下操作来跳过该 DDL 语句： cdc cli changefeed remove --server=http://127.0.0.1:8300 --changefeed-id simple-replication-task cdc cli changefeed create --server=http://127.0.0.1:8300 --sink-uri=\"mysql://root:123456@127.0.0.1:3306/\" --changefeed-id=\"simple-replication-task\" --sort-engine=\"unified\" --start-ts 415241823337054210 3、当 changefeed 的下游为类 MySQL 数据库时，TiCDC 执行了一个耗时较长的 DDL 语句，阻塞了所有其他 changefeed，应该怎样处理？ 首先暂停执行耗时较长的 DDL 的 changefeed。此时可以观察到，这个 changefeed 暂停后，其他的 changefeed 不再阻塞了。 在 TiCDC log 中搜寻 apply job 字段，确认耗时较长的 DDL 的 start-ts。 手动在下游执行该 DDL 语句，执行完毕后进行下面的操作。 修改 changefeed 配置，将上述 start-ts 添加到 ignore-txn-start-ts 配置项中。 恢复被暂停的 changefeed。 4、如何处理 TiCDC 创建同步任务或同步到 MySQL 时遇到 Error 1298: Unknown or incorrect time zone: 'UTC' 错误？ 这是因为下游 MySQL 没有加载时区，可以通过 mysql_tzinfo_to_sql 命令加载时区，加载后就可以正常创建任务或同步任务。 mysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -u root mysql -p 显示类似于下面的输出则表示导入已经成功： Enter password: Warning: Unable to load '/usr/share/zoneinfo/iso3166.tab' as time zone. Skipping it. Warning: Unable to load '/usr/share/zoneinfo/leap-seconds.list' as time zone. Skipping it. Warning: Unable to load '/usr/share/zoneinfo/zone.tab' as time zone. Skipping it. Warning: Unable to load '/usr/share/zoneinfo/zone1970.tab' as time zone. Skipping it. 如果下游是特殊的 MySQL 环境（某种公有云 RDS 或某些 MySQL 衍生版本等），使用上述方式导入时区失败，就需要通过 sink-uri 中的 time-zone 参数指定下游的 MySQL 时区。可以首先在 MySQL 中查询其使用的时区： show variables like '%time_zone%'; +------------------+--------+ | Variable_name | Value | +------------------+--------+ | system_time_zone | CST | | time_zone | SYSTEM | +------------------+--------+ 然后在创建同步任务和创建 TiCDC 服务时使用该时区： cdc cli changefeed create --sink-uri=\"mysql://root@127.0.0.1:3306/?time-zone=CST\" --server=http://127.0.0.1:8300 注意 CST 可能是以下四个不同时区的缩写： 美国中部时间：Central Standard Time (USA) UT-6:00 澳大利亚中部时间：Central Standard Time (Australia) UT+9:30 中国标准时间：China Standard Time UT+8:00 古巴标准时间：Cuba Standard Time UT-4:00 在中国，CST 通常表示中国标准时间，使用时请注意甄别。 参考： https://docs.pingcap.com/zh/tidb/v6.1/manage-ticdc#%E8%87%AA%E5%AE%9A%E4%B9%89-kafka-sink-%E7%9A%84-topic-%E5%92%8C-partition-%E7%9A%84%E5%88%86%E5%8F%91%E8%A7%84%E5%88%99 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-08-11 11:16:52 "},"origin/bigdata-migrate-operation.html":{"url":"origin/bigdata-migrate-operation.html","title":"大数据量的迁移方案对比","keywords":"","body":"大数据的数据迁移 一、简介 迁移全量MySQL数据到TIDB。情况如下： 源库：RDS备份文件启动的MySQL实例，一个DB，数据量大约800张表，数据大小500+GB 目标库：TIDB集群 将源库中的数据导入TiDB 二、方案 1、Navicat的数据传输工具 直接使用Navicat的数据传输工具，配置数据源连接和目标源连接。 2、TiDB生态圈工具 TiDB Dumping导出：导出源MySQL中的数据为SQL文件 修改SQL文件命名：修改TiDB Dumping导出的SQL文件命名格式 TiDB Lighting导入：将SQL文件导入到TiDB TiDB Dumpling version=v4.0.5 && \\ curl -# https://download.pingcap.org/tidb-toolkit-$version-linux-amd64.tar.gz | tar -zxC /opt && \\ ln -s /opt/tidb-toolkit-$version-linux-amd64 /opt/tidb-toolkit-$version && \\ echo \"export PATH=/opt/tidb-toolkit-$version/bin:$PATH\" >> /etc/profile && \\ mkdir -p /data/dumping-export/sql && \\ source /etc/profile && \\ nohup dumpling \\ -u 用于导出数据的用户 \\ # 用于导出数据的用户要拥有SELECT、RELOAD、LOCK TABLES、REPLICATION CLIENT服务器权限 -p 用于导出数据的用户密码 \\ -P 3306 \\ -h 192.168.1.4 \\ -B database \\ # 指定要导出的Database --filetype sql \\ # 指定导出文件类型（可为csv/sql） --threads 32 \\ # 指定备份并发线程数 -o /data/dumping-export/sql \\ # 指定导出文件存储路径 -F 256MiB \\ # 指定导出文件最大大小 --logfile /data/dumping-export/export-task.log >/data/dumping-export/dumpling-nohupout.log 2>&1 & 批量修改SQL文件 # 例如源库DB为Test，想把数据导入到目标库Test-2中 old_database_name=test new_database_name=Test-2 for i in $(ls /data/dumping-export/sql/*.sql | grep -v schema-create );do mv /data/dumping-export/sql/$i /data/dumping-export/sql/$new_database_name.${i#*.}; done mv /data/dumping-export/sql/${old_database_name}-schema-create.sql /data/dumping-export/sql/${new_database_name}-schema-create.sql echo \"\" > ${new_database_name}-schema-create.sql TiDB Lighting nohup /opt/tidb-toolkit-v4.0.5-linux-amd64/bin/tidb-lightning \\ -config /data/dumping-export/tidb-lightning.toml \\ --log-file /data/dumping-export/import-task.log > /data/dumping-export/lightning-nohupout.log 2>&1 & nohup tidb-lightning \\ -L info \\ -log-file /data/dumping-export/import-task.log \\ -backend tidb \\ -status-addr 10080 \\ -d /data/dumping-export/sql \\ -tidb-host 192.168.1.4 \\ -tidb-port 4000 \\ -tidb-user root \\ -tidb-password ***** > /data/dumping-export/lightning-nohupout.log 2>&1 & 三、结论 大约三千万条记录的表，Navicat数据传输工具同步完耗时约3个小时，而使用TiDB生态圈的工具耗时26分钟 四、其他 1、查询DB下所有表的行数 由于从INFORMATION_SCHEMA.TABLES中显示的表的行数不准确，需要使用count函数来统计表的行数 SELECT CONCAT( 'SELECT \"', TABLE_NAME, '\", COUNT(*) FROM ', TABLE_SCHEMA, '.', TABLE_NAME, ' UNION ALL' ) EXEC_SQL FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'DB名字'; 上述SQL会输出用于统计表的SQL语句，复制以后，删除最后一行末尾的UNION ALL，然后执行 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/tidb-management.html":{"url":"origin/tidb-management.html","title":"TiDB管理","keywords":"","body":"TiDB管理 一、系统表 mysql 库里存储的是 TiDB 系统表 1、权限系统表 user：用户账户，全局权限 user 表的权限是全局的，并且不管默认数据库是哪一个。比如 user 里面有 DELETE 权限，任何一行，任何的表，任何的数据库。 User+Host 可能会匹配 user 表里面多行，为了处理这种情况，user 表的行是排序过的，客户端连接时会依次去匹配，并使用首次匹配到的那一行做权限验证。排序是按 Host 在前，User 在后。 db：数据库级别的权限 db表里面，User 为空是匹配匿名用户，User 里面不能有通配符。Host 和 Db 列里面可以有 % 和 _，可以模式匹配。 tables_priv：表级别的权限 tables_priv 和 columns_priv 中使用 % 是类似的，但是在Db, Table_name, Column_name 这些列不能包含 %。加载进来时排序也是类似的。 columns_priv：列级别的权限，当前暂不支持 权限生效时机 TiDB 启动时，将一些权限检查的表加载到内存，之后使用缓存的数据来验证权限。系统会周期性的将授权表从数据库同步到缓存，生效则是由同步的周期决定，目前这个值设定的是 5 分钟。 如果授权表已被直接修改，则不会通知 TiDB 节点更新缓存，如果需要立即生效，可以手动执行FLUSH PRIVILEGES; 2、服务端帮助信息系统表 help_topic 目前为空 3、统计信息相关系统表 stats_buckets 统计信息的桶 stats_histograms 统计信息的直方图 stats_meta 表的元信息，比如总行数和修改数 analyze_jobs 正在执行的统计信息收集任务以及过去 7 天内的历史任务记录 4、GC Worker 相关系统表 gc_delete_range 5、其它系统表 GLOBAL_VARIABLES 全局系统变量表 tidb 用于 TiDB 在 bootstrap 的时候记录相关版本信息 二、用户管理 1、用户管理 -- 创建用户 CREATE USER `testuser`@`192.168.1.1` IDENTIFIED BY '密码'; -- 给用户授权 GRANT Select,UPDATE,INSERT,CREATE,DELETE ON `test`.* TO `testuser`@`192.168.1.1`; -- 查看用户权限 show grants for 'test'@'192.168.1.1'; -- 回收用户权限 REVOKE ALL PRIVILEGES ON `test`.* FROM 'testuser'@'192.168.1.1'; -- 删除用户 DROP USER 'testuser'@'192.168.1.1'; 2、创建Dashboard访问用户 Dashboard访问用户所需权限 当所连接的 TiDB 服务器未启用安全增强模式 (SEM) 时，要访问 TiDB Dashboard，SQL 用户应当拥有以下所有权限： PROCESS SHOW DATABASES CONFIG DASHBOARD_CLIENT 无IP限制，创建时使用%不限制用户登录IP CREATE USER 'dashboardAdmin'@'%' IDENTIFIED BY ''; GRANT PROCESS, CONFIG ON *.* TO 'dashboardAdmin'@'%'; GRANT SHOW DATABASES ON *.* TO 'dashboardAdmin'@'%'; GRANT DASHBOARD_CLIENT ON *.* TO 'dashboardAdmin'@'%'; 当所连接的 TiDB 服务器启用了安全增强模式 (SEM) 时，要访问 TiDB Dashboard，SQL 用户应当拥有以下所有权限： PROCESS SHOW DATABASES CONFIG DASHBOARD_CLIENT RESTRICTED_TABLES_ADMIN RESTRICTED_STATUS_ADMIN RESTRICTED_VARIABLES_ADMIN 无IP限制，创建时使用%不限制用户登录IP CREATE USER 'dashboardAdmin'@'%' IDENTIFIED BY ''; GRANT PROCESS, CONFIG ON *.* TO 'dashboardAdmin'@'%'; GRANT SHOW DATABASES ON *.* TO 'dashboardAdmin'@'%'; GRANT DASHBOARD_CLIENT ON *.* TO 'dashboardAdmin'@'%'; GRANT RESTRICTED_STATUS_ADMIN ON *.* TO 'dashboardAdmin'@'%'; GRANT RESTRICTED_TABLES_ADMIN ON *.* TO 'dashboardAdmin'@'%'; GRANT RESTRICTED_VARIABLES_ADMIN ON *.* TO 'dashboardAdmin'@'%'; 若希望 SQL 用户在登录 TiDB Dashboard 后允许修改界面上的各项配置，SQL 用户还应当拥有以下权限： SYSTEM_VARIABLES_ADMIN -- 如果要使自定义的 SQL 用户能修改 TiDB Dashboard 界面上的各项配置，可以增加以下权限 GRANT SYSTEM_VARIABLES_ADMIN ON *.* TO 'dashboardAdmin'@'%'; 三、慢SQL TiDB 会将执行时间超过 slow-threshold（默认值为 300 毫秒）的语句输出到 slow-query-file（默认值：\"tidb-slow.log\"）日志文件中，用于帮助用户定位慢查询语句，分析和解决 SQL 执行的性能问题。 TiDB 默认启用慢查询日志，可以修改配置 enable-slow-log 来启用或禁用它。 Slow Query 基础信息 Time：表示日志打印时间。 Query_time：表示执行这个语句花费的时间。 Parse_time：表示这个语句在语法解析阶段花费的时间。 Compile_time：表示这个语句在查询优化阶段花费的时间。 Query：表示 SQL 语句。慢日志里面不会打印 Query，但映射到内存表后，对应的字段叫 Query。 Digest：表示 SQL 语句的指纹。 Txn_start_ts：表示事务的开始时间戳，也是事务的唯一 ID，可以用这个值在 TiDB 日志中查找事务相关的其他日志。 Is_internal：表示是否为 TiDB 内部的 SQL 语句。true 表示 TiDB 系统内部执行的 SQL 语句，false 表示用户执行的 SQL 语句。 Index_ids：表示语句涉及到的索引的 ID。 Succ：表示语句是否执行成功。 Backoff_time：表示语句遇到需要重试的错误时在重试前等待的时间，常见的需要重试的错误有以下几种：遇到了 lock、Region 分裂、tikv server is busy。 Plan：表示语句的执行计划，用 select tidb_decode_plan('xxx...') SQL 语句可以解析出具体的执行计划。 Prepared：表示这个语句是否是 Prepare 或 Execute 的请求。 Plan_from_cache：表示这个语句是否命中了执行计划缓存。 Rewrite_time：表示这个语句在查询改写阶段花费的时间。 Preproc_subqueries：表示这个语句中被提前执行的子查询个数，如 where id in (select if from t) 这个子查询就可能被提前执行。 Preproc_subqueries_time：表示这个语句中被提前执行的子查询耗时。 Exec_retry_count：表示这个语句执行的重试次数。一般出现在悲观事务中，上锁失败时重试执行该语句。 Exec_retry_time：表示这个语句的重试执行时间。例如某个查询一共执行了三次（前两次失败），则 Exec_retry_time 表示前两次的执行时间之和，Query_time 减去 Exec_retry_time 则为最后一次执行时间。 和事务执行相关的字段 Prewrite_time：表示事务两阶段提交中第一阶段（prewrite 阶段）的耗时。 Commit_time：表示事务两阶段提交中第二阶段（commit 阶段）的耗时。 Get_commit_ts_time：表示事务两阶段提交中第二阶段（commit 阶段）获取 commit 时间戳的耗时。 Local_latch_wait_time：表示事务两阶段提交中第二阶段（commit 阶段）发起前在 TiDB 侧等锁的耗时。 Write_keys：表示该事务向 TiKV 的 Write CF 写入 Key 的数量。 Write_size：表示事务提交时写 key 或 value 的总大小。 Prewrite_region：表示事务两阶段提交中第一阶段（prewrite 阶段）涉及的 TiKV Region 数量。每个 Region 会触发一次远程过程调用。 和内存使用相关的字段 Mem_max：表示执行期间 TiDB 使用的最大内存空间，单位为 byte。 和硬盘使用相关的字段 Disk_max: 表示执行期间 TiDB 使用的最大硬盘空间，单位为 byte。 和 SQL 执行的用户相关的字段 User：表示执行语句的用户名。 Conn_ID：表示用户的链接 ID，可以用类似 con:3 的关键字在 TiDB 日志中查找该链接相关的其他日志。 DB：表示执行语句时使用的 database。 和 TiKV Coprocessor Task 相关的字段 Request_count：表示这个语句发送的 Coprocessor 请求的数量。 Total_keys：表示 Coprocessor 扫过的 key 的数量。 Process_time：执行 SQL 在 TiKV 的处理时间之和，因为数据会并行的发到 TiKV 执行，这个值可能会超过 Query_time。 Wait_time：表示这个语句在 TiKV 的等待时间之和，因为 TiKV 的 Coprocessor 线程数是有限的，当所有的 Coprocessor 线程都在工作的时候，请求会排队；当队列中有某些请求耗时很长的时候，后面的请求的等待时间都会增加。 Process_keys：表示 Coprocessor 处理的 key 的数量。相比 total_keys，processed_keys 不包含 MVCC 的旧版本。如果 processed_keys 和 total_keys 相差很大，说明旧版本比较多。 Cop_proc_avg：cop-task 的平均执行时间。 Cop_proc_p90：cop-task 的 P90 分位执行时间。 Cop_proc_max：cop-task 的最大执行时间。 Cop_proc_addr：执行时间最长的 cop-task 所在地址。 Cop_wait_avg：cop-task 的平均等待时间。 Cop_wait_p90：cop-task 的 P90 分位等待时间。 Cop_wait_max：cop-task 的最大等待时间。 Cop_wait_addr：等待时间最长的 cop-task 所在地址。 Cop_backoff_{backoff-type}_total_times：因某种错误造成的 backoff 总次数。 Cop_backoff_{backoff-type}_total_time：因某种错误造成的 backoff 总时间。 Cop_backoff_{backoff-type}_max_time：因某种错误造成的最大 backoff 时间。 Cop_backoff_{backoff-type}_max_addr：因某种错误造成的最大 backoff 时间的 cop-task 地址。 Cop_backoff_{backoff-type}_avg_time：因某种错误造成的平均 backoff 时间。 Cop_backoff_{backoff-type}_p90_time：因某种错误造成的 P90 分位 backoff 时间。 相关系统变量 tidb_slow_log_threshold：设置慢日志的阈值，执行时间超过阈值的 SQL 语句将被记录到慢日志中。默认值是 300 ms。 tidb_query_log_max_len：设置慢日志记录 SQL 语句的最大长度。默认值是 4096 byte。 tidb_redact_log：设置慢日志记录 SQL 时是否将用户数据脱敏用 ? 代替。默认值是 0 ，即关闭该功能。 tidb_enable_collect_execution_info：设置是否记录执行计划中各个算子的物理执行信息，默认值是 1。该功能对性能的影响约为 3%。 统计慢SQL SELECT DB AS '数据库', `Query` AS 'SQL语句', Query_time AS '执行耗时Query_time', Time AS '执行时间', `User` AS '执行用户', `Host` AS '主机', Total_keys AS '扫描总Key个数Total_keys', Index_names AS '涉及索引名' FROM INFORMATION_SCHEMA.SLOW_QUERY WHERE DB = '数据库名' ORDER BY Query_time DESC LIMIT 100; 四、TIDB集群安全相关 1、日志脱敏 ①TiDB 组件日志脱敏 ②TiKV 组件日志脱敏 ③PD 组件日志脱敏 ④TiFlash 组件日志脱敏 https://docs.pingcap.com/zh/tidb/stable/log-redaction 2、关闭发送遥测数据 ①TiDB禁用遥测 动态禁用 # 修改系统全局变量tidb_enable_telemetry动态禁用 TiDB 遥测功能： SET GLOBAL tidb_enable_telemetry = 0; 配置文件的禁用优先级高于全局变量。若通过配置文件禁用了遥测功能，则全局变量的配置将不起作用，遥测功能总是处于关闭状态。 集群级别禁用 修改集群配置文件 server_configs: tidb: enable-telemetry: false 重启集群TiDB组件 ②TiDB Dashboard禁用遥测 修改集群配置文件 server_configs: pd: dashboard.enable-telemetry: false 重启集群PD组件 ③TiUP禁用遥测 tiup telemetry disable # 禁用之后~/.tiup/telemetry/meta.yaml文件中的status字段会显示Disable ④TiSpark禁用遥测 在 Spark 配置文件设置 spark.tispark.telemetry.enable = false 来禁用 TiSpark 的遥测功能。 https://docs.pingcap.com/zh/tidb/stable/telemetry 五、下载的性能测试数据展示 在TIDB Dashboard中做的性能测试，数组只保存几天。可下载下来进行长期保存。如果要再次以Web形式展示，使用以下命令 go tool pprof --http=0.0.0.0:1234 cpu_xxx.proto 六、Kill进程 语法：KILL TIDB? ( 'CONNECTION' | 'QUERY' )? CONNECTION_ID SELECT ID, USER, INSTANCE, MEM, TIME, INFO FROM INFORMATION_SCHEMA.CLUSTER_PROCESSLIST order by TIME desc; +---------------------+------+-----------------+-----------------------------------------------------------------------------+ | ID | USER | INSTANCE | INFO | +---------------------+------+-----------------+-----------------------------------------------------------------------------+ | 8306449708033769879 | root | 127.0.0.1:10082 | select sleep(30), 'foo' | | 5857102839209263511 | root | 127.0.0.1:10080 | select sleep(50) | | 5857102839209263513 | root | 127.0.0.1:10080 | SELECT ID, USER, INSTANCE, INFO FROM INFORMATION_SCHEMA.CLUSTER_PROCESSLIST | +---------------------+------+-----------------+-----------------------------------------------------------------------------+ KILL 5857102839209263513; MySQL 兼容性 MySQL 的 KILL 语句仅能终止当前连接的 MySQL 实例上的连接，TiDB 的 KILL 语句能终止整个集群中任意一个 TiDB 实例上的连接。 暂时不支持使用 MySQL 命令行 ctrl+c 终止查询或连接。 行为变更说明 TiDB 从 v6.1.0 起新增 Global Kill 功能（由 enable-global-kill 配置项控制，默认启用）。启用 Global Kill 功能时，KILL 语句和 KILL TIDB 语句均能跨节点终止查询或连接，且无需担心错误地终止其他查询或连接。当你使用客户端连接到任何一个 TiDB 节点执行 KILL 语句或 KILL TIDB 语句时，该语句会被转发给对应的 TiDB 节点。当客户端和 TiDB 中间有代理时，KILL 及 KILL TIDB 语句也会被转发给对应的 TiDB 节点执行。 对于 TiDB v6.1.0 之前的版本，或未启用 Global Kill 功能时： KILL 语句与 MySQL 不兼容，负载均衡器后面通常放有多个 TiDB 服务器，这种不兼容有助于防止在错误的 TiDB 服务器上终止连接。你需要显式地增加 TIDB 后缀，通过执行 KILL TIDB 语句来终止当前连接的 TiDB 实例上的其他连接。 强烈不建议在配置文件里设置 compatible-kill-query = true，除非你确定客户端将始终连接到同一个 TiDB 节点。这是因为当你在默认的 MySQL 客户端按下 ctrl+c 时，客户端会开启一个新连接，并在这个新连接中执行 KILL 语句。此时，如果客户端和 TiDB 中间有代理，新连接可能会被路由到其他的 TiDB 节点，从而错误地终止其他会话。 KILL TIDB 语句是 TiDB 的扩展语法，其功能与 MySQL 命令 KILL [CONNECTION|QUERY] 和 MySQL 命令行 ctrl+c 相同。在同一个 TiDB 节点上，你可以安全地使用 KILL TIDB 语句。 https://docs.pingcap.com/zh/tidb/stable/sql-statement-kill#kill 七、忘记 root 密码 修改 TiDB 配置文件： 登录其中一台 tidb-server 实例所在的机器。 进入 TiDB 节点的部署目录下的 conf 目录，找到 tidb.toml 配置文件。在配置文件的 security 部分添加配置项 skip-grant-table。如无 security 部分，则将以下两行内容添加至 tidb.toml 配置文件尾部： [security] skip-grant-table = true 终止该 tidb-server 的进程： 找到 tidb-server 对应的进程 ID (PID) 并使用 kill 命令停掉该进程： ps aux | grep tidb-server kill -9 使用修改之后的配置启动 TiDB： 注意：设置 skip-grant-table 之后，启动 TiDB 进程会增加操作系统用户检查，只有操作系统的 root 用户才能启动 TiDB 进程。 进入 TiDB 节点部署目录下的 scripts 目录。 切换到操作系统 root 账号。 在前台执行目录中的 run_tidb.sh 脚本。 在新的终端窗口中使用 root 登录后修改密码： mysql -h 127.0.0.1 -P 4000 -u root 停止运行 run_tidb.sh 脚本，并去掉第 1 步中在 TiDB 配置文件中添加的内容，等待 tidb-server 自启动。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:31:42 "},"origin/tidb-performance.html":{"url":"origin/tidb-performance.html","title":"TiDB性能调优","keywords":"","body":"TiDB 性能调优 一、TiDB 中的各种超时 1. GC 超时 TiDB 的事务的实现采用了 MVCC（多版本并发控制）机制，当新写入的数据覆盖旧的数据时，旧的数据不会被替换掉，而是与新写入的数据同时保留，并以时间戳来区分版本。TiDB 通过定期 GC 的机制来清理不再需要的旧数据。 默认配置下 TiDB 可以保障每个 MVCC 版本（一致性快照）保存 10 分钟，读取时间超过 10 分钟的事务，会收到报错 GC life time is shorter than transaction duration。 当用户确信自己需要更长的读取时间时，比如在使用了 Mydumper 做全量备份的场景中（Mydumper 备份的是一致性的快照），可以通过调整 TiDB 中mysql.tidb 表中的 tikv_gc_life_time 的值来调大 MVCC 版本保留时间，需要注意的是 tikv_gc_life_time 的配置是立刻影响全局的，调大它会为当前所有存在的快照增加生命时长，调小它会立即缩短所有快照的生命时长。过多的 MVCC 版本会拖慢 TiKV 的处理效率，在使用 Mydumper 做完全量备份后需要及时把 tikv_gc_life_time 调整回之前的设置。 更多关于 GC 的信息，请参考 GC 机制简介文档。 2. 事务超时 垃圾回收 (GC) 不会影响到正在执行的事务。但悲观事务的运行仍有上限，有基于事务超时的限制（TiDB 配置文件 [performance] 类别下的 max-txn-ttl 修改，默认为 60 分钟）和 基于事务使用内存的限制。 形如 INSERT INTO t10 SELECT * FROM t1 的 SQL 语句，不会受到 GC 的影响，但超过了 max-txn-ttl 的时间后，会由于超时而回滚。 3. SQL 执行时间超时 TiDB 还提供了一个系统变量来限制单条 SQL 语句的执行时间：max_execution_time，它的默认值为 0，表示无限制。max_execution_time 目前对所有类型的 statement 生效，并非只对 SELECT 语句生效。其单位为 ms，但实际精度在 100ms 级别，而非更准确的毫秒级别。 4. JDBC 查询超时 MySQL jdbc 的查询超时设置 setQueryTimeout() 对 TiDB 不起作用。这是因为现实客户端感知超时时，向数据库发送一个 KILL 命令。但是由于 tidb-server 是负载均衡的， 为防止在错误的 tidb-server 上终止连接， tidb-server 不会执行这个 KILL。这时就要用 MAX_EXECUTION_TIME 实现查询超时的效果。 TiDB 提供了三个与 MySQL 兼容的超时控制参数： wait_timeout，控制与 Java 应用连接的非交互式空闲超时时间。在 TiDB v5.4 及以上版本中，默认值为 28800 秒，即空闲超时为 8 小时。在 v5.4 之前，默认值为 0，即没有时间限制。 interactive_timeout，控制与 Java 应用连接的交互式空闲超时时间，默认值为 8 小时。 max_execution_time，控制连接中 SQL 执行的超时时间，默认值是 0，即允许连接无限忙碌（一个 SQL 语句执行无限的长的时间）。 但在实际生产环境中，空闲连接和一直无限执行的 SQL 对数据库和应用都有不好的影响。你可以通过在应用的连接字符串中配置这两个 session 级的变量来避免空闲连接和执行时间过长的 SQL 语句。例如，设置 sessionVariables=wait_timeout=3600（1 小时）和 sessionVariables=max_execution_time=300000（5 分钟）。 参考 https://docs.pingcap.com/zh/tidb/dev/performance-tuning-overview https://docs.pingcap.com/zh/tidb/dev/dev-guide-timeouts-in-tidb Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-11-10 09:28:52 "},"origin/tidb-tiflash.html":{"url":"origin/tidb-tiflash.html","title":"TiDB TiFlash","keywords":"","body":"TiDB TiFlash 一、简介 TiFlash 是 TiDB HTAP 形态的关键组件，它是 TiKV 的列存扩展，在提供了良好的隔离性的同时，也兼顾了强一致性。列存副本通过 Raft Learner 协议异步复制，但是在读取的时候通过 Raft 校对索引配合 MVCC 的方式获得 Snapshot Isolation 的一致性隔离级别。这个架构很好地解决了 HTAP 场景的隔离性以及列存同步的问题。 TiFlash架构 TiFlash 提供列式存储，且拥有借助 ClickHouse 高效实现的协处理器层。除此以外，它与 TiKV 非常类似，依赖同样的 Multi-Raft 体系，以 Region 为单位进行数据复制和分散。 TiFlash 以低消耗不阻塞 TiKV 写入的方式，实时复制 TiKV 集群中的数据，并同时提供与 TiKV 一样的一致性读取，且可以保证读取到最新的数据。TiFlash 中的 Region 副本与 TiKV 中完全对应，且会跟随 TiKV 中的 Leader 副本同时进行分裂与合并。 TiFlash 可以兼容 TiDB 与 TiSpark，用户可以选择使用不同的计算引擎。 TiFlash 推荐使用和 TiKV 不同的节点以做到 Workload 隔离，但在无业务隔离的前提下，也可以选择与 TiKV 同节点部署。 TiFlash 暂时无法直接接受数据写入，任何数据必须先写入 TiKV 再同步到 TiFlash。TiFlash 以 learner 角色接入 TiDB 集群，TiFlash 支持表粒度的数据同步，部署后默认情况下不会同步任何数据，需要按照第三章手动完成指定表的数据同步。 TiFlash 主要包含三个组件，除了主要的存储引擎组件，另外包含 tiflash proxy 和 pd buddy 组件，其中 tiflash proxy 主要用于处理 Multi-Raft 协议通信的相关工作，pd buddy 负责与 PD 协同工作，将 TiKV 数据按表同步到 TiFlash。 对于按表构建 TiFlash 副本的流程，TiDB 接收到相应的 DDL 命令后 pd buddy 组件会通过 TiDB 的 status 端口获取到需要同步的数据表信息，然后会将需要同步的数据信息发送到 PD，PD 根据该信息进行相关的数据调度。 TiFlash特性 异步复制 TiFlash 中的副本以特殊角色 (Raft Learner) 进行异步的数据复制。这表示当 TiFlash 节点宕机或者网络高延迟等状况发生时，TiKV 的业务仍然能确保正常进行。 这套复制机制也继承了 TiKV 体系的自动负载均衡和高可用：并不用依赖附加的复制管道，而是直接以多对多方式接收 TiKV 的数据传输；且只要 TiKV 中数据不丢失，就可以随时恢复 TiFlash 的副本。 一致性 TiFlash 提供与 TiKV 一样的快照隔离支持，且保证读取数据最新（确保之前写入的数据能被读取）。这个一致性是通过对数据进行复制进度校验做到的。 每次收到读取请求，TiFlash 中的 Region 副本会向 Leader 副本发起进度校对（一个非常轻的 RPC 请求），只有当进度确保至少所包含读取请求时间戳所覆盖的数据之后才响应读取。 智能选择 TiDB 可以自动选择使用 TiFlash 列存或者 TiKV 行存，甚至在同一查询内混合使用提供最佳查询速度。这个选择机制与 TiDB 选取不同索引提供查询类似：根据统计信息判断读取代价并作出合理选择。 计算加速 TiFlash 对 TiDB 的计算加速分为两部分：列存本身的读取效率提升以及为 TiDB 分担计算。其中分担计算的原理和 TiKV 的协处理器一致：TiDB 会将可以由存储层分担的计算下推。能否下推取决于 TiFlash 是否可以支持相关下推。具体介绍请参阅“TiFlash 支持的计算下推”一节。 二、部署 1. 添加TiFlash节点配置 编写 tiflash-scale-out.yaml 文件，添加该 TiFlash 节点信息（目前只支持 ip，不支持域名）： tiflash_servers: - host: 10.0.1.4 config: logger.level: info TiFlash节点部署配置参数 配置 含义 host 指定部署到哪台机器，字段值填 IP 地址，不可省略 ssh_port 指定连接目标机器进行操作的时候使用的 SSH 端口，若不指定，则使用 global 区块中的 ssh_port tcp_port TiFlash TCP 服务的端口，默认 9000 http_port TiFlash HTTP 服务的端口，默认 8123 flash_service_port TiFlash 提供服务的端口，TiDB 通过该端口从 TiFlash 读数据，默认 3930 metrics_port TiFlash 的状态端口，用于输出 metric 数据，默认 8234 flash_proxy_port 内置 TiKV 的端口，默认 20170 flash_proxy_status_port 内置 TiKV 的状态端口，默认为 20292 deploy_dir 指定部署目录，若不指定，或指定为相对目录，则按照 global 中配置的 deploy_dir 生成 data_dir 指定数据目录，若不指定，或指定为相对目录，则按照 global 中配置的 data_dir 生成，TiFlash 的数据目录支持多个，采用逗号分割 log_dir 指定日志目录，若不指定，或指定为相对目录，则按照 global 中配置的 log_dir 生成 tmp_path: TiFlash 临时文件的存放路径，默认使用 [path 或者 storage.latest.dir 的第一个目录] + \"/tmp\" numa_node 为该实例分配 NUMA 策略，如果指定了该参数，需要确保目标机装了 numactl，在指定该参数的情况下会通过 numactl 分配 cpubind 和 membind 策略。该字段参数为 string 类型，字段值填 NUMA 节点的 ID，例如 \"0,1\" config 该字段配置规则和 server_configs 里的 tiflash 配置规则相同，若配置了该字段，会将该字段内容和 server_configs 里的 tiflash 内容合并（若字段重叠，以本字段内容为准），然后生成配置文件并下发到 host 指定的机器 learner_config 每个 TiFlash 中内置了一个特殊的 TiKV 模块，该配置项用于配置这个特殊的 TiKV 模块，一般不建议修改这个配置项下的内容 os host 字段所指定的机器的操作系统，若不指定该字段，则默认为 global 中的 os arch host 字段所指定的机器的架构，若不指定该字段，则默认为 global 中的 arch resource_control 针对该服务的资源控制，如果配置了该字段，会将该字段和 global 中的 resource_control 内容合并（若字段重叠，以本字段内容为准），然后生成 systemd 配置文件并下发到 host 指定机器。resource_control 的配置规则同 global 中的 resource_control 部署完成之后不能再修改的参数：host、tcp_port、http_port、flash_service_port、flash_proxy_port、flash_proxy_status_port、metrics_port、deploy_dir、log_dir、tmp_path、arch、os 2. 扩容TiFlash节点 tiup cluster scale-out scale-out.yaml # 需要当前执行命令的用户和新增的机器打通了互信，如果不满足已打通互信的条件，需要通过 `-p` 来输入新机器的密码，或通过 `-i` 指定私钥文件。 3. 查看集群TiFlash状态 tiup cluster display 4. 缩容TiFlash节点 ①调整数据表的副本数 在下线节点之前，确保 TiFlash 集群剩余节点数大于等于所有数据表的最大副本数，否则需要修改相关表的 TiFlash 副本数。 在 TiDB 客户端中针对所有副本数大于集群剩余 TiFlash 节点数的表执行： alter table . set tiflash replica 0; 等待相关表的 TiFlash 副本被删除 ②缩容TiFlash节点 tiup cluster scale-in --node 10.0.1.4:9000 # 在缩容集群时，对于某些组件，并不会立即停止服务并删除数据，而是需要等数据调度完成之后，用户手动执行 tiup cluster prune 命令清理。 tiup cluster prune ③手动缩容 TiFlash 节点 在特殊情况下（比如需要强制下线节点），或者 TiUP 操作失败的情况下，可以使用以下方法手动下线 TiFlash 节点。 使用 pd-ctl 的 store 命令在 PD 中查看该 TiFlash 节点对应的 store id。 tiup ctl: pd -u http://: store # 如果集群中有多个 PD 实例，只需在以上命令中指定一个活跃 PD 实例的IP:端口即可。 在 pd-ctl 中下线该 TiFlash 节点。 tiup ctl: pd -u http://: store delete # 为上一步查到的该 TiFlash 节点对应的 store id。 # 如果集群中有多个 PD 实例，只需在以上命令中指定一个活跃 PD 实例的IP:端口即可。 等待该 TiFlash 节点对应的 store 消失或者 state_name 变成 Tombstone 再关闭 TiFlash 进程。 手动删除 TiFlash 的数据文件，具体位置可查看在集群拓扑配置文件中 TiFlash 配置部分下的 data_dir 目录。 从 TiUP 拓扑信息中删除已经下线的 TiFlash 节点信息： tiup cluster scale-in --node : --force 注意 如果在集群中所有的 TiFlash 节点停止运行之前，没有取消所有同步到 TiFlash 的表，则需要手动在 PD 中清除同步规则，否则无法成功完成 TiFlash 节点的下线。 查询当前 PD 实例中所有与 TiFlash 相关的数据同步规则。 curl http://:/pd/api/v1/config/rules/group/tiflash [ { \"group_id\": \"tiflash\", \"id\": \"table-45-r\", \"override\": true, \"start_key\": \"7480000000000000FF2D5F720000000000FA\", \"end_key\": \"7480000000000000FF2E00000000000000F8\", \"role\": \"learner\", \"count\": 1, \"label_constraints\": [ { \"key\": \"engine\", \"op\": \"in\", \"values\": [ \"tiflash\" ] } ] } ] 删除所有与 TiFlash 相关的数据同步规则。以 id 为 table-45-r 的规则为例 curl -v -X DELETE http://:/pd/api/v1/config/rule/tiflash/table-45-r 5. TiFlash 系统表 information_schema.tiflash_replica 系统表的列名及含义如下： 列名 含义 TABLE_SCHEMA 数据库名 TABLE_NAME 表名 TABLE_ID 表 ID REPLICA_COUNT TiFlash 副本数 LOCATION_LABELS 给 PD 的 hint，让 Region 的多个副本尽可能按照 LOCATION_LABELS 里的设置分散 AVAILABLE 是否可用（0/1） PROGRESS 同步进度 [0.0~1.0] 5. TiFlash重要日志 日志信息 日志含义 [INFO] [] [\"KVStore: Start to persist [region 47, applied: term 6 index 10]\"] [thread_id=23] 表示数据开始同步了 [DEBUG] [] [\"CoprocessorHandler: grpc::Status DB::CoprocessorHandler::execute(): Handling DAG request\"] [thread_id=30] 表示开始处理一个 Coprocessor 请求 [DEBUG] [] [\"CoprocessorHandler: grpc::Status DB::CoprocessorHandler::execute(): Handle DAG request done\"] [thread_id=30] 表示完成Coprocessor 请求的处理 你可以找到一个 Coprocessor 请求的开始或结束，然后通过日志前面打印的线程号找到该 Coprocessor 请求的其他相关日志。 注意：TiFlash默认配置的日志级别为Debug级别，会导致日志文件特别大，可修改配置设置级别为INFO。 # 编辑集群组件配置 tiup cluster edit-config ... tiflash_servers: - host: 10.0.1.4 ... config: logger.level: info # 重启TiFlash tiup cluster reload -R tiflash -N tiflash主机:9000 6. TiFlash 配置参数 ## TiFlash TCP/HTTP 等辅助服务的监听 host。建议配置成 0.0.0.0，即监听本机所有 IP 地址。 listen_host = \"0.0.0.0\" ## TiFlash TCP 服务的端口 tcp_port = 9000 ## TiFlash HTTP 服务的端口 http_port = 8123 ## 数据块元信息的内存 cache 大小限制，通常不需要修改 mark_cache_size = 5368709120 ## 数据块 min-max 索引的内存 cache 大小限制，通常不需要修改 minmax_index_cache_size = 5368709120 ## DeltaIndex 内存 cache 大小限制，默认为 0，代表没有限制 delta_index_cache_size = 0 ## TiFlash 数据的存储路径。如果有多个目录，以英文逗号分隔。 ## 从 v4.0.9 版本开始，不推荐使用 path 及 path_realtime_mode 参数。推荐使用 [storage] 下的配置项代替，这样在多盘部署的场景下能更好地利用节点性能。 ## 从 v5.2.0 版本开始，如果要使用配置项 storage.io_rate_limit，需要同时将 TiFlash 的数据存储路径设置为 storage.main.dir。 ## 当 [storage] 配置项存在的情况下，path 和 path_realtime_mode 两个配置会被忽略。 # path = \"/tidb-data/tiflash-9000\" ## 或 # path = \"/ssd0/tidb-data/tiflash,/ssd1/tidb-data/tiflash,/ssd2/tidb-data/tiflash\" ## 默认为 false。如果设为 true，且 path 配置了多个目录，表示在第一个目录存放最新数据，在其他目录存放较旧的数据。 # path_realtime_mode = false ## TiFlash 临时文件的存放路径。默认使用 [`path` 或者 `storage.latest.dir` 的第一个目录] + \"/tmp\" # tmp_path = \"/tidb-data/tiflash-9000/tmp\" ## 存储路径相关配置，从 v4.0.9 开始生效 [storage] ## 该参数从 v5.2.0 开始废弃，请使用 `[storage.io_rate_limit]` 相关配置 # bg_task_io_rate_limit = 0 ## DTFile 储存文件格式 ## * format_version = 1 老旧文件格式，已废弃 ## * format_version = 2 v6.0.0 以前版本的默认文件格式 ## * format_version = 3 v6.0.0 及以后版本的默认文件格式，具有更完善的检验功能 # format_version = 3 [storage.main] ## 用于存储主要的数据，该目录列表中的数据占总数据的 90% 以上。 dir = [ \"/tidb-data/tiflash-9000\" ] ## 或 # dir = [ \"/ssd0/tidb-data/tiflash\", \"/ssd1/tidb-data/tiflash\" ] ## storage.main.dir 存储目录列表中每个目录的最大可用容量。 ## * 在未定义配置项，或者列表中全填 0 时，会使用目录所在的硬盘容量 ## * 以 byte 为单位。目前不支持如 \"10GB\" 的设置 ## * capacity 列表的长度应当与 dir 列表长度保持一致 ## 例如： # capacity = [ 10737418240, 10737418240 ] [storage.latest] ## 用于存储最新的数据，大约占总数据量的 10% 以内，需要较高的 IOPS。 ## 默认情况该项可留空。在未配置或者为空列表的情况下，会使用 storage.main.dir 的值。 # dir = [ ] ## storage.latest.dir 存储目录列表中，每个目录的最大可用容量。 # capacity = [ 10737418240, 10737418240 ] ## [storage.io_rate_limit] 相关配置从 v5.2.0 开始引入。 [storage.io_rate_limit] ## 该配置项是 I/O 限流功能的开关，默认关闭。TiFlash 的 I/O 限流功能适用于磁盘带宽较小且磁盘带宽大小明确的云盘场景。 ## I/O 限流功能限制下的读写流量总带宽，单位为 Byte，默认值为 0，即默认关闭 I/O 限流功能。 # max_bytes_per_sec = 0 ## max_read_bytes_per_sec 和 max_write_bytes_per_sec 的含义和 max_bytes_per_sec 类似，分别指 I/O 限流功能限制下的读流量总带宽和写流量总带宽。 ## 分别用两个配置项控制读写带宽限制，适用于一些读写带宽限制分开计算的云盘，例如 GCP 上的 persistent disk。 ## 当 max_bytes_per_sec 配置不为 0 时，优先使用 max_bytes_per_sec。 # max_read_bytes_per_sec = 0 # max_write_bytes_per_sec = 0 ## 下面的参数用于控制不同 I/O 流量类型分配到的带宽权重，一般不需要调整。 ## TiFlash 内部将 I/O 请求分成 4 种类型：前台写、后台写、前台读、后台读。 ## I/O 限流初始化时，TiFlash 会根据下面的权重 (weight) 比例分配带宽。 ## 以下默认配置表示每一种流量将获得 25 / (25 + 25 + 25 + 25) = 25% 的权重。 ## 如果将 weight 配置为 0，则对应的 I/O 操作不会被限流。 # foreground_write_weight = 25 # background_write_weight = 25 # foreground_read_weight = 25 # background_read_weight = 25 ## TiFlash 支持根据当前的 I/O 负载情况自动调整各种 I/O 类型的限流带宽，有可能会超过设置的权重。 ## auto_tune_sec 表示自动调整的执行间隔，单位为秒。设为 0 表示关闭自动调整。 # auto_tune_sec = 5 [flash] tidb_status_addr = tidb status 端口地址 # 多个地址以逗号分割 service_addr = TiFlash raft 服务 和 coprocessor 服务监听地址 # 多个 TiFlash 节点会选一个 master 来负责往 PD 增删 placement rule，通过 flash.flash_cluster 中的参数控制。 [flash.flash_cluster] refresh_interval = master 定时刷新有效期 update_rule_interval = master 定时向 tidb 获取 TiFlash 副本状态并与 pd 交互 master_ttl = master 选出后的有效期 cluster_manager_path = pd buddy 所在目录的绝对路径 log = pd buddy log 路径 [flash.proxy] addr = proxy 监听地址，不填则默认是 127.0.0.1:20170 advertise-addr = 外部访问 addr 的地址，不填则默认是 \"addr\" data-dir = proxy 数据存储路径 config = proxy 配置文件路径 log-file = proxy log 路径 log-level = proxy log 级别，默认是 \"info\" status-addr = 拉取 proxy metrics｜status 信息的监听地址，不填则默认是 127.0.0.1:20292 advertise-status-addr = 外部访问 status-addr 的地址，不填则默认是 \"status-addr\" [logger] ## log 级别（支持 trace、debug、information、warning、error），默认是 \"debug\" level = debug log = TiFlash log 路径 errorlog = TiFlash error log 路径 ## 单个日志文件的大小，默认是 \"100M\" size = \"100M\" ## 最多保留日志文件个数，默认是 10 count = 10 [raft] pd_addr = pd 服务地址 # 多个地址以逗号隔开 [status] ## Prometheus 拉取 metrics 信息的端口，默认是 8234 metrics_port = 8234 [profiles] [profiles.default] ## 存储引擎的 segment 分裂是否使用逻辑分裂。使用逻辑分裂可以减小写放大，但是会造成一定程度的硬盘空间回收不及时。默认为 false。不建议修改默认选项。 # dt_enable_logical_split = false ## 单次 coprocessor 查询过程中，对中间数据的内存限制，单位为 byte，默认为 0，表示不限制 max_memory_usage = 0 ## 所有查询过程中，对中间数据的内存限制，单位为 byte，默认为 0，表示不限制 max_memory_usage_for_all_queries = 0 ## 从 v5.0 引入，表示 TiFlash Coprocessor 最多同时执行的 cop 请求数量。如果请求数量超过了该配置指定的值，多出的请求会排队等待。如果设为 0 或不设置，则使用默认值，即物理核数的两倍。 cop_pool_size = 0 ## 从 v5.0 引入，表示 TiFlash Coprocessor 最多同时执行的 batch 请求数量。如果请求数量超过了该配置指定的值，多出的请求会排队等待。如果设为 0 或不设置，则使用默认值，即物理核数的两倍。 batch_cop_pool_size = 0 ## 从 v6.1 引入，指定 TiFlash 执行来自 TiDB 的 ALTER TABLE ... COMPACT 请求时，能同时并行处理的请求数量。 ## 如果这个值没有设置或设为了 0，则会采用默认值（1）。 manual_compact_pool_size = 1 ## 从 v5.4.0 引入，表示是否启用弹性线程池，这项功能可以显著提高 TiFlash 在高并发场景的 CPU 利用率。默认为 true。 # enable_elastic_threadpool = true # TiFlash 存储引擎的压缩算法，支持 LZ4、zstd 和 LZ4HC，大小写不敏感。默认使用 LZ4 算法。 dt_compression_method = \"LZ4\" ## TiFlash 存储引擎的压缩级别，默认为 1。 ## 如果 dt_compression_method 设置为 LZ4，推荐将该值设为 1； ## 如果 dt_compression_method 设置为 zstd ，推荐将该值设为 -1 或 1，设置为 -1 的压缩率更小，但是读性能会更好； ## 如果 dt_compression_method 设置为 LZ4HC，推荐将该值设为 9。 dt_compression_level = 1 ## 安全相关配置，从 v4.0.5 开始生效 [security] ## 从 v5.0 引入，控制是否开启日志脱敏 ## 若开启该选项，日志中的用户数据会以 `?` 代替显示 ## 注意，tiflash-learner 对应的安全配置选项为 `security.redact-info-log`，需要在tiflash-learner.toml中另外开启 # redact_info_log = false ## 包含可信 SSL CA 列表的文件路径。如果你设置了该值，`cert_path` 和 `key_path` 中的路径也需要填写 # ca_path = \"/path/to/ca.pem\" ## 包含 PEM 格式的 X509 certificate 文件路径 # cert_path = \"/path/to/tiflash-server.pem\" ## 包含 PEM 格式的 X509 key 文件路径 # key_path = \"/path/to/tiflash-server-key.pem\" 7. TiDB参数调优 对于 OLAP/TiFlash 专属的 TiDB 节点，建议调大读取并发数 tidb_distsql_scan_concurrency 到 80 set @@tidb_distsql_scan_concurrency = 80; 开启 Super batch 功能 tidb_allow_batch_cop 变量用来设置从 TiFlash 读取时，是否把 Region 的请求进行合并。当查询中涉及的 Region 数量比较大，可以尝试设置该变量为 1（对带 aggregation 下推到 TiFlash Coprocessor 的请求生效），或设置该变量为 2（对全部下推到 TiFlash Coprocessor 请求生效）。 set @@tidb_allow_batch_cop = 1; 尝试开启聚合推过 Join / Union 等 TiDB 算子的优化 tidb_opt_agg_push_down 变量用来设置优化器是否执行聚合函数下推到 Join 之前的优化操作。当查询中聚合操作执行很慢时，可以尝试设置该变量为 1。 set @@tidb_opt_agg_push_down = 1; 尝试开启 Distinct 推过 Join / Union 等 TiDB 算子的优化 tidb_opt_distinct_agg_push_down 变量用来设置优化器是否执行带有 Distinct 的聚合函数（比如 select count(distinct a) from t）下推到 Coprocessor 的优化操作。当查询中带有 Distinct 的聚合操作执行很慢时，可以尝试设置该变量为 1。 set @@tidb_opt_distinct_agg_push_down = 1; 三、构建 TiFlash 副本 TiFlash 部署完成后并不会自动同步数据，而需要手动指定需要同步的表。可通过 MySQL 客户端向 TiDB 发送 DDL 命令来为特定的表建立 TiFlash 副本。 1. 按表构建 TiFlash 副本 # 按表构建TiFlash副本SQL语法格式 ALTER TABLE 表名 SET TIFLASH REPLICA 副本个数; # count 表示副本数，0 表示删除现有的TiFlash 副本 # 为表建立1个副本 ALTER TABLE `test`.`testtable` SET TIFLASH REPLICA 1; # 删除副本 ALTER TABLE `test`.`testtable` SET TIFLASH REPLICA 0; 注意事项： 对于相同表的多次 DDL 命令，仅保证最后一次能生效。 假设有一张表 t 已经通过上述的 DDL 语句同步到 TiFlash，则通过以下语句创建的表也会自动同步到 TiFlash： CREATE TABLE table_name like t; 如果集群版本 如果集群版本以及 TiDB Lightning 版本均 >= v4.0.6，无论一个表是否已经创建 TiFlash 副本，你均可以使用 TiDB Lightning 导入数据至该表。但注意此情况会导致 TiDB Lightning 导入数据耗费的时间延长，具体取决于 TiDB Lightning 部署机器的网卡带宽、TiFlash 节点的 CPU 及磁盘负载、TiFlash 副本数等因素。 不推荐同步 1000 张以上的表，这会降低 PD 的调度性能。这个限制将在后续版本去除。 v5.1 版本及后续版本将不再支持设置系统表的 replica。在集群升级前，需要清除相关系统表的 replica，否则升级到较高版本后将无法再修改系统表的 replica 设置。 2. 按库构建 TiFlash 副本 # 按库构建TiFlash副本SQL语法格式 ALTER DATABASE 库名 SET TIFLASH REPLICA 副本个数; # count 表示副本数，0 表示删除现有的TiFlash 副本 # 为 `test` 库中的所有表建立1个TiFlash副本。 ALTER DATABASE `test` SET TIFLASH REPLICA 1; # 删除为 `tpch50` 库建立的 TiFlash 副本 ALTER DATABASE `test` SET TIFLASH REPLICA 0; 注意事项： 该命令实际是为用户执行一系列 DDL 操作，对资源要求比较高。如果在执行过程中出现中断，已经执行成功的操作不会回退，未执行的操作不会继续执行。 从命令执行开始到该库中所有表都已同步完成之前，不建议执行和该库相关的 TiFlash 副本数量设置或其他 DDL 操作，否则最终状态可能非预期。非预期场景包括： 先设置 TiFlash 副本数量为 2，在库中所有的表都同步完成前，再设置 TiFlash 副本数量为 1，不能保证最终所有表的 TiFlash 副本数量都为 1 或都为 2。 在命令执行到结束期间，如果在该库下创建表，则可能会对这些新增表创建 TiFlash 副本。 在命令执行到结束期间，如果为该库下的表添加索引，则该命令可能陷入等待，直到添加索引完成。 该命令会跳过系统表、视图、临时表以及包含了 TiFlash 不支持字符集的表。 3. 查看库表同步进度 ①查看库表同步进度 # 查看库的同步进度 SELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = ''; select table_schema,table_name,replica_count,progress from information_schema.tiflash_replica where progress !=1 ; # 查看指定表的同步进度 SELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = '' and TABLE_NAME = ''; 查询结果字段： AVAILABLE 表示该表的 TiFlash 副本是否可用。1 代表可用，0 代表不可用。副本状态为可用之后就不再改变，如果通过 DDL 命令修改副本数则会重新计算同步进度。 PROGRESS 示同步进度，在 0.0~1.0 之间，1 代表至少 1 个副本已经完成同步。 ②查看未设置同步的表 SELECT TABLE_NAME FROM information_schema.tables where TABLE_SCHEMA = \"\" and TABLE_NAME not in (SELECT TABLE_NAME FROM information_schema.tiflash_replica where TABLE_SCHEMA = \"\"); 4. 设置副本可用区 在配置副本时，如果为了考虑容灾，需要将 TiFlash 的不同数据副本分布到多个数据中心 ①为TiFlash节点指定label tiflash_servers: - host: 172.16.5.81 config: logger.level: \"info\" learner_config: server.labels: zone: \"z1\" - host: 172.16.5.82 config: logger.level: \"info\" learner_config: server.labels: zone: \"z1\" - host: 172.16.5.85 config: logger.level: \"info\" learner_config: server.labels: zone: \"z2\" 注：旧版本中的 flash.proxy.labels 配置无法处理可用区名字中的特殊字符，建议使用 learner_config 中的 server.labels 来进行配置。 ②创建副本时指定label ALTER TABLE 表名 SET TIFLASH REPLICA 副本个数 LOCATION LABELS 标签名; # ALTER TABLE test SET TIFLASH REPLICA 2 LOCATION LABELS \"zone\"; ③验证副本可用区调度状态 此时 PD 会根据设置的 label 进行调度，将表 t 的两个副本分别调度到两个可用区中。可以通过监控或 pd-ctl 来验证这一点： $ pd -u: store ... \"address\": \"172.16.5.82:23913\", \"labels\": [ { \"key\": \"engine\", \"value\": \"tiflash\"}, { \"key\": \"zone\", \"value\": \"z1\" } ], \"region_count\": 4, ... \"address\": \"172.16.5.81:23913\", \"labels\": [ { \"key\": \"engine\", \"value\": \"tiflash\"}, { \"key\": \"zone\", \"value\": \"z1\" } ], \"region_count\": 5, ... \"address\": \"172.16.5.85:23913\", \"labels\": [ { \"key\": \"engine\", \"value\": \"tiflash\"}, { \"key\": \"zone\", \"value\": \"z2\" } ], \"region_count\": 9, 四、TiDB读取TiFlash TiDB 提供三种读取 TiFlash 副本的方式。如果添加了 TiFlash 副本，而没有做任何 engine 的配置，则默认使用 CBO(Cost Based Optimization) 方式。 具体有没有选择 TiFlash 副本，可以通过 explain analyze 和desc sql语句查看 desc select count(*) from test.t; +--------------------------+---------+--------------+---------------+--------------------------------+ | id | estRows | task | access object | operator info | +--------------------------+---------+--------------+---------------+--------------------------------+ | StreamAgg_9 | 1.00 | root | | funcs:count(1)->Column#4 | | └─TableReader_17 | 1.00 | root | | data:TableFullScan_16 | | └─TableFullScan_16 | 1.00 | cop[tiflash] | table:t | keep order:false, stats:pseudo | +--------------------------+---------+--------------+---------------+--------------------------------+ explain analyze select count(*) from test.t; +-----+---------+---------+------+-------------+--------------+-------------+--------+------+ | id | estRows | actRows | task |access object|execution info|operator info| memory | disk | +-----+---------+---------+------+-------------+--------------+-------------+--------+------+ | StreamAgg_9 |1.0|1|root|| time:83.8372ms, loops:2 | funcs:count(1)->Column#4 | 372 Bytes | N/A | | └─TableReader_17|1.0|1|root||time:83.76ms,loops:2,rpc num: 1,rpc time:83.57ms,proc keys:0|data:TableFullScan_16|152 Bytes|N/A | | └─TableFullScan_16|1.0|1|cop[tiflash]|table:t|time:43ms,loops:1|keep order:false,stats:pseudo|N/A|N/A| +-----+---------+---------+------+-------------+--------------+-------------+--------+------+ cop[tiflash] 表示该任务会发送至 TiFlash 进行处理。如果没有选择 TiFlash 副本，可尝试通过 analyze table 语句更新统计信息后，再查看 explain analyze 结果。 需要注意的是，如果表仅有单个 TiFlash 副本且相关节点无法服务，智能选择模式下的查询会不断重试，需要指定 Engine 或者手工 Hint 来读取 TiKV 副本。 1. 智能选择 对于创建了 TiFlash 副本的表，TiDB 优化器会自动根据代价估算选择是否使用 TiFlash 副本。 2. Engine 隔离 Engine 隔离是通过配置变量来指定所有的查询均使用指定 engine 的副本，可选 engine 为 \"tikv\"、\"tidb\" 和 \"tiflash\"（其中 \"tidb\" 表示 TiDB 内部的内存表区，主要用于存储一些 TiDB 系统表，用户不能主动使用），分别有 2 个配置级别： TiDB 实例级别，即 INSTANCE 级别 在 TiDB 的配置文件添加如下配置项： [isolation-read] engines = [\"tikv\", \"tidb\", \"tiflash\"] 实例级别的默认配置为 [\"tikv\", \"tidb\", \"tiflash\"] 会话级别，即 SESSION 级别 会话级别的默认配置继承自 TiDB 实例级别的配置。最终的 engine 配置为会话级别配置，即会话级别配置会覆盖实例级别配置。比如实例级别配置了 \"tikv\"，而会话级别配置了 \"tiflash\"，则会读取 TiFlash 副本。当 engine 配置为 \"tikv, tiflash\"，即可以同时读取 TiKV 和 TiFlash 副本，优化器会自动选择。 set @@session.tidb_isolation_read_engines = \"逗号分隔的 engine list\"; # ....sql.... 或者 set SESSION tidb_isolation_read_engines = \"逗号分隔的engine\"; # ....sql.... 注意 由于 TiDB Dashboard等组件需要读取一些存储于 TiDB 内存表区的系统表，因此建议实例级别 engine 配置中始终加入 \"tidb\" engine。 如果查询中的表没有对应engin的副本，比如配置了engine为 \"tiflash\"而该表没有 TiFlash 副本，则查询会报该表不存在该 engine 副本的错。 3. 手工Hint 手工 Hint 可以在满足 engine 隔离的前提下，强制 TiDB 对于某张或某几张表使用指定的副本，使用方法为： select /*+ read_from_storage(tiflash[table_name]) */ ... from table_name; 如果在查询语句中对表设置了别名，在 Hint 语句中必须使用别名才能使 Hint 生效 select /*+ read_from_storage(tiflash[alias_a,alias_b]) */ ... from table_name_1 as alias_a, table_name_2 as alias_b where alias_a.column_1 = alias_b.column_2; 其中 tiflash[] 是提示优化器读取 TiFlash 副本，亦可以根据需要使用 tikv[] 来提示优化器读取 TiKV 副本。更多关于该 Hint 语句的语法可以参考 READ_FROM_STORAGE。 如果 Hint 指定的表在指定的引擎上不存在副本，则 Hint 会被忽略，并产生 warning。另外 Hint 必须在满足 engine 隔离的前提下才会生效，如果 Hint 中指定的引擎不在 engine 隔离列表中，Hint 同样会被忽略，并产生 warning。 注意 MySQL 命令行客户端在 5.7.7 版本之前默认清除了 Optimizer Hints。如果需要在这些早期版本的客户端中使用 Hint 语法，需要在启动客户端时加上 --comments 选项，例如 mysql -h 127.0.0.1 -P 4000 -uroot --comments。 三种方式之间关系的总结 Engine 隔离规定了总的可使用副本 engine 的范围 手工 Hint 可以在该范围内进一步实现语句级别及表级别的细粒度的 engine 指定，最终由 CBO 在指定的 engine 范围内根据代价估算最终选取某个 engine 上的副本。 SQL注意事项 TiDB 4.0.3 版本之前，在非只读 SQL 语句中（比如 INSERT INTO ... SELECT、SELECT ... FOR UPDATE、UPDATE ...、DELETE ...）读取 TiFlash，行为是未定义。 TiDB 4.0.3 以及后续的版本，TiDB 内部会对非只读 SQL 语句忽略 TiFlash 副本以保证数据写入、更新、删除的正确性。对应的，如果使用了智能选择的方式，TiDB 会自动选择非 TiFlash 副本；如果使用了 Engine 隔离的方式指定仅读取 TiFlash 副本，则查询会报错；而如果使用了手工 Hint 的方式，则 Hint 会被忽略。 五、TiFlash常见问题 1. TiFlash未能正常启动 该问题可能由多个因素构成，可以通过以下步骤依次排查： 检查系统环境是否是 CentOS8。 CentOS8 中缺少 libnsl.so 系统库，可以通过手动安装的方式解决： dnf install libnsl 检查系统的 ulimit 参数设置。 ulimit -n 1000000 使用 PD Control 工具检查在该节点（相同 IP 和 Port）是否有之前未成功下线的 TiFlash 实例，并将它们强制下线。（下线步骤参考手动缩容 TiFlash 节点） 2. TiFlash副本始终处于不可用状态 该问题一般由于配置错误或者环境问题导致 TiFlash 处于异常状态，可以先通过以下步骤定位问题组件 使用 pd-ctl 检查 PD 的 Placement Rules 功能是否开启 echo 'config show replication' | /path/to/pd-ctl -u http://${pd-ip}:${pd-port} 如果返回 true，进入下一步。 如果返回 false，你需要先开启 Placement Rules 特性 后再进入下一步。 通过 TiFlash-Summary 监控面板下的 UpTime 检查操作系统中 TiFlash 进程是否正常。 通过 pd-ctl 查看 TiFlash proxy 状态是否正常： echo \"store\" | /path/to/pd-ctl -u http://${pd-ip}:${pd-port} # store.labels 中含有 `{\"key\": \"engine\", \"value\": \"tiflash\"}` 信息的为 TiFlash proxy。 查看 pd buddy 是否正常打印日志（日志路径的对应配置项 [flash.flash_cluster] log 设置的值，默认为 TiFlash 配置文件配置的 tmp 目录下）。 检查配置的副本数是否小于等于集群 TiKV 节点数。若配置的副本数超过 TiKV 节点数，则 PD 不会向 TiFlash 同步数据； echo 'config placement-rules show' | /path/to/pd-ctl -u http://${pd-ip}:${pd-port} 再确认 \"default: count\" 参数值。 注意 开启 Placement Rules 后，原先的 max-replicas 及 location-labels 配置项将不再生效。如果需要调整副本策略，应当使用 Placement Rules 相关接口。 检查 TiFlash 节点对应 store 所在机器剩余的磁盘空间是否充足。默认情况下当磁盘剩余空间小于该 store 的 capacity 的 20%（通过 low-space-ratio 参数控制）时，PD 不会向 TiFlash 调度数据。 3. TiFlash查询时间不稳定 TiFlash 查询时间不稳定，同时错误日志中打印出大量的 Lock Exception 该问题是由于集群中存在大量写入，导致 TiFlash 查询时遇到锁并发生查询重试。 可以在 TiDB 中将查询时间戳设置为 1 秒前（例如：假设当前时间为 '2020-04-08 20:15:01'，可以在执行 query 前执行 set @@tidb_snapshot='2020-04-08 20:15:00';），来减小 TiFlash 查询碰到锁的可能性，从而减轻查询时间不稳定的程度。 4. 部分查询返回Region Unavailable错误 如果在 TiFlash 上的负载压力过大，会导致 TiFlash 数据同步落后，部分查询可能会返回 Region Unavailable 的错误。 在这种情况下，可以增加 TiFlash 节点分担负载压力。 5. 数据文件损坏 可依照如下步骤进行处理： 参照下线 TiFlash 节点下线对应的 TiFlash 节点。 清除该 TiFlash 节点的相关数据。 重新在集群中部署 TiFlash 节点。 6. TiFlash 分析慢 如果语句中含有 MPP 模式不支持的算子或函数等，TiDB 不会选择 MPP 模式，可能导致分析慢。此时，可以执行 EXPLAIN 语句检查 SQL 中是否含有 TiFlash 不支持的函数或算子。 create table t(a datetime); alter table t set tiflash replica 1; insert into t values('2022-01-13'); set @@session.tidb_enforce_mpp=1; explain select count(*) from t where subtime(a, '12:00:00') > '2022-01-01' group by a; show warnings; 示例中，warning 消息显示，因为 TiDB 5.4 及更早的版本尚不支持 subtime 函数的下推，因此 TiDB 没有选择 MPP 模式。 +---------+------+-----------------------------------------------------------------------------+ > | Level | Code | Message | +---------+------+-----------------------------------------------------------------------------+ | Warning | 1105 | Scalar function 'subtime'(signature: SubDatetimeAndString, return type: datetime) is not supported to push down to tiflash now. | +---------+------+-----------------------------------------------------------------------------+ 7. TiFlash 数据不同步 在部署完 TiFlash 节点且进行了数据的同步操作（ALTER 操作）之后，如果实际没有数据同步到 TiFlash 节点，可以通过以下步骤确认或解决问题： 检查同步操作是否执行 执行 ALTER table set tiflash replica 操作，查看是否有正常返回: 如果有正常返回，进入下一步。 如果无正常返回，请执行 SELECT * FROM information_schema.tiflash_replica 检查是否已经创建 TiFlash replica。如果没有，请重新执行 ALTER table ${tbl_name} set tiflash replica ${num}，查看是否有其他执行语句（如 add index ），或者检查 DDL 操作是否正常。 检查 TiFlash Region 同步是否正常。 查看 progress 是否有变化: 如果有变化，说明 TiFlash 同步正常，进入下一步。 如果没有变化，说明 TiFlash 同步异常，在 tidb.log 中，搜索 Tiflash replica is not available 相关日志。检查对应表的 region have 是否更新。如果无更新，请通过 tiflash 日志进一步排查。 使用 pd-ctl 检查 PD 的 Placement Rules 功能是否开启： echo 'config show replication' | /path/to/pd-ctl -u http://: 如果返回 true，进入下一步。 如果返回 false，你需要先开启 Placement Rules 特性，然后进入下一步。 检查集群副本数 max-replicas 配置是否合理。 如果 max-replicas 取值未超过 TiKV 节点数，进入下一步。 如果 max-replicas 超过 TiKV 节点数，PD 不会向 TiFlash 同步数据。此时，请将 max-replicas 修改为小于等于 TiKV 节点数的整数。 注意： max-replicas 的默认值是 3。在生产环境中，TiKV 节点数一般大于该值；在测试环境中，可以修改为 1。 curl -X POST -d '{ \"group_id\": \"pd\", \"id\": \"default\", \"start_key\": \"\", \"end_key\": \"\", \"role\": \"voter\", \"count\": 3, \"location_labels\": [ \"host\" ] }' 检查 TiDB 是否为表创建 placement-rule。 搜索 TiDB DDL Owner 的日志，检查 TiDB 是否通知 PD 添加 placement-rule。对于非分区表搜索 ConfigureTiFlashPDForTable；对于分区表，搜索 ConfigureTiFlashPDForPartitions。有关键字，进入下一步；没有关键字，收集相关组件的日志进行排查。 检查 PD 是否为表设置 placement-rule 可以通过 curl http://:/pd/api/v1/config/rules/group/tiflash 查询比较当前 PD 上的所有 TiFlash Placement Rule。如果观察到有 id 为 table--r 的 Rule ，则表示 PD Rule 设置成功。 检查 PD 是否正常发起调度 查看 pd.log 日志是否出现 table--r 关键字，且之后是否出现 add operator 之类的调度行为。是，PD 调度正常；否，PD 调度异常。 8. TiFlash 数据同步卡住 如果 TiFlash 数据一开始可以正常同步，过一段时间后全部或者部分数据无法继续同步，你可以通过以下步骤确认或解决问题： 检查磁盘空间。 检查磁盘使用空间比例是否高于 low-space-ratio 的值（默认值 0.8，即当节点的空间占用比例超过 80% 时，为避免磁盘空间被耗尽，PD 会尽可能避免往该节点迁移数据）。 如果磁盘使用率大于等于 low-space-ratio，说明磁盘空间不足。此时，请删除不必要的文件，如 ${data}/flash/ 目录下的 space_placeholder_file 文件（必要时可在删除文件后将 reserve-space 设置为 0MB）。 如果磁盘使用率小于 low-space-ratio ，说明磁盘空间正常，进入下一步。 检查是否有 down peer （down peer 没有清理干净可能会导致同步卡住）。 执行 pd-ctl region check-down-peer 命令检查是否有 down peer。 如果存在 down peer，执行 pd-ctl operator add remove-peer 命令将其清除。 9. 数据同步慢 同步慢可能由多种原因引起，你可以按以下步骤进行排查。 调整调度参数取值。 调大 store limit，加快同步速度。 调整 TiFlash 侧负载。 TiFlash 负载过大会引起同步慢，可通过 Grafana 中的 TiFlash-Summary 面板查看各个指标的负载情况： Applying snapshots Count: TiFlash-summary > raft > Applying snapshots Count Snapshot Predecode Duration: TiFlash-summary > raft > Snapshot Predecode Duration Snapshot Flush Duration: TiFlash-summary > raft > Snapshot Flush Duration Write Stall Duration: TiFlash-summary > Storage Write Stall > Write Stall Duration generate snapshot CPU: TiFlash-Proxy-Details > Thread CPU > Region task worker pre-handle/generate snapshot CPU 根据业务优先级，调整负载情况。 参考 https://docs.pingcap.com/zh/tidb/stable/tiflash-overview https://docs.pingcap.com/zh/tidb/stable/tiup-cluster-topology-reference#tiflash_servers Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-07-20 17:45:42 "},"origin/tidb-resource-limit.html":{"url":"origin/tidb-resource-limit.html","title":"TiDB资源限制隔离","keywords":"","body":"TiDB 资源限制隔离 一、简介 TiDB 资源管控特性提供了两层资源管理能力，包括在TiDB 层的流控能力和TiKV 层的优先级调度的能力。两个能力可以单独或者同时开启，详情请参见参数组合效果表。将用户绑定到某个资源组后，TiDB 层会根据用户所绑定资源组设定的配额对用户的读写请求做流控，TiKV 层会根据配额映射的优先级来对请求做调度。通过流控和调度这两层控制，可以实现应用的资源隔离，满足服务质量 (QoS) 要求。 TiDB 流控：TiDB 流控使用令牌桶算法 做流控。如果桶内令牌数不够，而且资源组没有指定 BURSTABLE 特性，属于该资源组的请求会等待令牌桶回填令牌并重试，重试可能会超时失败。 TiKV 调度：你可以为资源组设置绝对优先级 (PRIORITY)，不同的资源按照 PRIORITY 的设置进行调度，PRIORITY 高的任务会被优先调度。如果没有设置绝对优先级 (PRIORITY)，TiKV 会将资源组的 RU_PER_SEC 取值映射成各自资源组读写请求的优先级，并基于各自的优先级在存储层使用优先级队列调度处理请求。 RU 简介 Request Unit (RU) 是 TiDB 对 CPU、IO 等系统资源的统一抽象的计量单位，用于表示对数据库的单个请求消耗的资源量。请求消耗的 RU 数量取决于多种因素，例如操作类型或正在检索或修改的数据量。目前，RU 包含以下资源的统计信息： 资源类型 RU 消耗 Read 2 storage read batches 消耗 1 RU 8 storage read requests 消耗 1 RU 64 KiB read request payload 消耗 1 RU Write 1 storage write batch 消耗 1 RU 1 storage write request 消耗 1 RU 1 KiB write request payload 消耗 1 RU SQL CPU 3 ms 消耗 1 RU 预估集群容量 二、操作 1、开起资源限制 资源管控特性引入了如下系统变量或参数： TiDB：通过配置全局变量 tidb_enable_resource_control 控制是否打开资源组流控。 TiKV：通过配置参数 resource-control.enabled 控制是否使用基于资源组配额的请求调度。 TiFlash：通过配置全局变量 tidb_enable_resource_control 和 TiFlash 配置项 enable_resource_control（v7.4.0 开始引入）控制是否开启 TiFlash 资源管控。 从 v7.0.0 开始，tidb_enable_resource_control 和 resource-control.enabled 开关都被默认打开。这两个参数的组合效果见下表： esource-control.enabled tidb_enable_resource_control= ON tidb_enable_resource_control= OFF resource-control.enabled= true 流控和调度（推荐组合） 无效配置 resource-control.enabled= false 仅流控（不推荐） 特性被关闭 从 v7.4.0 开始，TiFlash 配置项 enable_resource_control 默认打开，与 tidb_enable_resource_control 一起控制 TiFlash 资源管控功能。只有二者都启用时，TiFlash 资源管控功能才能进行流控以及优先级调度。同时，在开启 enable_resource_control 时，TiFlash 会使用 Pipeline Model 执行模型。 2、估算 SQL 消耗的 RU 你可以通过 EXPLAIN ANALYZE 语句获取到 SQL 执行时所消耗的 RU。注意 RU 的大小会受缓存的影响（比如下推计算结果缓存），多次执行同一条 SQL 所消耗的 RU 可能会有不同。因此这个 RU 值并不代表每次执行的精确值，但可以作为估算的参考。 EXPLAIN ANALYZE .....SQL..... 3、创建资源组 ①创建语句 create resource group if not exists rg1 ru_per_sec = 200 priority = high QUERY_LIMIT=(EXEC_ELAPSED='100ms', ACTION=KILL, WATCH=EXACT DURATION='10m'); ②参数详解 参数 含义 举例 RU_PER_SEC 每秒 RU 填充的速度 RU_PER_SEC = 500 表示此资源组每秒回填 500 个 RU。 PRIORITY 任务在 TiKV 上处理的绝对优先级 PRIORITY = HIGH 表示优先级高。若未指定，则默认为 MEDIUM。 BURSTABLE 允许对应的资源组超出配额后使用空余的系统资源。 QUERY_LIMIT 当查询执行满足该条件时，识别该查询为 Runaway Query 并进行相应的控制 QUERY_LIMIT=(EXEC_ELAPSED='60s', ACTION=KILL, WATCH=EXACT DURATION='10m') 表示当执行时间超过 60 秒后识别为 Runaway Query，对该查询执行终止操作，并在 10 分钟内对同样的 SQL 直接执行终止操作。QUERY_LIMIT=() 或 QUERY_LIMIT=NULL 则表示不进行 Runaway 控制。具体参数介绍详见管理资源消耗超出预期的查询 (Runaway Queries)。 ｜ ③注意 TiDB 在创建资源组时不会检查容量。只要系统有足够的空闲资源，TiDB 就会满足每个资源组的用量设置。当系统资源超过限制时，TiDB 会优先满足高优先级 (PRIORITY) 资源组的请求。如果同一优先级的请求无法全部满足，TiDB 会根据用量 (RU_PER_SEC) 的大小按比例分配。 4、QUERY_LIMIT管理资源消耗超出预期的查询 针对已触发Runaway Queries条件的 SQL，设置指定时间的自动操作。 自 v7.2.0 起，TiDB 资源管控引入了对 Runaway Queries 的管理。你可以针对某个资源组设置条件来识别 Runaway Queries，并自动发起应对操作，防止集群资源完全被 Runaway Queries 占用而影响其他正常查询。你可以在 CREATE RESOURCE GROUP 或者 ALTER RESOURCE GROUP 中配置 QUERY_LIMIT 字段，通过规则识别来管理资源组的 Runaway Queries。 自 v7.3.0 起，TiDB 资源管控引入了手动管理 Runaway Queries 监控列表的功能，将给定的 SQL 或者 Digest 添加到隔离监控列表，从而实现快速隔离 Runaway Queries。执行语句 QUERY WATCH，手动管理资源组中的 Runaway Queries 监控列表。 ①QUERY_LIMIT 参数详解 支持的条件设置： EXEC_ELAPSED: 当查询执行的时间超限时，识别为 Runaway Query。 支持的应对操作 (ACTION)： DRYRUN：对执行 Query 不做任何操作，仅记录识别的 Runaway Query。主要用于观测设置条件是否合理。 COOLDOWN：将查询的执行优先级降到最低，查询仍旧会以低优先级继续执行，不占用其他操作的资源。 KILL：识别到的查询将被自动终止，报错 Query execution was interrupted, identified as runaway query。 WATCH 当某一个查询被识别为 Runaway Query 之后，会提取这个查询的匹配特征（由 WATCH 后的匹配方式参数决定）。在接下来DURATION 定义的一段时间里，这个 Runaway Query 的匹配特征会被加入到监控列表，TiDB 实例会将查询和监控列表进行匹配，匹配到的查询直接标记为 Runaway Query，而不再等待其被条件识别，并按照当前应对操作进行隔离。有三种匹配方式： EXACT 表示完全相同的 SQL 才会被快速识别 SIMILAR 表示会忽略字面值 (Literal)，通过 SQL Digest 匹配所有模式 (Pattern) 相同的 SQL PLAN 表示通过 Plan Digest 匹配所有模式 (Pattern) 相同的 SQL WATCH 中的 DURATION 选项，用于表示此识别项的持续时间，默认为无限长。 ②查询Runaway Queries历史记录 mysql.tidb_runaway_queries 表中包含了过去 7 天内所有识别到的 Runaway Queries 的历史记录 SELECT * FROM mysql.tidb_runaway_queries LIMIT 1; match_type 为该 Runaway Query 的来源，其值如下： identify 表示命中条件。 watch 表示被快速识别机制命中。 5、管理资源组 ①查看资源组 SELECT * FROM information_schema.resource_groups WHERE NAME ='rg1'; ②删除资源组 DROP RESOURCE GROUP IF EXISTS rg1; ③修改资源组 ALTER RESOURCE GROUP rg1 RU_PER_SEC = 200 PRIORITY = LOW QUERY_LIMIT = (EXEC_ELAPSED='1s' ACTION=COOLDOWN WATCH=EXACT DURATION '30s'); 修改 rg1 资源组，取消 Runaway Queries 检查。 ALTER RESOURCE GROUP rg1 QUERY_LIMIT=NULL; 6、资源组绑定 ①绑定用户到资源组 ALTER USER 'usr2'@'%' RESOURCE GROUP rg1; 查看当前用户绑定的资源组 SELECT USER, JSON_EXTRACT(User_attributes, \"$.resource_group\") FROM mysql.user WHERE user = \"usr2\"; 解除用户与资源组的绑定（只需将其重新绑定到 default 资源组即可） ALTER USER 'usr2'@'%' RESOURCE GROUP `default`; 注意： 没有指定任何资源组的用户，将被放入系统预定义的 default 资源组，而 default 资源组默认拥有无限用量。当所有用户都属于 default 资源组时，资源分配方式与关闭资源管控时相同。 ②绑定会话到资源组 将当前的会话绑定至资源组 rg1 SET RESOURCE GROUP rg1; ③绑定SQL到资源组 通过在 SQL 语句中添加 RESOURCE_GROUP(resource_group_name) Hint，可以将该语句绑定到指定的资源组。此 Hint 支持 SELECT、INSERT、UPDATE、DELETE 四种语句。 SELECT /*+ RESOURCE_GROUP(rg1) */ * FROM t limit 10; 三、针对 Metabase进行资源限制隔离 方案：限制指定用户执行超过 60s的 SQL 进行kill，或者降低优先级处理 ，对于触发限制的相同SQL 在 15 分钟内自动进行之前的处理操作，限制其频繁操作。 新建metabase使用的 TIDB用户，绑定该用户到特定的资源组。资源组规则设置：判断执行时间超过 60s的SQL create resource group if not exists rg1 ru_per_sec = 20000 priority = MEDIUM QUERY_LIMIT=(EXEC_ELAPSED='60s', ACTION=COOLDOWN, WATCH=SIMILAR DURATION='5m'); Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:32:15 "},"origin/minio.html":{"url":"origin/minio.html","title":"Minio","keywords":"","body":"Minio对象存储 一、简介 MinIO 是在 GNU Affero 通用公共许可证 v3.0 下发布的高性能对象存储。兼容与 Amazon S3 云存储服务的 API。 中文文档：http://minio.org.cn/ 二、安装部署 MinIO 部署开始使用默认的 root 凭据 minioadmin:minioadmin Docker mkdir -p ~/minio/data docker run \\ -p 9000:9000 \\ -p 9090:9090 \\ --name minio \\ -v ~/minio/data:/data \\ -e \"MINIO_ACCESS_KEY=*****\" \\ -e \"MINIO_SECRET_KEY=*****\" \\ quay.io/minio/minio server /data --console-address \":9090\" Brew brew install minio/stable/minio minio server /data Unix wget http://dl.minio.org.cn/server/minio/release/linux-amd64/minio chmod +x minio ./minio server /data Windows http://dl.minio.org.cn/server/minio/release/windows-amd64/minio.exe minio.exe server D:\\ 三、mc命令 命令文档：https://min.io/docs/minio/linux/reference/minio-mc.html# 1、mc命令安装 Linux curl https://dl.min.io/client/mc/release/linux-amd64/mc \\ --create-dirs \\ -o $HOME/minio-binaries/mc chmod +x $HOME/minio-binaries/mc export PATH=$PATH:$HOME/minio-binaries/ MacOS brew install minio/stable/mc WIndows https://dl.min.io/client/mc/release/windows-amd64/mc.exe Docker docker run -it --rm minio/mc --version mc命令shell自动补全 wget https://raw.githubusercontent.com/minio/mc/master/autocomplete/bash_autocomplete -O /etc/bash_completion.d/mc source /etc/bash_completion.d/mc 2、实例管理 # 添加minio实例配置 mc config host add synology-minio http://127.0.0.1:9000 用户名 密码 # 查看已配置的minio实例实例配置 mc config host list # 删除minio实例实例配置 mc config host remove synology-minio # 获取实例信息 mc admin info synology-minio 3、用户管理 # 列出所有用户 mc admin user list synology-minio # 添加新用户 mc admin user add synology-minio 用户名 密码 # 禁用用户 mc admin user disable synology-minio 用户名 # 启用用户 mc admin user enable synology-minio 用户名 # 显示用户信息 mc admin user info synology-minio 用户名 4、用户组管理 # 列出所有组 mc admin group list synology-minio # 获取组信息 mc admin group info synology-minio 用户组 # 将用户添加到组中。如果组不存在，则会创建该组。 mc admin group add synology-minio 用户组 用户1 用户2 # 删除组中用户 mc admin group remove synology-minio 用户组 用户1 用户2 # 删除组 mc admin group remove synology-minio 用户组 # 启用组 mc admin group enable synology-minio 用户组 # 禁用组 mc admin group disable synology-minio 用户组 5、Bucket管理 # 创建Bucket mc mb synology-minio/juicefs # 列出Bucket mc ls synology-minio # 删除没有文件的bucket mc rb synology-minio/juicefs # 删除有文件的bucket mc rb synology-minio/juicefs --force 6、策略管理 命令格式： mc admin policy 子命令 子命令: add 添加新策略 remove 删除策略 list 列出所有策略 info 显示策略信息 set 在用户或组上设置策略 # 列出所有策略 mc admin policy list synology-minio # 显示策略的信息 mc admin policy info synology-minio writeonly # 创建策略 mc admin policy add synology-minio listbucketsonly ./listbucketsonly.json # 删除策略 mc admin policy remove synology-minio listbucketsonly # 在用户或组上设置策略 mc admin policy set synology-minio listbucketsonly user=用户 mc admin policy set synology-minio listbucketsonly group=用户组 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:myListBuckets\" ], \"Resource\": [ \"arn:aws:s3:::*\" ] } ] } 策略模板 { \"Version\" : \"2012-10-17\", \"Statement\" : [ { \"Effect\" : \"Allow\", \"Action\" : [ \"s3:\", ... ], \"Resource\" : \"arn:aws:s3:::*\", \"Condition\" : { ... } }, { \"Effect\" : \"Deny\", \"Action\" : [ \"s3:\", ... ], \"Resource\" : \"arn:aws:s3:::*\", \"Condition\" : { ... } } ] } 7、文件管理 # 列出Bucket中的文件 mc ls synology-minio/ # 查看Bucket中的文件内容 mc cat synology-minio//文件路径 # 删除文件 mc rm synology-minio//文件路径 # 删除目录 mc rm synology-minio//目录路径 --recursive --force # --force 参数指定强制删除 # --recursive 递归删除 # 上传一个文件到bucket中 mc cp ./testfile synology-minio//文件路径 # 使用pipe管道直接上传stdout内容到Bucket文件中 ls -al | mc pipe synology-minio/juicefs/aaa/test.txt # 上传一个目录到bucket中 mc cp ./testdir synology-minio//目录路径 --recursive 8、查找文件与对象 find命令通过指定参数查找文件，它只列出满足条件的数据。 用法： mc find PATH [参数] 参数: --help, -h 显示帮助。 --exec value 为每个匹配对象生成一个外部进程（请参阅FORMAT） --name value 查找匹配通配符模式的对象。 ... mc find synology-minio/test1/ --name \"*.jpg\" # 持续从test1存储桶中查找所有jpeg图像，并复制到test2存储桶中 mc find synology-minio/test1/ --name \"*.jpg\" --watch --exec \"mc cp {} synology-minio/test2/\" 9、共享文件下载 share download命令生成不需要access key和secret key即可下载的URL，过期参数设置成最大有效期（不大于7天），过期之后权限自动回收。 用法： mc share download [参数] TARGET [TARGET...] 参数: --help, -h 显示帮助。 --recursive, -r 递归共享所有对象。 --expire, -E \"168h\" 设置过期时限，NN[h|m|s]。 mc share download --expire 4h synology-minio//文件路径 # 列出先前共享的有下载权限的URL。 mc share list download # 列出先前共享的有上传权限的URL。 mc share list upload 10、显示目录差异 diff命令计算两个目录之间的差异。它只列出缺少的或者大小不同的内容。 不比较内容，所以名称相同，大小相同但内容不同的对象没有被检测到。可以在不同站点或者大量数据的情况下快速比较。 用法： mc diff [FLAGS] 路径1 路径2 # 路径既可以是本地路径，又可以是minio bucket路径 # 比较一个本地文件夹和一个远程对象存储服务* mc diff localdir synology-minio/juicefs # localdir/notes.txt and https://play.min.io/mybucket/notes.txt - only in first. 11、共享文件上传 share upload命令生成不需要access key和secret key即可上传的URL。过期参数设置成最大有效期（不大于7天），过期之后权限自动回收。 Content-type参数限制只允许上传指定类型的文件。 用法： mc share upload [FLAGS] TARGET [TARGET...] 参数: --help, -h 显示帮助。 --recursive, -r 递归共享所有对象。 --expire, -E \"168h\" 设置过期时限，NN[h|m|s]. mc share upload synology-minio/juicefs/aaa/test1.txt # Expire: 7 days 0 hours 0 minutes 0 seconds # Share: curl http://127.0.0.1:9000/test/ -F AWSAccessKeyId=**** -F signature=*** -F bucket=test -F policy=****= -F key=aa/test1.txt -F file=@待上传的文件 # 使用共享上传URL上传本地文件内容 echo \"hahahhahhahah\" > test.hh curl http://127.0.0.1:9000/test/ -F AWSAccessKeyId=**** -F signature=*** -F bucket=test -F policy=****= -F key=aa/test1.txt -F file=@test.hh 12、显示服务端的http跟踪 mc admin trace synology-minio # 2022-10-21T14:37:34.601 [0 ] s3.GetBucketLocation 0s ↑ 0 B ↓ 0 B # 2022-10-21T14:37:34.614 [0 ] s3.ListObjectsV2 0s ↑ 0 B ↓ 0 B 13、监听文件和对象存储事件 watch命令提供了一种方便监听对象存储和文件系统上不同类型事件的方式。 用法： mc watch [参数] 0 参数: --events value 过滤不同类型的事件，默认是所有类型的事件 (默认： \"put,delete,get\") --prefix value 基于前缀过滤事件。 --suffix value 基于后缀过滤事件。 --recursive 递归方式监听事件。 --help, -h 显示帮助。 mc watch synology-minio # [2022-10-21T07:02:46.369Z] 7 B s3:ObjectCreated:Put http://127.0.0.1:9000/test/aaa/test1.txt 14、Prometheus监控 MinIO中的Prometheus端点默认需要身份验证。Prometheus支持使用承载令牌方法对Prometheus抓取请求进行身份验证 # 生成带有验证Token的Prometheus刮取minio监控数据的配置，将配置添加到Prometheus配置文件即可 mc admin prometheus generate synology-minio # - job_name: minio-job # bearer_token: ****** # metrics_path: /minio/v2/metrics/cluster # scheme: http # static_configs: # - targets: ['127.0.0.1:9000'] 指标 含义 disk_storage_used 磁盘使用的磁盘空间。 disk_storage_available 磁盘上剩余的可用磁盘空间。 disk_storage_total 磁盘上的总磁盘空间。 minio_disks_offline 当前MinIO实例中的脱机磁盘总数。 minio_disks_total 当前MinIO实例中的磁盘总数。 s3_requests_total 当前MinIO实例中s3请求的总数。 s3_errors_total 当前MinIO实例中s3请求中的错误总数。 s3_requests_current 当前MinIO实例中活动s3请求的总数。 internode_rx_bytes_total 当前MinIO服务器实例接收到的节点间字节总数。 internode_tx_bytes_total 当前MinIO服务器实例发送到其他节点的字节总数。 s3_rx_bytes_total 当前MinIO服务器实例接收的s3字节总数。 s3_tx_bytes_total 当前MinIO服务器实例发送的s3字节总数。 minio_version_info 具有提交ID的当前MinIO版本。 s3_ttfb_seconds 保存请求的延迟信息的直方图。 cache_data_served:cache_data_served 从缓存提供的总字节数。 cache_hits_total:cache_hits_total 缓存命中总数。 cache_misses_total:cache_misses_total 缓存未命中总数。 gateway__requests 向云后端发出的请求总数。此度量标准具有method标识GET，HEAD，PUT和POST请求的标签。 gateway__bytes_sent 发送到云后端的总字节数（在PUT和POST请求中）。 gateway__bytes_received 从云后端接收的字节总数（在GET和HEAD请求中）。 self_heal_time_since_last_activity 自上一次自我修复相关活动以来经过的时间。 self_heal_objects_scanned 在当前运行中由自愈线程扫描的对象数。重新开始自我修复运行时，它将重置。这用扫描的对象类型标记。 self_heal_objects_healed 当前运行中通过自愈线程修复的对象数。重新开始自我修复运行时，它将重置。这用扫描的对象类型标记。 self_heal_objects_heal_failed 当前运行中自愈失败的对象数。重新开始自我修复运行时，它将重置。这被标记为磁盘状态及其端点。 四、Minio服务端管理 1、升级 MinIO 服务器支持滚动升级，即可以在分布式集群中一次更新一个 MinIO 实例。 允许在不停机的情况下进行升级。 升级可以通过用最新版本替换二进制文件并以滚动方式重新启动所有服务器来手动完成 建议从客户端使用 mc admin update。 将同时更新集群中的所有节点并重新启动 某些版本可能不允许滚动升级，这总是在发行说明中提到，通常建议在升级前阅读发行说明 mc admin update 仅在运行 MinIO 的用户对二进制文件所在的父目录具有写访问权限时才有效，例如，如果当前二进制文件位于 /usr/local/bin/minio，则需要写入访问/usr/local/bin。 mc admin update 同时更新和重启所有服务器，应用程序会在升级后重试并继续各自的操作。 mc admin update 在 kubernetes/container 环境中被禁用，容器环境提供自己的机制来推出更新。 在联合设置的情况下，mc admin update 应该单独针对每个集群运行。在所有集群成功更新之前，避免将 mc 更新为任何新版本。 如果使用 kes 作为 MinIO 的 KMS，只需替换二进制文件并重新启动 kes 有关 kes 的更多信息可以在 here 中找到 如果将 Vault 作为 KMS 与 MinIO 一起使用，请确保遵循Vault升级过程：https://www.vaultproject.io/docs/upgrading/index.html 如果将 etcd 与 MinIO 用于联合，请确保遵循etcd升级过程：https://github.com/etcd-io/etcd/blob/master/Documentation/upgrades/upgrading-etcd.md 2、重启 mc admin service restart synology-minio 五、Minio事件通知 存储桶（Bucket）如果发生改变,比如上传对象和删除对象，可以使用存储桶事件通知机制进行监控，并通过以下方式发布出去: 参考：http://docs.minio.org.cn/docs/master/minio-bucket-notification-guide 1、使用Redis存储事件通知 ①配置Redis SQS mc admin config set synology-minio notify_redis \\ address=\"127.0.0.1:6379\" \\ password=\"redis密码\" \\ key=\"event在redis中的key名\" \\ format=\"namespace\" \\ queue_dir=\"\" \\ queue_limit=0 \\ comment=\"\" 参数 类型 描述 format string (必须) 是 namespace 还是 access。 如果是namespace格式的话，则存储到Redis的key的类型是一个hash。对于每一个条目，对应一个存储桶里的对象，其key都被设为\"存储桶名称/对象名称\"，value都是一个有关这个MinIO对象的JSON格式的事件数据。如果对象更新或者删除，hash中对象的条目也会相应的更新或者删除。； 如果是access格式的话，则是一个list。list中又有两个元素，第一个元素是时间戳的字符串，第二个元素是一个含有在这个存储桶上进行操作的事件数据的JSON对象。在这种格式下，list中的元素不会更新或者删除。 address string (必须) Redis服务地址，比如: localhost:6379。无法选择DB，默认使用Redis的0 Database password string (可选) Redis服务密码 key string (必须) 事件要存储到redis key的名称 ②重启Minio mc admin service restart synology-minio ③查看SQS的资源标识ARN mc admin info --json synology-minio | jq .info.sqsARN # 会输出类似于arn:minio:sqs::_:redis的信息 ④查看Bucket设置的SQS mc event list synology-minio/juicefs ⑤查看Bucket设置的SQS信息 mc admin config get synology-minio/ notify_redis ⑥在Bucket上配置SQS规则 mc event add synology-minio/juicefs arn:minio:sqs::_:redis # arn:minio:sqs::_:redis s3:ObjectCreated:*,s3:ObjectRemoved:*,s3:ObjectAccessed:* Filter: mc event add synology-minio/juicefs arn:minio:sqs::_:redis --suffix=\".txt\" # arn:minio:sqs::_:redis s3:ObjectCreated:*,s3:ObjectRemoved:*,s3:ObjectAccessed:* Filter: suffix=\".txt\" mc event add synology-minio/juicefs arn:minio:sqs::_:redis --event delete # arn:minio:sqs::_:redis s3:ObjectRemoved:* Filter: --events 过滤不同类型的事件，默认是所有类型的事件 (默认： \"put,delete,get\") --prefix 基于前缀过滤事件。 --suffix 基于后缀过滤事件。 --ignore-existing, -p 如果事件已经存在则忽略 ⑦删除Bucket上设置的SQS规则 mc event remove synology-minio/juicefs arn:minio:sqs::_:redis mc event remove synology-minio/juicefs arn:minio:sqs::_:redis --suffix=\".txt\" ⑧重置Redis SQS mc admin config reset synology-minio/ notify_redis # 重启Minio mc admin service restart synology-minio ⑨event时间样本 Key名：minio-events Key类型：hash { \"Records\": [ { \"eventVersion\": \"2.0\", \"eventSource\": \"minio:s3\", \"awsRegion\": \"\", \"eventTime\": \"2022-10-21T10:02:37.163Z\", \"eventName\": \"s3:ObjectCreated:Put\", \"userIdentity\": { \"principalId\": \"curiouser\" }, \"requestParameters\": { \"principalId\": \"curiouser\", \"region\": \"\", \"sourceIPAddress\": \"10.8.0.9\" }, \"responseElements\": { \"content-length\": \"0\", \"x-amz-request-id\": \"27200D52709D8C10\", \"x-minio-deployment-id\": \"5b27aw9c3-5ab2-4377-bebe-2a321143bdeb\", \"x-minio-origin-endpoint\": \"http://1237.0.0.1:9000\" }, \"s3\": { \"s3SchemaVersion\": \"1.0\", \"configurationId\": \"Config\", \"bucket\": { \"name\": \"juicefs\", \"ownerIdentity\": { \"principalId\": \"curiouser\" }, \"arn\": \"arn:aws:s3:::juicefs\" }, \"object\": { \"key\": \"aaa%2F5.txt\", \"size\": 331, \"eTag\": \"9b6a60f5b7707b436a33afb280fd6d8a\", \"contentType\": \"text/x-c\", \"userMetadata\": { \"content-type\": \"text/x-c\" }, \"sequencer\": \"17200D5D74683FBA\" } }, \"source\": { \"host\": \"10.8.0.9\", \"port\": \"\", \"userAgent\": \"MinIO (darwin; amd64) minio-go/v7.0.27 mc/RELEASE.2022-06-26T18-51-48Z\" } } ] } 六、Minio性能测试 mc support perf 是一个易用的测试工具，它会先运行PUTS，然后运行GETS，通过增量的方式测试得到最大吞吐量。而warp则是一个完整的工具链，提供了很独立的测试项，能够测试GET;PUT;DELETE等都可以测试得到。同时通过cs的结构设计，更符合真实的使用场景，得到最贴近应用的性能结果，有利于性能分析。 1、mc support perf mc support perf可以测试Minio的 S3 API（读/写）、网络 IO 和存储（驱动器读/写）性能 命令格式：mc [全局参数] support 性能测试对象 参数 minio实例别名 测试性能对象： Drive：测试驱动器的速度 mc [全局参数] support perf drive \\ [--concurrent 指定每个服务器要测试的并发请求数。默认32个] \\ [--verbose, -v] \\ [--filesize 指定要读取或写入每个驱动器的数据的总大小。默认1GiB，单位KiB, MiB,GiB] \\ [--blocksize 指定读/写块大小。默认4MiB，单位KiB, MiB,GiB] \\ [--serial，对驱动器逐个运行性能测试。] \\ minio实例别名 object：测试对象的读写速度 mc [全局参数] support perf object \\ [--duration 指定性能测试运行的持续时间。默认10s，单位s、m] \\ [--size] \\ [--concurrent 指定每个服务器要测试的并发请求数。默认32个] \\ [--verbose, -v] \\ minio实例别名 mc support perf object synology-minio # THROUGHPUT IOPS # PUT 77 MiB/s 1 objs/s # GET 6.4 MiB/s 0 objs/s # Speedtest: MinIO 2022-10-08T20:11:00Z, 1 servers, 1 drives, 64 MiB objects, 9 threads mc support perf drive synology-minio # NODE PATH READ WRITE # http://127.0.0.1:9000 /volume2/minio 109 MiB/s 138 MiB/s # Driveperf: ✔ 2、warp wrap是minio项目下的一个开源测试工具。wrap会上传一定随机数据到oss对象存储服务器上，统计过程消耗时间得出整个oss性能分析。 GitHub：https://github.com/minio/warp ①下载安装 二进制 下载连接：https://github.com/minio/warp/releases Docker docker run -it --rm --name minio-warp minio/warp:latest -h ②命令参数 命令格式 warp [全局参数] 子命令 [子命令参数] 子命令 含义 mixed benchmark mixed objects get benchmark get objects put benchmark put objects delete benchmark delete objects list benchmark list objects stat benchmark stat objects (get file info) select benchmark select objects versioned benchmark mixed versioned objects retention benchmark PutObjectRetention multipart benchmark multipart object analyze analyze existing benchmark data cmp compare existing benchmark data merge merge existing benchmark data client run warp in client mode, accepting connections to run benchmarks 参数 含义 --no-color disable color theme --debug enable debug output --insecure disable TLS certificate verification --autocompletion install auto-completion for your shell --host value host. Multiple hosts can be specified as a comma separated list. (默认: \"127.0.0.1:9000\") [$WARP_HOST] --access-key value Specify access key [$WARP_ACCESS_KEY] --secret-key value Specify secret key [$WARP_SECRET_KEY] --tls Use TLS (HTTPS) for transport [$WARP_TLS] --region value Specify a custom region [$WARP_REGION] --encrypt encrypt/decrypt objects (using server-side encryption with random keys) --bucket value Bucket to use for benchmark data. ALL DATA WILL BE DELETED IN BUCKET! (默认: \"warp-benchmark-bucket\") --host-select value Host selection algorithm. Can be \"weighed\" or \"roundrobin\" (默认: \"weighed\") --concurrent value Run this many concurrent operations (默认: 20) --noprefix Do not use separate prefix for each thread --prefix value Use a custom prefix for each thread --disable-multipart disable multipart uploads --md5 Add MD5 sum to uploads --storage-class value Specify custom storage class, for instance 'STANDARD' or 'REDUCED_REDUNDANCY'. --objects value Number of objects to upload. (默认: 2500) --obj.size value Size of each generated object. Can be a number or 10KiB/MiB/GiB. All sizes are base 2 binary. (默认: \"10MiB\") --get-distrib value The amount of GET operations. (默认: 45) --stat-distrib value The amount of STAT operations. (默认: 30) --put-distrib value The amount of PUT operations. (默认: 15) --delete-distrib value The amount of DELETE operations. Must be at least the same as PUT. (默认: 10) --obj.generator value Use specific data generator (默认: \"random\") --obj.randsize Randomize size of objects so they will be up to the specified size --benchdata value Output benchmark+profile data to this file. By 默认 unique filename is generated. --serverprof value Run MinIO server profiling during benchmark; possible values are 'cpu', 'mem', 'block', 'mutex' and 'trace'. --duration value Duration to run the benchmark. Use 's' and 'm' to specify seconds and minutes. (默认: 5m0s) --autoterm Auto terminate when benchmark is considered stable. --autoterm.dur value Minimum duration where output must have been stable to allow automatic termination. (默认: 10s) --autoterm.pct value The percentage the last 6/25 time blocks must be within current speed to auto terminate. (默认: 7.5) --noclear Do not clear bucket before or after running benchmarks. Use when running multiple clients. --syncstart value Specify a benchmark start time. Time format is 'hh:mm' where hours are specified in 24h format, server TZ. --warp-client value Connect to warp clients and run benchmarks there. --analyze.dur value Split analysis into durations of this length. Can be '1s', '5s', '1m', etc. --analyze.out value Output aggregated data as to file --analyze.op value Only output for this op. Can be GET/PUT/DELETE, etc. --analyze.host value Only output for this host. --analyze.skip value Additional duration to skip when analyzing data. (默认: 0s) --analyze.v Display additional analysis data. --serve value When running benchmarks open a webserver to fetch results remotely, eg: localhost:7762 --help, -h show help ③测试实例 # 1千个测试对象，每个对象文件2kb(2048byte) warp mixed \\ --host=192.168.1.7:9000 \\ --objects=1000 \\ --obj.seze=2048 \\ --access-key=**** \\ --secret-key=**** \\ --autoterm ④分析测试结果数据 warp analyze --analyze.op=GET --analyze.v warp-mixed-2022-10-25[174035]-RQ0u.csv.zst # 138 operations loaded... Done! # --------------------------------------- # Operation: GET (59). Ran 5m59s. Concurrency: 20. # Requests considered: 43: # * Avg: 1m11.796s, 50%: 1m11.55s, 90%: 1m44.48s, 99%: 1m50.607s, Fastest: 29.335s, Slowest: 1m50.607s # * TTFB: Avg: 1.89s, Best: 22ms, 25th: 299ms, Median: 1.451s, 75th: 3s, 90th: 4.824s, 99th: 7.65s, Worst: 7.65s # * First Access: Avg: 1m18.361s, 50%: 1m25.648s, 90%: 1m45.163s, 99%: 2m0.957s, Fastest: 31.606s, Slowest: 2m0.957s # * First Access TTFB: Avg: 1.957s, Best: 22ms, 25th: 299ms, Median: 1.451s, 75th: 3.417s, 90th: 4.824s, 99th: 7.65s, Worst: 7.65s # * Last Access: Avg: 1m7.275s, 50%: 1m4.377s, 90%: 1m43.378s, 99%: 2m0.957s, Fastest: 31.606s, Slowest: 2m0.957s # * Last Access TTFB: Avg: 2.87s, Best: 85ms, 25th: 1.466s, Median: 3.107s, 75th: 4.008s, 90th: 6.056s, 99th: 7.65s, Worst: 7.65s # Throughput: # * Average: 1.68 MiB/s, 0.17 obj/s # Throughput, split into 271 x 1s: # * Fastest: 1922.0KiB/s, 0.19 obj/s (1s, starting 17:42:48 CST) # * 50% Median: 1854.0KiB/s, 0.18 obj/s (1s, starting 17:42:28 CST) # * Slowest: 930.9KiB/s, 0.09 obj/s (1s, starting 17:44:16 CST) 七、Minio应用场景 1、同步备份MySQL物理文件 # 创建Bucket mc mb synology-minio/mysql-backups mc mirror --overwrite --watch /usr/local/var/mysql56 synology-minio/mysql-backups/20221024-15 八、不支持的S3 API BucketAPI BucketACL (可以用 bucket policies) BucketCORS (所有HTTP方法的所有存储桶都默认启用CORS) BucketLifecycle (Minio纠删码不需要) BucketReplication (可以用 mc mirror) BucketVersions, BucketVersioning (可以用 s3git) BucketWebsite (可以用 caddy or nginx) BucketAnalytics, BucketMetrics, BucketLogging (可以用 bucket notification APIs) BucketRequestPayment BucketTagging Object API ObjectACL (可以用 bucket policies) ObjectTorrent Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-26 13:40:53 "},"origin/juicefs.html":{"url":"origin/juicefs.html","title":"JuiceFS","keywords":"","body":"JucisFS 一、简介 Github：https://github.com/juicedata/juicefs 中文文档：https://juicefs.com/docs/zh/community/introduction/# JuiceFS 采用「数据」与「元数据」分离存储的架构，从而实现文件系统的分布式设计。使用 JuiceFS 存储数据，数据本身会被持久化在对象存储（例如，Amazon S3），相对应的元数据可以按需持久化在 Redis、MySQL、TiKV、SQLite 等多种数据库中。 核心特性 POSIX 兼容：像本地文件系统一样使用，无缝对接已有应用，无业务侵入性； HDFS 兼容：完整兼容 HDFS API，提供更强的元数据性能； S3 兼容：提供 S3 网关 实现 S3 协议兼容的访问接口； 云原生：通过 Kubernetes CSI 驱动 轻松地在 Kubernetes 中使用 JuiceFS； 分布式设计：同一文件系统可在上千台服务器同时挂载，高性能并发读写，共享数据； 强一致性：确认的文件修改会在所有服务器上立即可见，保证强一致性； 强悍性能：毫秒级延迟，近乎无限的吞吐量（取决于对象存储规模），查看性能测试结果； 数据安全：支持传输中加密（encryption in transit）和静态加密（encryption at rest），查看详情； 文件锁：支持 BSD 锁（flock）和 POSIX 锁（fcntl）； 数据压缩：支持 LZ4 和 Zstandard 压缩算法，节省存储空间。 应用场景 JuiceFS 为海量数据存储设计，可以作为很多分布式文件系统和网络文件系统的替代，特别是以下场景： 大数据分析：HDFS 兼容，没有任何特殊 API 侵入业务；与主流计算引擎（Spark、Presto、Hive 等）无缝衔接；无限扩展的存储空间；运维成本几乎为 0；完善的缓存机制，高于对象存储性能数倍。 机器学习：POSIX 兼容，可以支持所有机器学习、深度学习框架；共享能力提升团队管理、使用数据效率。 容器集群中的持久卷：Kubernetes CSI 支持；持久存储并与容器生存期独立；强一致性保证数据正确；接管数据存储需求，保证服务的无状态化。 共享工作区：可以在任意主机挂载；没有客户端并发读写限制；POSIX 兼容已有的数据流和脚本操作。 数据备份：在无限平滑扩展的存储空间备份各种数据，结合共享挂载功能，可以将多主机数据汇总至一处，做统一备份。 文件存储原理 元数据存储Redis内存使用量与文件系统的关系 JuiceFS 元数据引擎的使用空间主要与文件系统中的文件数量有关。每一个文件的元数据会大约占用 300 字节内存 # 如果最大1GB内存的Redis，减去124M的系统消耗，可用900M，可存储3145728个JuiceFS文件元数据 1024M-124M=900M=943718400 Bytes 943718400 Bytes / 300 Bytes = 314,5728 # 如果最大1GB内存的Redis，减去88M的系统消耗，可用944M，可存储3299519个JuiceFS文件元数据 1024M-88M=944 M=989855744 Bytes 989855744 Bytes / 300 Bytes = 329,9519 # 如要存储1亿个文件，大约需要 30GiB 内存 300Bytes * 1,0000,0000 =300,0000,0000 Bytes ~= 28610.3Mb ~= 27.94GB 通过 Redis 的 INFO memory 命令查看具体的内存使用量 > INFO memory used_memory: 19167628056 used_memory_human: 17.85G used_memory_rss: 20684886016 used_memory_rss_human: 19.26G ... used_memory_overhead: 5727954464 ... used_memory_dataset: 13439673592 used_memory_dataset_perc: 70.12% 其中 used_memory_rss 是 Redis 实际使用的总内存大小，这里既包含了存储在 Redis 中的数据大小（也就是上面的 used_memory_dataset），也包含了一些 Redis 的系统开销（也就是上面的 used_memory_overhead）。前面提到每个文件的元数据大约占用 300 字节是通过 used_memory_dataset 来计算的，如果你发现你的 JuiceFS 文件系统中单个文件元数据占用空间远大于 300 字节，可以尝试运行 juicefs gc命令来清理可能存在的冗余数据。 JuiceFS安全问题 元数据存储安全 客户端挂载安全 二、安装 Linux JFS_LATEST_TAG=$(curl -s https://api.github.com/repos/juicedata/juicefs/releases/latest | grep 'tag_name' | cut -d '\"' -f 4 | tr -d 'v') wget \"https://github.com/juicedata/juicefs/releases/download/v${JFS_LATEST_TAG}/juicefs-${JFS_LATEST_TAG}-linux-amd64.tar.gz\" tar -zxf \"juicefs-${JFS_LATEST_TAG}-linux-amd64.tar.gz\" mv juicefs /usr/local/bin MacOS brew tap juicedata/homebrew-tap brew install juicefs Windows 参考：https://juicefs.com/docs/zh/community/installation#windows-%E7%B3%BB%E7%BB%9F Docker FROM ubuntu:20.04 RUN apt update && apt install -y curl fuse && \\ apt-get autoremove && \\ apt-get clean && \\ rm -rf \\ /tmp/* \\ /var/lib/apt/lists/* \\ /var/tmp/* RUN set -x && \\ mkdir /juicefs && \\ cd /juicefs && \\ JFS_LATEST_TAG=$(curl -s https://api.github.com/repos/juicedata/juicefs/releases/latest | grep 'tag_name' | cut -d '\"' -f 4 | tr -d 'v') && \\ curl -s -L \"https://github.com/juicedata/juicefs/releases/download/v${JFS_LATEST_TAG}/juicefs-${JFS_LATEST_TAG}-linux-amd64.tar.gz\" \\ | tar -zx && \\ install juicefs /usr/bin && \\ cd .. && \\ rm -rf /juicefs CMD [ \"juicefs\" ] 客户端更新 JuiceFS 客户端只有一个二进制程序，升级新版只需用新版程序替换旧版程序即可。 使用预编译客户端：可以参照「安装」文档中相应系统的安装方法，下载最新的客户端，覆盖旧版客户端即可。 手动编译客户端：可以拉取最新的源代码重新编译，覆盖旧版客户端即可，具体请参考「安装」文档。 **注意** 对于已经使用旧版 JuiceFS 客户端挂载好的文件系统，需要先[卸载文件系统](https://juicefs.com/docs/zh/community/getting-started/for_distributed#7-卸载文件系统)，然后用新版 JuiceFS 客户端重新挂载。 卸载文件系统时需确保没有任何应用正在访问，否则将会卸载失败。不可强行卸载文件系统，有可能造成应用无法继续正常访问。 三、部署 1、元数据存储Redis配置要求 是 Redis 6.0 之后引入的，如果没有用户名可以忽略，但密码前面的 : 冒号需要保留，如 redis://:@:6379/1 如果没有改变Redis默认端口号6379，可以不用填写，如 redis://:@/1，否则需要显式指定端口号 对于Redis哨兵模式，META-URL 可以指定为redis[s]://[[USER]:PASSWORD@]MASTER_NAME,SENTINEL_ADDR[,SENTINEL_ADDR]:SENTINEL_PORT[/DB] 2、对象存储OSS配置 ①创建RAM子用户 禁止控制台访问 获取Access Key和Access Secret ②创建自定义策略 { \"Version\": \"1\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"oss:DeleteObject\", \"oss:GetObject\", \"oss:HeadObject\", \"oss:PutObject\" ], \"Resource\": \"acs:oss:*::/*\" } ] } ③RAM用户绑定自定义策略 3、创建文件系统 ①Redis+OSS set +o history export ACCESS_KEY=对象存储引擎OSS的用户名 export SECRET_KEY=对象存储引擎OSS的用户密码 export META_PASSWORD=元数据存储引擎redis密码 juicefs format \\ --storage oss \\ --bucket https://OSS Bucket内网访问URL \\ --capacity 100 \\ --inodes 100 \\ \"redis://127.0.0.1:6379/1\" \\ testjfs ②Redis+Minio MinIO是开源的轻量级对象存储，兼容 Amazon S3 API。 JuiceFS 仅支持路径风格的 MinIO URI 地址，例如：http://127.0.0.1:9000/myjfs。 MINIO_REGION 环境变量可以用于设置 MinIO 的 region，如果不设置，默认为 us-east-1。 set +o history export ACCESS_KEY=对象存储引擎Minio的用户名 export SECRET_KEY=对象存储引擎Minio的用户密码 export META_PASSWORD=元数据存储引擎redis密码 juicefs format \\ --storage minio \\ --bucket http://127.0.0.1:9000/juicefs \\ \"redis://127.0.0.1:6379/1\" \\ testjfs ③Redis+WebDAV WebDAV 是 HTTP 的扩展协议，有利于用户间协同编辑和管理存储在万维网服务器的文档 set +o history export ACCESS_KEY=对象存储引擎WebDAV的用户名 export SECRET_KEY=对象存储引擎WebDAV的用户密码 export META_PASSWORD=元数据存储引擎redis密码 juicefs format \\ --storage webdav \\ --bucket http:/// \\ --capacity 100 \\ --inodes 100 \\ \"redis://127.0.0.1:6379/1\" \\ testjfs 三、挂载 0 、挂载参数 参数 含义 默认值 --metrics value 监控数据导出地址 127.0.0.1:9567 --consul value consul 注册中心地址 127.0.0.1:8500 --no-usage-report 不发送使用量信息 false -d, --background 后台运行 false --no-syslog 禁用系统日志 false --log value 后台运行时日志文件的位置 $HOME/.juicefs/juicefs.log /var/log/juicefs.log -o value 其他 FUSE 选项 --attr-cache value 属性缓存过期时间；单位为秒 1 --entry-cache value 文件项缓存过期时间；单位为秒 1 --dir-entry-cache value 目录项缓存过期时间；单位为秒 1 --enable-xattr 启用扩展属性 (xattr) 功能 false --bucket value 为当前挂载点指定访问访对象存储的 endpoint --get-timeout value 下载一个对象的超时时间；单位为秒 60 --put-timeout value 上传一个对象的超时时间；单位为秒 60 --io-retries value 网络异常时的重试次数 10 --max-uploads value 上传对象的连接数 20 --max-deletes value 删除对象的连接数 2 --buffer-size value 读写缓存的总大小；单位为 MiB 300 --upload-limit value 上传带宽限制，单位为 Mbps 0 --download-limit value 下载带宽限制，单位为 Mbps 0 --prefetch value 并发预读 N 个块 1 --writeback 后台异步上传对象 false --cache-dir value 本地缓存目录路径；使用 : (Linux、macOS)或 ; (Windows)隔离多个路径 --cache-size value 缓存对象的总大小；单位为 MiB 102400 --free-space-ratio value 最小剩余空间比例 0.1 --cache-partial-only 仅缓存随机小块读 false --read-only 只读模式 false --open-cache value 打开的文件的缓存过期时间(0代表关闭这个特性）单位为秒 0 --subdir value 将某个子目录挂载为根 \"$HOME/.juicefs/cache\" \"/var/jfsCache\" --backup-meta value 自动备份元数据到对象存储的间隔时间；单位秒 (0 表示不备份) 3600 --heartbeat value 发送心跳的间隔 (秒);建议所有客户端使用相同的心跳值 12 --upload-delay value 数据上传到对象存储的延迟时间，支持秒分时精度，对应格式分别为 (\"s\", \"m\", \"h\") 0 --no-bgjob 禁用后台作业（清理、备份等） false 1、Linux挂载 juicefs mount 挂载参数 元数据存储引擎访问地址 挂载点 set +o history export META_PASSWORD=元数据存储引擎redis密码 juicefs mount -d --metrics 0.0.0.0:9567 redis://127.0.0.1/1 /root/test 2、MacOS挂载 JuiceFS 通过 FUSE 实现 POSIX 接口的兼容，在 macOS 系统上原生并没有提供 FUSE 的支持，需要借助 macFUSE 来实现相应接口的抽象。 MacFUSE官网：https://osxfuse.github.io set +o history export META_PASSWORD=元数据存储引擎redis密码 juicefs mount -d --metrics 0.0.0.0:9567 redis://127.0.0.1/1 /root/test 参考：https://juicefs.com/zh-cn/blog/usage-tips/using-juicefs-on-apple-m1 3、自动挂载 拷贝 juicefs 为 /sbin/mount.juicefs，然后按照下面的格式添加一行到 /etc/fstab： juicefs _netdev[,其他挂载参数] 0 0 # 例如： redis://localhost:6379/1 /jfs juicefs _netdev,max-uploads=50,writeback,cache-size=204800 0 0 set +o history export META_PASSWORD=元数据存储引擎redis密码 && mount -a 默认情况下，CentOS 6 在启动后不会自动挂载网络文件系统，你可以使用下面的命令开启它： sudo chkconfig --add netfs 四、管理 1、配置文件系统 juicefs config redis://127.0.0.1:6379/1 juicefs config redis://127.0.0.1:6379/1 --capacity 100 juicefs config redis://127.0.0.1:6379/1 --inodes 100 2、回收站设置 JuiceFS的回收站功能需要使用 1.0.0 及以上版本 回收站本身是一个名为 .trash 的目录，会被自动创建在 JuiceFS 的根目录 / 下，一般即对应挂载点目录。 回收站内固定只有两级深度。第一级为根据时间以 年-月-日-小时 格式命名的目录（如 2021-11-30-10），系统会自动创建它，并将在这个小时内删除的所有文件都放在此目录下。第二级即为平铺的用户文件和空目录（通常的 rm -r 命令实际会先逐个删除目标目录下文件，再删除空目录）。第一级目录的命名取自 UTC 时间，与中国北京时间相差 8 个小时。 回收站内不再保留原来的目录树结构，为了能在不影响正常操作性能的前提下，尽可能提供恢复原树型结构的信息，回收站内的文件被自动重命名成 {父目录 inode}-{文件 inode}-{原始文件名} 格式。其中 inode 是文件系统内部的管理信息，如果用户并不需求文件原始路径，则直接关注最后的原始文件名即可。 JuiceFS 设计并默认开启了回收站功能，会自动将用户删除的文件移动到隐藏的回收站目录内，保留一段时间后才将数据真正清理。 旧版本 JuiceFS 欲使用回收站，需要在升级所有挂载点后通过 config 命令手动将 --trash-days 改为需要的正整数值。 juicefs config META-URL --trash-days 7 # 默认值为 1，意味着回收站内文件会在一天后被自动清理。 # 参数值设为 0 即可禁用回收站功能，系统会在短时间内清空回收站，并使得后续应用删除的文件能被立即清理。 3、销毁文件系统 销毁文件系统时，需要确认卸载了所有挂载点后再行操作。不然则会出现 1 sessions are active, please disconnect them first的报错 juicefs destroy ：元数据引擎的 URL 地址； ：文件系统的 UUID。 # 获取文件系统UUID juicefs status redis://127.0.0.1:6379/1 # 销毁文件系统。在销毁文件系统时，客户端会发出确认提示，请务必仔细核对文件系统信息，确认无误后输入 y 确认。 juicefs destroy redis://127.0.0.1:6379/1 文件系统UUID 4、限制存储限额 JuiceFS v0.14.2 开始支持文件系统级别的存储配额，该功能包括： 限制文件系统的总可用容量 --capacity 设置容量限额，单位 GiB set +o history export ACCESS_KEY=对象存储引擎WebDAV的用户名 export SECRET_KEY=对象存储引擎WebDAV的用户密码 export META_PASSWORD=元数据存储引擎redis密码 juicefs format \\ --storage oss \\ --bucket https://OSS Bucket内网访问URL \\ --capacity 100 \\ \"redis://127.0.0.1:6379/1\" \\ testjfs # 对于已创建的文件系统，可再进行设置 juicefs config redis://127.0.0.1:6379/1 --capacity 1000 限制文件系统的 inode 总数 在 Linux 系统中，每个文件（文件夹也是文件的一种）不论大小都有一个 inode，因此限制 inode 数量等同于限制文件数量。 --inodes 设置限额 juicefs format \\ --storage oss \\ --bucket https://OSS Bucket内网访问URL \\ --inodes 100 \\ \"redis://127.0.0.1:6379/1\" \\ testjfs # 对于已创建的文件系统，可再进行设置 juicefs config redis://127.0.0.1:6379/1 --inodes 1000 存储限额设置会保存在元数据引擎中以供所有挂载点读取，每个挂载点的客户端也会缓存自己的已用容量和 inodes 数，每秒向元数据引擎同步一次。与此同时，客户端每 10 秒会从元数据引擎读取最新的用量值，从而实现用量信息在每个挂载点之间同步，但这种信息同步机制并不能保证用量数据被精确统计。 五、监控 1、命令监控信息 ①挂载命令 juicefs stats 命令，以类似 Linux dstat 工具的形式可以实时打印各个指标的每秒变化情况 juicefs stats ~/juicefsf-test3 ②监控指标 参数 指标 含义 usage cpu 进程的 CPU 使用率 mem 进程的物理内存使用量 buf 进程已使用的 Buffer 大小；此值受限于挂载选项 --buffer-size fuse ops/lat 通过 FUSE 接口处理的每秒请求数及其平均时延（单位为毫秒） read/write 通过 FUSE 接口处理的读写带宽 meta ops/lat 每秒处理的元数据请求数和平均时延（单位为毫秒）注意部分能在缓存中直接处理的元数据请求未列入统计，以更好地体现客户端与元数据引擎交互的耗时 blockcache read/write 客户端本地数据缓存的每秒读写流量 object get/put 客户端与对象存储交互的 Get/Put 每秒流量 ③参考 https://juicefs.com/docs/zh/community/stats_watcher 2、Prometheus ①挂载配置 挂载时，juicefs命令可使用--metrics参数自定义暴露出挂载时Metrics Endpoint juicefs mount -d --metrics 0.0.0.0:9567 redis://127.0.0.1/1 /root/test1 juicefs mount -d --metrics 0.0.0.0:9568 redis://127.0.0.1/1 /root/test2 ②Prometheus配置 Prometheus添加刮取juicefs客户端的配置 ....省略.... - job_name: \"juicefs-client\" static_configs: - targets: - \"127.0.0.1:9567\" - \"127.0.0.1:9568\" ③Grafana配置 Grafana导入图表JSON数据，文件地址：https://github.com/juicedata/juicefs/blob/main/docs/en/grafana_template.json ④监控指标 文件系统 | 名称 | 描述 | 单位 | | --------------------- | -------------- | ---- | | juicefs_used_space | 总使用空间 | 字节 | | juicefs_used_inodes | 总 inodes 数量 | | 操作系统 | 名称 | 描述 | 单位 | | ------------------- | ---------- | ---- | | juicefs_uptime | 总运行时间 | 秒 | | juicefs_cpu_usage | CPU 使用量 | 秒 | | juicefs_memory | 内存使用量 | 字节 | 元数据引擎 | 名称 | 描述 | 单位 | | ------------------------------------------------- | -------------- | ---- | | juicefs_transaction_durations_histogram_seconds | 事务的延时分布 | 秒 | | juicefs_transaction_restart | 事务重启的次数 | | FUSE | 名称 | 描述 | 单位 | | ---------------------------------------------- | -------------------- | ---- | | juicefs_fuse_read_size_bytes | 读请求的大小分布 | 字节 | | juicefs_fuse_written_size_bytes | 写请求的大小分布 | 字节 | | juicefs_fuse_ops_durations_histogram_seconds | 所有请求的延时分布 | 秒 | | juicefs_fuse_open_handlers | 打开的文件和目录数量 | | SDK | 名称 | 描述 | 单位 | | --------------------------------------------- | ------------------ | ---- | | juicefs_sdk_read_size_bytes | 读请求的大小分布 | 字节 | | juicefs_sdk_written_size_bytes | 写请求的大小分布 | 字节 | | juicefs_sdk_ops_durations_histogram_seconds | 所有请求的延时分布 | 秒 | 缓存 | 名称 | 描述 | 单位 | | --------------------------------------- | ---------------------- | ---- | | juicefs_blockcache_blocks | 缓存块的总个数 | | | juicefs_blockcache_bytes | 缓存块的总大小 | 字节 | | juicefs_blockcache_hits | 命中缓存块的总次数 | | | juicefs_blockcache_miss | 没有命中缓存块的总次数 | | | juicefs_blockcache_writes | 写入缓存块的总次数 | | | juicefs_blockcache_drops | 丢弃缓存块的总次数 | | | juicefs_blockcache_evicts | 淘汰缓存块的总次数 | | | juicefs_blockcache_hit_bytes | 命中缓存块的总大小 | 字节 | | juicefs_blockcache_miss_bytes | 没有命中缓存块的总大小 | 字节 | | juicefs_blockcache_write_bytes | 写入缓存块的总大小 | 字节 | | juicefs_blockcache_read_hist_seconds | 读缓存块的延时分布 | 秒 | | juicefs_blockcache_write_hist_seconds | 写缓存块的延时分布 | 秒 | 对象存储 | 名称 | 描述 | | -------- | ------------------------------------------------- | | method | 请求对象存储的方法（例如 GET、PUT、HEAD、DELETE） | 指标 | 名称 | 描述 | 单位 | | ---------------------------------------------------- | ------------------------ | ---- | | juicefs_object_request_durations_histogram_seconds | 请求对象存储的延时分布 | 秒 | | juicefs_object_request_errors | 请求失败的总次数 | | | juicefs_object_request_data_bytes | 请求对象存储的总数据大小 | 字节 | 内部特性 | 名称 | 描述 | 单位 | | -------------------------------------- | ------------------ | ---- | | juicefs_compact_size_histogram_bytes | 合并数据的大小分布 | 字节 | ⑤参考 https://juicefs.com/docs/zh/community/administration/monitoring/ https://juicefs.com/docs/zh/community/p8s_metrics 六、测试 1、基础测试 基准性能测试流程 N 并发各写 1 个 1 GiB 的大文件，IO 大小为 1 MiB N 并发各读 1 个之前写的 1 GiB 的大文件，IO 大小为 1 MiB N 并发各写 100 个 128 KiB 的小文件，IO 大小为 128 KiB N 并发各读 100 个之前写的 128 KiB 的小文件，IO 大小为 128 KiB N 并发各 stat 100 个之前写的 128 KiB 的小文件 清理测试用的临时目录 并发数 N 的值即由 bench 命令中的 -p 参数指定 juicefs bench -p 4 /root/test 2、FIO基准测试 fio --name=sequential-read --directory=/root/test --rw=read --refill_buffers --bs=4M --size=4G fio --name=sequential-read --directory=/root/test --rw=read --refill_buffers --bs=4M --size=4G fio --name=sequential-read --directory=/root/test --rw=read --refill_buffers --bs=4M --size=4G 3、手动文件读写测试 大量小文件读写测试 for ((i = 1; i /dev/null ;done for ((i = 1; i /dev/null ;done 大文件读写测试 5、测试总结对比 测试文件 JuiceFS 阿里云NAS Copy 单个900M文件 5.232s 8.35s Copy 1W个8K文件 3m33s 2m01s Copy 10W个8K文件 36m30s 18m16s DELETE 1W个8K文件 18.76s 54.28s DELETE 10W个8K文件 七、元数据存储备份 JuiceFS v0.15.2 开始支持元数据手动备份、恢复和引擎间迁移。 JuiceFS v1.0.0 开始支持元数据自动备份 JuiceFS 支持多种元数据存储引擎，且各引擎内部的数据管理格式各有不同。为了便于管理，JuiceFS 提供了 dump 命令允许将所有元数据以统一格式写入到 JSON 文件进行备份。同时，JuiceFS 也提供了 load 命令，允许将备份恢复或迁移到任意元数据存储引擎 JSON 备份只能恢复到 新创建的数据库 或 空数据库 中。 1、手动备份 juicefs dump 仅保证单个文件自身的完整性，不提供全局时间点快照的功能，如在 dump 过程中业务仍在写入，最终结果会包含不同时间点的信息。 juicefs dump redis://127.0.0.1:6379/1 meta.dump 该命令默认从根目录 / 开始，深度遍历目录树下所有文件，将每个文件的元数据信息按 JSON 格式写入到文件。 2、自动备份 从 JuiceFS v1.0.0 开始，不论文件系统通过 mount 命令挂载，还是通过 JuiceFS S3 网关及 Hadoop Java SDK 访问，客户端每小时都会自动备份元数据并拷贝到对象存储。 备份的文件存储在对象存储的 meta 目录中，它是一个独立于数据存储的目录，在挂载点中不可见，也不会与数据存储之间产生影响，用对象存储的文件浏览器即可查看和管理。 虽然自动备份元数据成为了客户端的默认动作，但在多主机共享挂载同一个文件系统时并不会发生备份冲突。 JuiceFS 维护了一个全局的时间戳，确保同一时刻只有一个客户端执行备份操作。当客户端之间设置了不同的备份周期，那么就会以周期最短的设置为准进行备份。 默认情况下，JuiceFS 客户端每小时备份一次元数据，自动备份的频率可以在挂载文件系统时通过--backup-meta选项进行调整 juicefs mount -d --backup-meta 8h redis://127.0.0.1:6379/1 /mnt # 备份频率可以精确到秒，支持的单位如下： # h：精确到小时，如 1h； # m：精确到分钟，如 30m、1h30m； # s：精确到秒，如 50s、30m50s、1h30m50s; 自动备份清理策略 保留 2 天以内全部的备份； 超过 2 天不足 2 周的，保留每天中的 1 个备份； 超过 2 周不足 2 月的，保留每周中的 1 个备份； 超过 2 个月的，保留每个月中的 1 个备份。 3、恢复 JSON 备份只能恢复到 新创建的数据库 或 空数据库 中。 为了保证对象存储 SecretKey 与 SessionToken 的安全性，juicefs dump 得到的备份文件中的 SecretKey 与 SessionToken 会被改写为“removed”，所以在对其执行 juicefs load 恢复到元数据引擎后，需要使用 juicefs config --secret-key xxxxx META-URL 来重新设置 SecretKey。 juicefs load会自动处理因包含不同时间点文件而产生的冲突问题，并重新计算文件系统的统计信息（空间使用量，inode 计数器等），最后在数据库中生成一份全局一致的元数据。 如果你想自定义某些元数据（请务必小心），可以尝试在 load 前手动修改 JSON 文件。 juicefs load redis://127.0.0.1:26379/1 meta.dump 4、迁移 得益于 JSON 格式的通用性，JuiceFS 支持的所有元数据存储引擎都能识别，因此可以将元数据信息从一种引擎中导出为 JSON 备份，然后再导入到另外一种引擎，从而实现元数据在不同类型引擎间的迁移 juicefs dump redis://127.0.0.1:6379/1 | juicefs load mysql://user:password@(127.0.0.1:3306)/juicefs 参考：https://juicefs.com/docs/zh/community/metadata_dump_load 八、在 Kubernetes 中使用 JuiceFS 参考： https://juicefs.com/docs/zh/cloud/use_juicefs_in_kubernetes https://juicefs.com/docs/zh/csi/introduction/ 参考 https://juicefs.com/zh-cn/blog/engineering/speed-up-oss/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-23 17:43:52 "},"origin/pulsar-basic.html":{"url":"origin/pulsar-basic.html","title":"基础概念","keywords":"","body":"Pulsar 一、简介 1、功能特性 2、Pulsar架构 Broker 一个或多个Broker 处理和负载接受到的生产者发送的消息数据 调度消息发送给消费者 与Zookeeper进行通信以处理各种协调任务， 将消息存储在BookKeeper实例（又称为bookies）中 依赖于ZooKeeper集群来执行某些任务， 等等 Apache Zookeeper（Standby/Cluster） Pulsar使用ZK存储元数据、集群配置，还有协调各Broker 协调由那个Broker响应数据处理 存储Topic主题的元数据 Apache BookKeeper（又称为bookies） 由一个或多个bookies组成的BookKeeper集群来存储需要持久化的消息数据，和消费者消费消息的游标offset Apache BookKeeper是一个分布式的WAL(write-ahead log)系统 二、基础概念 1、namespace namespace是Pulsar中最基本的管理单元， 可以在namespace中进行设置权限、调整副本设置，管理跨集群的消息复制，控制消息策略等关键操作。 一个主题topic可以继承其所对应的namespace的属性，因此我们只需对namespace的属性进行设置，就可以一次性设置该namespace中所有主题topic的属性。 2、namespace分类 namespace有本地namespace和全局namespace两种 本地namespace仅对定义它的集群可见 全局namespace跨集群可见，可以使同一个数据中心的集群，也可以是跨地域中心的集群，这取决于namespace中是否设置了跨集群拷贝数据功能 本地和全局namespace都可以通过一定设置进行跨团队和跨组织共享。一旦生产者获得了namespace的写入权限，那么它就可以向namespace中的所有topic主题写入数据。若主题不存在，则在第一次写入时动态创建。 3、Topic Schema {persistent|non-persistent}://tenant/namespace/topic {持久化|非持久化}://租户/命名空间/主题 4、Topic主题类型 持久化的：persistent://tenant/namespace/topic-name 默认情况下，Pulsar将所有未确认的消息持久存储在多个 BookKeeper（存储节点）上。因此，持久主题上的消息数据可以在broker重新启动和订阅者故障切换后继续存在。Pulsar 在收到消息之后，将消息发送给多个 BookKeeper 节点（具体由复制系数来定），节点将数据写入预写式日志（write ahead log），同时在内存里也保存一份。节点在对消息进行确认之前，强制将日志写入到持久化的存储上，因此即使出现电力故障，数据也不会丢失。 非持久化的：non-persistent://tenant/namespace/topic-name 消息不会持久化到磁盘，而只存在于内存中。当使用非持久性主题时，停止Pulsar Broker或断开主题订阅者的连接，意味着该（非持久性）主题上所有传输中的消息都会丢失，同时客户端可能会看到消息丢失。 5、支持的消息压缩格式： LZ4 ZLIB ZSTD SNAPPY 6、生产者发送消息模式 同步：发送每条消息后，生产者将等待Broker的确认。如果未收到确认，则生产者将发送操作视为失败 异步：将把消息放于阻塞队列中，并立即返回。然后，客户端将在后台将消息发送给 broker 如果队列已满(最大大小可配置)，则调用 API 时，producer 可能会立即被阻止或失败，具体取决于传递给 producer 的参数。 7、消费者接受消息模式 同步 异步 8、订阅模式 exclusive(独家)：只允许有一个消费者 shared（共享）：允许多个消费者，消费者间机会均等。消息通过轮询机制分发给不同的消费者，并且每个消息仅会被分发给一个消费者。当消费者断开连接，所有发送给他，但没有被确认的消息将被重新安排，分发给其它存活的消费者。 failover(灾备)：允许多个消费者，消费者有主从之分，主消费者负责接受数据，主消费者挂掉以后，从消费者代替主消费者接着接受数据 key_shared：允许多个消费者，具有相同key或相同订阅key的消息仅传递给一个使用者。 不管消息被重新发送多少次，它都会被发送到同一使用者。当消费者连接或断开连接时，将导致服务的消费者更改某些消息键。 9、多主题订阅 当consumer订阅pulsar的主题时，它默认指定订阅了一个主题，例如：persistent://public/default/my-topic。 从Pulsar的1.23.0-incubating的版本开始，Pulsar消费者可以同时订阅多个topic 多主题订阅方式： 正则匹配：persistent://public/default/finance-.* 明确指定的topic列表 非持久化主题内的数据处理速度比持久化主题快 消息的默认保留，过期处理方式 立即删除消费者已确认的所有消息 以消息backlog的形式，持久保存所有的未被确认消息 Pulsar 支持保证一条消息只能在broker服务端被持久化一次的特性，即消息去重功能 10、消息路由策略 单独分区（SinglePartition）：生产者随机挑选一个分区，并将数据写入该分区。该策略与非分区主题提供的保证是一样的，不过如果有多个生产者向同一个主题写入数据，该策略就会很有用。 轮询分区（RoundRobinPartition）：生产者通过轮询的方式将数据平均地分布到各个分区上。比如，第一个消息写入第一个分区，第二个消息写入第二个分区，并以此类推。 哈希分区（hash）：每个消息会带上一个键，要写入哪个分区取决于它所带的键。这种分区方式可以保证次序。 自定义分区：生产者使用自定义函数生成分区对应的数值，然后根据这个数值将消息写入对应的分区。 11、消息保留策略 消息存留策略 存留规则会被用于某namespace下所有的topic，指明哪些消息会被持久存储，即使已经被确认过。 没有被留存规则覆盖的消息将会被删除。 消息过期策略 设置在namespace上的TTL，消息即使没有被确认消费，也会被删除掉。 参考 https://jack-vanlightly.com/blog/2018/10/2/understanding-how-apache-pulsar-works https://www.infoq.cn/article/2017/11/apache-pulsar-brief-introduction Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-28 10:57:00 "},"origin/pulsar-install.html":{"url":"origin/pulsar-install.html","title":"安装部署","keywords":"","body":"Apache Pulsar的安装部署 一、简介 二、二进制安装 1、prerequisite 三台机器硬件：8核16G内存100G系统磁盘500G数据磁盘的CentOS 7（数据磁盘挂载到/data目录） 三台机器IP地址：192.168.1.121~123 三台都为Zookeeper集群、Apache Bookkeeper集群、Broker集群 三台机器安装Java JDK（过程省略） 2、下载 pulsar_version=2.7.1 curl -s -# https://archive.apache.org/dist/pulsar/pulsar-$pulsar_version/apache-pulsar-$pulsar_version-bin.tar.gz | tar zxvf - -C /opt ln -s /opt/apache-pulsar-$pulsar_version /opt/pulsar echo \"export PULSAR_HOME=/opt/pulsar\\nexport PATH=$PATH:$PULSAR_HOME/bin\" >> /etc/profile mkdir -p /data/pulsar/{data/bookkeeper/journal,logs} /data/zookeeper/{data,logs} source /etc/profile 3、下载connectors pulsar_version=2.7.1 && \\ mkdir /opt/pulsar/connectors && \\ nohup wget https://apachemirror.sg.wuchna.com/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-kafka-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.kddi-research.jp/infosystems/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-redis-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://mirror-hk.koddos.net/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-netty-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.tsukuba.wide.ad.jp/software/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-jdbc-mariadb-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.wayne.edu/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-file-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.jaist.ac.jp/pub/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-elastic-search-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.jaist.ac.jp/pub/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-kafka-connect-adaptor-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.tsukuba.wide.ad.jp/software/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-canal-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.kddi-research.jp/infosystems/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-influxdb-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://apache.website-solution.net/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-rabbitmq-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.yz.yamagata-u.ac.jp/pub/network/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-hdfs2-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.jaist.ac.jp/pub/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-hdfs3-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! nohup wget https://ftp.jaist.ac.jp/pub/apache/pulsar/pulsar-$pulsar_version/connectors/pulsar-io-jdbc-postgres-$pulsar_version.nar -P /opt/pulsar/connectors >/dev/null &! 之后将整个/opt/apache-pulsar-2.7.1目录拷贝到另外台主机上，设置一下软连，配置一下系统变量 for i in {2..3};do scp -r /opt/apache-pulsar-2.7.1 root@192.168.1.12$i:/opt/ ; ssh root@192.168.1.12$i -c 'ln -s /opt/apache-pulsar-2.7.1 /opt/pulsar && ' done 4、部署Zookeeper集群 sed -i \\ -e 's/dataDir=data\\/zookeeper/dataDir=\\/data\\/zookeeper\\/data/g' \\ -e '$a server.1=192.168.1.121:2888:3888\\nserver.2=192.168.1.122:2888:3888\\nserver.3=192.168.1.123:2888:3888\\n' \\ /opt/pulsar/conf/zookeeper.conf && \\ echo 1 > /data/zookeeper/data/myid && \\ PULSAR_EXTRA_OPTS=\"-Dstats_server_port=8001\" pulsar-daemon start zookeeper && \\ jps -l && \\ netstat -lanp|grep 2181 上述命令三台机器要执行 5、初始化pulsar集群元数据到Zookeeper中 下述命令只用执行一遍即可 pulsar initialize-cluster-metadata \\ --cluster pulsar-cluster-prod \\ --zookeeper 192.168.1.121:2181 \\ --configuration-store 192.168.1.121:2181 \\ --web-service-url http://192.168.1.121:8080,192.168.1.122:8080,192.168.1.123:8080 \\ --broker-service-url pulsar://192.168.1.121:6650,192.168.1.122:6650,192.168.1.123:6650 6、部署Apache Bookkeeper sed -i \\ -e 's/journalDirectory=data\\/bookkeeper\\/journal/journalDirectory=\\/data\\/pulsar\\/data\\/bookkeeper\\/journal/g' \\ -e 's/ledgerDirectories=data\\/bookkeeper\\/ledgers/ledgerDirectories=\\/data\\/pulsar\\/data\\/bookkeeper\\/ledgers/g' \\ -e 's/zkServers=localhost:2181/zkServers=192.168.1.121:2181,192.168.1.122:2181,192.168.1.123:2181/g' \\ /opt/pulsar/conf/bookkeeper.conf && \\ pulsar-daemon start bookie && \\ jps -l 7、部署Broker sed -i \\ -e 's/zookeeperServers=/zookeeperServers=192.168.1.121:2181,192.168.1.122:2181,192.168.1.123:2181/g' \\ -e 's/configurationStoreServers=/configurationStoreServers=192.168.1.121:2181,192.168.1.122:2181,192.168.1.123:2181/g' \\ -e 's/clusterName=/clusterName=pulsar-cluster-prod/g' \\ /opt/pulsar/conf/broker.conf && \\ pulsar-daemon start broker && \\ jps -l 8、测试 ①配置客户端配置 sed -i \\ -e 's/webServiceUrl=http:\\/\\/localhost:8080\\//webServiceUrl=http:\\/\\/192.168.1.121:8080,192.168.1.122:8080,192.168.1.123:8080\\//g' \\ -e 's/brokerServiceUrl=pulsar:\\/\\/localhost:6650\\//brokerServiceUrl=pulsar:\\/\\/192.168.1.121:6650,192.168.1.122:6650,192.168.1.123:6650\\//g' \\ /opt/pulsar/conf/client.conf ②配置命名空间的权限 pulsar-admin namespaces set-persistence -a 3 -e 3 -w 3 -r 3 public/default pulsar-admin namespaces get-persistence public/default ③创建消费者消费消息 pulsar-client consume persistent://public/default/test -n 100 -s \"consumer-test\" -t \"Exclusive\" ④创建生产者产生消息 pulsar-client produce persistent://public/default/test -n 1 -m \"Hello Pulsar\" 三、Docker(standalone) version: \"3.4\" services: standalone: container_name: standalone-pulsar hostname: localhost image: streamnative/sn-pulsar:2.10.1.6 volumes: - /data/pulsar/data:/pulsar/data command: > bash -c \"bin/apply-config-from-env.py conf/standalone.conf && exec bin/pulsar standalone -nss -nfw\" # disable stream storage and functions worker environment: allowAutoTopicCreationType: partitioned brokerDeleteInactiveTopicsEnabled: \"false\" PULSAR_PREFIX_messagingProtocols: kafka PULSAR_PREFIX_kafkaListeners: PLAINTEXT://0.0.0.0:9092 PULSAR_PREFIX_kafkaAdvertisedListeners: PLAINTEXT://0.0.0.0:9092 PULSAR_PREFIX_brokerEntryMetadataInterceptors: org.apache.pulsar.common.intercept.AppendIndexMetadataInterceptor ports: - 6650:6650 - 8080:8080 - 9092:9092 四、Kubernetes helm repo add pulsar https://pulsar.apache.org/charts helm update helm search repo pulsar -l latest_version=$(helm search repo pulsar -l | grep -v \"CHART VERSION\" | awk '{print $3}' | sort -n | tail -1) helm show values pulsar/pulsar > pulsar-$latest_version-values.yaml helm upgrade --install pulsar -n pulsar -f pulsar-$latest_version-values.yaml pulsar/pulsar 五、Pulsar Manager Github：https://github.com/apache/pulsar-manager#access-pulsar-manager 以二进制方式安装为例，docker或k8s相关安装配置的参考：https://github.com/apache/pulsar-manager#access-pulsar-manager 和 https://github.com/apache/pulsar-manager/blob/master/src/README.md 1、下载安装 pulsar_manager_version=0.2.0 curl -s -# https://dist.apache.org/repos/dist/release/pulsar/pulsar-manager/pulsar-manager-0.2.0/apache-pulsar-manager-$pulsar_manager_version-bin.tar.gz | tar zxvf - -C /tmp && \\ tar -xvf /tmp/pulsar-manager/pulsar-manager.tar -C /opt && \\ cp -r /tmp/pulsar-manager/dist /opt/pulsar-manager/ui && \\ rm -rf /tmp/pulsar-manager 2、编辑配置文件 只修改/opt/pulsar-manager/application.properties的以下配置项，其他不用 # 开启Swagger swagger.enabled=true # 设置默认集群 default.environment.name=pulsar-cluster-prod default.environment.service_url=http://127.0.0.1:8080 3、启动 nohup /opt/pulsar-manager/bin/pulsar-manager >/dev/null 2>&1 & 4、设置用户名密码 CSRF_TOKEN=$(curl http://localhost:7750/pulsar-manager/csrf-token) && echo $CSRF_TOKEN curl \\ -H 'X-XSRF-TOKEN: $CSRF_TOKEN' \\ -H 'Cookie: XSRF-TOKEN=$CSRF_TOKEN;' \\ -H \"Content-Type: application/json\" \\ -X PUT http://localhost:7750/pulsar-manager/users/superuser \\ -d '{\"name\": \"用户名\", \"password\": \"密码\", \"description\": \"Administrator\", \"email\": \"邮箱地址\"}' 5、访问 http://pulsar-manager服务器地址:7750/ui/index.html http://pulsar-manager服务器地址:7750/swagger-ui.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-11-20 17:34:23 "},"origin/pulsar-cli.html":{"url":"origin/pulsar-cli.html","title":"Pulsar的CLI命令","keywords":"","body":"Pulsar命令行pulsar-admin 一、pulsar-admin pulsar-admin工具，可以用来管理Pulsar的集群、Brokers、名称空间，租户等。 参考文档：https://pulsar.apache.org/docs/en/pulsar-admin/#list-failure-domains 1、命令格式 pulsar-admin 命令 子命令 参数 子命令 broker-stats：收集Brokers的统计信息 brokers：操作Brokers clusters：操作集群 functions：操作Pulsar函数 functions-worker：收集Pulsar函数Brokers的统计信息 namespaces：操作管理命令空间 ns-isolation-policy：操作管理命令空间的隔离策略 sources： sinks topics：操作管理主题 tenants：操作管理多租户 resource-quotas：操作管理资源配额 schemas：操作管理主题关联的模式 二、pulsar-admin常用操作 1、创建Topic 创建一个没有分区的Topic pulsar-admin topics create persistent://tenant/namespace/topic 创建一个有分区的Topic pulsar-admin topics create-partitioned-topic persistent://tenant/namespace/topic --partitions 4 无论是有分区还是没有分区, 创建topic后,如果没有任何操作, 60s后pulsar会认为此topic是不活动的, 会自动进行删除, 以避免生成垃圾数据 2、列出Topic pulsar-admin topics list tenant/namespace 3、删除Topic 删除没有分区的Topic pulsar-admin topics delete persistent://tenant/namespace/topic 删除有分区的Topic pulsar-admin topics delete-partitioned-topic persistent://tenant/namespace/topic 注意：如果Topic仍有producer和subscription。Topics无法删除。需要先剔除producer和subscription。请参照13小节剔除subscription后再次尝试删除Topic。相关文档 4、授权 pulsar-admin topics grant-permission --actions produce,consume --role application1 persistent://tenant/namespace/topic 5、获取权限 pulsar-admin topics grant-permission --actions produce,consume --role application1 persistent://tenant/namespace/topic 6、收回权限 pulsar-admin topics revoke-permission --role application persistent://tenant/namespace/topic { \"application1\": [ \"consume\", \"produce\" ] } 7、获取租户列表 pulsar-admin tenants list 8、创建租户 pulsar-admin tenants create my-tenant # 在创建租户时，可以使用-r或者–admin-roles标志分配管理角色。可以用逗号分隔的列表指定多个角色； pulsar-admin tenants create my-tenant -r role1,role2,role3 9、在指定的租户下创建名称空间 pulsar-admin namespaces create tenant/namespace 10、获取所有的名称空间列表 pulsar-admin namespaces list tenant 11、删除名称空间 pulsar-admin namespaces delete tenant/namespace 12、获取名称空间的配置策略 pulsar-admin namespaces policies tenant/namespace 13、剔除消费者的订阅 pulsar-admin topics unsubscribe persistent://public/default/topic-partition-0 --subscription test --force curl -XDELETE \\ http://brokers:8080/admin/v2/persistent/{tenant}/{namespace}/{topic}/subscription/{sub_name}?force=true 14、获取Topic状态 pulsar-admin topics stats persistent://public/default/topic-partition-0 具体返回值字段含义参考：https://pulsar.apache.org/docs/2.9.x/admin-api-topics#get-internal-stats 15、查找topic所在的broker信息 pulsar-admin persistent lookup persistent://public/default/topic-partition-0 16、查询topic的订阅信息 pulsar-admin persistent subscriptions persistent://public/default/topic-partition-0 17、查询最后一条消息的MessageID pulsar-admin topics last-message-id persistent://public/default/topic-partition-0 18、跳过消费部分消息 pulsar-admin persistent skip --count 10 --subscription my-subscription persistent://public/default/topic-partition-0 19、跳过所有数据 pulsar-admin persistent skip-all --subscription my-subscription persistent://public/default/topic-partition-0 20、重置消费cursor到几分钟之前 pulsar-admin persistent reset-cursor --subscription my-subscription --time 10 persistent://public/default/topic-partition-0 21、设置消息保留策略 pulsar-admin namespaces set-retention persistent://public/default/topic-partition-0 \\ --size 10G \\ --time 3h # size和time都为-1 ，则无限保留 # size和time都为0 ，则禁用保留策略 # size和time其中一个为0，则以另外一个为准作为保留策略 # 设置TTL，单位为秒 pulsar-admin namespaces set-message-ttl my-tenant/my-ns \\ --messageTTL 120 三、pulsar-client常用操作 1、生产数据 如果topic不存在，pulsar会自动创建 pulsar-client produce persistent://public/default/topic --messages \"Hello Pulsar I'm python client 1,Hello Pulsar I'm python client 2\" # -m, --messages ：以逗号分隔的消息字符串 # -n, --num-produce 2、消费数据 pulsar-client consume persistent://public/default/topic -s \"first-subscription\" # -s, --subscription-name : 订阅者名称 # -t, --subscription-type : 订阅类型，Exclusive, Shared, Failover, Key_Shared # -p, --subscription-position : 订阅位置。Latest, Earliest. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-10-27 21:17:59 "},"origin/pulsar-perf-test.html":{"url":"origin/pulsar-perf-test.html","title":"Pulsar性能测试","keywords":"","body":"一、方案 使用官方压测工具pulsar-perf ，利用多线程模拟生产者和消费者在并发情况下发送或消费数据的情形，以测试pulsar的读写性能 详细文档参考：https://pulsar.apache.org/docs/en/performance-pulsar-perf/ 命令参数：https://pulsar.apache.org/docs/en/reference-cli-tools/#pulsar-perf 二、测试指标 作为生产者并发写入不同类型Topic的TPS 作为消费者并发读取消息的QPS 三、测试环境 基础 三台8核，16G内存，100G SSD系统盘(4800IOPS)、1T 高效数据盘(5000 IOPS)的阿里云ECS服务器 操作系统为Ubuntu 20.04，均安装JDK 11.0.10 数据盘以LVM挂载/data路径下 IP地址：192.168.170.121~123 Pulsar Pulsar版本：2.7.1 使用内置Zookeeper Apache Zookeeper、Apache Bookkeeper 、Pulsar Broker均以集群形式分布在三台集群上。 Apache Zookeeper、Apache Bookkeeper 、Pulsar Broker的JVM -Xms -Xmx均设置为8Gb 测试命令 在192.168.170.121上执行测试命令 四、测试命令及结果 1、并发写入持久化Topic pulsar-perf produce \\ --batch-max-messages 10000 \\ # 设置批量发送批次的最大消息数 --num-producers 5 \\ # 设置每个主题的生产者数量 --num-test-threads 5 \\ # 设置进程个数 --rate 5000 \\ # 设置发送消息的速率 --size 4096 \\ # 设置发送单个消息的大小，单位bytes字节 persistent://public/default/perf-test11 其他默认参数 单个pulsar broker建立的最大TCP连接数：100 发送的消息数量： 0（一直发,不限制.不停不中断） 预热时间： 1s 主题个数： 1个 批量发送数据的窗口时间: 1ms 单次批量发送数据大小: 4194304 bytes Throughput produced: 4073.5 msg/s --- 127.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 472.911 ms - med: 103.862 - 95pct: 1210.079 - 99pct: 1223.351 - 99.9pct: 1236.391 - 99.99pct: 1244.527 - Max: 1249.999 Throughput produced: 5000.5 msg/s --- 156.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 227.782 ms - med: 7.242 - 95pct: 1557.415 - 99pct: 1883.111 - 99.9pct: 1968.999 - 99.99pct: 1977.207 - Max: 1978.335 Throughput produced: 5000.7 msg/s --- 156.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 363.717 ms - med: 8.834 - 95pct: 1605.079 - 99pct: 1796.095 - 99.9pct: 1856.335 - 99.99pct: 1864.623 - Max: 1865.927 Throughput produced: 4296.8 msg/s --- 134.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 151.693 ms - med: 7.453 - 95pct: 1121.647 - 99pct: 1417.671 - 99.9pct: 1486.287 - 99.99pct: 1492.615 - Max: 1493.135 Throughput produced: 5704.3 msg/s --- 178.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 249.943 ms - med: 8.440 - 95pct: 1692.911 - 99pct: 1980.311 - 99.9pct: 2052.695 - 99.99pct: 2068.639 - Max: 2073.943 pulsar-perf produce \\ --batch-max-messages 10000 \\ # 设置批量发送批次的最大消息数 --num-producers 5 \\ # 设置每个主题的生产者数量 --num-test-threads 5 \\ # 设置进程个数 --rate 8000 \\ # 设置发送消息的速率 --size 4096 \\ # 设置发送单个消息的大小，单位bytes字节 persistent://public/default/perf-test 其他默认参数 单个pulsar broker建立的最大TCP连接数：100 发送的消息数量： 0（一直发,不限制.不停不中断） 预热时间： 1s 主题个数： 1个 批量发送数据的窗口时间: 1ms 单次批量发送数据大小: 4194304 bytes Throughput produced: 6552.2 msg/s --- 204.8 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 8.609 ms - med: 6.987 - 95pct: 16.778 - 99pct: 37.272 - 99.9pct: 68.692 - 99.99pct: 72.774 - Max: 73.817 Throughput produced: 8772.0 msg/s --- 274.1 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 596.719 ms - med: 273.773 - 95pct: 1966.231 - 99pct: 2160.895 - 99.9pct: 2225.311 - 99.99pct: 2235.519 - Max: 2238.799 Throughput produced: 7993.3 msg/s --- 249.8 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 537.369 ms - med: 168.972 - 95pct: 1815.615 - 99pct: 2078.911 - 99.9pct: 2164.095 - 99.99pct: 2177.231 - Max: 2179.247 Throughput produced: 6447.4 msg/s --- 201.5 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 600.288 ms - med: 336.561 - 95pct: 1775.967 - 99pct: 1957.775 - 99.9pct: 2022.439 - 99.99pct: 2033.391 - Max: 2034.239 Throughput produced: 9575.8 msg/s --- 299.2 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 728.759 ms - med: 482.111 - 95pct: 2022.175 - 99pct: 2234.175 - 99.9pct: 2332.463 - 99.99pct: 2341.391 - Max: 2345.439 Throughput produced: 6827.5 msg/s --- 213.4 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 756.798 ms - med: 463.187 - 95pct: 2216.655 - 99pct: 2511.359 - 99.9pct: 2572.815 - 99.99pct: 2586.511 - Max: 2590.655 2、并发写入非持久化Topic pulsar-perf produce \\ --batch-max-messages 10000 \\ # 设置批量发送批次的最大消息数 --num-producers 5 \\ # 设置每个主题的生产者数量 --num-test-threads 5 \\ # 设置进程个数 --rate 5000 \\ # 设置发送消息的速率 --size 4096 \\ # 设置发送单个消息的大小，单位bytes字节 non-persistent://public/default/perf-test6 其他默认参数 单个pulsar broker建立的最大TCP连接数：100 发送的消息数量： 0（一直发,不限制.不停不中断） 预热时间： 1s 主题个数： 1个 批量发送数据的窗口时间: 1ms 单次批量发送数据大小: 4194304 bytes Throughput produced: 4562.4 msg/s --- 142.6 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.718 ms - med: 0.718 - 95pct: 1.162 - 99pct: 1.228 - 99.9pct: 2.022 - 99.99pct: 8.470 - Max: 11.320 Throughput produced: 4999.8 msg/s --- 156.2 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.709 ms - med: 0.702 - 95pct: 1.149 - 99pct: 1.198 - 99.9pct: 3.810 - 99.99pct: 12.648 - Max: 23.229 Throughput produced: 5000.2 msg/s --- 156.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.701 ms - med: 0.701 - 95pct: 1.145 - 99pct: 1.193 - 99.9pct: 1.802 - 99.99pct: 7.321 - Max: 15.233 Throughput produced: 4999.9 msg/s --- 156.2 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.706 ms - med: 0.705 - 95pct: 1.151 - 99pct: 1.203 - 99.9pct: 2.960 - 99.99pct: 5.138 - Max: 5.723 Throughput produced: 5000.1 msg/s --- 156.3 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.703 ms - med: 0.701 - 95pct: 1.148 - 99pct: 1.194 - 99.9pct: 3.385 - 99.99pct: 5.997 - Max: 6.320 Throughput produced: 4999.9 msg/s --- 156.2 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.697 ms - med: 0.697 - 95pct: 1.147 - 99pct: 1.194 - 99.9pct: 1.298 - 99.99pct: 2.889 - Max: 4.888 pulsar-perf produce \\ --batch-max-messages 10000 \\ # 设置批量发送批次的最大消息数 --num-producers 5 \\ # 设置每个主题的生产者数量 --num-test-threads 5 \\ # 设置进程个数 --rate 8000 \\ # 设置发送消息的速率 --size 4096 \\ # 设置发送单个消息的大小，单位bytes字节 non-persistent://public/default/perf-test8 其他默认参数 单个pulsar broker建立的最大TCP连接数：100 发送的消息数量： 0（一直发,不限制.不停不中断） 预热时间： 1s 主题个数： 1个 批量发送数据的窗口时间: 1ms 单次批量发送数据大小: 4194304 bytes Throughput produced: 7329.1 msg/s --- 229.0 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.710 ms - med: 0.703 - 95pct: 1.159 - 99pct: 1.230 - 99.9pct: 3.044 - 99.99pct: 10.701 - Max: 13.688 Throughput produced: 8000.3 msg/s --- 250.0 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.699 ms - med: 0.694 - 95pct: 1.149 - 99pct: 1.199 - 99.9pct: 2.149 - 99.99pct: 7.919 - Max: 11.846 Throughput produced: 8000.8 msg/s --- 250.0 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.697 ms - med: 0.693 - 95pct: 1.146 - 99pct: 1.198 - 99.9pct: 2.326 - 99.99pct: 5.694 - Max: 6.710 Throughput produced: 8000.9 msg/s --- 250.0 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.741 ms - med: 0.700 - 95pct: 1.156 - 99pct: 1.228 - 99.9pct: 12.492 - 99.99pct: 22.429 - Max: 33.309 Throughput produced: 8000.7 msg/s --- 250.0 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.696 ms - med: 0.693 - 95pct: 1.146 - 99pct: 1.196 - 99.9pct: 1.295 - 99.99pct: 5.035 - Max: 5.478 Throughput produced: 8000.7 msg/s --- 250.0 Mbit/s --- failure 0.0 msg/s --- Latency: mean: 0.696 ms - med: 0.693 - 95pct: 1.146 - 99pct: 1.195 - 99.9pct: 1.279 - 99.99pct: 5.308 - Max: 8.603 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/pulsar-kafka-kop.html":{"url":"origin/pulsar-kafka-kop.html","title":"Pulsar的Kafka协议适配器KoP","keywords":"","body":"Pulsar的Kafka协议适配器KoP 一、简介 为了能平滑、不改写代码、无侵入地迁移应用使用pulsar，KoP以插件形式支持Kafka协议。 Github地址：https://github.com/streamnative/kop 二、部署配置 从Pulsar的2.6.2.0开始，KoP x.y.z.m基于Pulsar x.y.z，而m是补丁版本号。 1、下载KoP 下载KoP的nar包到各个Broker节点 mkdir /opt/pulsar/protocol-handler && \\ wget https://github.com/streamnative/kop/releases/download/v2.7.1.5/pulsar-protocol-handler-kafka-2.7.1.5.nar -P /opt/pulsar/protocol-handler 2、配置Broker的配置文件 pulsar_broker_config_file=/opt/pulsar/conf/broker.conf sed -i '/allowAutoTopicCreationType=non-partitioned/d' $pulsar_broker_config_file echo \"### --- KoP Configuration----\" >> $pulsar_broker_config_file echo \"messagingProtocols=kafka\" >> $pulsar_broker_config_file echo \"protocolHandlerDirectory=./protocol-handler\" >> $pulsar_broker_config_file echo \"kafkaListeners=PLAINTEXT://$(ip a |grep eth0|grep inet|awk '{print $2}'|awk -F\"/\" '{print $1}'):9092\" >> $pulsar_broker_config_file echo \"allowAutoTopicCreationType=partitioned\" >> $pulsar_broker_config_file echo \"brokerEntryMetadataInterceptors=org.apache.pulsar.common.intercept.AppendIndexMetadataInterceptor\" >> $pulsar_broker_config_file echo \"advertisedAddress=$(ip a |grep eth0|grep inet|awk '{print $2}'|awk -F\"/\" '{print $1}')\" >> $pulsar_broker_config_file 3、重启Pulsar Broker节点 kill -9 `jps -l |grep \"org.apache.pulsar.PulsarBrokerStarter\" |awk '{print $1}'` ; sleep 3 && \\ pulsar-daemon start broker && \\ tail -f /data/pulsar/logs/pulsar-broker-$(hostname -s).pulsar.prod.log 4、验证 ①验证Broker节点是否开起9092端口 netstat -lanp|grep 9092 && \\ jps -l | grep \"org.apache.pulsar.PulsarBrokerStarter\" ②使用kaf工具 kaf config add-cluster prod-pulsar -b pulsar_broker_ip:9092 kaf config select-cluster -c kaf config select-cluster kaf topic create kop1 -p 10 -r 1 echo \"hello pulsar kop\" | kaf produce kop1 kaf consume kop1 -f ③使用kafka原生客户端 创建Topic kafka-topics.sh --bootstrap-server pulsar_broker_ip:9092 --create --replication 1 --partitions 5 --topic kop 创建生产者 kafka-console-producer.sh --bootstrap-server pulsar_broker_ip:9092 --topic kop 创建消费者 kafka-console-consumer.sh --bootstrap-server pulsar_broker_ip:9092 --topic kop --from-beginning 三、KoP的配置详解 其他配置项参考：https://github.com/streamnative/kop/blob/master/docs/configuration.md 配置项 含义 默认值 messagingProtocols kafka null protocolHandlerDirectory KoP NAR文件相对于安装路径所处的目录路径 ./protocols allowAutoTopicCreationType KoP仅支持分区的Topic。因此，最好设置为partitioned。如果默认情况下将其设置为未分区，则KoP自动创建的主题仍为分区主题。但是，由Pulsar Broker自动创建的主题是未分区的主题。 non-partitioned Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-25 20:48:15 "},"origin/pulsar-websocket-api.html":{"url":"origin/pulsar-websocket-api.html","title":"Pulsar的WebSocket API","keywords":"","body":"Pulsar WebSocket API 一、简介 Pulsar WebSocket API 提供了一种使用没有官方客户端库的语言与 Pulsar 交互的简单方法。通过 WebSocket，您可以发布和使用消息并使用客户端功能矩阵页面上提供的功能。 官方文档：https://pulsar.apache.org/docs/client-libraries-websocket/ 二、配置 1、在Broker的8080端口开起 conf/broker.conf webSocketServiceEnabled=true 2、作为独立组件启用 conf/websocket.conf configurationMetadataStoreUrl=zk1:2181,zk2:2181,zk3:2181 webServicePort=8080 clusterName=my-cluster # 如果要开起TLS，需要设置一下参数 tlsEnabled=true tlsAllowInsecureConnection=false tlsCertificateFilePath=/path/to/client-websocket.cert.pem tlsKeyFilePath=/path/to/client-websocket.key-pk8.pem tlsTrustCertsFilePath=/path/to/ca.cert.pem bin/pulsar-daemon start websocket 三、Websocket工具 Chrome插件：PieSocket WebSocket Tester 网页工具：http://wstool.js.org/ Python：pip3 install websocket-client 文档：https://pulsar.apache.org/docs/client-libraries-websocket/#python 四、API Endpoint 1、生产者Endpoint 请求URL ws://broker-service-url:8080/ws/v2/producer/persistent/:tenant/:namespace/:topic?参数1=值&参数2=值 参数 类型 必须 描述 sendTimeoutMillis long no Send timeout (默认: 30s) batchingEnabled boolean no Enable batching of messages (默认: false) batchingMaxMessages int no 批处理中允许的最大消息数 (默认: 1000) maxPendingMessages int no Set the max size of the internal-queue holding the messages (默认: 1000) batchingMaxPublishDelay long no Time period within which the messages will be batched (默认: 10ms) messageRoutingMode string no Message routing mode for the partitioned producer: SinglePartition, RoundRobinPartition compressionType string no Compression type: LZ4, ZLIB producerName string no Specify the name for the producer. Pulsar will enforce only one producer with same name can be publishing on a topic initialSequenceId long no Set the baseline for the sequence ids for messages published by the producer. hashingScheme string no Hashing function to use when publishing on a partitioned topic: JavaStringHash, Murmur3_32Hash token string no Authentication token, this is used for the browser javascript client 消息体样本 { \"payload\": \"SGVsbG8gV29ybGQ=\", \"properties\": {\"key1\": \"value1\", \"key2\": \"value2\"}, \"context\": \"1\" } 消息体属性 属性 类型 必须 描述 payload string yes Base-64 encoded payload properties key-value pairs no Application-defined properties context string no Application-defined request identifier key string no For partitioned topics, decides which partition to use replicationClusters array no Restrict replication to this list of clusters, specified by name 2、消费者Endpoint 请求URL ws://broker-service-url:8080/ws/v2/consumer/persistent/:tenant/:namespace/:topic/:subscription?参数1=值&参数2=值 Key Type Required? Explanation ackTimeoutMillis long no 设置未确认消息的超时时间 (默认: 0) subscriptionType string no 订阅类型: Exclusive, Failover, Shared, Key_Shared receiverQueueSize int no 设置消费者接收队列的大小 (默认: 1000) consumerName string no 消费者名字 priorityLevel int no 设置消费者定义优先级 maxRedeliverCount int no Define a maxRedeliverCount for the consumer (默认: 0). Activates Dead Letter Topic feature. deadLetterTopic string no Define a deadLetterTopic for the consumer (默认: {topic}-{subscription}-DLQ). Activates Dead Letter Topic feature. pullMode boolean no 是否开起pull模式 (默认: false) negativeAckRedeliveryDelay int no When a message is negatively acknowledged, the delay time before the message is redelivered (in milliseconds). 默认: 60000. token string no Authentication token, this is used for the browser javascript client 消息体样本 { \"messageId\": \"CAMQADAA\", \"payload\": \"hvXcJvHW7kOSrUn17P2q71RA5SdiXwZBqw==\", \"properties\": {}, \"publishTime\": \"2021-10-29T16:01:38.967-07:00\", \"redeliveryCount\": 0, \"encryptionContext\": { \"keys\": { \"client-rsa.pem\": { \"keyValue\": \"jEuwS+PeUzmCo7IfLNxqoj4h7txbLjCQjkwpaw5AWJfZ2xoIdMkOuWDkOsqgFmWwxiecakS6GOZHs94x3sxzKHQx9Oe1jpwBg2e7L4fd26pp+WmAiLm/ArZJo6JotTeFSvKO3u/yQtGTZojDDQxiqFOQ1ZbMdtMZA8DpSMuq+Zx7PqLo43UdW1+krjQfE5WD+y+qE3LJQfwyVDnXxoRtqWLpVsAROlN2LxaMbaftv5HckoejJoB4xpf/dPOUqhnRstwQHf6klKT5iNhjsY4usACt78uILT0pEPd14h8wEBidBz/vAlC/zVMEqiDVzgNS7dqEYS4iHbf7cnWVCn3Hxw==\", \"metadata\": {} } }, \"param\": \"Tfu1PxVm6S9D3+Hk\", \"compressionType\": \"NONE\", \"uncompressedMessageSize\": 0, \"batchSize\": { \"empty\": false, \"present\": true } } } 消息体属性 基础属性 | Key | Type | Required? | Explanation | | ------------------------- | ----------------- | --------- | ------------------------------------------------------------ | | messageId | string | yes | 消息 ID | | payload | string | yes | Base-64 encoded payload | | publishTime | string | yes | 推送时间戳 | | redeliveryCount | number | yes | Number of times this message was already delivered | | properties | key-value pairs | no | Application-defined properties | | key | string | no | Original routing key set by producer | | encryptionContext | EncryptionContext | no | Encryption context that consumers can use to decrypt received messages | | param | string | no | Initialization vector for cipher (Base64 encoding) | | batchSize | string | no | Number of entries in a message (if it is a batch message) | | uncompressedMessageSize | string | no | Message size before compression | | compressionType | string | no | Algorithm used to compress the message payload | encryptionContext related parameter | Key | Type | Required? | Explanation | | ------ | ----------------------- | --------- | ------------------------------------------------------------ | | keys | key-EncryptionKey pairs | yes | Key in key-EncryptionKey pairs is an encryption key name. Value in key-EncryptionKey pairs is an encryption key object. | encryptionKey related parameters | Key | Type | Required? | Explanation | | ---------- | --------------- | --------- | -------------------------------- | | keyValue | string | yes | Encryption key (Base64 encoding) | | metadata | key-value pairs | no | Application-defined metadata | Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-29 13:38:24 "},"origin/pulsar-python-client.html":{"url":"origin/pulsar-python-client.html","title":"Python Pulsar Client ","keywords":"","body":"Python Pulsar Client 一、Python Client pip3 install pulsar-client pulsar-client.py #!/usr/bin/python3 # -*- coding: UTF-8 -*- import pulsar,sys,socket def consume(client,topic): consumer = client.subscribe(topic,subscription_name=(socket.gethostbyname(socket.gethostname()))) while True: msg = consumer.receive() print(\"Received message: '%s'\" % msg.data()) consumer.acknowledge(msg) client.close() def produce(client,topic,msg): producer = client.create_producer(topic) producer.send((msg).encode('utf-8')) client.close() def main(args): client = pulsar.Client('pulsar://localhost:6650') if args[1] == \"consume\": consume(client,args[2]) elif args[1] == \"produce\": produce(client,args[2],args[3]) if __name__ == \"__main__\": main(sys.argv) 1、生产数据 python3 ./pulsar-client.py produce test \"Hello Pulsar , I'm python client\" 2、消费数据 python3 ./pulsar-client.py consume test Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-08-29 13:45:29 "},"origin/kettle-basic.html":{"url":"origin/kettle-basic.html","title":"安装部署及基础概念","keywords":"","body":"Pentaho Data Integration：Kettle 一、简介 一个用java开发的软件，可以对数据进行一系列操作，例如数据整合、数据转换、数据迁移、数据导出报表等。 Kettle其实叫Pentaho Data Integration，Pentaho是公司名，Data Integration是数据整合。 Pentaho Data Integration分为商业版与开源版，在中国，一般人仍习惯把Pentaho Data Integration的开源版称为Kettle。 二、基础概念 三、安装部署 MacOS windows Linux 参考 https://blog.csdn.net/weixin_43281875/article/details/121161789 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-03-09 18:11:17 "},"origin/canal-install-basic.html":{"url":"origin/canal-install-basic.html","title":"简介安装部署","keywords":"","body":"Canal 一、简介 Github：https://github.com/alibaba/canal 功能简介 基于日志增量订阅和消费的业务包括 数据库镜像 数据库实时备份 索引构建和实时维护(拆分异构索引、倒排索引等) 业务 cache 刷新 带业务逻辑的增量数据处理 当前的 canal 支持源端 MySQL 版本包括 5.1.x , 5.5.x , 5.6.x , 5.7.x , 8.0.x MySQL主备复制原理 MySQL master 将数据变更写入二进制日志( binary log, 其中记录叫做二进制日志事件binary log events，可以通过 show binlog events 进行查看) MySQL slave 将 master 的 binary log events 拷贝到它的中继日志(relay log) MySQL slave 重放 relay log 中事件，将数据变更反映它自己的数据 canal 工作原理 canal 模拟 MySQL slave 的交互协议，伪装自己为 MySQL slave ，向 MySQL master 发送dump 协议 MySQL master 收到 dump 请求，开始推送 binary log 给 slave (即 canal ) canal 解析 binary log 对象(原始为 byte 流) 二、部署 1、二进制 canal_version=1.1.6 wget https://github.com/alibaba/canal/releases/download/canal-$canal_version/canal.adapter-$canal_version.tar.gz -P /tmp mkdir /opt/canal.adapter-$canal_version && tar -zxvf /tmp/canal.adapter-$canal_version.tar.gz -C /opt/canal.adapter-$canal_version ln -s /opt/canal.adapter-$canal_version /opt/canal.adapter canal_version=1.1.6 wget https://github.com/alibaba/canal/releases/download/canal-$canal_version/canal.admin-$canal_version.tar.gz -P /tmp mkdir /opt/canal.admin-$canal_version && tar -zxvf /tmp/canal.admin-$canal_version.tar.gz -C /opt/canal.admin-$canal_version ln -s /opt/canal.admin-$canal_version /opt/canal.admin canal_version=1.1.6 wget https://github.com/alibaba/canal/releases/download/canal-$canal_version/canal.deployer-$canal_version.tar.gz -P /tmp mkdir /opt/canal.deployer-$canal_version && tar -zxvf /tmp/canal.deployer-$canal_version.tar.gz -C /opt/canal.deployer-$canal_version ln -s /opt/canal.deployer-$canal_version /opt/canal.deployer 2、Docker 文档：https://github.com/alibaba/canal/wiki/Docker-QuickStart docker镜像：https://hub.docker.com/r/canal/canal-server/tags/ docker模式下，单docker实例只能运行一个instance 三、启动 1、debug方式启动 默认使用suspend=y，阻塞等待你remote debug链接成功 sh startup.sh debug 9099 2、设置输出日志级别 修改conf/logback.xml 四、配置 1、数据库设置 开启 Binlog 写入功能，配置 binlog-format 为 ROW 模式，配置如下 [mysqld] log-bin=mysql-bin # 开启 binlog binlog-format=ROW # 选择 ROW 模式 server_id=1 # 配置 MySQL replaction 需要定义，不要和 canal 的 slaveId 重复 show variables like 'binlog_format'; show variables like 'log_bin'; 授权 canal 链接 MySQL 账号具有作为 MySQL slave 的权限, 如果已有账户可直接 grant CREATE USER canal IDENTIFIED BY 'canal'; GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'canal'@'%'; -- GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ; FLUSH PRIVILEGES; 2、配置文件 canal.properties (系统根配置文件) ################################################# ######### Canal配置 ############# ################################################# # canal server绑定的本地IP信息，如果不配置，默认选择一个本机IP进行启动服务 canal.ip = # canal server注册到外部zookeeper、admin的ip信息 (针对docker的外部可见ip) canal.register.ip = # canal server提供socket服务的端口 canal.port = 11111 canal.metrics.pull.port = 11112 # canal instance user/passwd # canal.user = canal # canal.passwd = E3619321C1A937C46A0D8BD1DAC39F93B27D4458 # canal链接canal-admin的地址 # canal.admin.manager = 127.0.0.1:8089 # admin管理链接端口 canal.admin.port = 11110 # admin管理指令链接的ACL配置 canal.admin.user = admin canal.admin.passwd = 4ACFE3202A5FF5CF467898FC58AAB1D615029441 # admin auto register #canal.admin.register.auto = true #canal.admin.register.cluster = #canal.admin.register.name = # canal server链接zookeeper集群的链接信息 canal.zkServers = # canal持久化数据到zookeeper上的更新频率，单位毫秒 canal.zookeeper.flush.period = 1000 canal.withoutNetty = false # tcp, kafka, rocketMQ, rabbitMQ, pulsarMQ canal.serverMode = tcp # flush meta cursor/parse position to file canal.file.data.dir = ${canal.conf.dir} canal.file.flush.period = 1000 # canal内存store中可缓存buffer记录数，需要为2的指数 canal.instance.memory.buffer.size = 16384 # 内存记录的单位大小，默认1KB，和buffer.size组合决定最终的内存使用大小 canal.instance.memory.buffer.memunit = 1024 # canal内存store中数据缓存模式 # 1. ITEMSIZE : 根据buffer.size进行限制，只限制记录的数量 # 2. MEMSIZE : 根据buffer.size * buffer.memunit的大小，限制缓存记录的大小 canal.instance.memory.batch.mode = MEMSIZE canal.instance.memory.rawEntry = true ################################################# ### Canal 数据库连接心跳检查、切换配置 ### ################################################# ## 是否开启心跳检查 canal.instance.detecting.enable = false # 心跳检查sql canal.instance.detecting.sql = select 1 # 心跳检查频率，单位秒 canal.instance.detecting.interval.time = 3 # 心跳检查失败重试次数 canal.instance.detecting.retry.threshold = 3 # 心跳检查失败后，是否开启自动mysql自动切换。 # 说明：比如心跳检查失败超过阀值后，如果该配置为true，canal就会自动链到mysql备库获取binlog数据 # 发生master/standby的切换的条件：(heartbeatHaEnable = true) && (失败次数>=retry.threshold). canal.instance.detecting.heartbeatHaEnable = false # 最大事务完整解析的长度支持超过该长度后，一个事务可能会被拆分成多次提交到canal store中，无法保证事务的完整可见性 canal.instance.transaction.size = 1024 # canal发生mysql切换时，在新的mysql库上查找binlog时需要往前查找的时间，单位秒。 # 说明：mysql主备库可能存在解析延迟或者时钟不统一，需要回退一段时间，保证数据不丢 canal.instance.fallbackIntervalInSeconds = 60 ################################################# ######### Canal 网络连接配置 ############# ################################################# # 网络链接参数，SocketOptions.SO_RCVBUF canal.instance.network.receiveBufferSize = 16384 # 网络链接参数，SocketOptions.SO_SNDBUF canal.instance.network.sendBufferSize = 16384 # 网络链接参数，SocketOptions.SO_TIMEOUT canal.instance.network.soTimeout = 30 ################################################# ######### Canal Binlog配置 ############# ################################################# # 是否使用druid处理所有的ddl解析来获取库和表名 canal.instance.filter.druid.ddl = true # 是否忽略dcl语句 canal.instance.filter.query.dcl = false # 是否忽略dml语句 # (mysql5.6之后，在row模式下每条DML语句也会记录SQL到binlog中,可参考MySQL文档) canal.instance.filter.query.dml = false # 是否忽略ddl语句 canal.instance.filter.query.ddl = false # 是否忽略binlog表结构获取失败的异常 # 主要解决回溯binlog时,对应表已被删除或者表结构和binlog不一致的情况 canal.instance.filter.table.error = false # 是否dml的数据变更事件.(主要针对用户只订阅ddl/dcl的操作) canal.instance.filter.rows = false # 是否忽略事务头和尾,比如针对写入kakfa的消息时，不需要写入TransactionBegin/Transactionend事件 canal.instance.filter.transaction.entry = false canal.instance.filter.dml.insert = false canal.instance.filter.dml.update = false canal.instance.filter.dml.delete = false # 支持的binlog format格式列表(otter会有支持format格式限制) canal.instance.binlog.format = ROW,STATEMENT,MIXED # 支持的binlog image格式列表(otter会有支持format格式限制) canal.instance.binlog.image = FULL,MINIMAL,NOBLOB # ddl语句是否单独一个batch返回 (比如下游dml/ddl如果做batch内无序并发处理,会导致结构不一致) canal.instance.get.ddl.isolation = false # 是否开启binlog并行解析模式(串行解析资源占用少,但性能有瓶颈, 并行解析可以提升近2.5倍+) # 如果系统是1个 cpu，需要将 canal.instance.parser.parallel 设置为 false canal.instance.parser.parallel = true # binlog并行解析的异步ringbuffer队列(必须为2的指数) canal.instance.parser.parallelBufferSize = 256 # 是否开启tablemeta的tsdb能力 canal.instance.tsdb.enable = true # 主要针对h2-tsdb.xml时对应h2文件的存放目录,默认为conf/xx/h2.mv.db canal.instance.tsdb.dir = ${canal.file.data.dir:../conf}/${canal.instance.destination:} # jdbc url的配置(h2的地址为默认值，如果是mysql需要自行定义) canal.instance.tsdb.url = jdbc:h2:${canal.instance.tsdb.dir}/h2;CACHE_SIZE=1000;MODE=MYSQL; canal.instance.tsdb.dbUsername = canal canal.instance.tsdb.dbPassword = canal # dump snapshot interval, default 24 hour canal.instance.tsdb.snapshot.interval = 24 # purge snapshot expire , default 360 hour(15 days) canal.instance.tsdb.snapshot.expire = 360 ################################################# ######### Canal配置 ############# ################################################# canal.destinations = example # conf root dir canal.conf.dir = ../conf # auto scan instance dir add/remove and start/stop instance canal.auto.scan = true canal.auto.scan.interval = 5 # set this value to 'true' means that when binlog pos not found, skip to latest. # WARN: pls keep 'false' in production env, or if you know what you want. canal.auto.reset.latest.pos.mode = false canal.instance.tsdb.spring.xml = classpath:spring/tsdb/h2-tsdb.xml #canal.instance.tsdb.spring.xml = classpath:spring/tsdb/mysql-tsdb.xml canal.instance.global.mode = spring canal.instance.global.lazy = false canal.instance.global.manager.address = ${canal.admin.manager} #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml canal.instance.global.spring.xml = classpath:spring/file-instance.xml #canal.instance.global.spring.xml = classpath:spring/default-instance.xml ################################################## ######### 消息队列通用配置 ############# ################################################## # aliyun ak/sk , support rds/mq canal.aliyun.accessKey = canal.aliyun.secretKey = canal.aliyun.uid= # 是否为json格式,如果设置为false,对应MQ收到的消息为protobuf格式.需要通过CanalMessageDeserializer进行解码 canal.mq.flatMessage = true # Canal的批处理数据大小, 默认50K, 由于kafka最大消息体限制请勿超过1M(900K以下) canal.mq.canalBatchSize = 50 # Canal get数据的超时时间, 单位: 毫秒, 空为不限超时 canal.mq.canalGetTimeout = 100 # 是否为阿里云模式，可选值local/cloud canal.mq.accessChannel = local # 是否开启database混淆hash,确保不同库的数据可以均匀分散,如果关闭可以确保只按照业务字段做MQ分区计算 canal.mq.database.hash = true # MQ消息发送并行度 canal.mq.send.thread.size = 30 # MQ消息构建并行度 canal.mq.build.thread.size = 8 ################################################## ######### Kafka ############# ################################################## # kafka为bootstrap.servers，rocketMQ中为nameserver列表 kafka.bootstrap.servers = 127.0.0.1:9092 kafka.acks = all # 压缩类型 kafka.compression.type = none kafka.batch.size = 16384 kafka.linger.ms = 1 kafka.max.request.size = 1048576 kafka.buffer.memory = 33554432 kafka.max.in.flight.requests.per.connection = 1 # 发送失败重试次数 kafka.retries = 0 kafka.kerberos.enable = false kafka.kerberos.krb5.file = \"../conf/kerberos/krb5.conf\" kafka.kerberos.jaas.file = \"../conf/kerberos/jaas.conf\" ################################################## ######### RocketMQ ############# ################################################## rocketmq.producer.group = test rocketmq.enable.message.trace = false rocketmq.customized.trace.topic = rocketmq.namespace = rocketmq.namesrv.addr = 127.0.0.1:9876 rocketmq.retry.times.when.send.failed = 0 rocketmq.vip.channel.enabled = false rocketmq.tag = rabbitmq.host = rabbitmq.virtual.host = rabbitmq.exchange = rabbitmq.username = rabbitmq.password = rabbitmq.deliveryMode = ################################################## ######### Pulsar ############# ################################################## pulsarmq.serverUrl = pulsarmq.roleToken = pulsarmq.topicTenantPrefix = instance.properties (instance级别的配置文件，每个instance一份) # 是否启用mysql gtid的订阅模式 canal.instance.gtidon=false # mysql主库链接地址 canal.instance.master.address=127.0.0.1:3306 # mysql主库链接时起始的binlog文件 canal.instance.master.journal.name= # mysql主库链接时起始的binlog偏移量 canal.instance.master.position= # mysql主库链接时起始的binlog的时间戳 canal.instance.master.timestamp= # mysql主库链接时对应的gtid位点 canal.instance.master.gtid= # 目前配置只支持一个standby配置 #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = #canal.instance.standby.gtid= # rds oss binlog canal.instance.rds.accesskey= canal.instance.rds.secretkey= canal.instance.rds.instanceId= # table meta tsdb info canal.instance.tsdb.enable=true #canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb #canal.instance.tsdb.dbUsername=canal #canal.instance.tsdb.dbPassword=canal # username/password canal.instance.dbUsername=canal canal.instance.dbPassword=canal # 数据库的编码方式对应到 java 中的编码类型，比如 UTF-8，GBK , ISO-8859-1 canal.instance.connectionCharset = UTF-8 # enable druid Decrypt database password canal.instance.enableDruid=false #canal.instance.pwdPublicKey=MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBALK4BUxdDltRRE5/zXpVEVPUgunvscYFtEip3pmLlhrWpacX7y7GCMo2/JM6LeHmiiNdH1FWgGCpUfircSwlWKUCAwEAAQ== # table regex canal.instance.filter.regex=.*\\\\..* # table black regex canal.instance.filter.black.regex=mysql\\\\.slave_.* # table field filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2) #canal.instance.filter.field=test1.t_product:id/subject/keywords,test2.t_company:id/name/contact/ch # table field black filter(format: schema1.tableName1:field1/field2,schema2.tableName2:field1/field2) #canal.instance.filter.black.field=test1.t_product:subject/product_image,test2.t_company:id/name/contact/ch # mq config canal.mq.topic=example # dynamic topic route by schema or table regex #canal.mq.dynamicTopic=mytest1.user,topic2:mytest2\\\\..*,.*\\\\..* canal.mq.partition=0 # hash partition config #canal.mq.enableDynamicQueuePartition=false #canal.mq.partitionsNum=3 #canal.mq.dynamicTopicPartitionNum=test.*:4,mycanal:6 # 库名.表名: 唯一主键，多个表之间用逗号分隔 canal.mq.partitionHash=test.table:id^name,.*\\\\..* 3、源库DB-表正则匹配设置 格式：schema.table canal.instance.filter.regex=A.*,B.*,C.* 4、消息队列Topic正则匹配设置 使用正则路由schema和table到不同的Topic 格式：topic:schema.table,topic:schema.table,topic:schema.table canal.mq.dynamicTopic=topic_A:db_test,topic_B:db_test\\.*,topic_C:db_test.test_table # topic_A:db_test :db_test库的所有表都会发送到topic_A下 # topic_B:db_test\\.* :db_test库正则*匹配的所有表都会发送到topic_B下 # topic_C:db_test.test_table :db_test库的test_table表都会发送到topic_C下 默认的topic canal.mq.topic=canal_ddl 一些mysql schema的binlog也会读进来（建表语句，grant语句等），如果没有这个默认的topic，会报找不到分区的错误，从而导致canal停止写入 参考：https://www.cnblogs.com/xiexiandong/p/12881011.html 五、MySQL To Kafka conf/canal.properties ... canal.serverMode = kafka ... kafka.bootstrap.servers = localhost:9092 kafka.acks = all kafka.compression.type = none kafka.batch.size = 16384 kafka.linger.ms = 1 kafka.max.request.size = 1048576 kafka.buffer.memory = 33554432 kafka.max.in.flight.requests.per.connection = 1 kafka.retries = 0 kafka.kerberos.enable = false kafka.kerberos.krb5.file = \"../conf/kerberos/krb5.conf\" kafka.kerberos.jaas.file = \"../conf/kerberos/jaas.conf\" conf/example/instance.properties ... # mq config canal.mq.topic=canal_ddl # dynamic topic route by schema or table regex canal.mq.dynamicTopic=db.table:db\\\\.table canal.mq.partition=0 # hash partition config canal.mq.enableDynamicQueuePartition=false canal.mq.partitionsNum=3 canal.mq.dynamicTopicPartitionNum=test.*:4,mycanal:6 canal.mq.partitionHash=test.table:id^name,.*\\\\..* ... 六、MySQL To Pulsar 文档：https://pulsar.apache.org/docs/io-canal-source/#configuration conf/canal.properties ... canal.serverMode = pulsarMQ ... pulsarmq.serverUrl = pulsar://127.0.0.1:30065 pulsarmq.roleToken = pulsarmq.topicTenantPrefix = public/default conf/example/instance.properties ... # mq config canal.mq.topic=canal_ddl # dynamic topic route by schema or table regex canal.mq.dynamicTopic=db.table:db\\\\.table canal.mq.partition=0 # hash partition config canal.mq.enableDynamicQueuePartition=false canal.mq.partitionsNum=3 canal.mq.dynamicTopicPartitionNum=test.*:4,mycanal:6 canal.mq.partitionHash=test.table:id^name,.*\\\\..* ... 七、Aliyun RDS MySQL To Pulsar 同第六章的相同配置。 八、表与消费队列Topic对应实操 适应于投递MySQL Binlog到kafka和pulsar 1、同库多表对应一个topic canal.instance.filter.regex=test.jobs1,test.jobs2 canal.mq.dynamicTopic=test.jobs:test.* 九、监控 1、配置canal conf/canal.properties canal.metrics.pull.port = 11112 2、配置prometheus刮取canal端点数据 - job_name: \"test-stg-canal\" scrape_interval: 15s scrape_timeout: 10s metrics_path: / scheme: http static_configs: - targets: ['192.168.1.20:11112'] 3、Grafana导入Dashboard Dashboard josn文件 4、配置告警 去除默认无用的两条数据显示，只保存上图中的两条，同时查询条件进行精确匹配不要用变量引用进行可变搜索。不然无法创建告警Alert，会显示“Template variables are not supported in alert queries” 5、效果 6、监控指标释义 指标 说明 单位 精度 canal_instance_transactions instance接收transactions计数 - - canal_instance instance基本信息 - - canal_instance_subscriptions instance订阅数量 - - canal_instance_publish_blocking_time instance dump线程提交到异步解析队列过程中的阻塞时间(仅parallel解析模式) ms ns canal_instance_received_binlog_bytes instance接收binlog字节数 byte - canal_instance_parser_mode instance解析模式(是否开启parallel解析) - - canal_instance_client_packets instance client请求次数的计数 - - canal_instance_client_bytes 向instance client发送数据包字节计数 byte - canal_instance_client_empty_batches 向instance client发送get接口的空结果计数 - - canal_instance_client_request_error instance client请求失败计数 - - canal_instance_client_request_latency instance client请求的响应时间概况 - - canal_instance_sink_blocking_time instance sink线程put数据至store的阻塞时间 ms ns canal_instance_store_produce_seq instance store接收到的events sequence number - - canal_instance_store_consume_seq instance store成功消费的events sequence number - - canal_instance_store instance store基本信息 - - canal_instance_store_produce_mem instance store接收到的所有events占用内存总量 byte - canal_instance_store_consume_mem instance store成功消费的所有events占用内存总量 byte - canal_instance_put_rows store put操作完成的table rows - - canal_instance_get_rows client get请求返回的table rows - - canal_instance_ack_rows client ack操作释放的table rows - - canal_instance_traffic_delay server与MySQL master的延时 ms ms canal_instance_put_delay store put操作events的延时 ms ms canal_instance_get_delay client get请求返回events的延时 ms ms canal_instance_ack_delay client ack操作释放events的延时 ms ms 7、Dashboard 指标 简述 多指标 Basic Canal instance 基本信息。 是 Network bandwith 网络带宽。包含inbound(canal server读取binlog的网络带宽)和outbound(canal server返回给canal client的网络带宽) 是 Delay Canal server与master延时；store 的put, get, ack操作对应的延时。 是 Blocking sink线程blocking占比；dump线程blocking占比(仅parallel mode)。 是 TPS(transaction) Canal instance 处理binlog的TPS，以MySQL transaction为单位计算。 否 TPS(tableRows) 分别对应store的put, get, ack操作针对数据表变更行的TPS 是 Client requests Canal client请求server的请求数统计，结果按请求类型分类(比如get/ack/sub/rollback等)。 否 Response time Canal client请求server的响应时间统计。 否 Empty packets Canal client请求server返回空结果的统计。 是 Store remain events Canal instance ringbuffer中堆积的events数量。 否 Store remain mem Canal instance ringbuffer中堆积的events内存使用量。 否 Client QPS client发送请求的QPS，按GET与CLIENTACK分类统计 是 参考：https://github.com/alibaba/canal/wiki/Prometheus-QuickStart 十、错误处理 1、如果canal启动时候从日志看到报这个错误：can't find start position for example。 解决方法： 单机 删除meta.dat文件，重启canal，问题解决。 集群 进入canal对应的zookeeper集群下，删除节点/otter/canal/destinations/实例/1001/cursor，重启canal即可恢复 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-09-05 10:08:20 "},"origin/sqlite-cipher.html":{"url":"origin/sqlite-cipher.html","title":"SQLCipher","keywords":"","body":"SQLCipher 一、简介 SQLCipher是一个在SQLite基础之上进行扩展的开源数据库，它主要是在SQLite的基础之上增加了数据加密功能。SQLCipher采用的是256位AES对称加密算法 Github：https://github.com/sqlcipher/sqlcipher 二、安装 MacOS brew install sqlcipher 三、操作 1、创建加密数据库 PRAGMA key = 'thisiskey'; create table test (id integer, name text); 2、打开加密数据库 PRAGMA key = 'thisiskey'; .schema 3、修改数据库密码 PRAGMA KEY = 'thisiskey'; PRAGMA REKEY = 'newkey'; 4、加密已有的数据库 ATTACH DATABASE 'encrypted.db' AS encrypted KEY 'thisiskey'; SELECT sqlcipher_export('encrypted'); DETACH DATABASE encrypted; 参考： https://www.modb.pro/db/65057 https://blog.csdn.net/u010333084/article/details/104984553 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-03-16 09:47:49 "},"origin/python-basic.html":{"url":"origin/python-basic.html","title":"环境搭建：安装配置","keywords":"","body":"Python 环境的搭建 一、安装 1、源码编译安装 CentOS version=3.7.7 yum install -y sqlite-devel readline-devel tk-devel wget https://www.python.org/ftp/python/$version/Python-$version.tgz tar -xzf Python-$version.tgz cd Python-$version ./configure --enable-optimizations make make install Ubuntu version=3.7.7 apt update apt install -y build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev \\ libsqlite3-dev libreadline-dev libffi-dev wget libbz2-dev wget https://www.python.org/ftp/python/$version/Python-$version.tgz tar -xzf Python-$version.tgz cd Python-$version ./configure --enable-optimizations make make install 2、包管理器安装 APT(Ubuntu/Debian) apt install software-properties-common add-apt-repository ppa:deadsnakes/ppa apt-get update apt-get install python3.6 二、Python的包管理器pip 1、 安装 YUM yum install -y epel-release ;\\ yum install python-pip APT apt-get install python3-pip 2、升级 pip3 install -U pip # 或者 python3 -m pip install -U pip python3 -m pip install --upgrade pip 3、安装依赖 pip3 install django # 或者 python3.6 -m pip install django 4、固定安装的依赖 pip3 freeze > requirements.txt 三、Python虚拟隔离环境Virtualenv 如果我们要同时开发多个应用程序，那这些应用程序都会共用一个Python，就是安装在系统的Python 3。如果应用A需要jinja 2.7，而应用B需要jinja 2.6怎么办？这种情况下，每个应用可能需要各自拥有一套“独立”的Python运行环境。virtualenv就是用来为一个应用创建一套“隔离”的Python运行环境。 1、安装 pip3 install virtualenv 2、配置使用 virtualenv --python=python3.6 . # 上述命令会在当前路径创建lib,include,bin目录 # --no-site-packages，参数设置已经安装到系统Python环境中的所有第三方包都不会复制过来，可以得到一个不带任何第三方包的“干净”的Python运行环境。 3、激活当前virtualenv source ./bin/activate # 注意终端发生了变化 4、安装依赖 (venv) pip install -y requirement.txt 5、关闭virtualenv (venv)deactivate 6、打包当前虚拟环境 (venv) -relocatable ./ 7、固定当前虚拟环境中安装的依赖 (venv) pip freeze > requirements.txt 四、AnaConda、MiniConda 1、设置 conda 源 ~/.condarc # 配置了 Conda 包搜索的顺序。Conda 将按照列表中的顺序搜索包，直到找到第一个匹配的包为止 channels: - defaults show_channel_urls: true # 是否在安装过程中显示从哪个频道下载的包 auto_update_conda: false # 配置了是否在每次使用 Conda 命令时自动更新 Conda。 create_default_packages: # 配置了创建新环境时默认安装的包 - numpy - pandas anaconda_anon_usage: false # 配置是否允许匿名使用数据的收集 channel_priority: strict # 配置了Conda是否优先使用指定的频道.\"strict\"(严格)表示只使用指定频道,\"flexible\"(灵活)表示可以使用其他频道 channel_alias: # 设置频道别名，以便更方便地引用频道 my_alias: https://my_channel_url default_channels: # 设置了 Conda 搜索包时的默认频道列表 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: # 设置自定义频道 conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/ conda clean -i 清除索引缓存 参考：https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/ 2、包管理 # 列出当前激活环境的所有包 conda list # 列出一个非激活环境的所有包 conda list -n base # 为指定环境安装某个包 conda install -n base package_name # 从指定 channel 为指定环境安装某个包 conda install -n base -c defaults package_name 3、环境管理 # 列出当前所有环境 conda info --envs conda env list # 创建包含某些包的环境 conda create --name your_env_name numpy scipy # 创建指定Python版本下包含某些包的环境 conda create --name your_env_name python=3.8 numpy scipy # 激活某个环境 activate your_env_name # 关闭某个环境 deactivate your_env_name # 克隆某个环境 conda create --name new_env_name --clone old_env_name # 删除某个环境 conda remove --name your_env_name --all # 分享环境 conda env export > share_env.yml # 创建该环境 conda env create -f share_env.yml 4、配置操作 # 清除一下缓存 conda clean -i # 清除所有缓存 conda clean --all # 查看全部配置信息 conda config --show # 查看源的配置信息 conda config --show-sources # 查看源的详细信息 conda info # 升级 conda conda update -n base conda Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:40:50 "},"origin/python-workerbench-jupyterhub.html":{"url":"origin/python-workerbench-jupyterhub.html","title":"JupyterHub/RStuido","keywords":"","body":"Python/R语言多用户工作台JupyterHub/RStuido 一、Jupyter简介 Jupyter是一款基于python的web notebook服务，目前有大多python数据挖掘与机器学习爱好者使用这款服务，其特性其实与Ipytohn Notebook差不多，准确说Ipython Notebook是一款提供增强型交互的功能的shell，而Jupyter除了Ipython的功能，还加入了普通编辑器的通用功能，是一款带代码交互的动态文档web编辑器 二、安装及命令详解 1、各种方式安装 二进制安装 以Ubuntu为例 apt-get install npm python3-pip npm config set registry https://registry.npm.taobao.org --global mkdir ~/.pip echo -e \"[global]\\nindex-url = https://mirrors.aliyun.com/pypi/simple/\\n[install]\\ntrusted-host=mirrors.aliyun.com\\n\" > ~/.pip/pip.conf python3 -m pip install jupyterhub notebook npm install -g configurable-http-proxy 生成默认配置文件 jupyterhub --generate-config 修改配置文件~/jupyterhub_config.py并启动jupyterhub nohup jupyterhub -f ~/jupyterhub_config.py 2>&1 >> /var/log/jupyterhub.log & echo $! > /var/log/jupyterhub.pid # 或者 nohup jupyterhub -f ~/jupyterhub_config.py 2>&1 >> /var/log/jupyterhub.log &! echo $! > /var/log/jupyterhub.pid docker安装 version: \"3\" services: jupyterhub: image: jupyterhub/jupyterhub:4.0.2 container_name: jupyterhub restart: always ports: - \"8000:8000\" environment: TZ: Asia/Shanghai volumes: - /root/jupyterhub/data:/home kubernetes安装 helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/ && \\ helm repo update RELEASE=jhub NAMESPACE=jhub helm upgrade --cleanup-on-fail \\ --install $RELEASE jupyterhub/jupyterhub \\ --namespace $NAMESPACE \\ --create-namespace \\ --version=0.9.0 \\ --values config.yaml 2、jupyterhub 命令详解 命令格式 jupyterhub cmd [args] 全局命令参数 --debug set log level to logging.DEBUG (maximize logging output) --generate-config generate default config file --generate-certs generate certificates used for internal ssl --no-db disable persisting state database to disk --upgrade-db Automatically upgrade the database if needed on startup. Only safe if the database has been backed up. Only SQLite database files will be backed up automatically. --no-ssl [DEPRECATED in 0.7: does nothing] --base-url= (JupyterHub.base_url) Default: '/' The base URL of the entire application. Add this to the beginning of all JupyterHub URLs. Use base_url to run JupyterHub within an existing website. .. deprecated: 0.9 Use JupyterHub.bind_url -y (JupyterHub.answer_yes) Default: False Answer yes to any questions (e.g. confirm overwrite) --ssl-key= (JupyterHub.ssl_key) Default: '' Path to SSL key file for the public facing interface of the proxy When setting this, you should also set ssl_cert --ssl-cert= (JupyterHub.ssl_cert) Default: '' Path to SSL certificate file for the public facing interface of the proxy When setting this, you should also set ssl_key --url= (JupyterHub.bind_url) Default: 'http://:8000' The public facing URL of the whole JupyterHub application. This is the address on which the proxy will bind. Sets protocol, ip, base_url --ip= (JupyterHub.ip) Default: '' The public facing ip of the whole JupyterHub application (specifically referred to as the proxy). This is the address on which the proxy will listen. The default is to listen on all interfaces. This is the only address through which JupyterHub should be accessed by users. .. deprecated: 0.9 Use JupyterHub.bind_url --port= (JupyterHub.port) Default: 8000 The public facing port of the proxy. This is the port on which the proxy will listen. This is the only port through which JupyterHub should be accessed by users. .. deprecated: 0.9 Use JupyterHub.bind_url --pid-file= (JupyterHub.pid_file) Default: '' File to write PID Useful for daemonizing JupyterHub. --log-file= (JupyterHub.extra_log_file) Default: '' DEPRECATED: use output redirection instead, e.g. jupyterhub &>> /var/log/jupyterhub.log --log-level= (Application.log_level) Default: 30 Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL') Set the log level by value or name. -f (JupyterHub.config_file) Default: 'jupyterhub_config.py' The config file to load --config= (JupyterHub.config_file) Default: 'jupyterhub_config.py' The config file to load --db= (JupyterHub.db_url) Default: 'sqlite:///jupyterhub.sqlite' url for the database. e.g. `sqlite:///jupyterhub.sqlite` 子命令 token：生成用户API token 命令格式 jupyterhub token [username] 命令参数 --log-level= (Application.log_level) Default: 30 Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL') Set the log level by value or name. -f (JupyterHub.config_file) Default: 'jupyterhub_config.py' The config file to load --config= (JupyterHub.config_file) Default: 'jupyterhub_config.py' The config file to load --db= (JupyterHub.db_url) Default: 'sqlite:///jupyterhub.sqlite' url for the database. e.g. `sqlite:///jupyterhub.sqlite` # 示例 $> jupyterhub token kaylee ab01cd23ef45 3、其他命令 Jupyter kernel的管理 jupyter-kernelspec list jupyter-kernelspec install jupyter-kernelspec uninstall jupyter-kernelspec remove 三、Jupyter功能扩展 1、使用LDAP进行用户认证 Github：https://github.com/jupyterhub/ldapauthenticator pip3 install jupyterhub-ldapauthenticator c.JupyterHub.authenticator_class = 'ldapauthenticator.LDAPAuthenticator' #c.LDAPAuthenticator.server_address = '192.168.1.7' c.LDAPAuthenticator.server_hosts = ['ldap://192.168.1.7:389'] c.LDAPAuthenticator.bind_user_dn = 'uid=root,cn=users,dc=ldap,dc=synology,dc=curiouser,dc=com' c.LDAPAuthenticator.bind_user_password = 'jL6u49t5A9P5' c.LDAPAuthenticator.user_search_base = 'cn=users,dc=ldap,dc=synology,dc=curiouser,dc=com' c.LDAPAuthenticator.user_search_filte = '(&(memberOf=cn=jupyterhub,cn=groups,dc=ldap,dc=synology,dc=curiouser,dc=com)(cn={0}))' c.LDAPAuthenticator.user_attribute = 'cn' c.LDAPAuthenticator.create_user_home_dir = True c.LDAPAuthenticator.create_user_home_dir_cmd = ['mkhomedir_helper'] c.LDAPAuthenticator.lookup_dn = True c.LDAPAuthenticator.lookup_dn_search_filter = '({login_attr}={login})' c.LDAPAuthenticator.lookup_dn_search_user = 'ldap_search_user_technical_account' c.LDAPAuthenticator.lookup_dn_search_password = 'secret' c.LDAPAuthenticator.user_search_base = 'ou=people,dc=wikimedia,dc=org' c.LDAPAuthenticator.user_attribute = 'sAMAccountName' c.LDAPAuthenticator.lookup_dn_user_dn_attribute = 'cn' c.LDAPAuthenticator.escape_userdn = False c.LDAPAuthenticator.bind_dn_template = '{username}' 2、添加扩展插件管理器 pip3 install jupyter_contrib_nbextensions # 安装完之后需要配置 nbextension，注意配置的时候要确保已关闭Jupyter Notebook jupyter contrib nbextension install --skip-running-check # 命令执行完后，会生成配置文件/usr/local/etc/jupyter/jupyter_nbconvert_config.json 重新启动 Jupyterhub后，上面选项栏会出现 Nbextensions 的选项。 3、支持R语言kernel 安装R语言 apt-get install r-base r-base-core r-base-dev pip3 install jupyterlab R安装基础工具 R > install.packages(c('pbdZMQ', 'repr', 'devtools', 'IRkernel')) R > IRkernel::installspec(user = FALSE) # 安装完成后会在目录/root/.local/share/jupyter/kernels/ir生成一份配置信息 jupyter labextension install @techrah/text-shortcuts # 刷新页面就可以看到R的kernel了！ 参考：https://irkernel.github.io/installation/ 四、R语言 1、安装 ①包管理器安装 以Ubuntu 18.04 bionic安装R 4.x.x版本 为例（包管理器默认仓库的R版本大多是3.x.x） echo \"deb https://mirrors.tuna.tsinghua.edu.cn/CRAN/bin/linux/ubuntu bionic-cran40/\">> /etc/apt/sources.list.d/r-tuna.list apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 apt-get update apt-get install r-base r-base-dev ②源码编译安装 apt-get install libxt-dev libcurl4-openssl-dev export R_VERSION=4.3.2 curl -O \"https://cran.rstudio.com/src/base/R-4/R-${R_VERSION}.tar.gz\" tar -xzvf R-${R_VERSION}.tar.gz cd R-${R_VERSION} JAVA_HOME=/opt/java ./configure \\ --prefix=/opt/R/4.3.2\\ --enable-R-shlib \\ --enable-memory-profiling \\ --with-blas \\ --with-lapack \\ --enable-R-shlib # --prefix Specifies the directory where R is installed when executing make install. Change this to install R at a different location than /opt/R/${R_VERSION}. # --enable-R-shlib Required to use R with RStudio. # --enable-memory-profiling Enables support for Rprofmem() and tracemem(), used to measure memory use in R code. # --with-blas, --with-lapack Configures R to link against external BLAS and LAPACK libraries on the system. Recommended only on Ubuntu/Debian, where the alternatives system may be used to switch the BLAS library at runtime. If unspecified, R uses an internal BLAS library that can be switched at runtime. See Configure R to use a different BLAS library for more details. make make install /opt/R/4.3.2/bin/R --version ln -s /opt/R/4.3.2/bin/R /usr/local/bin/R432 ln -s /opt/R/4.3.2/bin/Rscript /usr/local/bin/RscriptR432 2、包的管理 https://cloud.r-project.org/bin/linux/ubuntu/bionic-cran40/ 包的安装 从镜像源仓库安装 # 在R CLI中 install.packages(\"RMySQL\",repos=\"https://mirrors.ustc.edu.cn/CRAN\") install.packages(\"ape\") # 在linux命令行 su - -c \"R -e \\\"install.packages('shiny', repos='https://cran.rstudio.com/')\\\"\" 从包文件安装 # 在R CLI中 install.packages(\"/root/mgcv_1.8-29.tar.gz\", repos = NULL,type=\"source\") # 或者 packageurl 包的查看 installed.packages() 包的删除 remove. packages(c(\"pkg1\",\"pkg2\") , lib = file .path(\"path\", \"to\", \"library\")) # shili remove.packages(\"mgcv\", lib=\"/usr/lib/R/library\") 包的清除 detach(\"package:rjson\") 包的加载 library(rjson) require(rjson)---便于写脚本特性 3、升级R 3.x.x到4.x.x Windows下 # 安装包，如果已经有此包可跳过此步骤 install.packages(\"installr\") # 加载包，升级 library(installr) updateR() Ubuntu下 参考：https://cloud.r-project.org/bin/linux/ubuntu/#get-5000-cran-packages apt update -qq apt install --no-install-recommends software-properties-common dirmngr apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 # 添加CRAN的R 4.0仓库 add-apt-repository \"deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/\" apt-get install r-base-dev 五、RStudio Server 1、简介 RStudio分为桌面版、Web服务端版。这种两种类型都有开源和商业版本 官方文档：https://docs.rstudio.com/ide/server-pro/1.2.1293-1/index.html 2、安装 ①YUM（ RedHat / CentOS 6+） 参考：https://cran.rstudio.com/bin/linux/redhat/ yum install -y epel-release yum install # 或者 gpg --keyserver keys.gnupg.net --recv-keys 3F32EE77E331692F gpg --armor --export 3F32EE77E331692F > rstudio-code-signing.key rpm --import rstudio-code-signing.key rpm -K ②APT（ Debian 8+ / Ubuntu 12.04+） 文档：https://cran.rstudio.com/bin/linux/ubuntu/ apt-get install r-base # 或者 gpg --keyserver keys.gnupg.net --recv-keys 3F32EE77E331692F dpkg-sig --verify ③Docker-compose services: rstudio-server: image: rocker/rstudio:4.3.2 container_name: rstudio-server hostname: rstudio-server restart: always # 设置容器自启模式 ports: - \"8087:8787\" environment: TZ: Asia/Shanghai # 设置容器时区与宿主机保持一致 PASSWORD: 123456 # 设置root密码 ROOT: true USERID: 1001 GROUPID: 1001 volumes: - ./workspace:/home/ 参考： https://github.com/rocker-org/rocker/wiki https://github.com/rocker-org/rocker/wiki/Using-the-RStudio-image#multiple-users 3、配置 ①配置多版本 R https://support.posit.co/hc/en-us/articles/226872207-Managing-R-versions-in-RStudio-Connect 4、测试代码 install.packages('RMySQL') library(RMySQL) # 创建数据库连接 con Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-18 20:41:27 "},"origin/python-summary.html":{"url":"origin/python-summary.html","title":"Python总结","keywords":"","body":"Python 语法总结 一、通过SMTP发送邮件 1. 发送带附件的邮件 #!/usr/bin/python3 # -*- coding: UTF-8 -*- import smtplib import datetime from email import encoders from email.header import Header from email.mime.base import MIMEBase from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText smtp_server = 'smtp.163.com' smtp_user = '' smtp_user_pasw = '' email_sender = smtp_user # 接收者邮件地址 email_receivers = ['test1@163.com', 'test2@163.com'] def sendEmail(attachfilename): message = MIMEMultipart() message['From'] = Header(email_sender, 'utf-8') message['To'] = Header('; '.join(str(e) for e in email_receivers) + '; ', 'utf-8') # 邮件主题内容 message['Subject'] = Header(datetime.datetime.now().strftime('%Y%m%d%H') + \"Python测试SMTP发送邮件\", 'utf-8') # 邮件正文内容 message.attach(MIMEText(\"Python测试SMTP发送邮件，详情见附件Excel文件\", 'plain', 'utf-8')) # 附件内容 attachment = MIMEBase('application', \"octet-stream\") attachment.set_payload(open(attachfilename, \"rb\").read()) encoders.encode_base64(attachment) attachment.add_header('Content-Disposition', 'attachment; filename=\"附件文件名\"') message.attach(attachment) try: smtpObj = smtplib.SMTP(smtp_server) smtpObj.login(smtp_user, smtp_user_pasw) smtpObj.sendmail(email_sender, email_receivers, message.as_string()) print(\"邮件发送成功\") except smtplib.SMTPException: print(\"Error: 无法发送邮件\") def main(): sendEmail(\"/tmp/test-email-attachfile.txt\") if __name__ == \"__main__\": main() 2. 发送邮件 #!/usr/bin/python3 # -*- coding: UTF-8 -*- import smtplib import datetime from email.header import Header from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText smtp_server = 'smtp.163.com' smtp_user = '' smtp_user_pasw = '' email_sender = smtp_user email_receivers = ['test1@163.com','test2@163.com'] def sendEmail(email_context): message = MIMEMultipart() message['From'] = Header(email_sender, 'utf-8') message['To'] = Header(''.join(str(e) for e in email_receivers) + '; ', 'utf-8') message['Subject'] = Header(datetime.datetime.now().strftime('%Y%m%d%H') + \"Python测试SMTP发送邮件\", 'utf-8') message = MIMEText(email_context, 'plain', 'utf-8') try: smtpObj = smtplib.SMTP(smtp_server) smtpObj.login(smtp_user, smtp_user_pasw) smtpObj.sendmail(email_sender, email_receivers, message.as_string()) print(\"邮件发送成功\") except smtplib.SMTPException: print(\"Error: 无法发送邮件\") def main(): sendEmail(\"test,test\") if __name__ == \"__main__\": main() 参考： https://www.runoob.com/python/python-email.html https://www.runoon.com/python3/python3-smtp-sendmail.html 二、写入EXCEL 参考：https://blog.csdn.net/u013250071/article/details/81911434 #!/usr/bin/python3 # -*- coding: UTF-8 -*- import xlwt def createExcel(data,filename): # 创建excel工作表 workbook = xlwt.Workbook(encoding='utf-8') # 给表格新建一个名为 sheer1 的工作簿 worksheet = workbook.add_sheet('sheet1') # 创建一个样式对象，初始化样式 style = xlwt.XFStyle() al = xlwt.Alignment() # 其中horz代表水平对齐方式，vert代表垂直对齐方式 # VERT_TOP = 0x00 上端对齐 # VERT_CENTER = 0x01 居中对齐（垂直方向上） # VERT_BOTTOM = 0x02 低端对齐 # HORZ_LEFT = 0x01 左端对齐 # HORZ_CENTER = 0x02 居中对齐（水平方向上） # HORZ_RIGHT = 0x03 右端对齐 # style.alignment = al # 设置表头 worksheet.write(0, 0, label='id') worksheet.write(0, 1, label='用户名') worksheet.write(0, 2, label='性别') worksheet.write(0, 3, label='年龄') worksheet.write(0, 4, label='学历') ordered_list = [\"id\", \"用户名\", \"性别\", \"年龄\", \"学历\"] row = 1 for player in data: for _key, _value in player.items(): col = ordered_list.index(_key) worksheet.write(row, col, _value, style) row += 1 workbook.save(filename) def main(): testdata=[ {'id': '1', '用户名': \"test1\", '性别': \"男\", '年龄': 18,'学历': \"大学本科\"}, {'id': '2', '用户名': \"test2\", '性别': \"男\", '年龄': 25,'学历': \"大学本科\"} ] createExcel(testdata,\"test.xls\") if __name__ == \"__main__\": main() 三、列表操作 1、遍历字典列表 #!/usr/bin/python3 # -*- coding: UTF-8 -*- def main(): testdata=[ {'id': '1', '用户名': \"test1\", '性别': \"男\", '年龄': 18,'学历': \"大学本科\"}, {'id': '2', '用户名': \"test2\", '性别': \"男\", '年龄': 25,'学历': \"大学本科\"} ] row = 1 for i in testdata: for _key,_value in i.items(): print(_key,\"=\",_value) row += 1 if __name__ == \"__main__\": main() 2、列表去重添加 #!/usr/bin/python3 # -*- coding: UTF-8 -*- data_example={\"1\": \"aa\", \"2\": \"bb\", \"3\": \"cc\"} target = [] u_name = \"2\" if u_name in data_example: target.append(json.loads(data_example).get(u_name)) print(target) # 输出：['bb'] 3. 删除字典列表元素 ①For循环删除 test_list = [{\"id\" : 1, \"data\" : \"1111\"}, {\"id\" : 2, \"data\" : \"2222\"}, {\"id\" : 3, \"data\" : \"3333\"}] for i in range(len(test_list)): if test_list[i]['id'] == 2: del test_list[i] break # 输出： [{‘id’: 1, ‘data’: ‘1111’}, {‘id’: 3, ‘data’: ‘2222’}] ②lambda删除 test_list = [{\"id\" : 1, \"data\" : \"1111\"}, {\"id\" : 2, \"data\" : \"2222\"}, {\"id\" : 3, \"data\" : \"3333\"}] res = list(filter(lambda i: i['id'] != 2, test_list)) print (str(res)) # 输出： [{‘id’: 1, ‘data’: ‘1111’}, {‘id’: 3, ‘data’: ‘2222’}] 参考：https://www.geeksforgeeks.org/python-removing-dictionary-from-list-of-dictionaries/ 4. 列表的遍历与拼接 #!/usr/bin/python3 # -*- coding: UTF-8 -*- def main(): test_list = ['hello','world','I\\'m','Python'] print(' '.join(str(e) for e in test_list) + '!') if __name__ == \"__main__\": main() # 输出 hello world I'm Python! 四、时间处理 ISO 8601 #!/usr/bin/python3 # -*- coding: UTF-8 -*- import datetime created_at=\"2022-08-16T06:43:49.869Z\" updated_at=\"2022-08-16T06:48:09.429Z\" print( datetime.datetime.strptime(updated_at, \"%Y-%m-%dT%H:%M:%S.%fZ\") - datetime.datetime.strptime(created_at,\"%Y-%m-%dT%H:%M:%S.%fZ\") ) from datetime import datetime, timedelta # 时间格式为ISO 8601 str_time = \"2023-01-13T01:30:43.559Z\" f_time = (datetime.strptime(str_time, \"%Y-%m-%dT%H:%M:%S.%fZ\") + timedelta(hours=8)).strftime(\"%Y-%m-%d %H:%M:%S\") print(f_time) # 输出：2023-01-13 09:30:43 五、发送钉钉通知 #!/usr/bin/python3 # encoding: utf-8 import hmac, json, base64, hashlib, urllib, mureq, time dingding_secret = \"钉钉自定义机器人Webhook的Token\" dingding_token = \"钉钉自定义机器人消息加签的Secret\" ddmsg = { \"msgtype\": \"markdown\", \"markdown\": { \"title\": \"\", \"text\": \"\", }, \"at\": { \"atMobiles\": [], } } def dingdingnotify(msg): timestamp = (round(time.time() * 1000)) secret_enc = dingding_secret.encode('utf-8') string_to_sign = '{}\\n{}'.format(timestamp, dingding_secret) string_to_sign_enc = string_to_sign.encode('utf-8') hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest() sign = urllib.parse.quote(base64.b64encode(hmac_code)) url = \"https://oapi.dingtalk.com/robot/send?access_token=\" + dingding_token + \"&timestamp=\" + str( timestamp) + \"&sign=\" + str(sign) headers = {\"Content-Type\": \"application/json\", \"Charset\": \"UTF-8\"} return mureq.post(url, str.encode(json.dumps(msg)), headers=headers) if __name__ == '__main__': ddmsg['markdown']['title'] = \"主题\" ddmsg['markdown']['text'] = \"MarkDown格式的正文，在正文里添加@人的手机号，且只有在群内的成员才可被@，非群内成员手机号会被脱敏\" ddmsg['at']['atMobiles'] = [\"要@的人的手机号1\", \"要@的人的手机号2\", \"要@的人的手机号3\"] notify_respone = dingdingnotify(ddmsg) 参考：https://open.dingtalk.com/document/robots/custom-robot-access#title-zob-eyu-qse 六、命令参数 #!/usr/bin/python3 # encoding: utf-8 import sys,argparse def todo(arg_id, arg_name): print(arg_id, arg_name) def main(): if args.id and args.name: todo(args.id,args.name) if __name__ == \"__main__\": parser = argparse.ArgumentParser() parser.add_argument('--id', dest='id', nargs='?', default=None, type=int, required=True, help=\"请输入ID\" ) parser.add_argument('--name', dest='name', nargs='?', default=None, type=str, required=True, help=\"请输用户名\" ) args = parser.parse_args() sys.exit(main) 参考： https://docs.python.org/zh-cn/3/howto/argparse.html https://www.jianshu.com/p/5e28e0590b71 七、evdev from flask import Flask, render_template, request from flask_socketio import SocketIO import evdev app = Flask('test') socketio = SocketIO(app) scancodes = { 0: None, 1: u'ESC', 2: u'1', 3: u'2', 4: u'3', 5: u'4', 6: u'5', 7: u'6', 8: u'7', 9: u'8', 10: u'9', 11: u'0', 12: u'-', 13: u'=', 14: u'BKSP', 15: u'TAB', 16: u'Q', 17: u'W', 18: u'E', 19: u'R', 20: u'T', 21: u'Y', 22: u'U', 23: u'I', 24: u'O', 25: u'P', 26: u'[', 27: u']', 28: u'CRLF', 29: u'LCTRL', 30: u'A', 31: u'S', 32: u'D', 33: u'F', 34: u'G', 35: u'H', 36: u'J', 37: u'K', 38: u'L', 39: u';', 40: u'\"', 41: u'`', 42: u'LSHFT', 43: u'\\\\', 44: u'Z', 45: u'X', 46: u'C', 47: u'V', 48: u'B', 49: u'N', 50: u'M', 51: u',', 52: u'.', 53: u'/', 54: u'RSHFT', 56: u'LALT', 100: u'RALT' } @socketio.on(\"message\") def handle_message(data): print(\"message emit\") def getInputDevice(): devices = [evdev.InputDevice(path) for path in evdev.list_devices()] barcode_scanner = None for device in devices: print(device) if \"HID\" in device.name: barcode_scanner = device.path break if barcode_scanner is None: sys.exit(1) else: return barcode_scanner barcodescanner = evdev.InputDevice(getInputDevice()) def listen_barcode_scanner(): barcode = \"\" for event in barcodescanner.read_loop(): if event.type == evdev.ecodes.EV_KEY: key_event = evdev.categorize(event) if key_event.keystate == 1: key_code = key_event.keycode key_lookup = scancodes.get(key_event.scancode) or u'UNKNOWN:{}'.format(key_event.scancode) if key_code == \"KEY_ENTER\": socketio.emit('message', barcode) barcode = \"\" elif key_code != \"KEY_LEFTSHIFT\": barcode += key_lookup if __name__ == '__main__': import threading barcode_listener = threading.Thread(target=listen_barcode_scanner) barcode_listener.daemon = True barcode_listener.start() app.run(debug=True, port=8001) Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-08-11 11:17:49 "},"origin/flask.html":{"url":"origin/flask.html","title":"Flask","keywords":"","body":"Flask总结 1、工程目录 2、快速搭建 pip3 install flask main.py from flask import Flask,request import json app = Flask('test') f = open('test.log','w') @app.route('/test', methods=['POST']) def test(): print(json.dumps(request.get_json(), sort_keys=True, indent=4, separators=(',', ':'),ensure_ascii=False), file=f) f.close() return \"\" if __name__ == '__main__': app.run(debug=True, port=5005, host='0.0.0.0') 3、USGI wsgi pip3 install gunicorn Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-08-11 11:32:06 "},"origin/golang-basic.html":{"url":"origin/golang-basic.html","title":"基础语法","keywords":"","body":"一、推荐目录结构 /main.go 入口文件 /config 配置 /router 路由 |—— api.go |-- web.go /handler 请求处理器 |-- user.go /middleware 中间件 |-- jwtToken.go /model 模型 |-- user.go /test 测试 二、变量声明 1、变量范围 函数内定义的变量称为局部变量 函数外定义的变量称为全局变量 函数定义中的变量称为形式参数 2、可导出与不可导出变量规则 变量的可见性由其标识符的首字母是否大写来确定。 Exported（可导出）变量： 如果变量的标识符（名字）以大写字母开头，那么它是可导出的，可以在其他包中访问。可导出的变量是包外可见的。 Unexported（不可导出）变量： 如果变量的标识符以小写字母开头，那么它是不可导出的，只能在当前包中访问。不可导出的变量对于其他包是不可见的。 可见性规则适用于所有的标识符，不仅仅是变量，还包括函数、类型等。这个规则有助于封装和隐藏内部实现，使得包的使用更加清晰和可维护。 在同一个包内，无论变量是否可导出，都可以直接访问。但在不同的包中，只能访问可导出的变量。 三、类型转换 struct与json之间的互转 type Test struct { A string `json:\"a` B int `json:\"b\"` } struct转json var t Test t.A=\"测试\" t.B=2 jsonBytes, err := json.Marshal(t) if err != nil { fmt.Println(err) } fmt.Println(string(jsonBytes)) json转struct jsonStr := `{ \"a\": \"测试\", \"b\": 2 }` var t Test json.Unmarshal([]byte(jsonStr), &t) fmt.Println(t) 整型转 string var number int = 80 strconv.Itoa(number) bytes转 string var b byte string(b) 四、主函数的初始化 var ( router *gin.Engine ) func init (){ router = gin.Default() } func main(){ router.GET(\"/\",func(context *gin.Context) {}) } 五、数组 ①遍历 var test_array = [5]float32{1000.0, 2.0, 3.4, 7.0, 50.0} for _,a := range test_array { fmt.Println(a) } var test_array = [3]string{\"test1\",\"test2\"} for _,a := range test_array { fmt.Println(a) } 六、golang程序初始化 golang程序初始化先于main函数执行，由runtime进行初始化，初始化顺序如下： 初始化导入的包（包的初始化顺序并不是按导入顺序（“从上到下”）执行的，runtime需要解析包依赖关系，没有依赖的包最先初始化，与变量初始化依赖关系类似，参见golang变量的初始化）； 初始化包作用域的变量（该作用域的变量的初始化也并非按照“从上到下、从左到右”的顺序，runtime解析变量依赖关系，没有依赖的变量最先初始化，参见golang变量的初始化）； 执行包的init函数； 变量初始化->init()->main() 七、获取系统环境变量 方式一：env type config struct { App string Port int `default:\"8000\"` IsDebug bool `env:\"DEBUG\"` Hosts []string `slice_sep:\",\"` Timeout time.Duration Redis struct { Version string `sep:\"\"` // no sep between `CONFIG` and `REDIS` Host string Port int } MySQL struct { Version string `default:\"5.7\"` Host string Port int } } 方式二：os.Getenv var ( key1 string key2 string appConfig = &config.AppConfig{ AppID: os.Getenv(\"APP_NAME\"), Cluster: os.Getenv(\"APP_ENV\"), } ) ausername=\"test\" awk '!/^;/&&!/^#/&&$1==\"'${ausername}'\"{print $2;exit}' psw-file 八、读取json格式配置文件 config.json { \"host\": \"192.168.1.1\", \"ports\": { \"22\": \"ssh\", \"3306\": \"mysql\", \"6379\": \"redis\", \"9091\": \"kafka\" } } main.go package main import ( \"encoding/json\" \"fmt\" \"os\" ) // AppConfig 结构体定义了应用程序的配置结构 type AppConfig struct { Host string `json:\"host\"` PortsMap map[string]string `json:\"ports\"` } // 全局变量，存储应用程序配置 var appconfig = AppConfig{} func main() { // 读取配置文件内容 configfileData, err := os.ReadFile(\"config.json\") if err != nil { fmt.Println(\"当前目录下未发现配置文件config.json，本次启动将使用默认值。可创建配置，格式为json\") } else { // 解析 JSON err = json.Unmarshal(configfileData, &appconfig) if err != nil { fmt.Println(\"当前目录下的config.json无法解析，本次启动不使用其中设置。格式为json，请进行检查。\") } else { // 检查参数是否存在 if appconfig.Host == \"\" && appconfig.PortsMap == nil { println(\"当前目录下config.json没有配置host和ports。本次启动将使用默认值。格式为json，请进行检查。\") } // 检查参数是否缺少 if appconfig.Host == \"\" { println(\"当前目录下config.json没有配置host 。本次启动将使用默认值。格式为json，请进行检查。\") } if appconfig.PortsMap == nil { println(\"当前目录下config.json没有配置ports，本次启动将使用默认值。格式为json，请进行检查。\") } } } fmt.Printf(\"服务器地址: %s\\n\", appconfig.Host) for port, service := range appconfig.PortsMap { fmt.Printf(\" %s: %s\\n\", service, port) } } 九、其他 在 Go 语言中，map 是无序的，即使按照某种顺序插入数据，也不能保证遍历时是按照插入的顺序。最终的 map 遍历结果仍然是无序的。 十、struct 嵌套 对象继承在 Golang嵌套。 type Address struct { Street string City string PostalCode string } type Person struct { Name string Age int Address // 匿名字段 } p := Person{ Name: \"Alice\", Age: 30, Address: Address{ Street: \"123\", City: \"Any\", PostalCode: \"12345\", } } print(json.Marshal(p)) 十一、异常 err. Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-04-29 11:10:17 "},"origin/golang-concurrent-programming.html":{"url":"origin/golang-concurrent-programming.html","title":"go的并发","keywords":"","body":" 在一个函数调用前加上“go”关键词，那么本地调用就会在一个新的goroutine 中并发执行 如果该函数有返回值，则会被抛弃 函数结束，goroutine 也会结束 不要通过共享内存来通信，而应该通过通信来共享内存 go通过消息机制而非共享内存作为并发单元间的通信方式，这个消息机制在go中被称为channel 消息机制认为每一个并发单元都是一个自包含、独立的个体，并且有自己不共享的变量。 每个并发单元的输入输出都只能是消息。 channel是go在语言级别为goroutine 间的通信方式 channel是类型相关的，一个channel只能传递一种类型的值 channel语法 声明 var a chan int：声明一个传递类型为int的channel var b map[string] chan bool ：声明了一个map，元素是bool类型的channel 初始化 a := make(chan int)：声明并初始化一个int类型名为a的channel 读取与写入 a ：将1写入名为a的channel value := ：从名为a的channel中读取数据到value中 设置限制大小带有缓冲的channel 在需要传输大量数据的场景下，传递单个数据的channel就不合适啦 ch := make(chan init ,1024)：声明并创建一个大小1024的int类型的channel 在没有读取方的时候，写入方可以一直写，直到填充完channel前都不会阻塞 单向只读只写channel 默认情况下，通道 channel 是双向的，也就是，既可以往里面发送数据也可以同里面接收数据。但是，我们经常见一个通道作为参数进行传递而只希望对方是单向使用的，要么只让它发送数据，要么只让它接收数据，这时候我们可以指定通道的方向。而所谓的单向channel，可以理解为对channel的限制。例如限制某个函数只能往某个channel中写入数据 如果直接使用make创建单向channel（ch := make()，就毫无意义。通常声明初始化一个正常双向的channel，再使用类型转换创建单向的 ch := make(chan int) // 声明一个只能写入数据的通道类型, 并赋值为ch var chSendOnly chan func producer(out chan 关闭channel 关闭channel直接使用close()即可，但是如何确认channel是否已经关闭?可通过在读取时使用多重返回值的方式进行判断 close(ch) x , ok := 不要从接收端关闭channel，也不要关闭有多个并发发送者的channel。换句话说，如果sender(发送者)只是唯一的sender或者是channel最后一个活跃的sender，那么你应该在sender的goroutine关闭channel，从而通知receiver(s)(接收者们)已经没有值可以读了。维持这条原则将保证永远不会发生向一个已经关闭的channel发送值或者关闭一个已经关闭的channel。 channel的读写堵塞、超时问题的解决 问题：如果往channel中写数据，此时发现channel满了；如果从channel中读取数据，此时发现channel是空的，如果此时没有处理逻辑，会造成个goroutine 堵塞锁死 解决：使用select实现给channel的读写设置超时机制 func main() { ch := make(chan int) quit := make(chan bool) //新开一个协程 go func() { for { select { // select的每个 case 语句里必须是一个 IO 操作 // 如果ch成功读到数据，则进行该case处理语句 case num := Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/go-gin.html":{"url":"origin/go-gin.html","title":"Web框架Gin的使用总结","keywords":"","body":"Golang的Web框架Gin使用总结 一、Gin基础设置 func main(){ # 设置当前运行模式 gin.SetMode(gin.ReleaseMode) r := gin.Default() } 二、使用BasicAuth中间件限制指定接口 router := gin.Default() authorizedRoute = router.Group(\"/\", gin.BasicAuth(gin.Accounts{ \"admin\": \"123456\", //用户名：密码 \"root\": \"aaaaaaa\", })) // Group函数注册了一个群组路由，gin.BasicAuth是中间件，参数gin.Accounts是一个map[string]string类型的映射，用来记录用户名和密码。 authorizedRoute.StaticFile(\"/favicon.ico\", \"./public/favicon.ico\") authorizedRoute.GET(\"/secrets\", func(c *gin.Context) { // get user, it was set by the BasicAuth middleware user := c.MustGet(gin.AuthUserKey).(string) if secret, ok := secrets[user]; ok { c.JSON(http.StatusOK, gin.H{\"user\": user, \"secret\": secret}) } else { c.JSON(http.StatusOK, gin.H{\"user\": user, \"secret\": \"NO SECRET :(\"}) } }) 三、重定向路由 router := gin.Default() router.GET(\"/\", func(context *gin.Context) { context.Request.URL.Path = \"/public\" router.HandleContext(context) }) // 或者重定向到外部连接 router.GET(\"/test\", func(context *gin.Context) { context.Redirect(http.StatusMovedPermanently, \"http://www.google.com/\") }) // 重定向POST请求 router.POST(\"/test\", func(context *gin.Context) { context.Redirect(http.StatusFound, \"/foo\") }) 四、获取请求中的数据 1、打印请求Body router := gin.Default() router.POST(\"/welcome\", func(context *gin.Context) { requestBody, _ := ioutil.ReadAll(context.Request.Body) println(string(requestBody)) }) 2、获取URL路径 ①获取单个或多个请求URL路径 router := gin.Default() // 只会匹配“/user/john” 不会匹配 “/user/“ 或者 “/user“ router.GET(\"/user/:name\", func(context *gin.Context) { name := context.Param(\"name\") context.String(http.StatusOK, \"Hello %s\", name) }) // However, this one will match /user/john/ and also /user/john/send // If no other routers match /user/john, it will redirect to /user/john/ router.GET(\"/user/:name/*action\", func(context *gin.Context) { name := context.Param(\"name\") action := context.Param(\"action\") message := name + \" is \" + action context.String(http.StatusOK, message) }) // 对于每个匹配的请求，Context将保留路由定义 router.POST(\"/user/:name/*action\", func(context *gin.Context) { context.FullPath() == \"/user/:name/*action\" // true }) 3、获取URL参数 ①请求中的参数 /welcome?firstname=Jane&lastname=Doe router := gin.Default() router.GET(\"/welcome\", func(context *gin.Context) { firstname := context.DefaultQuery(\"firstname\", \"默认值\") lastname := context.Query(\"lastname\") // 也可以写成 context.Request.URL.Query().Get(\"lastname\") context.String(http.StatusOK, \"Hello %s %s\", firstname, lastname) }) ②请求中的参数数组 /post?ids[a]=1234&ids[b]=hello router := gin.Default() router.GET(\"/welcome\", func(context *gin.Context) { ids := context.QueryMap(\"ids\") context.String(http.StatusOK, \"Hello %s %s\", firstname, lastname) }) 4、获取POST请求表单数据 ①表单的多个数据 POST /post?id=1234&page=1 HTTP/1.1 Content-Type: application/x-www-form-urlencoded name=manu&message=this_is_great router := gin.Default() router.POST(\"/post\", func(context *gin.Context) { name := context.PostForm(\"name\") message := context.PostForm(\"message\") fmt.Printf(\"name: %s; message: %s\", name, message) }) ②表单的数组数据： POST /post?ids[a]=1234&ids[b]=hello HTTP/1.1 Content-Type: application/x-www-form-urlencoded names[first]=thinkerou&names[second]=tianou router := gin.Default() router.POST(\"/post\", func(context *gin.Context) { names := context.PostFormMap(\"names\") fmt.Printf(\"names: %v\", names) }) 五、文件上传 1、单个文件上传 router := gin.Default() // Set a lower memory limit for multipart forms (default is 32 MiB) router.MaxMultipartMemory = 8 curl -X POST http://localhost:8080/upload \\ -F \"file=@/Users/curiouser/test.txt\" \\ -H \"Content-Type: multipart/form-data\" 2、多个文件上传 router := gin.Default() // Set a lower memory limit for multipart forms (default is 32 MiB) router.MaxMultipartMemory = 8 curl -X POST http://localhost:8080/upload \\ -F \"upload[]=@/Users/curiouser/test1.txt\" \\ -F \"upload[]=@/Users/curiouser/test2.txt\" \\ -H \"Content-Type: multipart/form-data\" 六、绑定请求数据到结构体中 1、绑定请求路径到结构体中 GET /thinkerou/987fbc97-4bed-5078-9f07-9141ba07c9f3 GET /thinkerou/not-uuid type Person struct { ID string `uri:\"id\" binding:\"required,uuid\"` Name string `uri:\"name\" binding:\"required\"` } route := gin.Default() route.GET(\"/:name/:id\", func(context *gin.Context) { var person Person if err := context.ShouldBindUri(&person); err != nil { context.JSON(400, gin.H{\"msg\": err}) return } }) 2、绑定请求参数到结构体中 GET /testing?name=curiouser&address=xyz&birthday=1993&createTime=123&unixTime=15622 type Person struct { Name string `form:\"name\"` Address string `form:\"address\"` Birthday time.Time `form:\"birthday\" time_format:\"2006-01-02\" time_utc:\"1\"` CreateTime time.Time `form:\"createTime\" time_format:\"unixNano\"` UnixTime time.Time `form:\"unixTime\" time_format:\"unix\"` } route := gin.Default() route.GET(\"/testing\",func(context *gin.Context) { var person Person if context.ShouldBind(&person) == nil { log.Println(person.Name) log.Println(person.Address) log.Println(person.Birthday) log.Println(person.CreateTime) log.Println(person.UnixTime) } }) 3、绑定请求Header到结构体中 curl -H \"rate:300\" -H \"domain:music\" 127.0.0.1:8080/test type testHeader struct { Rate int `header:\"Rate\"` Domain string `header:\"Domain\"` } route := gin.Default() route.GET(\"/test\", func(context *gin.Context) { h := testHeader{} if err := context.ShouldBindHeader(&h); err != nil { context.JSON(200, err) } }) 4、绑定HTML复选框中的数据到结构体 Check some colors Red Green Blue type colorsForm struct { Colors []string `form:\"colors[]\"` } route := gin.Default() route.GET(\"/test\", func(context *gin.Context) { var foo colorsForm context.ShouldBind(&foo) context.JSON(200, gin.H{\"color\": foo.Colors}) }) 5、绑定表单数据到结构体 curl -X POST -v --form name=user --form \"avatar=@./avatar.png\" http://localhost:8080/profile type ProfileForm struct { Name string `form:\"name\" binding:\"required\"` Avatar *multipart.FileHeader `form:\"avatar\" binding:\"required\"` // or for multiple files // Avatars []*multipart.FileHeader `form:\"avatar\" binding:\"required\"` } router := gin.Default() router.POST(\"/profile\", func(context *gin.Context) { // you can bind multipart form with explicit binding declaration: // c.ShouldBindWith(&form, binding.Form) // or you can simply use autobinding with ShouldBind method: var form ProfileForm // in this case proper binding will be automatically selected if err := context.ShouldBind(&form); err != nil { context.String(http.StatusBadRequest, \"bad request\") return } err := context.SaveUploadedFile(form.Avatar, form.Avatar.Filename) if err != nil { context.String(http.StatusInternalServerError, \"unknown error\") return } // db.Save(&form) context.String(http.StatusOK, \"ok\") }) 6、绑定请求体到结构体 type Test struct { ID int `json:\"id\"` AppID string `json:\"appId\"` } router := gin.Default() router.POST(\"/welcome\", func(context *gin.Context) { var a Test if err := context.ShouldBind(&a); err != nil { context.String(http.StatusBadRequest, \"bad request\") return } }) 七、使用中间件 1、全局设置中间件 router := gin.Default() router.Use(gin.Logger()) router.Use(gin.Recovery()) 2、单个路由设置中间件 router := gin.Defalut() authorized := router.Group(\"/login\") authorized.Use(AuthRequired()) // 简短写法 // authorized = router.Group(\"/login\",gin.BasicAuth(gin.Accounts{ // \"admin\": \"111\", // )) // 或者 router.GET(\"/benchmark\", MyBenchLogger(), benchEndpoint) 八、设置接口支持跨域 router.GET(\"/login\",func(context *gin.Context) { context.Writer.Header().Set(\"Access-Control-Allow-Origin\", \"*\") context.Header(\"Access-Control-Allow-Origin\", \"*\") // 设置允许访问所有域 context.Header(\"Access-Control-Allow-Methods\", \"POST, GET, OPTIONS, PUT, DELETE,UPDATE\") context.Header(\"Access-Control-Allow-Headers\", \"Authorization, Content-Length, X-CSRF-Token, Token,session,X_Requested_With,Accept, Origin, Host, Connection, Accept-Encoding, Accept-Language,DNT, X-CustomHeader, Keep-Alive, User-Agent, X-Requested-With, If-Modified-Since, Cache-Control, Content-Type, Pragma,token,openid,opentoken\") context.Header(\"Access-Control-Expose-Headers\", \"Content-Length, Access-Control-Allow-Origin, Access-Control-Allow-Headers,Cache-Control,Content-Language,Content-Type,Expires,Last-Modified,Pragma,FooBar\") context.Header(\"Access-Control-Max-Age\", \"172800\") context.Header(\"Access-Control-Allow-Credentials\", \"false\") context.Set(\"content-type\", \"application/json\") //设置返回格式是json } 参考：https://www.cnblogs.com/you-men/p/14054348.html 九、压缩响应数据 常见浏览器支持的压缩算法：gzip, deflate, br(Brotli), zstd 具体各个浏览器的支持详情参考：https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Encoding 1、Gzip压缩 gzip是GNUzip的缩写，最早用于UNIX系统的文件压缩。HTTP协议上的gzip编码是一种用来改进web应用程序性能的技术，web服务器和客户端（浏览器）必须共同支持gzip。目前主流的浏览器，Chrome,firefox,IE等都支持该协议。常见的服务器如Apache，Nginx，IIS同样支持gzip。 gzip压缩比率在3到10倍左右 Go gin gzip的实现: https://github.com/gin-contrib/gzip go get github.com/gin-contrib/gzip import \"github.com/gin-contrib/gzip\" // 全局路由使用gzip压缩响应数据 router.Use(gzip.Gzip(gzip.DefaultCompression)) // 根据正则排除的路由不使用 gzip压缩响应数据 router.Use(gzip.Gzip(gzip.DefaultCompression, gzip.WithExcludedPathsRegexs([]string{\".*\"}))) // 排除指定路径的路由不使用 gzip压缩响应数据 router.Use(gzip.Gzip(gzip.DefaultCompression, gzip.WithExcludedPaths([]string{\"/api/\"}))) https://blog.rexskz.info/trip-for-finding-golang-memory-leak.html https://blog.hi917.com/detail/57.html 2、Brotli压缩 Brotli 是谷歌2015 年推出的开源无损压缩算法，它通过变种的LZ77 算法、Huffman 编码以及二阶文本建模等方式进行数据压缩，比常见的Gzip更高效。 注意：Brotli 压缩算法在浏览器下，接口只有部署在 https 请求下生效，因为在http请求头Accept-Encoding中默认是没有 br的，只有gzip,deflate ,同时浏览器禁止在http请求中修改Accept-Encoding Header。参考：https://developer.mozilla.org/zh-CN/docs/Glossary/Forbidden_header_name。但是可以使用 Curl命令 、PostMan等其他工具进行测试 ①Google基于C的go实现 Github : https://github.com/google/brotli/tree/master/go 压缩率设置：0 ~ 11。设置默认为 5。 import ( \"bytes\" \"net/http\" \"strings\" \"github.com/gin-gonic/gin\" \"github.com/google/brotli/go/cbrotli\" ) func BrotliCompress() gin.HandlerFunc { return func(c *gin.Context) { // 检查客户端是否支持Brotli压缩 if strings.Contains(c.Request.Header.Get(\"Accept-Encoding\"), \"br\") { writer := &brotliWriter{ResponseWriter: c.Writer, buf: &bytes.Buffer{}} c.Writer = writer c.Next() writer.compress() } else { c.Next() } } } type brotliWriter struct { gin.ResponseWriter buf *bytes.Buffer } func (b *brotliWriter) Write(data []byte) (int, error) { return b.buf.Write(data) } func (b *brotliWriter) compress() { b.Header().Set(\"Content-Encoding\", \"br\") b.Header().Del(\"Content-Length\") // 使用Brotli压缩 writer := cbrotli.NewWriter(b.ResponseWriter, cbrotli.WriterOptions{Quality: 5}) defer writer.Close() _, err := writer.Write(b.buf.Bytes()) if err != nil { http.Error(b.ResponseWriter, \"Failed to compress response\", http.StatusInternalServerError) return } } func main(){ router := gin.Default() // 指定路由使用 BR 压缩响应数据 router.POST(\"/getlogs\", BrotliCompress(), func(c *gin.Context) { ... } } 测试 curl -v -H \"Accept-Encoding: br\" https://localhost:8443 ②纯Go实现 Github：https://github.com/andybalholm/brotli import ( \"bytes\" \"strings\" \"github.com/andybalholm/brotli\" \"github.com/gin-gonic/gin\" ) func BrotliCompress() gin.HandlerFunc { return func(c *gin.Context) { if strings.Contains(c.Request.Header.Get(\"Accept-Encoding\"), \"br\") { writer := &brotliWriter{ResponseWriter: c.Writer, buf: &bytes.Buffer{}} c.Writer = writer c.Next() writer.compress() } else { c.Next() } } } type brotliWriter struct { gin.ResponseWriter buf *bytes.Buffer } func (b *brotliWriter) Write(data []byte) (int, error) { return b.buf.Write(data) } func (b *brotliWriter) compress() { b.Header().Set(\"Content-Encoding\", \"br\") b.Header().Del(\"Content-Length\") writer := brotli.NewWriter(b.ResponseWriter) defer writer.Close() writer.Write(b.buf.Bytes()) } func main(){ router := gin.Default() // 指定路由使用 BR 压缩响应数据 router.POST(\"/getlogs\", BrotliCompress(), func(c *gin.Context) { ... } } 参考：https://blog.csdn.net/qq_34556414/article/details/109112165 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-05 18:10:39 "},"origin/go-utils.html":{"url":"origin/go-utils.html","title":"常见工具包的使用总结","keywords":"","body":"常用工具包的使用 一、时间的处理 1、按照指定格式输出 println(time.Now().Format(\"2006-01-02 15:04:05\")) 2、输出毫秒级别时间戳 cts := fmt.Sprintf(\"%d\", time.Now().UnixNano()/1e6) 二、字符串的处理 1、分割字符串 多分割符分割字符串 目标字符串 := strings.FieldsFunc(\"待处理字符串\", func(r rune) bool { return r == '/' || r == '?' })[3] 单个分隔符分割字符串 str=\"aaaaaaaa\\r\\nBBBBBBBBBBB\\r\\n1111\" 目标字符串 := strings.Split(string(res[0:len(res)]), \"\\r\\n\") # 目标字符串类型为字符串数组 2、判断字符串前缀是否包含指定字符 str=\"aaaaaaaa\\r\\nBBBBBBBBBBB\\r\\n1111\" res := strings.HasPrefix(s, \"aaa\") // res为布尔值 三、命令行参数的设置 import (\"flag\" ) var ( omhost string omport string ompasswd string ) func init (){ flag.StringVar(&omhost, \"host\", \"\", \"OpenVPN服务端地址\") flag.StringVar(&omport, \"port\", \"\", \"OpenVPN服务端管理端口，默认为空\") flag.StringVar(&ompasswd, \"passwd\", \"\", \"OpenVPN服务端管理端口密码\") flag.Parse() } func main(){ if omhost == \"\" && omport == \"\" { fmt.Println(\"请在启动命令后添加'-host','-port'参数设置\") os.Exit(0) } else if omhost == \"\" { fmt.Println(\"请在启动命令后添加'-host'参数设置IP地址\") os.Exit(0) } else if omport == \"\" { fmt.Println(\"请在启动命令后添加'-port'参数设置管理端口号\") os.Exit(0) } } 四、本地文件操作 1、判断文件是否存在 func PathExists(path string) (bool, error) { /* 判断文件或文件夹是否存在 如果返回的错误为nil,说明文件或文件夹存在 如果返回的错误类型使用os.IsNotExist()判断为true,说明文件或文件夹不存在 如果返回的错误为其它类型,则不确定是否在存在 */ _, err := os.Stat(path) if err == nil { return true, nil } if os.IsNotExist(err) { return false, nil } return false, err } func main{ isFileExist, err := PathExists(\"文件路径\") if isFileExist == true && err == nil { // 文件存在时toDo }else{ // 文件不存在时toDo } 五、下载操作 1、文件下载 func main{ resp, err := http.Get(\"文件URL\") if err != nil { return err,\"\" } defer resp.Body.Close() out, err := os.Create(\"文件本地存储路径\") if err != nil { return err, \"\" } defer out.Close() // 然后将响应流和文件流对接起来 _, err = io.Copy(out, resp.Body) if err != nil { return err }else { return nil } } 六、钉钉验签机器人通知 func sign(secret string,cts string) string { sign := fmt.Sprintf(\"%s\\n%s\", cts, secret) signData := computeHmacSha256(sign, secret) encodeURL := url.QueryEscape(signData) return fmt.Sprintf(\"&timestamp=%s&sign=%s\", cts, encodeURL) } func computeHmacSha256(message string, secret string) string { key := []byte(secret) h := hmac.New(sha256.New, key) h.Write([]byte(message)) return base64.StdEncoding.EncodeToString(h.Sum(nil)) } func DingDingNotify(msg string) { cts := fmt.Sprintf(\"%d\", time.Now().UnixNano()/1e6) var dingDingRebotToken string=\"自定义机器人的Secret\" dsign:=genSignedURL(\"自定义机器人验签设置的秘钥\",cts) dd_robot_api_addr ,err :=url.Parse(\"https://oapi.dingtalk.com/robot/send?access_token=\"+dingDingRebotToken+\"&timestamp=\"+cts+\"&sign=\" + dsign) if err != nil{ panic(err) } ddmsgtmp := `{\"at\": {\"isAtAll\": true}, \"text\": {\"content\":\"`+msg+`\"},\"msgtype\":\"text\"}` res, err := http.Post(dd_robot_api_addr.String(), \"application/json\",bytes.NewReader([]byte(ddmsgtmp))) if err !=nil{ panic(err) println(time.Now().Format(\"2006-01-02 15:04:05\")+\" 钉钉通知消息发送失败，状态码为：\"+res.Status) }else { println(time.Now().Format(\"2006-01-02 15:04:05\")+\" 钉钉通知消息已发送，状态码为：\"+res.Status) } } 七、编译优化 1、删除调试符 默认情况下go编译出的程序在运行出错时会输出自己在哪个线程哪个文件哪个函数哪行出的错。这些信息属于DWARF信息。在编译二进制文件时，添加参数可以去除掉。（go > 1.7） go build -ldflags '-s -w' main.go # -s disable symbol table # -w disable DWARF generation 2、混淆编译运行时的环境变量 export GO111MODULE=on export GOROOT=\"/usr/local/go\" export ACTUAL_GOPATH=\"实际GOPAH\" export TMP_GOPATH=\"/tmp/go\" export GOBIN=\"$GOROOT/bin\" export CGO_LDFLAGS=\"-g -O2\" [ -z ${GOPATH+x} ] && [ -h $TMP_GOPATH ] && ln -s $ACTUAL_GOPATH $TMP_GOPATH && export GOPATH=\"/tmp/go\" export GOENV=$TMP_GOPATH/env export GOCACHE=$TMP_GOPATH/caches [[ ! $PATH =~ $GOPATH ]] && export PATH=$PATH:$GOROOT/bin 3、使用upx压缩二进制包 # 安装 brew instal upx yum install upx -y # 压缩 upx --brute 二进制文件 # --brute 尝试所有压缩算法 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/golang-embed.html":{"url":"origin/golang-embed.html","title":"embed: 嵌入静态资源文件","keywords":"","body":"Golang embed嵌入静态资源文件 一、简介 一般编译出来的可执行二进制文件都是单个的文件，非常适合复制和部署。在实际使用中，除了二进制文件，可能还需要一些配置文件，或者静态文件，比如html模板、静态的图片、CSS、javascript等文件。如果这些文件也能打进到二进制文件中，只需复制、按照单个的可执行文件即可。 一些开源的项目很久以前就开始做这方面的工作，比如gobuffalo/packr、markbates/pkger、rakyll/statik、knadh/stuffbin等。 自 Go 1.16 版本开始引入。它提供了一种将静态文件或整个目录嵌入到 Go 二进制文件中的机制，以便在运行时访问这些文件，而无需依赖外部文件系统。 embed 包定义了 FS 接口，表示嵌入的文件系统。通过 FS 接口，可以使用 ReadFile 和 ReadDir 等方法来读取嵌入的文件或目录的内容。支持嵌入整个目录，而不仅仅是单个文件。 二、功能 1、嵌入多个文件 //go:embed test.txt test2.txt var f embed.FS 或者 //go:embed test1.txt //go:embed test2.txt var f embed.FS 2、嵌入文件为字符串变量 ①嵌入单个文件内容为字符串变量 //go:embed hello.txt var s filecontent func main() { fmt.Println(filecontent) } ②嵌入同一个文件为多个字符串变量 //go:embed test.txt var test1content string //go:embed test.txt var test2content string func main() { fmt.Println(test1content) fmt.Println(test2content) } ③嵌入文件内容为可见变量和不可见变量 //go:embed hello.txt var filecontent string //go:embed hello.txt var Filecontent string func main() { fmt.Println(filecontent) } 2、嵌入文件为字节数组 把单个文件的内容嵌入为字节数组slice of byte。 //go:embed hello.txt var filecontext []byte func main() { fmt.Println(filecontext) } 3、嵌入文件夹 //go:embed statics var f embed.FS func main() { indexdata, _ := f.ReadFile(\"statics/index.html\") favicondata, _ = f.ReadFile(\"statics/favicon.ico\") } 4、嵌入相对路径文件夹路径 相对路径的根路径是go源文件所在的文件夹 支持使用双引号\"或者反引号的方式应用到嵌入的文件名或者文件夹名或者模式名上 //go:embed \"te st1.txt\" `test-1.txt` var f embed.FS func main() { data, _ := f.ReadFile(\"te st1.txt\") fmt.Println(string(data)) } 5、匹配模式嵌入 //go:embed statics/* var f embed.FS func main() { indexdata, _ := f.ReadFile(\"statics/index.html\") favicondata, _ = f.ReadFile(\"statics/js/jquery.js\") } 不支持绝对路径、不支持路径中包含.和..。如果想嵌入go源文件所在的路径，使用* //go:embed * var f embed.FS func main() { indexdata, _ := f.ReadFile(\"statics/index.html\") jquerydata, _ = f.ReadFile(\"statics/js/jquery.js\") } 三、示例 package main import ( \"embed\" \"fmt\" \"github.com/gin-gonic/gin\" \"net/http\" \"path\" ) // 创建一个全局的 Gin 引擎实例 var router = gin.Default() // 使用 embed.FS 类型声明一个变量 f，用于指定要嵌入的文件或目录。目的是将静态文件或目录嵌入到可执行文件中 // //go:embed statics/* var f embed.FS // serveEmbeddedFile 从嵌入文件系统中读取文件并响应客户端 func serveEmbeddedFile(context *gin.Context, relativePath string) { filePath := path.Join(\"statics\", relativePath) if fileContent, err := f.ReadFile(filePath); err == nil { // 根据文件类型设置响应头 context.Data(http.StatusOK, http.DetectContentType(fileContent), fileContent) } else { context.String(http.StatusInternalServerError, fmt.Sprintf(\"Error reading file %s\", relativePath)) } } func main() { // 处理根路径的请求，返回嵌入的 index.html 文件内容 router.GET(\"/\", func(context *gin.Context) { serveEmbeddedFile(context, \"index.html\") }) // 处理 /js/:filename 路径的请求，返回嵌入的 JavaScript 文件内容 router.GET(\"/js/:filename\", func(context *gin.Context) { filename := context.Param(\"filename\") serveEmbeddedFile(context, path.Join(\"js\", filename)) }) // 启动 Gin 服务器，监听端口 9091 if err := router.Run(\":9091\"); err != nil { fmt.Println(\"Error starting server:\", err) } } 参考 https://colobu.com/2021/01/17/go-embed-tutorial/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-04-29 11:13:20 "},"origin/golang-x509-certificate.html":{"url":"origin/golang-x509-certificate.html","title":"X509/PKI证书","keywords":"","body":"Go生成x509相关证书 一、简介 二、生成 CA 证书 三、签署证书 四、验证证书 参考： https://blog.yeziruo.com/archives/148.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-03-13 10:58:34 "},"origin/html-css-learn.html":{"url":"origin/html-css-learn.html","title":"HTML/CSS学习笔记","keywords":"","body":"HTML/CSS学习总结 一、模态框modal 1、简介 模态框（Modal）是一种用户界面元素，通常用于显示弹出式对话框或窗口，覆盖在应用程序的主界面之上。它在显示内容的同时阻止用户与应用程序的其他部分进行交互，直到用户关闭模态框为止。 2、用途 用户提示和通知： 模态框可用于显示警告、成功消息、错误信息等。 表单交互： 在模态框中显示表单，让用户填写或编辑信息。 确认对话框： 用于确认某个操作，例如删除确认框。 图像或媒体展示： 展示大图、视频或其他媒体内容。 登录/注册窗口： 提供用户登录或注册的界面。 3、功能 阻止交互： 模态框弹出时，通常会阻止用户与主应用程序的其他部分进行交互，使用户专注于模态框的内容。 动画效果： 可以通过CSS或JavaScript添加动画效果，使模态框的显示和隐藏更加平滑。 关闭按钮： 通常模态框内有关闭按钮，以便用户手动关闭模态框。 事件触发： 模态框的显示和隐藏通常是由特定事件触发的，例如点击按钮、链接或定时器。 响应式设计： 可以通过CSS媒体查询等技术，使模态框适应不同屏幕大小。 4、结构 一个简单的模态框通常包括以下几个部分： 触发按钮： 启动模态框的按钮，可以是一个按钮、链接或其他可点击的元素。 模态框容器： 包含模态框内容的容器，通常位于页面的中心。 内容区域： 显示具体内容的区域，可以是文本、表单、图像等。 关闭按钮： 允许用户手动关闭模态框的按钮，通常位于模态框的右上角。 5、属性 display： 控制模态框的显示和隐藏状态，通常使用CSS的display属性。 position： 设置模态框的定位方式，通常使用CSS的position属性。 z-index： 控制模态框在层叠上下文中的堆叠顺序，确保模态框位于其他元素之上。 动画效果： 可以通过CSS的过渡或动画效果实现模态框的平滑显示和隐藏。 事件监听器： 使用JavaScript添加事件监听器，例如点击按钮触发显示模态框，点击关闭按钮或覆盖层触发隐藏模态框等。 6、示例 https://code.z01.com/v4/components/modal.html 二、常用界面 body, h1, ul, li { margin: 0; padding: 0; } /* 整体容器样式 */ .container { display: flex; flex-direction: column; min-height: 100vh; /* 设置容器至少铺满整个视口高度 */ } /* 顶部 Banner 样式 */ header { background-color: #555; color: #fff; padding: 10px; text-align: center; } /* 导航栏和内容区域容器样式 */ .content-container { display: flex; flex: 1; /* 填充剩余空间 */ } /* 导航栏样式 */ nav { background-color: #333; color: #fff; text-align: center; width: 200px; /* 左侧导航栏宽度 */ display: flex; flex-direction: column; /* 将导航栏内元素垂直排列 */ align-items: stretch; /* 让导航栏高度延伸至底部 */ padding: 20px; } ul { list-style: none; } a { text-decoration: none; color: #fff; display: block; padding: 10px; border-bottom: 1px solid #555; /* 添加底边框，提高可读性 */ } /* 右侧内容区域样式 */ .content { flex: 1; /* 填充剩余空间 */ padding: 20px; } .page { display: none; /* 默认隐藏所有 div 元素 */ } .page.active { display: block; } /* 添加表格样式 */ table { width: 100%; border-collapse: collapse; margin-top: 20px; } th, td { border: 1px solid #ddd; padding: 8px; text-align: center; } 您的网站名称 选项1 选项2 选项3 页面1内容 列1 列2 列3 列4 列5 列6 列7 列8 列9 列10 列11 数据1 数据2 数据3 数据4 数据5 数据6 数据7 数据8 数据9 数据10 数据11 页面2内容 页面3内容 // JavaScript 函数，根据传入的页面 ID 显示对应的 div 元素 function showPage(pageId) { // 隐藏所有页面 var pages = document.querySelectorAll('.page'); pages.forEach(page => page.classList.remove('active')); // 显示选中的页面 var selectedPage = document.getElementById(pageId); if (selectedPage) { selectedPage.classList.add('active'); } } Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-01-29 13:14:38 "},"origin/js-jquery-vue-learn.html":{"url":"origin/js-jquery-vue-learn.html","title":"JS/JQuery/Vue3/TS基础知识学习","keywords":"","body":"一、JavaScript DOM是一个树形结构。操作一个DOM节点实际上就是这么几个操作： 更新：更新该DOM节点的内容，相当于更新了该DOM节点表示的HTML的内容； 遍历：遍历该DOM节点下的子节点，以便进行进一步操作； 添加：在该DOM节点下新增一个子节点，相当于动态增加了一个HTML节点； 删除：将该节点从HTML中删除，相当于删掉了该DOM节点的内容以及它包含的所有子节点。 DOM节点是指Element，但是DOM节点实际上是Node，在HTML中，Node包括Element、Comment、CDATA_SECTION等很多种，以及根节点Document类型，但是，绝大多数时候我们只关心Element，也就是实际控制页面结构的Node，其他类型的Node忽略即可。根节点Document已经自动绑定为全局变量document。 document对象表示当前页面。由于HTML在浏览器中以DOM形式表示为树形结构，document对象就是整个DOM树的根节点。 最常用的方法是 document.getElementById() document.getElementsByTagName() CSS选择器document.getElementsByClassName()。 document.getElementsByTagName()和document.getElementsByClassName()总是返回一组DOM节点。要精确地选择DOM，可以先定位父节点，再从父节点开始选择，以缩小范围。 ES6标准新增了一种新的函数：Arrow Function（箭头函数）。x => x * x 箭头函数相当于：function (x) { return x * x; } 如果仔细观察一个Form的提交，你就会发现，一旦用户点击“Submit”按钮，表单开始提交，浏览器就会刷新页面，然后在新页面里告诉你操作是成功了还是失败了。如果不幸由于网络太慢或者其他原因，就会得到一个404页面。 这就是Web的运作原理：一次HTTP请求对应一个页面。 如果要让用户留在当前页面中，同时发出新的HTTP请求，就必须用JavaScript发送这个新请求，接收到数据后，再用JavaScript更新页面，这样一来，用户就感觉自己仍然停留在当前页面，但是数据却可以不断地更新。 1、this 关键词 在JavaScript中，this 是一个特殊的关键字，它在不同的上下文中具有不同的值。this 的值取决于函数是如何被调用的。 全局上下文中： 当在全局上下文中使用 this 时，它指向全局对象，在浏览器环境中通常是 window 对象。 console.log(this); // 指向全局对象（在浏览器中通常是 window） 函数内部： 作为普通函数调用 如果函数是作为普通函数调用的，this 将指向全局对象（在浏览器中通常是 window） function myFunction() { console.log(this); } myFunction(); // 指向全局对象 作为对象的方法调用 如果函数是作为对象的方法调用的，this 将指向调用该方法的对象。 var obj = { myMethod: function() { console.log(this); } }; obj.myMethod(); // 指向 obj 使用 call 或 apply 方法 可以显式地设置函数的上下文。 function myFunction() { console.log(this); } var myObject = { name: \"Object\" }; myFunction.call(myObject); // 指向 myObject 事件处理函数中 当在事件处理函数中使用 this 时，它通常指向触发事件的元素。 document.getElementById(\"myButton\").addEventListener(\"click\", function() { console.log(this); // 指向触发点击事件的按钮元素 }); 箭头函数中 箭头函数的 this 始终指向定义函数时的上下文，而不是调用时的上下文。 var myObject = { myMethod: function() { var myArrowFunction = () => { console.log(this); }; myArrowFunction(); } }; myObject.myMethod(); // 指向 myObject 2、jQuery 中的this与$(this) 在 jQuery 中，this 关键字与原生 JavaScript 中的一些情况相似。行为也取决于上下文，但主要是与事件处理函数和遍历集合时的上下文有关。 事件处理函数中： 当你使用 jQuery 绑定事件处理函数时，this 通常指向触发事件的 DOM 元素。 $(\"button\").click(function() { console.log(this); // 指向触发点击事件的按钮元素 }); 遍历集合中： 当你使用 jQuery 的遍历方法（如 each）时，this 通常指向当前正在迭代的元素。 $(\"li\").each(function() { console.log(this); // 指向当前正在迭代的 元素 }); 而 $(this) 是 jQuery 提供的用于将当前 DOM 元素包装为 jQuery 对象的语法。在事件处理函数中，this 是事件触发元素的原生 DOM 引用，而 $(this) 是通过 jQuery 包装后的对象，可以方便地使用 jQuery 方法。 $(\"button\").click(function() { // 使用 $(this) 来引用当前 DOM 元素的 jQuery 对象 var $button = $(this); $button.text(\"Button Clicked!\"); }); 二、jQuery 1、简介 ①能做哪些事情 消除浏览器差异：你不需要自己写冗长的代码来针对不同的浏览器来绑定事件，编写AJAX等代码； 简洁的操作DOM的方法：写$('#test')肯定比document.getElementById('test')来得简洁； 轻松实现动画、修改CSS等各种操作。 $是著名的jQuery符号。实际上，jQuery把所有功能全部封装在一个全局变量jQuery中，而$也是一个合法的变量名，它是变量jQuery的别名： $本质上就是一个函数，但是函数也是对象，于是$除了可以直接调用外，也可以有很多其他属性。 注意，你看到的$函数名可能不是jQuery(selector, context)，因为很多JavaScript压缩工具可以对函数名和参数改名，所以压缩过的jQuery源码$函数可能变成a(b, c) ②jQuery和DOM对象互相转化 var div = $('#abc'); *// jQuery*对象 var divDom = div.get(0); *//* 假设存在*div*，获取第*1*个*DOM*元素 var another = $(divDom); *//* 重新把*DOM*包装为*jQuery*对象 通常情况下你不需要获取DOM对象，直接使用jQuery对象更加方便。如果你拿到了一个DOM对象，那可以简单地调用$(aDomObject)把它变成jQuery对象，这样就可以方便地使用jQuery的API了。 ③引入 2、选择器 jQuery的选择器就是帮助我们快速定位到一个或多个DOM节点。 如果查询的节点不存在，jQuery返回的对象是 [] ，不会返回undefined或者null，这样的好处是不必在下一行判断 if (div === undefined)。 按ID查找：$('#abc') 按tag查找：$('table') 按class查找：$('.button') 组合查找 tag组合ID：$('table#test_table1')查找页面两个 Tables 中的一个 多项选择器 多项选择器就是把多个选择器用,组合起来一块选。选出来的元素是按照它们在HTML中出现的顺序排列的，而且不会有重复元素。 $('table#projects_mr_table,input')查找页面两个 Tables 中的一个和页面中的 input 元素 层级选择器（Descendant Selector）：$('ancestor descendant') 如果两个DOM元素具有层级关系，就可以用$('ancestor descendant')来选择，层级之间用空格隔开。 层级选择器相比单个的选择器好处在于，它缩小了选择范围，因为首先要定位父节点，才能选择相应的子节点，这样避免了页面其他不相关的元素。 $('table#test_table1 tr td')查找页面两个 Table 中的一个的数据行所有单元格对象 子选择器（Child Selector） 子选择器$('parent>child')类似层级选择器，但是限定了层级关系必须是父子关系，就是节点必须是节点的直属子节点 $('table#test_table1>tbody>tr')查找页面两个 Tables 中的一个的所有行 过滤器（Filter） 过滤器一般不单独使用，它通常附加在选择器上，帮助我们更精确地定位元素。 $('table#test_table1>tbody>tr td:first-child') 查找页面两个 Tables 中的一个的所有数据行的第一个单元格 $('table#test_table1>tbody>tr td:last-child') 查找页面两个 Tables 中的一个的所有数据行的最后一个单元格 $('table#test_table1>tbody>tr td:nth-child(3)') 查找页面两个 Tables 中的一个的所有数据行的第三个单元格 $('table#test_table1>tbody>tr td:nth-child(even)') 查找页面两个Tables中的一个的所有数据行中序号为偶数的单元格 $('table#test_table1>tbody>tr td:nth-child(odd)') 查找页面两个Tables中的一个的所有数据行中序号为奇数的单元格 特殊的选择器 针对表单元素，jQuery还有一组特殊的选择器： :input：可以选择，，和； :file：可以选择，和input[type=file]一样； :checkbox：可以选择复选框，和input[type=checkbox]一样； :radio：可以选择单选框，和input[type=radio]一样； :focus：可以选择当前输入焦点的元素，例如把光标放到一个上，用$('input:focus')就可以选出； :checked：选择当前勾上的单选框和复选框，用这个选择器可以立刻获得用户选择的项目，如$('input[type=radio]:checked')； :enabled：可以选择可以正常输入的、 等，也就是没有灰掉的输入； :disabled：和:enabled正好相反，选择那些不能输入的。 其他选择器 选出可见的或隐藏的元素： $('div:visible'); // 所有可见的div $('div:hidden'); // 所有隐藏的div 3、DOM操作函数 ①添加节点元素 append 添加到目标元素内部的最后 // 使用 append 会直接添加到目标元素内部的最后，而 after 则会将内容插入到目标元素的同级位置。 // 如果目标元素有兄弟元素，after 会将内容插入到目标元素的后面，而 append 会将内容添加到目标元素内部的末尾。 $(\"#DomID\").append($('').attr('value', 'test'); prepend 插入到目标元素内的起始位置，即作为其子元素中的第一个子元素。 $(\"#DomID\").prepend($('').attr('value', 'test'); before 在指定节点前添加目标元素 $(\"#DomID\").before($('').attr('value', 'test'); after 在指定节点后添加目标元素 $(\"#DomID\").after($('').attr('value', 'test'); ②清空节点/多重操作 $('#DomID').empty().append($('').attr('value', 'test'); 4、效果和动画函数 ① 显示或隐藏元素 show()：显示元素 hide()：隐藏元素 ② 淡入或淡出元素 fadeIn()： 淡入元素 fadeOut()：淡出元素 ③ 上滑或下滑元素 slideUp()：上滑元素 slideDown()：下滑元素 ④ 创建自定义动画 animate(properties, duration, easing, complete) 5、事件处理函数 click(handler): 在元素被点击时执行函数。 change(handler): 在表单元素的值发生变化时执行函数。 mouseover(handler), mouseout(handler): 当鼠标移入或移出元素时执行函数。 keydown(handler), keyup(handler): 当键盘按下或释放时执行函数。 6、Ajax函数 ① 通用Ajax请求 $.ajax(options)：是 jQuery 提供的一个通用的 Ajax 请求方法，它具有更大的灵活性和可配置性 var id = parseInt(Math.random() * 100000000); var postData = { \"id\": id, key1: \"value1\", key2: \"value2\" }; $.ajax({ url: \"targetURL\", // 请求的目标 URL type: \"POST\", // 请求的类型，例如 \"GET\" 或 \"POST\" data: JSON.stringify(postData), // 要发送到服务器的数据，可以是对象、字符串或数组 dataType: \"json\", // 预期从服务器接收的数据类型，例如 \"json\" contentType: \"application/json\", // 发送数据到服务器时使用的内容类型 headers: { // 设置请求头 \"Authorization\": \"Bearer token\" }, beforeSend: function(xhr) { // 发送请求之前执行的回调函数 // 可以在这里进行一些预处理操作，例如设置请求头 }, success: function(response) { // 请求成功时执行的回调函数 console.log(\"服务器响应:\", response); }, error: function(jqXHR, textStatus, errorThrown) { // 请求失败时执行的回调函数 console.error(\"请求失败:\", textStatus, errorThrown); }, complete: function(xhr, status) { // 请求完成时执行的回调函数（无论成功或失败） console.log(\"请求完成，状态:\", status); }, timeout: 5000, // 超时时间，单位毫秒 async: true, // 是否使用异步请求，默认为 true cache: false // 是否启用缓存，默认为 true }); ② GET、POST请求 $.get(url, data, success, dataType) $.post(url, data, success, dataType) url (必需)： 发送请求的目标 URL。 data (可选)： 要发送到服务器的数据。可以是字符串、对象或数组。 success (可选)： 请求成功时执行的回调函数。该函数的参数是服务器返回的数据。 dataType (可选)： 预期从服务器接收的数据类型。常见的值包括 \"xml\"、\"json\"、\"html\" 和 \"text\"。 7、工具函数 ① 遍历数组或对象 $.each(数组, 回调函数) // 遍历 Ajax 返回的响应JSON数据，操作其中的值 $.each(response.data, function (index, data) { var row = $('').attr('id', index); row.append($('').text(data.name)); tbody.append(row); }); ② 过滤数组 $.grep(数组, 回调函数) // 例如过滤数组中所有的偶数 var numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]; var evenNumbers = $.grep(numbers, function (element) { return element % 2 === 0; }); ③ 合并对象 $.extend(target, object1, object2, ...): 将一个或多个对象的内容合并到目标对象中 三、Vue3 1、基础文件详解 App.vue 是 Vue.js 应用的根组件。它包含了整个应用的结构和行为。 ：定义组件的模板，它描述了组件的结构和布局。 ：定义组件的逻辑，它包含了组件的方法、数据和生命周期钩子。 ：定义组件的样式，它包含了组件的样式规则。 main.js 是 Vue.js 应用的入口文件。它负责创建 Vue 实例并挂载到 DOM 元素上。 import { createApp } from 'vue' import App from './App.vue' createApp(App).mount('#app') import { createApp } from 'vue'：导入 Vue.js 库中的 createApp 函数。 import App from './App.vue'：导入根组件 App.vue。 createApp(App).mount('#app')：使用 createApp() 函数创建 Vue 实例，并将其挂载到 #app 元素上。 index.html 是 HTML 入口文件。它定义了页面的结构和内容。 Vue.js App ：声明 HTML 文档类型。 ：HTML 根元素。 ：HTML 头部元素，包含元数据和脚本。 ：页面标题。 ：HTML 主体元素，包含页面的内容。 ：一个空 元素，用于挂载 Vue 实例。 ：导入 main.js 脚本文件。 package.json 是 Node.js 项目的配置文件。它包含了项目的基本信息、依赖项和其他元数据。 { \"name\": \"vue-app\", \"version\": \"1.0.0\", \"description\": \"A Vue.js application\", \"main\": \"main.js\", \"scripts\": { \"dev\": \"vite\", \"build\": \"vite build\" }, \"dependencies\": { \"vue\": \"^3.2.36\" } } \"name\"：项目的名称。 \"version\"：项目的版本。 \"description\"：项目的描述。 \"main\"：项目的入口文件。 \"scripts\"：项目中可以运行的脚本命令。 \"dependencies\"：项目所需的依赖项。 vite.config.js 是 Vite 配置文件。它用于配置 Vite 开发服务器和其他构建选项。 module.exports = { plugins: [], server: { port: 3000 } } \"plugins\"：Vite 插件列表。 \"server\"：Vite 开发服务器配置。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-02-07 16:22:39 "},"origin/vue3.html":{"url":"origin/vue3.html","title":"Vue3基础知识学习","keywords":"","body":"Vue 学习 一、简介 二、安装引入 三、工具 四、基础 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-02-07 16:24:28 "},"origin/vue-vite.html":{"url":"origin/vue-vite.html","title":"Vite打包工具","keywords":"","body":"Vite打包工具 一、简介 二、配置 三、迁移 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-02-07 16:23:02 "},"origin/js-kits.html":{"url":"origin/js-kits.html","title":"JavaScript常用工具函数","keywords":"","body":"JavaScript常用工具函数 1、把字节数字转换成易于阅读格式 function getBytesSize(size) { if (!size) return \"\"; var num = 1024.00; //byte if (size 2、将秒转换为时分秒 function formatDuring(s){ var days = parseInt(s / ( 60 * 60 * 24)); var hours = parseInt((s % ( 60 * 60 * 24)) / ( 60 * 60)); var minutes = parseInt((s % ( 60 * 60)) / 60); var seconds = (s % 60) ; return days + \" 天 \" + hours + \" 小时 \" + minutes + \" 分钟 \"; } 3、设置点击复制并弹出模态框提示复制内容 .modal { display: none; position: fixed; z-index: 1; left: 0; top: 0; width: 100%; height: 100%; overflow: auto; color: white; } .modal-content { background-color: #0d6efd; margin: 15% auto; padding: 20px; border: 1px solid #888; width: 300px; text-align: center; } function copyordernum(value) { var clipboardInput = document.createElement(\"input\"); clipboardInput.style.position = \"absolute\"; clipboardInput.style.left = \"-9999px\"; clipboardInput.style.top = \"-9999px\"; clipboardInput.value = value; document.body.appendChild(clipboardInput); clipboardInput.select(); document.execCommand(\"copy\"); document.body.removeChild(clipboardInput); var modalMessage = document.getElementById(\"modalMessage\"); modalMessage.textContent = \"已复制到剪贴板: \" + value; var modal = document.getElementById(\"myModal\"); modal.style.display = \"block\"; setTimeout(function () { modal.style.display = \"none\"; }, 900); } ID A B C D 1 1111***1211 111 2021-06-03 10:01:15 111 4、CSS 伪元素实现隐藏过长字符串，焦点悬停式显示完整字符串 方案：使用 BS5 的 CSS伪元素。 JS在生成表格元素时，将标签的 class 设置为class=\"hide-string\"；将原字符串赋予标签的 data-vaule属性；标签值则赋予隐藏字符。 JS 替换字符串前 4 位和后 4 位中间的字符为：`str.replace(/(^\\d{4})(.)(\\d{4}$)/, \"$1*$3\");` .hide-string { position: relative; } .hide-string::after { content: attr(data-value); position: absolute; top: 0; left: 0; visibility: hidden; opacity: 0; background-color: #cccccc; padding: 5px; border: 1px solid #cccccc; } .hide-string:hover::after { visibility: visible; opacity: 1; } ID A B C D 1 1111***1211 111 2021-06-03 10:01:15 111 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2023-07-21 09:40:24 "},"origin/web-devtools.html":{"url":"origin/web-devtools.html","title":"前端代码工具集","keywords":"","body":"前端代码工具集 一、包可视化分析工具rollup-plugin-visualizer Github: https://github.com/btd/rollup-plugin-visualizer 1、安装引入 集成在项目中 npm install --save-dev rollup-plugin-visualizer 配置vite.config.ts import { visualizer } from 'rollup-plugin-visualizer' //...., export default defineConfig({ // ...., build: { rollupOptions: { //...., plugins: [visualizer()] }, } }) 安装命令行 npm install -g rollup-plugin-visualizer rollup-plugin-visualizer 2、分析 每次构建时生成分析页面 npm run build 构建完成后会在项目根目录生成一个 stats.html文件 二、包可视化分析工具vite-bundle-visualizer Github: https://github.com/KusStar/vite-bundle-visualizer 1、安装引入 npm install -g vite-plugin-analyzer 2、分析命令 Treemap npx vite-bundle-visualizer Sunburst npx vite-bundle-visualizer -t sunburst Network npx vite-bundle-visualizer -t network 三、代码重复分析工具：jscpd 1、简介 Github：https://github.com/kucherenko/jscpd 2、安装配置 npm install -g jscpd 创建配置文件.jscpd.json { \"threshold\": 1, \"reporters\": [ \"html\", \"console\" ], \"absolute\": true } 或者配置在命令行 jscpd ./src -t 1 -l, --min-lines [number] min size of duplication in code lines (Default is 5) -k, --min-tokens [number] min size of duplication in code tokens (Default is 50) -x, --max-lines [number] 最大文件大小（以行为单位）。大于 max-lines 的文件将被跳过(默认1000行) -z, --max-size [string] 最大文件大小（以字节为单位）。大于 max-size 的文件将被跳过。例如: 1kb, 1mb, 120kb(默认100kb) -t, --threshold [number] 重复级别阈值，检查当前重复级别是否大于阈值 jscpd 错误退出。 -c, --config [string] 配置文件的路径。json 格式。配置文件中支持的选项可以与 cli 选项相同。 (默认读取当前路径下的.jscpd.json) -i, --ignore [string] 具有要在分析中忽略的全局模式的选项。对于多个glob，可以使用逗号作为分隔符。 例子：--ignore \"**/*.min.js,**/*.map\" /path/to/files --ignore-pattern [string] Ignore code blocks matching the regexp patterns -r, --reporters [string] 指定输出类型。console(控制台、consoleFull(控制台代码也输出)，json、xml、csv、markdown、html等格式文件 -o, --output [string] 指定报告存储目录的路径。 json、csv、markdown、html等格式文件 报告将保存在那里 (默认当前路径下./report/) -m, --mode [string] 检测质量的方式。可选strict、mild(默认)、weak -f, --format [string] 用于检测重复的格式列表。例如：php、javascript、python等150种格式 -p, --pattern [string] glob pattern to file search (Example **/*.txt) -b, --blame 从 git 获取有关重复块的作者和日期的信息。 -s, --silent 不要向控制台写入大量信息。 --store [string] 用于收集有关代码的信息的存储，默认情况下所有信息都收集在内存中 (e.g. --store leveldb used for big codebase) -a, --absolute use absolute path in reports -n, --noSymlinks dont use symlinks for detection in files --ignoreCase ignore case of symbols in code (experimental) -g, --gitignore ignore all files from .gitignore file --formats-exts [string] list of formats with file extensions (javascript:es,es6;dart:dt) -d, --debug show debug information, not run detection process(options list and selected files) --list show list of total supported formats --skipLocal skip duplicates in local folders, just detect cross folders duplications --exitCode [number] exit code to use when code duplications are detected 3、分析结果 结果可以输出在控制台，还可以生成报告文件在根目录下report 目录中。有 json格式，还有可视化的 HTML 文件。 四、自动按需导入工具 1、简介 在前端项目中，按需引入是优化项目的重要方法之一。 npm install -d unplugin-auto-import unplugin-vue-components 2、unplugin-auto-import unplugin-auto-import是 基于 unplugin ，支持 Vite、Webpack、Rollup 和 esbuild 按需自动导入。同时也支持 TypeScript。 Github：https://github.com/unplugin/unplugin-auto-import ①vite中配置 vit.config.ts import { defineConfig } from 'vite' import vue from '@vitejs/plugin-vue' // 使用unplugin-auto-import按需导入Vite import AutoImport from 'unplugin-auto-import/vite' export default defineConfig({ // ... plugins: [ vue(), // ...., AutoImport({ imports: ['vue'], // 如果使用的是Typescript，设置dts为true dts: true, // 如果使用了eslint，设置eslintrc字段为true eslintrc: { enabled: true }, // .... }) ] }) 插件会在项目根目录生成类型文件 .eslintrc-auto-import.json 、auto-imports.d.ts 在tsconfig.app.json设置包含auto-imports.d.ts { // ..... \"include\": [ \"env.d.ts\", \"src/**/*\", \"src/**/*.vue\" // 添加auto-imports.d.ts文件 \"auto-imports.d.ts\", ], // ..... } 在eslintrc.cjx设置包含auto-imports.d.ts ②使用 使用插件之前 import { ref, onMounted } from 'vue'; const test = ref(''); 使用插件之后 const test = ref(''); 3、unplugin-vue-components ①配置 vite.config.ts import { defineConfig } from 'vite' import Components from 'unplugin-vue-components/vite' import { ElementPlusResolver } from 'unplugin-vue-components/resolvers' //.... export default defineConfig({ plugins: [ // .... // 例如添加 ElementPlusResolver 解析器，这样我们在使用ElementPlus中的组件时就无需import 导入，可直接在template使用 Components({ resolvers: [ ElementPlusResolver() ], }), ] }) tsconfig.json { // ..... \"include\": [\"components.d.ts\"], } 以上全部设置后会在根目录下生成并会自动更新 components.d.ts此类型声明文件 /* eslint-disable */ /* prettier-ignore */ // @ts-nocheck // Generated by unplugin-vue-components // Read more: https://github.com/vuejs/core/pull/3399 export {} declare module 'vue' { export interface GlobalComponents { ElButton: typeof import('element-plus/es')['ElButton'] ElTable: typeof import('element-plus/es')['ElTable'] ElTableColumn: typeof import('element-plus/es')['ElTableColumn'] RouterLink: typeof import('vue-router')['RouterLink'] RouterView: typeof import('vue-router')['RouterView'] } } 全量导入时打包后 css文件大小 315.55 kB，按需之后仅有 58.93KB 参考: https://stackoverflow.com/questions/75746767/is-there-any-bundle-analyzer-for-vite https://github.com/KusStar/vite-bundle-visualizer 五、自动按需导入icon工具：unplugin-icons unplugin-icons插件 Github: https://github.com/unplugin/unplugin-icons iconify网站：https://icon-sets.iconify.design/ 1、功能 可以自动从iconify网站按需导入icon 可以加载本地 各种格式icon文件进行按需导入 2、安装插件 npm i -D unplugin-icons 3、配置插件 vite.config.ts // ..... // 自动导入Icon图标 import IconsResolver from 'unplugin-icons/resolver' import Icons from 'unplugin-icons/vite'; import { FileSystemIconLoader } from 'unplugin-icons/loaders' export default defineConfig({ plugins: [ // ..... Components({ resolvers: [ // ..... // 自动注册图标组件 IconsResolver({ // 修改Icon组件名字前缀，不设置则默认为i, 禁用则设置为false。 prefix: 'icon', // 指定collection图标集。ElementPlus图标集：ep enabledCollections: ['ep','game-icons'], // 使用自定义的本地icon集合名 customCollections: ['myIcons'] }) ] }), Icons({ autoInstall: true, // 配置自定义的icon集合文件的路径 customCollections: { 'myIcons': FileSystemIconLoader('src/assets/icons'), } }) ] }) 3、使用 使用组件解析器IconsResolver时，必须遵循名称转换才能正确推断图标，icon引用命名格式: {prefix}-{collection}-{icon} 如果在IconsResolver没有设置prefix。则前缀默认为i。icon引用命名格式为：i-{collection}-{icon} 例如：引用iconify网站game-icons图集中的high-kick图标。可以使用 下线该用户 参考： https://element-plus.org/zh-CN/component/icon.html https://www.cnblogs.com/fuct/p/17533365.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-02-21 18:09:00 "},"origin/elementplus-echarts.html":{"url":"origin/elementplus-echarts.html","title":"Element-Plus与Echarts","keywords":"","body":"一、Element Plus 文档：https://element-plus.org/zh-CN/guide/design.html 1、导入引用 ①全量导入 main.ts import { createApp } from 'vue' import ElementPlus from 'element-plus' import 'element-plus/dist/index.css' import App from './App.vue' const app = createApp(App) .use(ElementPlus) .mount('#app') ②按需导入 手动按需导入 // 按需导入 element-plus 组件，配置文件为 babel.config.js import { ElTable, ElTableColumn } from 'element-plus'; // 引入 Element Plus 默认样式文件 import 'element-plus/theme-chalk/index.css' 插件自动按需导入 安装unplugin-vue-components 和 unplugin-auto-import这两款插件 npm install -D unplugin-vue-components unplugin-auto-import vite.config.ts import { defineConfig } from 'vite' import AutoImport from 'unplugin-auto-import/vite' import Components from 'unplugin-vue-components/vite' import { ElementPlusResolver } from 'unplugin-vue-components/resolvers' export default defineConfig({ // ... plugins: [ // ... Components({ resolvers: [ElementPlusResolver()], }), ], }) 二、Echarts 文档：https://echarts.apache.org/handbook/zh/get-started/ 1、导入引用 ①全量导入 import * as echarts from 'echarts' ②按需导入 // 引入 echarts 核心模块，核心模块提供了 echarts 使用必须的接口。 import * as echarts from 'echarts/core'; // 引入要使用的图表类型，图表后缀都为 Chart import { PieChart, BarChart } from 'echarts/charts'; // 引入提示框，标题，直角坐标系，数据集，内置数据转换器组件，组件后缀都为 Component import { TitleComponent, TooltipComponent, GridComponent, DatasetComponent, TransformComponent } from 'echarts/components'; // 引入 Canvas 渲染器，注意引入 CanvasRenderer 或者 SVGRenderer 是必须的一步 import { CanvasRenderer } from 'echarts/renderers'; // 注册必须的组件 echarts.use([ PieChart, BarChart, TooltipComponent, TitleComponent, CanvasRenderer, GridComponent ]); 使用跟之前一样，初始化图表，设置配置项。 Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-04-29 10:59:30 "},"origin/hcakintosh-opencore.html":{"url":"origin/hcakintosh-opencore.html","title":"OpenCore","keywords":"","body":"一、OpenCore简介 GitHub 地址：https://github.com/acidanthera/OpenCorePkg 下载地址：https://github.com/acidanthera/OpenCorePkg/releases 文档： https://dortania.github.io/OpenCore-Post-Install/ 中文文档 https://blog.daliansky.net/OpenCore-BootLoader.html https://oc.skk.moe/ https://apple.sqlsec.com/#reloaded 二、编辑工具 OpenCore Configurator ProperTree 安装部署 git clone https://github.com/corpnewt/ProperTree python ./ProperTree/ProperTree.py # 或者 python3 ./ProperTree/ProperTree.py # 或者 MacOS下直接双击ProperTree.command 其他工具 Hackintool 三、目录文件详解 Kexts Lilu.kext：Acidanthera驱动全家桶的SDK Applealc.kext：声卡驱动 VirtualSMC.kext：传感器驱动依赖 SMCProcessor.kext：CPU核传感器 SMCSuperIO.kext：IO传感器 WhateverGreen.kext：显卡驱动 IntelMausi.kext：Intel类千兆网卡驱动 Usbinjectall.kext：USB驱动 （你也可以定制自己的USB补丁） Drivers AudioDxe.efi：开机UEFI界面若需要声音效果需要加载。 CrScreenshotDxe.efi：开机UEFI的截图工具。 HiiDatabase.efi：用于给 Ivy Bridge (3 代酷睿) 或更老代主板上支持 UEFI 字体渲染, UEFI Shell 中文字渲染异常时使用, 新主板不需要。 NvmExpressDxe.efi：用于在 Haswell (4 代酷睿) 或更老的主板上支持 NVMe 硬盘, 新主板不需要。 OpenCanopy.efi：加载第三方开机主题。 OpenRuntime.efi： 内存运用等必要的插件，必须加载。 OpenUsbKbDxe.efi： 给使用模拟 UEFI 的老主板在 OpenCore 界面正常输入用的, 请勿在 Ivy Bridge (3 代酷睿)及以上的主板上使用。 Ps2KeyboardDxe.efi： PS2键盘所需插件。 Ps2MouseDxe.efi： PS2鼠标所需插件。 UsbMouseDxe.efi： 当MacOS被安装在虚拟机上所需要的鼠标插件。 XhciDxe.efi： 用于在 Sandy Bridge（2代）及之前或更老的主板上加载XHCI控制器。 HfsPlus.efi： 用于HFS格式文件系统，这是必须加载的。 Tools OC 小工具文件夹，像下面的 CleanNvram.efi 和 ResetSystem.efi 就是其中的一些小工具。OC 官方自带的工具文件以及说明如下： BootKicker.efi：调用苹果原生的引导切换 GUI, 给白苹果使用的，黑苹果不支持 ChipTune.efi：测试 BeepGen 协议并生成不同风格和长度的音频信号。 CleanNvram.efi：NVRAM 清理工具，实际上 OC 自带的 NVRAM 清理功能已经足够了 ControlMsrE2.efi：检查所有内核的CFG 锁定（MSR 0xE2写保护）一致性，并在更改此类隐藏选项 CsrUtil.efi：简单实现苹果csrutil的 SIP 相关功能 GopStop.efi：停止显卡 GOP，排错时使用用一个简单的场景测试 GraphicsOutput 协议 KeyTester.efi：在 SimpleText 模式下测试键盘输入。 MmapDump.efi：ProvideCustomSlide 选项的必要性 OpenControl.efi：为其他工具提供 NVRAM 保护，以便在从 OC 启动时能够获得完整的 NVRAM 访问权限。 OpenShell.efi：OpenCore 配置的 UEFI Shell ResetSystem.efi：用于执行系统重置的实用程序 RtcRw.efi：用于读取和写入 RTC (CMOS) 存储器的实用程序。 TpmInfo.efi：检查平台上的英特尔 PTT（平台信任技术）功能，如果启用，则允许使用 fTPM 2.0 四、重要配置 1、个性定制启动菜单主题 ①下载跟OpenCore版本一致的OpenCanopy.efi放在EFI/OC/Drivers文件夹中（OpenCanopy.efi是在OpenCorePkg压缩包中的，只提取这个文件即可）。 ②配置 使用OpenCore Configurator进行配置，可进行预览，点击下载，自动校验相关文件是否存在合法。 手动配置 在主题仓库：https://github.com/chris1111/My-Simple-OC-Themes下载文件到`EFI/OC/Resources/Image` 配置EFI/OC/config.plist中的 Misc --> Boot --> PickerAttributes（Number）= 17 Misc --> Boot --> PickerVariant（String）= 主题名字 参考：https://dortania.github.io/OpenCore-Post-Install/cosmetic/gui.html#setting-up-boot-chime-with-audiodxe 2、关闭启动菜单中的“Reset NvmRAM”选项 配置EFI/OC/config.plist中的Misc > Security > AllowNvramReset = False 参考：https://www.tonymacx86.com/threads/how-to-remove-reset-nvram-boot-entry-in-opencore-0-7-2.315495/ 3、不显示启动日志 配置EFI/OC/config.plist中的 Misc --> Debug --> AppleDebug = NO Misc --> Debug --> ApplePanic = NO Misc --> Debug --> Target = 0 特别注意：OpenCore如果是0.7.9，删除Misc --> Debug --> LogModules 参考：https://dortania.github.io/OpenCore-Install-Guide/troubleshooting/debug.html#config-changes 4、在启动菜单中启用内置工具 使用OpenCore Configurator打开config.plist ① Misc -> Boot -> LauncherOption = Full ② 使用OpenCore Configurator自动检测内置工具，并勾选启用 5、禁用单个显卡 ⓪为什么要禁用显卡 ​ 双系统，Windows+Hackinsh MacOS；有两张显卡，一张RTX 2060S，Windows下游戏用，hackinsh下没有驱动，无法使用；一张RTX 765，MacOS下免驱动亮机卡。在Hackinsh MacOS启动过程中要禁用掉不支持的RTX 2060S才能正常启动。 ①查询设备的ACPI路径 ②方法选择 Boot Flag：禁用除核显之外的所有独显 DeviceProperties（首选）：基于单个PCIE插槽禁用 GPU SSDT：基于单个PCIE插槽禁用 GPU ③DeviceProperties方法 DeviceProperties --> Add 参考： https://dortania.github.io/OpenCore-Install-Guide/extras/spoof.html https://dortania.github.io/Getting-Started-With-ACPI/Desktops/desktop-disable.html#finding-the-acpi-path-of-the-gpu 6、Windoows双系统下时区同步 使用Hackintool生成在windows下修改注册表的脚本。会自动生成 Windows 可用的注册表文件，一个结尾是On，一个是Off，分别对应开启和关闭。 WinUTCOff.reg WinUTCOn.reg 在Windows下执行注册表脚本，重启即可。 参考：https://heipg.cn/tutorial/solving-time-sync-problem.html 7、开启启动菜单密码保护 使用OpenCore核心安装包中的工具OpenCore-0.7.9-RELEASE/Utilities/ocpasswordgen/ocpasswordgen生成密码的盐值和Hash值 注意点：macOS双击ocpasswordgen运行。复制密码Hash值时，<>也要复制 使用OpenCore Configurator设置启用密码 参考： https://www.youtube.com/watch?v=aGDeRLimNzY https://dortania.github.io/OpenCore-Post-Install/universal/security/password.html 8、挂载EFI分区 sudo mkdir /Volumes/EFI && sudo mount -t msdos /dev/disk0s1 /Volumes/EFI 9、EFI分区被覆盖如何处理 双系统情况下，如果想重装Windows，在装完以后，磁盘的EFI分区会被覆盖，引导程序会变成Windows Boot Manager。此时该如何重新使用OpenCore进行引导。步骤如下： 前提：原先的双系统的EFI文件有做备份 ① 使用U盘内置的OpenCore启动，进入装好的MacOS ② 挂载当前磁盘的EFI分区（一般是磁盘的第一个分区），备份现有的EFI分区，并清空文件夹 方式一：使用工具挂载 方式二(推荐)： 使用命令行挂载 sudo mkdir /Volumes/EFI && sudo mount -t msdos /dev/disk0s1 /Volumes/EFI ③ 将原先备份的EFI分区文件复制到现有EFI分区文件夹中 ④删除复制到EFI分区中的MicroSoft文件夹，将第二步备份的WIndows Boot Manager中的MicroSoft文件复制过来 ⑤启动后，首先默认进入的Windows，开机以后，安装EasyUEFI。 ⑥使用EasyUEFI创建新的启动项，指定启动boot文件 ⑦此时重启，进入BIOS。设置好启动项，选择OpenCore。再次启动就可以使用之前的OpenCore启动程序啦。 五、EFI Shell命令详解 1、bcfg # 帮助文档 help bcfg -v -b # 或者 bcfg -? -v -b # bcfg boot dump -v # 删除第4个启动选项: bcfg boot rm 3 # 把第3个启动选项移动到第0 bcfg boot mv 3 0 # bcfg boot add N fsV:\\vmlinuz-linux \"Arch Linux\" 2、map 文件系统：fs0 存储设备：blk0 参考： https://linuxhint.com/use-uefi-interactive-shell-and-its-common-commands/#9 https://wiki.archlinux.org/title/Unified_Extensible_Firmware_Interface_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87) 参考 https://blog.daliansky.net/OpenCore-BootLoader.html Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2022-05-16 17:26:46 "},"origin/linux-security.html":{"url":"origin/linux-security.html","title":"Linux安全","keywords":"","body":"Linux 安全架构 一、SSH相关 1、只允许某用户从指定IP地址登陆 sed -i '$a AllowUsers CR@192.168.1.12 root@192.168.1.12' /etc/ssh/sshd_config ;\\ systemctl restart sshd 2、设置SSH空闲超时退出时间 /etc/ssh/sshd_config #ClientAliveInterval 0 #ClientAliveCountMax 3 修改成 ClientAliveInterval 30 #（每30秒往客户端发送会话请求，保持连接） ClientAliveCountMax 3 #（去掉注释即可，3表示重连3次失败后，重启SSH会话） systemctl restart sshd 3、 限制登陆访问尝试的验证次数 MaxAuthTries 20 4、允许root用户登录 sed -i 's/PermitRootLogin no/PermitRootLogin yes/g' /etc/ssh/sshd_config ;\\ systemctl restart sshd 5、设置登录方式 # AuthorizedKeysFile .ssh/authorized_keys //公钥公钥认证文件 # PubkeyAuthentication yes //可以使用公钥登录 # PasswordAuthentication no //不允许使用密码登录 6、禁止使用空白密码用户访问 PermitEmptyPasswords no 7、程序用户禁止登录 不创建家目录 不允许 ssh登录 useradd -s /usr/sbin/nologin test 8、禁止通过 SSH 通道进行端口转发 AllowTcpForwarding no X11Forwarding no AllowAgentForwarding no 9、SSH登录事件通知至ntfy /etc/pam.d/sshd session optional pam_exec.so /usr/local/bin/ntfy-ssh-login.sh ntfy-ssh-login.sh #!/bin/bash TOPIC_URL=http://test.curiouser.com:8080/ssh-notify if [ \"${PAM_TYPE}\" = \"open_session\" ]; then curl -H tags:warning -H prio:high -d \"SSH login to $(hostname): ${PAM_USER} from ${PAM_RHOST}\" \"${TOPIC_URL}\" fi 部署私有的ntfy ntfy_server: image: 'binwiederhier/ntfy' restart: always container_name: ntfy_server command: serve --config /etc/ntfy/server.yml --cache-file /var/cache/ntfy/cache.db --listen-http :18070 ports: - '18070:18070' volumes: - '/data/ntfy/data:/var/cache/ntfy' - '/data/ntfy/config:/etc/ntfy' - '/data/ntfy/data:/var/lib/ntfy/' 二、OS系统设置 1、访问控制配置文件的权限设置 chown root:root /etc/passwd /etc/shadow /etc/group /etc/gshadow chmod 0644 /etc/group chmod 0644 /etc/passwd chmod 0400 /etc/shadow chmod 0400 /etc/gshadow 2、密码安全策略 /etc/pam.d/password-auth password requisite pam_pwquality.so try_first_pass local_users_only retry=3 authtok_type= difok=1 minlen=8 ucredit=-1 lcredit=-1 dcredit=-1 difok= 定义新密码中必须要有几个字符和旧密码不同 minlen=新密码的最小长度 ucredit= 新密码中可以包含的大写字母的最大数目。-1 至少一个 lcredit=新密码中可以包含的小写字母的最大数 dcredit=定新密码中可以包含的数字的最大数目 注：这个密码强度的设定只对\"普通用户\"有限制作用，root用户无论修改自己的密码还是修改普通用户的时候，不符合强度设置依然可以设置成功 3、用户帐号限制 /etc/login.defs配置文件是设置用户帐号限制的文件，可配置密码的最大过期天数，密码的最大长度约束等内容。 该文件里的配置对root用户无效。此文件中的配置与 /etc/passwd 和 /etc/shadow 文件中的用户信息有冲突时，系统会以/etc/passwd 和 /etc/shadow 为准。 /etc/login.defs 文件用于在Linux创建用户时，对用户的一些基本属性做默认设置，例如指定用户 UID 和 GID 的范围，用户的过期时间，密码的最大长度，等等。 设置项 含义 MAIL_DIR /var/spool/mail 创建用户时，系统会在目录 /var/spool/mail 中创建一个用户邮箱，比如 lamp 用户的邮箱是 /var/spool/mail/lamp。 PASS_MAX_DAYS 99999 密码有效期，99999 是自 1970 年 1 月 1 日起密码有效的天数，相当于 273 年，可理解为密码始终有效。 PASS_MIN_DAYS 0 表示自上次修改密码以来，最少隔多少天后用户才能再次修改密码，默认值是 0。 PASS_MIN_LEN 5 指定密码的最小长度，默认不小于 5 位，但是现在用户登录时验证已经被 PAM 模块取代，所以这个选项并不生效。 PASS_WARN_AGE 7 指定在密码到期前多少天，系统就开始通过用户密码即将到期，默认为 7 天。 UID_MIN 500 指定最小 UID 为 500，也就是说，添加用户时，默认 UID 从 500 开始。注意，如果手工指定了一个用户的 UID 是 550，那么下一个创建的用户的 UID 就会从 551 开始，哪怕 500~549 之间的 UID 没有使用。 UID_MAX 60000 指定用户最大的 UID 为 60000。 GID_MIN 500 指定最小 GID 为 500，也就是在添加组时，组的 GID 从 500 开始。 GID_MAX 60000 用户 GID 最大为 60000。 CREATE_HOME yes 指定在创建用户时，是否同时创建用户主目录，yes 表示创建，no 则不创建，默认是 yes。 UMASK 077 用户主目录的权限默认设置为 077。 USERGROUPS_ENAB yes 指定删除用户的时候是否同时删除用户组，准备地说，这里指的是删除用户的初始组，此项的默认值为 yes。 ENCRYPT_METHOD SHA512 指定用户密码采用的加密规则，默认采用 SHA512，这是新的密码加密模式，原先的 Linux 只能用 DES 或 MD5 加密。 4、用户登录次数限制 5、系统超时退出 # 600表示超过600秒无操作即断开连接 echo \"export TMOUT=600\" >> /etc/profile source /etc/profile 三、shell安全 1、历史命令安全 禁止shell的history记录密码相关的环境变量 Bash # 开起不记录模式 set +o history # 关闭不记录模式 set -o history ZSH setopt HIST_IGNORE_SPACE # 之后命令前面加一个空格即可不被记录 command 四、Fail2Ban Fail2Ban 是一款入侵防御软件，可以保护服务器免受暴力攻击。 它是用 Python 编程语言编写的。 Fail2Ban 基于auth 日志文件工作，默认情况下它会扫描所有 auth 日志文件，如 /var/log/auth.log、 /var/log/apache/access.log 等，并禁止带有恶意标志的IP，比如密码失败太多，寻找漏洞等等标志。 通常，Fail2Ban 用于更新防火墙规则，用于在指定的时间内拒绝 IP 地址。 它也会发送邮件通知。 Fail2Ban 为各种服务提供了许多过滤器，如 ssh、apache、nginx、squid、named、mysql、nagios 等。 Fail2Ban 能够降低错误认证尝试的速度，但是它不能消除弱认证带来的风险。 这只是服务器防止暴力攻击的安全手段之一。 # Debian / Ubuntu apt install fail2ban ssh [sshd] enabled = true port = ssh backend = systemd maxretry = 3 findtime = 300 bantime = 3600 ignoreip = 127.0.0.1 # enabled – 是否启用该规则 # port – 要监听的端口。例如：ssh的22端口 # backend – 指定用于获取文件修改的后端。由于所有现代 Linux 系统都依赖于 systemd 的日志服务，因此我们将其指定为后端。 # maxretry – 某个 IP 在被禁止之前尝试失败的次数。 # findtime – “maxretry”登录失败将导致禁止的时间范围（以秒为单位）。我们指定了 300 秒，即 5 分钟。 # bantime – IP 应保持禁止状态的持续时间（以秒为单位）。在我们的例子中，我们设置了 3600 秒，这意味着在接下来的一小时内，来自该 IP 地址的任何后续请求（不仅仅是到 SSH 端口）都将被阻止。 # ignoreip – 忽略的IP地址白名单。可确保给定的IP地址，即使超过“maxretry”中指定的失败尝试次数，也不会被阻止。 fail2ban-client status sshd iptables -L -n fail2ban-client unban --all fail2ban-client unban 参考：https://linuxiac.com/how-to-protect-ssh-with-fail2ban/ Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-06-21 16:12:53 "},"origin/mysql-data-security.html":{"url":"origin/mysql-data-security.html","title":"MySQL数据安全","keywords":"","body":"MySQL 数据安全 一、简介 数据库中存储着各种各样的数据，但当涉及到用户敏感信息，比如手机号、身份证号码等，应该以密文形式存储在数据库中，mysql数据库有自带的加密函数 ENCODE / DECODE 传入两个值，一个是要加密的记录，一个是加密和解密的key.加密之后的二进制字符长度和原始长度是一样的，以blob类型存储 BLOB 类型的字段用于存储二进制数据 MySQL 中，BLOB 是个类型系列，包括：TinyBlob、Blob、MediumBlob、LongBlob，这几个类型之间的唯一区别是在存储文件的最大大小上不同。 MySQL 的四种 BLOB 类型 类型 大小 (单位：字节) TinyBlob 最大 255 Blob 最大 65K Medium。 AES_ENCRYPT / AES_DECRYPT 这种加密算法使用AES(高级加密标准，Advanced Encryption Standard)，使用key_str加密，key_str的长度可以达到256位，加密的结果是一个二进制的字符串，以blob类型存储 DES_ENCRYPT/DES_DECRYPT 这种加密方法使用了3DES（三重加密数据算法，听着就知道加密等级比较gap），加密时可以选择使用key_num还是key_str 二、数据加密 1. 测试加解密过程 加密 # 使用字符串“密码”SHA512值的十六进制值作为密码，以AES方式加密字符串“testpasswd” select hex(AES_ENCRYPT('testpasswd', HEX(SHA2('密码',512)))); 解密 # 获取密码的Hex值 SELECT HEX(SHA2('密码',512)) # 解密加密过后的字段值 SELECT AES_DECRYPT(UNHEX('加密后的字段值'),'密码的Hex值'); 2. 创建副本表，批量修改数据 set @saltpasswd=HEX(SHA2('密码',512)) # 创建副本表 CREATE TABLE IF NOT EXISTS 副本表 (LIKE 源表); # 修改副本表字段长度。字段长度太小，数值会被截断。长度最小50 alter table 副本表 modify column 长度小的字段 varchar(50); # 复制源表数据并加密指定字段到副本表中 INSERT INTO 副本表 ( `字段名1`, `字段名2`, `....`, `副本表的所有字段` ) SELECT `字段名1`, `字段名2`, hex(AES_ENCRYPT(要加密的字段1,@saltpasswd)), `....`, hex(AES_ENCRYPT(要加密的字段2,@saltpasswd)), `源表的所有字段` FROM 源表; 3. 查询加密的数据 set @saltpasswd=HEX(SHA2('密码',512)); SELECT *, AES_DECRYPT( unhex( 加密的字段 ), @saltpasswd ) AS decrypt_context FROM 副本表; 4. 测试 CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(50) NOT NULL AUTO_INCREMENT, `phone` varchar(50) DEFAULT NULL, `sfz` varchar(50) DEFAULT NULL, `birthday` varchar(50) DEFAULT NULL PRIMARY KEY (`id`) ) ENGINE=InnoDB insert into user(name,phone,sfz,birthday) values('test1','13111110112','310000000000','20001118'); insert into user(name,phone,sfz,birthday) values('test1','13111110113','310000000001','20000118'); set @saltpasswd=HEX(SHA2('密码',512)) insert into user(phone) values(hex(AES_ENCRYPT('13111110111',@saltpasswd))); insert into user(phone) values(hex(AES_ENCRYPT('13111110112',@saltpasswd))); 三、备份文件加密 参考：Xtrabackup备份第五章第一小节 四、用户权限安全 场景 用户名命名规则 IP限制 权限 模式 备注 应用使用 app_* 应用所在服务器 IP 地址 select, insert, delete, update DML、DQL 应用账号不能创建表，更改表结构 管理工具使用 yearning_* 管理工具所在服务器 IP 地址 select, insert, delete, create, alter, update DDL、DML、DQL 创建表，更改表结构只能在审计管理工具上执行 个人直连使用 姓名简拼_* 个人电脑出口 IP 地址 select DQL 报表程序使用 report_* 报表程序所在服务器 IP 地址 select DQL 运维工具使用 tool_工具名简拼 运维工具所在服务器 IP 地址 不同用途不同权限 禁止给 Drop 密码位数至少10位以上，必须包含英文大小写、数字、特殊字符(_#@*-)等。 所有用户授予权限到具体库，禁止授权*所有库 五、命令行密码安全 mysql_config_editor 是 MySQL 提供的一个命令行工具，用于安全地存储 MySQL 登录凭据。它允许用户创建和管理 MySQL 登录信息文件，以避免在脚本或命令行中明文传递密码。 创建登录信息文件： mysql_config_editor set --login-path=test --host=hostname --user=username --port=3306 --password 提示输入密码，并将登录信息保存在指定的 login_path_name 中。 显示登录信息 mysql_config_editor print --all 删除登录信息 mysql_config_editor remove --login-path=test 使用登录信息执行 MySQL 命令： mysql --login-path=test -e \"show tables;\" 参考 http://www.gimoo.net/t/1712/5a4080a95a1fd.html https://dev.mysql.com/doc/refman/5.6/en/encryption-functions.html#function_aes-decrypt Copyright Curiouser all right reserved，powered by Gitbook该文件最后修改时间： 2024-04-29 11:17:01 "}}